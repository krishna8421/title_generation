[
    {
      "_id": "64e08914b72e199dda603dae",
      "title": "Beginning Serverless Framework V.",
      "content": "!building_version_ The goal of the Serverless Framework (formerly JAWS) is to help developers build and operate serverless architectures. In less than a year it has garnered almost , Github stars and hundreds of companies, from startups to large enterprises, are using it to develop and deploy serverless, event-driven architectures on AWS Lambda and AWS API Gateway. Overall, its been an extraordinary ride. Meanwhile, the landscape is changing. AWS Lambda is improving and evolving. Other IaaS providers are introducing their own serverless offerings. As a result, best practices and the underlying definition of the serverless architecture is shifting. Its time to recap our observations, acknowledge our key learnings, and begin work on a better, bolder Serverless Framework V.. Functions Can Be Too Small In the serverless architecture, functions are the unit of scale. This provides useful isolation of your logic, leaving you in a very agile position when you are in production and you want to modify your application components individually, without affecting the entire system. The downside is when functions are also treated as the unit of development. Every function requires its own scaffolding and code dependencies, which is a pain to manage. Who wants to do `npm install yada-yada --save` or `npm update` different times? The focus on individual Lambdas has been the recommended practice. We implemented it in V. of the Framework, but it hasnt made our users as happy as they could be, and will be, in Version ... Functions Hang Out In Groups A common pattern we see is functions often appear in small groups.To group functions, people rely on the Frameworks ability to put related functions in subfolders and have them share one common set of code dependencies. Functions often appear in groups because they have a common theme and as a result they may share resources, configuration, or code. For example, functions that each perform a CRUD operation on a common resource (e.g., Users CRUD) or functions that are part of a workflow (e.g., save data, process data after save). Wed like to make this experience better in Version , since this is natural behavior with clear usability gains. Flexible Code The recommended practice for writing Lambda functions is to write your function code to perform a single job. For example, creating a data record or resizing an image. This allows for easy debugging since there is one expected result. But we often see people putting more logic in a Lambda function, and for valid reasons. The first reason is they simply dont want to manage several Lambda functions. The second reason is they want to optimize the response time of their Lambda functions. Lambda has a well-known cold-start issue and the more tasks a Lambda performs, the higher the likelihood that it will always be warm and execute more quickly. How people choose to containerize their code via Lambda is something wed like to keep flexible in Version . Application/Project-level Thinking Is Broken The serverless architecture on AWS Lambda is very much a Service/Microservice architecture, and we try to adhere to their principles and best practices. However, by addressing our functions together as a project or application we fall into patterns that break the most important SOA/Microservices principle service independence. Whats great about service independence is it means independence for the teams working on those services. Whenever teams become reliant on each other, their progress can be blocked. If we want to build complex serverless systems, and build them quickly, teams must remain independent. In the serverless architecture and the Framework, creating a single stack of resources (e.g., S, DynamoDB, API Gateway REST APIs), at the project or application level, which all of the functions depend on, is the main blocking issue. Whenever one team must wait for another team to update a CloudFormation stack, before their Lambda functions can have the resources they need, they cant move forward. We got into this pattern largely because AWS API Gateway demands it. A single API Gateway REST API is designed to sit in front of all Lambda functions in your application, forcing a single shared resource. On top of that, API Gateway requires you create deployments of your REST API, to push your changes into production. As an unintended consequence, a deployment blocks teams from pushing their own endpoints and potentially code to production. Generally, wed like to move away from the concepts project and application. In the serverless architecture, everything is a service, that is all. With this new way of thinking, we're going to enable a lot of new interesting features and usability improvements in Version , which we'll write about soon. New Serverless Compute Services Since the release of AWS Lambda other major IaaS providers have introduced similar services (Google CloudFunctions, Azure Functions, IBM OpenWhisk, etc.). All of them look interesting and similar to Lambda, but underneath the surface, they are quite different in very interesting ways. Weve met all of the technical leaders of those respective services, and they each have a unique take as to how best to build a great Functions-as-a-Service platform. Its our goal to provide a great developer experience, regardless of the infrastructure provider, so multi-provider support will be a central feature of Serverless Framework V.. Doubling Down On Lambda The new serverless offerings look incredible. But, we absolutely cannot forget Lambda, the most mature and robust FaaS product around, and our original inspiration. The majority of our anticipated changes in Version are inspired by Lambda-based architectures first and foremote. Lambda and its related services (e.g., CloudFormation) have been progressing wonderfully. We will be incorporating all of that progress into the Lambda workflow for Version , offering a much better AWS Lambda experience. Wrapping Up Serverless Framework Version is currently being built in the v. branch. Above, are some hints as to the direction we're taking. Progress is rapid and we expect to release an alpha in a few weeks. If you'd like to contribute, please check out our issues. If you have questions, please email Flo, our CTO :) The Serverless Team",
      "__v": 0
    },
    {
      "_id": "64e08914b72e199dda603db0",
      "title": "Serverless V. Alpha",
      "content": "!valpha We've released Alpha- in the meantime, please check out the corresponding post as there are changes to how things are presented in this blogpost. After giving you a little more insight into our thoughts regarding the future of Serverless and event driven infrastructure in general in our last blogpost were happy to release the first alpha of Serverless V. today. Serverless Framework V. features strong extensibility through a new Plugin System and the ability to work with multiple providers from the start, though this release focuses purely on AWS. We already have most of the functionality that was available in .x reimplemented with many optimizations to the developer experience going forward. With all those new plans coming we decided that this is our time to introduce breaking changes, so from V. onwards we can improve incrementally. We will provide more information, documentation and direct help to our community going forward to transition from .X of Serverless to .x. The new system is very easy to set up and should be fast to reconfigure. We will release more alpha versions over the next weeks and months until we finally release the stable version of Serverless V.\\. We want to include you early on in this process for feedback and you can see the direction and help us on the way. We already have most of the functionality that was available in .x reimplemented and will put a special focus on developer tooling and developer experience going forward. With all of that said let's dig into Setting up a new serverless service, a new and fundamental concept in V.. Introducing The Serverless Service A serverless service is one or multiple functions (FaaS, Lambda, etc.), grouped together, with their resource/infrastructure requirements. Services should only contain functions that are related (e.g., functions for CRUD operations on a data resource, or functions for a workflow, like a data processing pipeline), or functions that all depend on the same infrastructure resource (e.g., DynamoDB table, S bucket). By grouping these related functions together, they are easier to develop and deploy. These services are also designed to be autonomous units, which helps teams involved in larger projects own and work on serverless services independently, without blocking each other. In Serverless Framework V., there are only services, instead of projects or single functions. The serverless service represents the optimal level of organization. Setting up Serverless Service We will walk you through all the steps necessary to set up a service with AWS as the provider, invoke it, add events and custom resources and then remove it again. For more detailed documentation you can check out our docs. At first we need to install serverless of course. You can do this by running the following command. It will automatically install the latest Alpha version of Serverless.  To get an overview on the available commands and the plugins that are currently loaded you can check out the help:  !Screenshot -- .. Create a new Service Let's create a new service. You need to set the name and provider for a specific service.  This will create a subfolder first-service with the necessary files for your Node.js service. At the moment we only support Node.js in this alpha, but other languages will follow. !Screenshot -- .. This will create the following simple serverless.yml file for deployment, the file that declares and describes a serverless service:  Deploy the Service Now that we have created the new service lets deploy it to AWS. The new implementation of Serverless on AWS builds completely on CloudFormation. When we deploy a serverless service, a new CloudFormation stack gets created and functions get updated through that Stack. You can check out the details of the AWS Deployment in our documentation.  Following is the log output during the deployment. Were also working on having a little nicer output during the deployment so we dont repeat the same message: !Screenshot -- .. Invoke the function with test data Now we can invoke the function with some test data and see the results. Were going to use the following json file for that   !Screenshot -- .. Add other Events and Custom Resources The serverless.yml doesnt just allow you to add your functions, but also add events and custom resources to your stack. Events will be automatically translated into CloudFormation Resources. The resources section lets you define a custom CloudFormation template that we will use and extend, so you have full control over the infrastructure that is deployed. In the following example weve added a schedule event that will run the function every minutes and an S bucket as a custom resource. You can read all about the events we currently support in our AWS Events documentation  After redeploying the service the function will be called every minutes and the bucket will be created. Remove A Service Of course you can also remove a serverless service, which will remove the whole CloudFormation template, so nothing stays behind:  !Screenshot -- .. Next Steps for Serverless Framework V. Now that weve walked you through setting up a serverless service lets talk about the next steps we have planned. Multi Provider Support Were currently working with Google Cloud Functions, Microsoft Azure, IBM OpenWhisk and others to bring Serverless to many more providers. We believe the future of cloud infrastructure is cross provider and we want to make sure you have the same great experience with every one of them. Each provider has their own advantages and disadvantages and with the simplicity of Function as a Service and Serverless Architectures were getting to a place where it's possible to use all of them together. Local Developer Tooling We want our community to have the best local developer experience possible. From working on services offline to easily and automatically deploying them into your functions while developing. With great insights into logs and metrics and good debugging support we will make sure you really know and understand your functions and your infrastructure while youre developing it. Now that we have the first iteration of the serverless tooling out there this is one of the most important steps we want to take. How to Contribute With that first release of Serverless V. we want to get more feedback from the community. Please help us by testing it, playing around with it, breaking it and reporting back to us. Create new issues in our Github repo issues and report any ideas that you have for V.. You can join our Gitter chat to discuss with us and the community about Serverless, our next steps and get help and support. We are currently planning all the milestones until the V release so keep an eye on the next milestones that are coming up in the repository if you want to help out contributing code. Wed love to have you as as part of our project. If you have any further question you can also simply email me at florian@serverless.com. Happy to point you in the best direction to chat. Conclusions Were very excited for this first release. It's the start of a very exciting future we have in mind for Serverless and for our Community. We couldnt do it without you and hope for your help in moving Serverless forward. And we will work hard to give you the best tools and services for building your infrastructure and products.",
      "__v": 0
    },
    {
      "_id": "64e08914b72e199dda603db2",
      "title": "Serverless V. Alpha",
      "content": "!img Today were proud to announce the release of Serverless V.-alpha\\. Two weeks have passed since the very first release of our scheduled rolling release roadmap. Thanks to our great community we were able to jump into discussions around different topics regarding the vision for Serverless v\\. With that help we were able to finish and release this second alpha of Serverless v. To install the new version of Serverless run the following:  Lets take a quick look at whats changed. New packaging mechanism See and Cloud applications have underdone a shift over the past several years toward service orientation (e.g. Microservices or even Nanoservices). Starting with Serverless v well also think in terms of services rather than a project (take a look here to read about services in Serverless v). This is now also reflected in the way we package your project for the deployment to your cloud provider. Starting with alpha your whole service will be zipped (instead of each function inside your service) and uploaded as a whole to your cloud provider. This approach has different upsides: Faster deployment as only one file needs to be uploaded Less brittle code parts which might break during packaging Easier sharing of code between functions Better integration for CI / CD systems Fundamental for multi langauge and provider support as you dont have to deal with the different special cases they might need when zipping is done on a per function basis Were also working on functionality to upload independent functions as well. If you have thoughts regarding that join the discussion here! Templates to kick-start your Serverless service See , and With alpha you would create a new service by running e.g.  This is an easy and simple way to create a basic scaffold for your new Serverless service. However weve discussed this way of creating a Serverless service with our community and saw a problem which arises when dealing with some of our core concepts for v which are Multi provider support Multi language support Starting with this release well provide different templates for different use cases. For example, you can now generate a scaffold for your Node.js . service which runs on AWS by using our aws-nodejs template:  Weve put together some basic templates here. More provider and language specific templates will follow soon. Better UI / UX and improved error reporting See and Dealing with errors/problems in your application is always painful. Weve overhauled our error reporting system to provide you with more detail on what went wrong and how you can resolve the problem. Additionally weve improved the overall UX to provide you a even better experience when working with our CLI. SNS event integration See and SNS events have finally arrived. You can now add a SNS topic as your event source to your function like this:  Take a look at the plugin code to get more in-depth information on how it works. Anonymized tracking of framework usage See and Our mission is to provide you the best tools and user experience when developing and dealing with Serverless infrastructures. Starting with the alpha release well gather anonymized usage data in order to determine where we can improve the user experience of the framework. We will always transparently communicate what data we collect and how we collect it. Additionally, we will NEVER collect any sensitive or private information. Please consult our documentation to get an always up to date reference regarding the tracking implementation. Keep in mind that this data is always anonymized and that you can always opt out of tracking if you wish! Workflow for contributing We love our community! Weve updated our CONTRIBUTING.md file to provide you with a better understand how you can get involved and contribute to Serverless. What's next? Take a look at our upcoming beta release milestone to get an overview of what we're working on next. It will include: Multi provider support Multi programming language support Better local developer experience More guides and a new, improved documentation We always appreciate feedback so feel free to comment on the issues or open up new ones! Introducing our new Serverless forum Weve successfully used Gitter in the past as a way to connect and interact with Serverless users from all over the world. In addition to that were also releasing our new Serverless forum alongside the alpha release and would like everyone to join this platform to discuss the future of the Serverless framework and serverless application development in general! Create your account today and join the discussion. Thats it for now. We hope that youll like the alpha and would love to hear your feedback about it!",
      "__v": 0
    },
    {
      "_id": "64e08914b72e199dda603db4",
      "title": "Serverless V. Beta",
      "content": "!announcement_vbeta Its been two weeks since our Serverless V.-alpha. release. Lots of valuable feedback and suggestions were submitted and discussed since then. Thanks to the help of our great community were proud to announce our first beta release today: Serverless V.-beta. Lets take a closer look what has changed / is new. Features Overview Video Getting Started Video Multi language support Up until now Serverless only supported Node.js as a runtime and JavaScript as the corresponding programming language. In Serverless v.-Beta. youll be able to use Node.js, Python . or Java (via Gradle or Maven) as a runtime for your projects. Using these new runtimes is easy and youre already familiar with how to create a service through the new runtimes. Just use the create command with the aws-java-maven template parameter to e.g. create a new Java (via Maven) based service on AWS:  To see a list of available templates and options for creating a new service run  You can also change the runtime of existing services by updating the provider runtime attribute in serverless.yml like this:  Take a look at the service templates docs to see how you can use the other templates. A big Thank you! goes out to Ilya Shindyapin (@licensee) who took the time to get Java support into Serverless. _See , and (Python support), , and (Java support)_ API Gateway updates The API Gateway event source is one of the most used and most complex event sources out there. There are many things one can configure. Take a look at the API Gateway documentation to see how you can use all those new settings. _See and (Custom Authorizers), , and (Proxy settings), and (API keys)_ VPC support Sometimes you want to configure your lambda functions so that they can be operated in a VPC environment. This feature was also added to the beta release! Take a look at the documentation to see how you can setup the corresponding configuration parameters so that your lambda function can be operated in a VPC environment. _See and _ Developer plugins / tooling Our mission for the Serverless framework is to provide an easy to use, yet powerful tool which helps you as a developer while working on your Serverless services and projects. Tedious tasks / configurations should be abstracted away so that you can focus on your code and be more productive. A feature rich toolset which supports you in your day to day development is vital here. Because of that weve started the discussion with the community what kind of developer tooling plugins are of interest and needed to be more productive while working with Serverless. V.-Beta. includes the following plugins which will help you develop your Serverless services: Info plugin Prints out what resources are deployed to AWS Logs plugin See all your functions logs and browse through them on your console (supports tailing as well) Single function deployment Blazing fast way to deploy a single function of your service (e.g. to test recent changes on a cloud provider infrastructure) Big thank you to Benny Bauer (@bennybauer) who implemented the whole info plugin! What plugins do you need for your daily work? Any ideas or thoughts? Please let us know and jump into the discussion here. _See (main discussion), , and (info plugin), and (logs plugin), and (single function deployment plugin)_ .yaml and .yml support YAMLs official file extension is .yaml which weve supported right from the beginning. However .yml is used often due to its brevity. Serverless V.-Beta. will now support both file extensions (.yaml and .yml). The default Serverless will choose (e.g. upon service creation) is .yml and we recommend that you switch to this file extension. _See , and _ Documentation updates, bug fixes and v as the main version The documentation is a key piece for every software project. Without it (or with poor documentation) developers wont be able to use the product and cant work with it. In this beta release weve updated our documentation with new examples, sections for new functionality and many different minor fixes. We would like to hear your feedback on our docs. What do you think is missing? Should we consider another structure? Please chime in on this issue and let us know what you think! Additional to the documentation updates weve also fixed a bunch of nasty bugs so that Serverless is way more stable and reliable. Thats why weve decided to merge the v. branch into the master branch and mark v as deprecated. Serverless v is now the go to version when youre about to work on a new project or want to get started with Serverless. A big thanks to our contributors! Weve seen a dramatic increase in contributions the past months and would like to use this blog post to say thank you to the community for all the work theyve done to make Serverless great! Contributing to Serverless is not hard at all! Weve just introduced the easy-pick label which shows what issues are easy for a first time contribution. Other than that were always happy if you discuss with us in the issues or pull requests. This blog post might also be interesting for you as it shows you what the usual workflow for open source contributions looks like. Heres a list of all people who have contributed code for the beta release (in no particular order): Ilya Shindyapin (@licensee) Sander van de Graaf (@svdgraaf) Benny Bauer (@bennybauer) Jamie Sutherland (@wedgybo) Sergio Arcos (@mt-sergio) Toby Hede (@tobyhede) There are way more awesome people who have helped by jumping in on issue discussions, pull requests, discussions on Gitter or the Serverless forum. Thanks everyone! Changelog Take a look at our v..-beta. milestone to get an overview of all changes in Beta . Whats next The next release will be the second beta release and is scheduled for mid of August. It will include the following: Improved stage / region / env variable and secrets support Better documentation and onboarding for new users Integration with Lambda versions / aliases Event integration test repository (to test all the event sources) Community plugin repository (a repository for community plugins) We would like to hear your feedback about this and have created an issue where well discuss this milestone goals (you can find all milestone discussion issues with the help of the recently introduced milestone-discussion label). Weve already created the beta milestone and added corresponding issues and pull requests. It would be great if you could give us some feedback on the things were about to implement so feel free to comment on the issues or open up new ones! Only a few weeks are left when well finally release Serverless v in fall Join the discussion in our Serverless forum GitHub is a great place to talk and discuss technical topics. For all other, more general topics about the Serverless framework, the serverless architecture, support questions, etc. you might want to look into our Serverless forum which is available at: http://forum.serverless.com Create your account today and join the discussion. Thats it for now. We hope that youre as excited as we are and like the first beta release. We would like to hear your feedback and future features you want to see in the Framework. You can find us on Twitter, GitHub and our Forum.",
      "__v": 0
    },
    {
      "_id": "64e08914b72e199dda603db6",
      "title": "Serverless V. Beta",
      "content": "Two weeks have passed since our first v beta. release. Today were proud to announce the second beta version of the Serverless framework which introduces lots of new features, improvements and bug fixes. You can install it via:  Here's whats changed... CORS support for API Gateway See , , and CORS support for API Gateway is finally here! You can now enable CORS support with a simple cors: true in your http event definition:  More in-depth documentation on how to e.g. set custom headers or the origin can be found in the corresponding CORS documentation. A big thanks goes out to Chris Paton (@patoncrispy) who worked hard to get this into place for the beta. release! Support for custom IAM role statements See and Once you start to build more complex serverless applications you need the ability to set IAM roles for services your serverless functions access (e.g. access to a DynamoDB through a Lambda function). This is now possible by defining an iamRoleStatement inside the provider property:  This way Serverless will automatically add specified statements to service's IAM role. You can read more about custom IAM role statements in the documentation. New implementation for custom provider resource mergings See , , and Weve updated the stack deployment and custom provider resource merging in this release so that your custom provider resources are merged once the initial stack is created. This prevents the stack from being removed on initial stack creation when theres an error with custom provider resource definitions. Furthermore well now support the merging / overwriting of everything with the help of the resources section in the `serverless.yml` file. This makes the whole system way more powerful and extendable (if e.g. n event source is yet not supported via a plugin). Take a look at this example taken from a `serverless.yml` file which will add a DisplayName property to the `mySNSEvent` and a Description to the `ServerlessDeploymentBucketName` (this is the S bucket Serverless creates to store your deployment artifacts) output:  A big thank you goes out to @fridaystreet who was working on an initial prototype for this feature. Creating Deployment artifacts only See and Ever wanted to look into the CloudFormation template Serverless will create? Or just wanted to get the deployment artifacts without an actual deployment? This is now possible. Just run `serverless deploy --noDeploy` to get the deployment artifacts inside the .serverless directory without an actual deployment (theyll be removed when the next deployment is run). Thank you John McKim (@johncmkim) for getting this up and running! New community plugin repository See You can find the GitHub repository here: https://github.com/serverless/community-plugins Plugins are the heart of the Serverless framework. We ship different core plugins right out of the box but there are many more use cases / functionality wishes where a plugin makes sense. Wed like to introduce our new official community plugin repository in order to provide a way to explore great, high quality Serverless Plugins which are actively maintained by the contributors but also signed off by the Serverless team. Feel free to contribute your Serverless plugin ideas / code there so that we can create a place where users can find great and useful Serverless plugins. It's not a requirement everyone put plugins in here (absolutely keep them in your own repo if you want to,). However, we help maintain the ones in the community repository. New integration test repository See You can find the GitHub repository here: https://github.com/serverless/integration-test-suite Serverless services and applications usually consist of different functions, events and maybe even more advanced custom resource definitions. Wed like to ensure that the Serverless framework always meets the high quality standards you and your team has while developing modern, large scale serverless applications. Our recently introduced integration test suite is used to test the Serverless framework autonomous in combination with complex services in real world scenarios. Weve just started the work on this one and would like to have your feedback / ideas regarding complex integration testing for the Serverless framework. Documentation for DynamoDB and Kinesis stream support See , and One of the feature requests weve heard over and over again is the setup of DynamoDB and Kinesis streams. After some discussion with our awesome community we came to the conclusion that we dont have enough feedback to build a complete plugin for those two event sources. Thats why we decided to add some documentation on how to setup those event sources with the help of custom provider resources. Heres an example how you can setup a Kinesis Stream: ```yaml this code should be added to the `serverless.yml` file resources: Resources: mapping: Type: AWS::Lambda::EventSourceMapping Properties: BatchSize: EventSourceArn: \"arn:aws:kinesis:::stream/\" FunctionName: Fn::GetAtt: - \"\" - \"Arn\" StartingPosition: \"TRIM_HORIZON\" ``` You can find the whole documentation for those event sources here: - DynamoDB stream event source - Kinesis stream event source Furthermore wed love to have your feedback on how you think the plugin solution for those event sources might look like. So feel free to join the conversation for the DynamoDB stream event here and Kinesis stream event source here. Other new features, improvements and fixes Those are just the features weve decided to highlight in this announcement blog post. You can find every issue and pull request in our corresponding Beta Milestone. Here are other features, improvements and fixes which will be also introduced in our beta release. API Gateway deployment fixes See , , and Unfortunately we had a bug which caused problems when deploying endpoints via API Gateway as old stages were not successfully updated and function order in `serverless.yml` caused trouble during deployment. While this has been resolved there is still an issue when functions get removed or renamed. API Methods are still deployed after a remove or rename and will only be fully removed on a following deployment. We're working with AWS to resolve this in the future. This has been (partially) resolved thanks to the work from Jamie Sutherland (@wedgybo) Set SNS topic via ARN See and Pre-existing topic ARNs can now be used to hook to a function with the help of the SNS event. Take a look at the documentation for the SNS event to see how this can be used. camelCase syntax for SNS event configuration (BREAKING CHANGE) See and Weve switched the configuration syntax to camelCase so that it sticks to the overall defined coding convention. After updating your SNS event definitions (e.g. from topic_name to topicName) everything should work again. New lifecycle event names for deploy plugin (BREAKING CHANGE) See and The deploy plugins lifecycle events are renamed so that the are more generalized and can be applied to more cloud providers. Weve not removed any lifecycle events but renamed them. You can see a list of the renaming here. Furthermore you can see all new lifecycle event names here. Set own variable syntax in defaults property See and The variableSyntax can be overwritten with the help of the defaults property in the `serverless.yml` file (You can see an example of a `serverless.yml` file which overwrites this variableSyntax here). Thank you John McKim (@johncmckim) for your work on this one! Reduced verbosity while loading See and The Serverless CLI had a very verbose output while loading / waiting for operations. This has been updated so that less screen real estate is used during usage. Include stage variables in API Gateway request body template See and You can now access the stage variables with the help of the velocity request template. Take a look here to see how you can access it. Thank you Patrick Brandt (@patrickbrandt) for your work on this functionality. Fix for populating boolean and variables See A bug caused problems with the population / usage of boolean and variables. This has been resolved thanks to Kengo Suzuki (@kengos). Refactor for the API Gateway endpoints outputs See CloudFormation has a limit for outputs per stack which means that you should be cautious what you want to put inside this outputs section. Nick den Engelsman (@nicka) pointed that out and worked on an improvement for the API Gateway outputs of the info plugin! Switch from node-zip to jszip See and Zipping the service is one of our core functionalities. Weve used the node-zip package for this task previously. However the package is a little bit dated so weve decided to move to the more maintained JSZip (which node-zip uses under the hood). This transition is finally implemented thanks to Chris Olszewski (@chris-olszewski). Alphabetical plugin sorting when running help command See and Used plugins are displayed when you run the serverless --help command on your terminal. Sander van de Graaf (@svdgraaf) submitted a fix which sorts those plugins by name which makes it way easier to grasp what plugins are in use. Removed individual lodash function requires See and Individual lodash function requires makes it tedious to work on code which uses the lodash library heavily. Sander van de Graaf (@svdgraaf) took some time to refactor the code so that the individual requires are removed. Breaking changes Moving fast and evaluation new ideas means that introducing some breaking changes are unavoidable. Heres the list of all the the breaking changes you need to be aware of for this release: - camelCase syntax for SNS event configuration - New lifecycle event names for deploy plugin A big thank you to our awesome community We couldnt have introduced such a large feature set for this release without the invaluable feedback and help from our incredible community who is always active on GitHub, Gitter, our Serverless forum and everywhere else on the internet! Thank you for making it a joy to work on Serverless everyday. You want to contribute / give feedback? You want to contribute to the Serverless project? Thats awesome! Weve recently introduced the following issue / pull request labels which makes it easy help us build a great Serverless framework: milestone-discussion  High level discussion about upcoming milestones discussion  Discussions about implementation details (feedback highly welcomed!) help-wanted-easy  A feature which can be implemented quite easily help-wanted  A feature we need your help with Additionally contributing is not limited to GitHub! Feel free to join the discussions on Gitter and our Serverless forum.",
      "__v": 0
    },
    {
      "_id": "64e08914b72e199dda603db8",
      "title": "'Defining Serverless and Why It Matters to Developers'",
      "content": "Youve probably heard the term _serverless._ But what does it actually mean? And more importantly, as a developer, why should you care? Serverless refers to a cloud architectural design pattern that abstracts servers away to the point that developers have little to no direct interaction with them. Of course technically there are still servers behind the scenes, but you dont have to worry about managing them. Serverless providers (e.g. AWS Lambda), also known as function-as-a-service (Faas) providers, remove servers from the equation by providing an event-driven, pay-per-execution compute service. In practice this means you write a function  a small fragment of code  and upload it to the service provider. You can then execute that function based on any event. For example, a user clicking a signup button or a new record being stored in your database. When that event occurs a machine spins up, runs your function, then shuts down. You only pay for the exact time it took to execute your function. In the serverless world your cloud provider is the one responsible for managing, provisioning and scaling servers  so you dont have to. This makes scaling your serverless apps much more efficient. Additionally, you no longer need to pay for servers when youre not using them. This makes going serverless dramatically cheaper than any previous compute service. Were excited about the possibilities of serverless compute services for a few reasons: \\. They allow developers to focus more time on building functionality and less time managing servers. \\. They significantly reduce the cost of cloud hosting. \\. They allow for new and interesting event based workflows. At Serverless Inc. we want to help developers take full advantage of these possibilities. Our goal is to build tools that empower developers to create the next generation of event-driven architectures. We believe this will allow you to produce higher levels of output while lowering your total cost of ownership, and in general make your work and life just a little bit happier. _Also published on Medium._",
      "__v": 0
    },
    {
      "_id": "64e08914b72e199dda603dba",
      "title": "'New Release: Serverless V. rc.'",
      "content": "Its time again. Serverless v beta. was released nearly two weeks ago. Today were happy and proud to announce the first release candidate of the Serverless Framework v (Serverless v rc.). You can install it via:  This release candidate comes with many new features, bug fixes and improvements. Before jumping into the details about whats changed wed like to tip our virtual hats and say thank you to our awesome community / contributors. The huge number of new features wouldnt have been possible without each one of you. Were happy to have all of you aboard and excited about the future of Serverless well build together! Alright. Lets take a look at all the new features in detail! Renaming of resources logical IDs (BREAKING CHANGE) See , , , , , , , , , and The deployment via CloudFormation makes it necessary for Serverless to generate logical ids for all the resources which are created on your behalf (e.g. resources which are used to setup your API Gateway with the help of the http event). Serverless also supports custom resources you can specify in the resources section of the serverless.yml file. Advanced usage and requirements made it necessary to change the merging logic of these custom resources so that youre able to e.g. overwrite the resources which are generated by Serverless using the custom provider resources. This new resource merging strategy was implemented in the last release (Serverless v beta.). A problem which still persisted since then was that the logical ids of the resources Serverless created were hard to predict and use to overwrite specific behavior. Thanks to the great discussions we came to a predictable rule well now follow to generate the resource logical ids. You can read more about the rule here. The new naming makes it way easier for developers to overwrite or extend resources generated by Serverless. Furthermore, several bugs (such as using one S bucket / SNS topic in several events) were resolved alongside! Note that this is a breaking change as existing services which access resources generated by Serverless need updates to work after updating. For example, if you deploy your existing service the Lambda Function Resource cant be created as the logical id changed, but the function name is the same (and has to be unique) so CloudFormation will fail. If you can, simply remove the existing stack and create a new one. In case this is not an option make sure to slowly update your stack and check the changes that have been happening during the deployment. Another change is that the API Gateway endpoint will change and you will get a new API Gateway for this deployment. In the event that you point something to that APIG endpoint make sure to update it. Thanks to Tapio Kukkonen (@oipat), @sung-hwang-zocdoc, Rogelio (@rogemita), Benny Bauer (@bennybauer), Timothy Caraballo (@openback) and Xavier Snelgrove (@wxs) for reporting the related bugs / helping out here! New Serverless variable system (BREAKING CHANGE) See , and Serverless v was started with a powerful way to reuse configuration throughout your whole service. Thats why we introduced the serverless.env.yml file back then. However, as more and more time was spent developing real world applications with the Serverless Framework the need for an easier to understand, yet powerful and flexible way to deal with configuration / environment variables came up. We had great discussions with you about all of your needs and came up with a way to introduce a powerful, flexible and easy to understand variable system. Serverless now introduces different ways to use variables inside your service related files. Lets take a look at some examples. Referencing environment variables Lets pretend that we have exported an environment variable with the name FUNC_PREFIX and want to access this variable inside our serverless.yml file. We can do this easily like this:  Referencing CLI options Accessing options which might be passed through the CLI can be referenced like this:  Including contents of other files You can also include contents of other files like this:  There are way more possibilities like nesting and combinations of all those options described above. We recommend checking out the documentation about the new variable system so that you can see how powerful it is and what you can do with it. Wed love to hear your feedback about this as this change is a crucial one! How does it behave in production-ready applications? Is something important missing? Let us know! Note that this change is a breaking change. Please check the environment variable usage of your service and note that the serverless.env.yml file has been dropped. The documentation about the new variable system will help you with the transition. Custom request / response configuration for API Gateway See , and This was an often discussed and mentioned feature. Its finally here thanks to the great discussion we had on how to implement it in a developer friendly way. Request improvements Serverless now supports two default request templates (one for the content type application/json and one for application/x-www-form-urlencoded) so that you can access all the necessary variables (e.g. like the body) inside of your functions code. Furthermore, youre now able to set your own templates for other content types (or overwrite the default request templates) like this:  Response improvements Response configuration was also improved. You are now able to set custom headers and a response template when setting up your API Gateway endpoint. Lets look at an example:  Take a look at our new documentation for more in-depth information about request and response configuration. Note: The new variable syntax which is shown here can help you to define external request / response templates like this:  Thanks to Oriol Gual (@oriolgual) for composing a request template for the content type application/x-www-form-urlencoded. Rename service and create directory on service creation See , , , and The create command helps you to create and setup a service scaffold in seconds. However, it would always create the necessary files in the CWD which meant that you had to create a directory and cd into that directory if youd like to encapsulate all your service files into a dedicated folder. After you created the service you were also forced to open up the serverless.yml file and rename it accordingly. This task is tedious and cumbersome. It would be great if Serverless could support a way a accomplish those steps in one command. The new version of Serverless adds the --path option to the create command which does exactly that. It works as follows:  This command will create a new directory (or reuse the existing one) with the name my-awesome-service for your service, copies all the necessary files in there and renames the service in the serverless.yml file to my-awesome-service. Youre also able to specify absolute and relative paths like this:  In this case Serverless will create the corresponding service in the nested directory. You services name will be my-awesome-service. Thank you Vaidas Mykolaitis (@codepreneur) for reporting and Roger Lam (@mrlamroger) for working on this issue! Show real time CloudFormation stack status See and Sometimes it just happens. You get an error when you try to deploy your service. Serverless returns some information about a CloudFormation stack problem but you need to see the whole CloudFormation output to know whats going on. What do you do? You open up the AWS console, navigate to the CloudFormation service, select your stack and read through the logs. Weve all been there. While this will help you better understand the real problem, its still bad UX. It would be great if you could simply see the CloudFormation stack trace in your console in real time while your stack is being deployed, updated or removed. This is now a reality! Nick den Engelsman (@nicka) jumped straight into this problem and implemented this amazing and very useful feature. All you need to do is to add the --verbose flag to your serverless deploy or serverless remove commands like this:  Thats it. Now youll see the CloudFormation outputs in real time! Nifty!  to get started. Also feel free to jump right into discussions on GitHub, our Serverless forum, Gitter and Twitter. Whats next This is our first v release candidate which means that Serverless v is right around the corner! Next up were planning to resolve the last issues / bugs to blaze the way for the release of Serverless v. You can follow the progress on our v milestone on GitHub.",
      "__v": 0
    },
    {
      "_id": "64e08914b72e199dda603dbc",
      "title": "'Interview with Peter Sbarski of Serverlessconf London + Registration Discount'",
      "content": ". There are many talks on building and scaling systems with AWS Lambda and Azure. Weve had talk submissions on how to: replace costly monolithic systems and architect systems to scale; build payment systems and replace expensive ETL pipelines; create massive IoT systems and introduce serverless technologies to large enterprises. Weve had at least three times the number of talks submitted for London as we did in NYC. And were still getting new submissions every day. What are some topics you personally are excited to share/learn about at Serverlessconf London? Im personally looking forward to user stories at Serverlessconf London. A lot of people have now had a chance to build large serverless systems so Id like to hear what they have learned. Id love to discuss the pros and cons of serverless applications; talk about how to evolve architectures and make serverless as ubiquitous and accessible to developers and solution architects as possible. There are a number of speakers returning from Serverlessconf NYC that I cant wait to hear from again. Is there anything else youre looking for right now? Serverlessconf is a community event, so we always welcome volunteers. We greatly appreciate whenever people donate their time to help grow the community. If youd like to volunteer and assist at the event, get in touch with us at hello@serverlessconf.io. Sponsors are also welcome to contact us. Our sponsors get great benefits such as an ability to exhibit their products directly to customers, find new leads, and interact with the community. At the same time, their support helps us put on the conference and grow the community. Any other info you want to share about Serverlessconf London? Were excited to run Serverlessconf London because there is so much passion and interest in this area. Serverless technologies and architectures are still relatively new, so everyone has a chance to become a pioneer and make a difference. Im convinced that at Serverlessconf London were going to hear from brilliant technologists, make new discoveries, debate, and walk away just a little bit smarter and a little more inspired. _Enter the code serverlessrocks at checkout to receive a % discount on registration  including early bird tickets. Well see you in London!_ _Also published on Medium._",
      "__v": 0
    },
    {
      "_id": "64e08914b72e199dda603dbe",
      "title": "'Serverless CEO Austen Collins on The New Stack Makers Podcast'",
      "content": "_Serverless Inc. CEO and founder Austen Collins was recently featured on the The New Stack Makers podcast hosted by Kiran Oliver. In this episode titled How the Serverless Framework is Reshaping AWS Lambda Austen talks about the Serverless Framework and the important role our open source community plays in shaping it. Listen to the conversation on SoundCloud or check out the transcript below._ How the Serverless Framework is Reshaping AWS Lambda [Transcript] KO: Hey everybody, this is Kiran Oliver, and welcome to this weeks episode of _The New Stack Makers_. Im here with Austen Collins of Serverless, and today were going to be talking about the new release of Serverless. So Austen, given the buzz was amazing leading up to this launch of Serverless, how was what went into . affected by feedback from the community in terms of what was given first priority, and what sort of features went into it? AC: Yeah, Ill jump right into it. Serverless Framework V. is definitely a reflection of feedback. Actually we get lots and lots of feedback  its really phenomenal how much is generated in an open source project, especially this one. What were making is a framework for building serverless applications, meaning apps built on the new event driven computer services like AWS Lambda, but deploying code on Lambda is only half of what developers do when they make a serverless application. The other half is people need to manage the infrastructure their Lambdas use, like S buckets, DynamoDB tables, manage permissions, environments, regions, meaning we basically have to build an application framework for using the entire cloud provider. Sometimes I say were building the AWS application framework, but then of course we interpret the provider from the perspective of a serverless architecture, as if AWS Lambda is the foundation of AWS Cloud. Then in V. were bringing on support for multiple cloud providers, so basically all this is a super ambitious scope of work and there are a lot of opinions on how to do all that, naturally resulting in a lot of feedback. On top of that you add in the surreal momentum that the serverless movement has received, and the strong attraction our project has had, all inspiring feedback coming in through multiple channels of communication: Gitter, GitHub, email, stack overflow, Twitter, Facebook. We have a Serverless Forum now. The result is a lot of feedback coming in at a rate of probably at least one comment every few minutes. Its a lot, especially for an open source venture that has no income at the moment. So yeah, feedback was a huge factor in V.. We cant escape it. Were honored to have as much feedback thats coming in the door right now. We have a few full time people on the framework. Id say the majority of their day is used to read feedback, its pretty phenomenal. So the next challenge is of course prioritizing all that, determining the features, determining the good feedback and not getting distracted. Thats a big challenge for us. The way I handle this personally is Im very theme based. I try to identify themes everywhere in life, and judge things based on how well they express the theme. You probably think about this all the time because youre a writer, right? KO: Definitely. Thats always something on your mind, how can you tie something together to make sure that its all cohesive? AC: Exactly right. The theme I started with when I first began the framework over a year ago now was I simply wanted to build more and manage less. I wanted to be more productive and minimize overhead  pretty universal themes. What feedback we decided to take on, and the overall direction the framework goes in is, and always will be, determined by how well it fits that theme. So thats the first criteria. Then getting into actual features, Id say the next biggest piece of feedback that we had was to stop making breaking challenges. The underlying technology that the framework depends on has changed so much since I started the framework by myself. AWS Lambda has changed. API Gateway has changed. They have evolved significantly, and the result is weve been chasing this moving target resulting in a lot of changes, which is one of the reasons we completely depend on AWS CloudFormation in V.\\. Serverless Framework V. is basically an abstraction of CloudFormation, and by embracing it, we build on top of something that AWS has signaled to be very stable, and many of the things have improved as a result. Lastly, theres no framework locking because you can always generate a full CloudFormation template of your application and be on your way. _Stop making breaking changes._ I think this decision was the best way to accomplish that goal and the next step. Since the framework was begun theres been a whole bunch of other Functions as a Service (FaaS) products that have launched and we received a lot of feedback that people want to use these products. Some examples are Azure Functions, Google Cloud Functions, IBM OpenWhisk. So we started building in support for other providers in the framework. And were fortunate enough to have some of the major providers actually writing the integrations themselves into the framework. Like the Azure Functions team is working on the Azure Functions integration. The IBM OpenWhisk team is also working on the OpenWhisk integration. So stuff is coming on that front and its going to be built by the experts themselves. Its super exciting. Lastly, we learned a lot about Lambda and FaaS work flows in general in version zero. Version zero was the trailblazer. Now were incorporating all those learnings and building something that is just hands down going to be the best tool to help you build more and manage less. Its absolutely the tool for anyone who wants to move fast, scale massively, be cost efficient, agile and super competitive. KO: I know theres a plugin system getting introduced. How is that working? Are things easy to swap out, piece together? Is there any particular language youre seeing people develop in in terms of those plugins? AC: Yes, we did have a plugin system in version zero and it was pretty successful. It was something actually introduced super early on in the framework, but weve made it much more stable, much simpler in V.\\. I opted towards a plugin architecture in the immediate beginning of the framework because I put it on the internet, it went super viral, everybody loved it, and that was like day one. It was a fantastic day. I got a lot of attention, a lot of positive feedback. Then the next day in the repo all the issues started to pour in. I realized wow, we have an ambitious scope of work, theres a lot of issues, theres a lot of feedback out there and everybody has different workflows. Startup requirements differ. Enterprise requirements differ. I think there are as many development styles as there are developers out there. So to meet all these peoples requirements it was clear that we had to have a plugin architecture on day one, and make it robust and easy to plugin to. Further, as a developer, I like to hack stuff. I think many other developers feel the same. If I cant hack it or modify it, extend it, then I dislike it to a great degree usually. That was another motivator behind it. So yeah, we started that in version zero and its been pretty successful. I probably have plugins just for version zero alone, and in V. very similar, but simpler, more stable. As far as languages go, Id still say we see a lot of interest in Node.js. This may be because our whole framework is written in Node.js were attracting that crowd, but the framework also works for Lambda functions written in Python and Java. I say next up on the list where we see the most interest is definitely Python. KO: Awesome. I noticed that the Serverless service experience is a really big part of the V. picture as a whole. In terms of how resources and infrastructure are operated, what are you seeing the community usually setup and accomplish with that? AC: We have a concept called the Serverless service. Its something new that were introducing in V.\\. Ill explain the backstory, or the problem it seeks to solve, before going into how it solves it. Id say the first thing that you notice when you start building a serverless architecture is that youre making lots of independent functions, or application components. Our theory, at least on our team, is that event driven, serverless apps are the future. The logical evolution of this is that people will end up building significant amounts of functions. And this is kind of a lot of what serverless applications, serverless architecture is. Its mostly a microservice architecture. So for example, larger companies already have a lot of microservices. I think Netflix operates over , microservices. Given that Lambda functions, Id say, are written to be even more granular than microservices a lot of the time, I think that if you took Netflix and expressed it as lambda functions theyd have probably a few thousand of them, instead of just , microservices. So we think this is where everything is heading, but the result is youre going to end up building a lot of functions. The only reason why we think people arent immediately making these vast amounts of functions is because the tooling to help you develop, deploy, and operate all these functions isnt there yet. In development, in operation when you have all these separate units of deployment, all these separate, totally independent components of your application, its awesome because when youre handling lots of volume, a lot of traffic, and you need to go update something, or you want to add to your application, all you have to do is modify one single piece. And youre not going to risk changing the whole application, the whole project, and thats really great. That leaves you an incredibly agile, less risky position in production. So in operation, having all these little independent units is absolutely fantastic. However, in development, if you have several thousand Lambda functions  we dont see that yet, but we do see people with a few hundred of them sometimes  this is super annoying, because you have to share code across all these separate files basically. You have to share infrastructure resources, like an S table or something, commonly across all these functions, and then add in multiple environments and regions, etc., across a growing project. So it gets complicated in the development and deployment phase. The way I put it sometimes is the function as the unit of deployment is super powerful, but the function as the unit of development is painful. So youre stuck with a situation where youre writing all these separate, independent units of code, and the framework seeks to make that process easier. Thats one of the big goals of the framework. In version zero we noticed something. And that is that people are writing these Lambda functions in groups a lot of the time. They do this because a lot of Lambda functions are related. Theyre sharing code, or within just a smaller larger group, or theyre sharing resource requirements. So for example, a backend service like a user service will probably share some code across five different Lambda functions, or four Lambda functions that handle CRUD operations on that service. Or like a data processing through some data pipeline or something will probably be a group of Lambda functions that are sharing code and resources. Functions, we learned, are easier to develop in groups, and people naturally like to group them, so we created this concept called the Serverless service. Its basically a group of functions  it could be one function or several functions. And in that Serverless service youre defining not only your functions, but also one set of infrastructure  like a DynamoDB table or an Amazon S bucket  that all those functions depend on. All this is defined in one file written in YML, so its pretty clean to look at. Its just the serverless.yml file. The great thing about this is that youre sort of, youre developing functions in a group, which makes sharing code and everything a lot easier. But when you go to deploy it the framework will actually split it up into separate units or deploy it all together if you want, so you have flexibility in how you deploy. Thats the premise behind the Serverless service. So far its been super successful. Its just a much easier development workflow. The only thing thats next that is really interesting is how we can make these Serverless services shareable, and how people can write these once and share them with the open source community so that you dont have to go rewrite the image resizer or some webhook handler for handling strike by votes or something. All that stuff should only be written once. People should be able to install it, and run it right away, so thats the next frontier for that. KO: Awesome. That sounds great. Another thing, too, is when working with things like Google Cloud Functions, Azure and OpenWhisk, what can developers and DevOps teams expect when setting those up, especially across a multi platform environment? AC: Good question. Were seeing increasing interest for this, some of the other FaaS products aside from Lambda. Lambda is the most mature out there. The other FaaS products, I dont think any of them are actually released yet, at least from the big providers. I know webtask.io is out there. Hook.io is out there. Those are really interesting products. First off, being open and accessible is a big goal of ours. Also, personally I think developers should be free to use any type of compute service they want to so they can take advantage of lower costs or features that any one platform is offering. The great thing about the FaaS product is that its the easiest way to get started on an Infrastructure as a Service (IaaS) provider, and I think that all these providers have caught onto this. Thats why theyre making these FaaS products. These providers are offering so many great proprietary services outside of just compute. And if you want to take advantage of those services, like DynamoDB for example, you dont need to go provision and maintain in EC Instance just to play a Lambda function. Youre ready to rock. You can start working with Dynamo DB right away. We think that deploying the multi-cloud future is coming more so than ever because its easier to do that type of architecture with FaaS. You could easily have a few functions on AWS, a few functions on Google, some on Azure, and IBM as well. So the framework is preparing for this multi-cloud future. Actually we sort of hope to enable it to some degree. However, how we get there is tricky, because unfortunately all the FaaS providers are pretty different once you get under the hood. I think in general were still on V. of event driven computing. Its going to evolve a lot. So weve backed away from making a single abstraction for now  just one way to write functions across all the clouds  mostly because as everything is changing so rapidly, we felt we would get in a position where we end up blocking users and eliminating usability across those services. Were not making this single abstraction; however, we are making a uniform experience in concepts, and in workflow and in some syntax. Again, not so much that it gets in the developers way and puts us in a position where we cant rapidly adopt new IaaS features. So multi-cloud stuff is coming. Its easier than ever to do, especially with Faas. And we hope to make it even easier with the framework. KO: In that vein actually, in terms of working with Serverless, I was wondering how thats been working out in terms of getting people to actually embrace serverless frameworks and start using them? AC: For the first question, we solve pretty much everything with our community. Were doing it in public, right? We cant not have these outside peoples opinions in there. So everything all day long in our GitHub issues, debates, discussions, proposals are being written on how to accomplish things best. We wouldnt be here today without these fantastic people. I mean theres been so many incidents where someone, a total stranger, has come by and given us a great idea. Its really remarkable to have that. Sometimes well never see that stranger again, but they came by, and they had a really significant contribution just for that short time window. We work with our community all day long. Were in Gitter, we have the Serverless forum, GitHub issues, tons of channels in which we hash stuff out and discuss stuff. Again, our scope is super ambitious, but its worthwhile, because if we could offer this great tooling, people are going to be building stuff faster than ever. Theyre going to be building more than ever. Regarding the education of serverless architectures and how we help people transition to them, I think the framework is the best at doing that. And thats why I started the framework to some degree. The framework is absolutely hands down the best tooling for getting started with a serverless architecture and Lambda. Just Lambda in general does tons of automation for you  optimization, best practices are all baked in, and it shows you how you scale those across a bigger team and a bigger project. So the framework is the biggest educational piece. Were going to be producing a lot more educational material here in the near future, but so far I dont think that people have had a hard time adopting a serverless architecture. I think that theyre usually getting stopped by just developer experience problems with AWS. Thats stuff that were trying to solve as much as we can, but some things like getting your credentials and dealing with permissions and stuff, theres only so much we can do there. But once they get it, theyre up and running. We also see a lot of big enterprise companies adopting the serverless architecture super rapidly, partially because its already the tried and true AWS infrastructure that theyre running. And Lambda is basically easy to write, so they already have buy in to some degree, and now theyre just looking at this more efficient version of AWS cloud computing. So theyve been quick to jump on the serverless architecture movement. We see lots of big companies making serverless teams with a new organization. Theyll start off with just one person who is a believer and we see that these teams are growing. Everyone else in the organization is starting to hand them more and more logic, more and more workloads to rearchitect in a serverless fashion. So I think people get it. I wish that AWS would do a few things to make the developer experience a bit better, especially around onboarding. We have some educational stuff coming out, so that should help out a lot. KO: Thats awesome. In terms of the problems facing developers when working with AWS, ones you see, minus the credentials issue, is there anything else that people are like please help us solve this? AC: Yeah, theres a lot. Thats why I started the project, just because the developer experience for building serverless architecture on AWS was not great in a lot of areas. As I think onboarding, you know the credential issues, thats something we see all the time. We get a lot of people who are completely new to AWS using the framework, so theyve never used AWS before. Some of them are coming over from Hiroku or something. I mean its pretty phenomenal. A lot of what happens in our community is really kind of a lot of AWS questions. So aside from the credentials, and just the general onboarding experience theres API Gateway. Its a wonderful service, super powerful, lots of functionality in there, but with all that complexity, with all those options, the basic user experience just sort of suffers as a consequence of that. There are lots of things there we think could be done better, but then again, thats what the framework is for. And the framework is solving that problem in particular. For example, when you want to pass in information or data from an HTTP request into a Lambda function, you just cant do that immediately with API Gateway. So if you hook up an end point, a Lambda function, nothing is really passed through unless you explicitly define it in API Gateway. That stops a lot of people immediately because theyre used to just HTTP bodies sort of going right into their functions or their logic, but again, thats something the framework solves right now. In V. weve created all the settings so that everything thats attached to an HTTP request is passed through by default and then you can go ahead and start stripping away stuff you dont want to be passed through. I say all those AWS problems are usually a consequence of all the functionality, sort of options, and great options that they provide. We like that that stuff exists and I think its kind of just the perfect fit for a framework now to go in and surface what really matters, and abstract a lot of that stuff away so that you can get to work faster. Its great that all that functionality exists. Theres a lot of complexity there, but its great that you can use it when you need it. And then of course rely on a framework or some tool just to help you move faster and not get distracted by it. KO: Absolutely. I just wanted to thank you for taking the time to talk with me today, and to let people know a little bit about Serverless, and what they can expect moving forward. When do you think that the next round of beta is going to be out for people to have a look at? AC: We release every two weeks. Were on an aggressive release schedule. KO: Sounds it. AC: Yeah, and were trying to move faster and faster. We wouldnt be able to do it without our community, too. The pull requests are coming in every day. Theres multiple pull requests coming in and its fantastic. A lot of our job is, as I said, its reading through all that feedback, and now its just doing code reviews, which is great. I cant name them all, but like Benny Bauer, Paton Crispy, John McKim, Kenuu, Wedgybo, Erik Erikson, Rob Gruhl, I mean theres so many great people in the community. Just had to give a shout out to them, too. Because again, we wouldnt be here without them, and were releasing so quickly, and were going to try and release it even more quickly just because of that. KO: All right. Well again, thank you so much for taking the time out to talk with us today. Definitely want to have you back once Serverless goes live, and we can really hash it out. AC: Awesome! Thanks for having me. _Also published on Medium._",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dc0",
      "title": "'Serverless Code Patterns'",
      "content": "Microservices and the Serverless Architecture have changed the way we think about web applications and partitioning logic. In this post, we'll share the best ways to structure your Serverless applications by applying the patterns directly on a simple example app were building  A Serverless Social Network. This fictional social network could have many features, but well focus on the _users_ and _comments_ features to keep things simple. Following months of experimentation and feedback weve summarized our findings into four patterns that the Serverless Framework embraces perfectly: \\. Microservices Pattern \\. Services Pattern \\. Monolithic Pattern \\. Graph Pattern Lets explore how we can build our Serverless Social Network using each one of these patterns, while outlining the benefits and drawbacks of each along with the configuration required for each pattern. Microservices Pattern In the Microservices Pattern each job or functionality is isolated within a separate Lambda function. In the case of our example app, each Lambda function would also have a single http endpoint that serves as the entry point for that function.  Benefits of Microservices Pattern: Total separation of concerns. Each job/operation is in a separate unit of deployment (i.e., a separate Lambda function), allowing you to modify your applications components individually, without affecting the system as a whole. This is a very agile and safe pattern, especially in production. Each Lambda function handles a single event making your functions easy to debug, since there is usually only one expected outcome. This separation of concerns is also great for autonomous teams. They can push functionality into production independently. Drawbacks of Microservices Pattern: You will end up with a lot of functions, which is harder to manage and can result in a lot of cognitive overhead. Lambda functions tend to be more granular than traditional microservices, so be ready for a lot of them! Performance could be slower. When functions handle a single job, they are called less, resulting in more cold starts. Deployments will be slower, since multiple functions have to be provisioned. You could reach the CloudFormation template file size limit quickly, especially if youre using custom resources. Services Pattern In the Services Pattern, a single Lambda function can handle a few (~) jobs that are usually related via a data model or a shared infrastructure dependency. In our example app, all operations on the Users data model are performed in a single Lambda function, and multiple HTTP endpoints are created for all CRUD operations.  You can inspect the incoming HTTP requests path and method by parsing the _event_ body in your code, and then perform the correct operation in response. Its like having a small router in the beginning of your Lambda code. Benefits of Services Pattern: This will result in less Lambda functions that you need to manage. Some separation of concerns still exists. Teams can still work autonomously. Faster deployments. Theoretically better performance. When multiple jobs are within a Lambda function, there is a higher likelihood that Lambda function will be called more regularly, which means the Lambda will stay warm and users will run into less cold-starts. Drawbacks of Services Pattern: Debugging gets slightly more complicated, since the Lambda function is handling multiple jobs, and has different outcomes. Requires creating a router to call the right logic based on the request method or endpoint. Bigger function sizes due to putting multiple operations within the same Lambda function. Monolithic Pattern In the Monolithic Pattern your entire application is crammed into a single Lambda function. In our example app, our entire app is in a single Lambda function, all HTTP endpoints point to that Lambda function.  Benefits of the Monolithic Pattern: A single Lambda function is much easier to comprehend and manage. Its more of a traditional set-up. Fast deployments, depending on the total code size. Theoretically faster performance. Your single Lambda function will be called frequently and it is less likely that your users will run into cold-starts. Drawbacks of the Monolithic Pattern: Requires building a more complex router within your Lambda function and ensuring it always directs calls the appropriate logic. Its harder to understand performance. The Lambda function will run for a variety of durations. You can easily hit the Lambda size limit in real world practical applications due to the larger function size. The Graph Pattern The Graph Pattern is similar to the Monolithic Pattern, but it allows you to take advantage of GraphQL to reduce your entire REST API and all of its endpoints into  endpoints. As a result, your entire application will be composed of a single function and a  endpoints that handle GraphQL queries. GraphQL will then fetch the correct data in any form you need.  Benefits of the Graph Pattern: Very easy to manage with a single Lambda function and a single endpoint for the entire application. Theoretically faster performance. Your single Lambda function will be called frequently and it is less likely that your users will run into cold-starts. Blazing fast to deploy since you only have a single function and a single endpoint A pay-per-execution, zero-administration, Graph API!!! It doesnt get more efficient than that! Drawbacks of the Graph Pattern: You can easily hit the Lambda size limit in real world practical applications due to the massive function size. You have to learn GraphQL. Our team is currently playing with the Graph pattern, except weve isolated each GraphQL query into a second tier of Lambda functions. This helps retain a microservices architecture w/ GraphQL. It is what were internally calling a Graph Gateway pattern. Well write more on this in the near future, as its still in the testing phase. Conclusion Weve explored four patterns that you can use to build your Serverless applications. Everyone has different requirements and preferences, so weve made sure the Serverless Framework can support all of the above patterns, easily. Enjoy, and good luck! _About the Author:_ _Eslam Hefnawy is a senior developer at Serverless Inc. He leads the team in charge of building and maintaining the Serverless Framework  an application framework for building web, mobile and IoT applications powered by AWS Lambda, AWS API Gateway and in the future other FaaS providers. Connect with Eslam at_ _http://eahefnawy.com/_ _or on_ _Github__._",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dc2",
      "title": "'Building A Better Australian Census Website with Serverless Architecture'",
      "content": "Austin Wilshire and Bernd Hartzer received world-wide attention in August when they built an alternative Australian Bureau of Statistics Census website for $ million less than the Australian government. The kicker? The official ABS site crashed almost immediately after launching. Whereas, the site created by Austin and Bernd at a weekend hack-a-thon remained stable because it was built using serverless architecture. It also cost less than $ compared to the governments $ million price tag. Heres what Austin had to say about the whole process when he recently connected with Serverless, Inc. CEO Austen Collins and Developer Evangelist David Wells. Watch the video or read the transcript below. [Transcript] AC: This is Austen Collins. Im the founder of Serverless, Inc. I created the Serverless Framework. Weve got David Wells here on the line. He is our head of community over at Serverless, as well. Then we have Austin here, who built, well Austin maybe you should just take it away and talk a bit about yourself and then tell us this classic story. Tell us what happened. AW: Just a little bit about me. Im just a web dev in Brisbane at the moment, working at a start-up called iRecruit. I went to my first hack-a-thon the other week, which was pretty awesome. A challenge was put out to build a better census. Because you guys are American, I dont know if youve heard, but the Australian census was a massive, massive fail. DW: We heard. AW: You did hear? Okay, good. So the challenge was put up to build it for web scale. Thats something I actually wasnt familiar with doing. So me and Bernd we were just like, sounds fun learning new tech, even if we dont do it right. So over the weekend we just built it, tested it, and turns out we did it. AC: What was the massive fail for those watching that dont know the story? What happened with the original census? AW: It crashed pretty much straight away. The government tried blaming it on DoS attacks, but they only tested for like one million requests per hour. They didnt go higher than that and so Im pretty sure what happened is just a bunch of people went on after dinner and the servers fell over. AC: To me, this is like a classic story. You hear about this stuff all the time. The government grossly overpays for some sort of simple solution. It goes totally wrong and crashes. Didnt even sound like anything unexpected, really. But from what Ive read, theyre doing the Australian census, and this happens  how frequently does this happen, every six years or four years? AW: Every five years, Im pretty sure. AC: Okay. Every five years. What I read was that million Australians were expected to complete the census entirely online, and so they spent over $,, creating this website so that they could collect the census information. I also read that they spent over $, on load testing alone. So four hundred grand on just load testing, making sure that it could perform at scale. They said that servers were load tested at a % of expected usage levels, but all the money spent, after all the testing done, servers crashed in the evening and everyone was sort of erroring out when they were trying to complete their information. As of today, I think the census was  lets see. What day is this? This is the th. The census was, what day was it Austin? It was like the th? AW: Yeah, I think so, yeah th, last week, Tuesday. AC: So as of today, less than half of all the Australian households have completed the survey. With this level of turnout theyre saying that the entire effort may be irrelevant because its just not enough data to draw sort of general patterns and stuff from it. So you guys went in and you did this hack-a-thon. How long did it take you to recreate it and how much detail did you put into recreating? AW: For the frontend, just the statistic pages, I just ripped them straight from the site because I didnt want to waste time on just rebuilding it completely. AC: Sure. AW: I just grabbed them and that took probably an hour because I had to get past the code. I wont go into how I did that but it happened. Then after that, to set up the actual architecture, it took me most of Saturday because Id never touched AWS in my life. AC: Oh, wow. AW: So I was feeling my way out. Then on the Sunday I actually had to redo it on a different AWS account that had credits and that took me half an hour. It was pretty great. AC: What was the total time involved, would you say? AW: Id give it under hours, just under hours total work. AC: Under hours and youre totally new to AWS which is amazing. Did you guys load test it at all? Did you test, howd you do that? AW: Yeah, we did load tests. We used an open-source tool called Goad which you can find at goad.io and it uses Lambda functions to smash websites with requests. Im actually not sure of the specifics but you can get up to k requests per second. DW: Was this hack-a-thon focused on this problem? Was everyone trying to AW: No. DW: Okay, it was just your guys idea? AC: Nice. DW: I like the initiative. AW: No, it was actually our challenge. How a hack-a-thon works is people pitch their ideas and then you could work on whatever you wanted. One of the guys just got up and was like, make it great again. So we decided to do it. Everyone else was just building cool shit with VR and automatic time and space recognition. Its crazy. AC: So youre totally new to AWS. Did you know about Lambda before you started this project? AW: Id heard whispers about it during the week like at work because were going to be using them Im pretty sure. Id heard a little bit about microservices and all of that, but it was really Friday night and Saturday where I picked up what serverless architecture was and all that. AC: What are the services that youre using on AWS? What services did you use to recreate this? AW: Its a really super basic stack. Its just an S bucket then one of the pages makes a call to an API Gateway, which triggers the Lambda function, which inserts the data into DynamoDB. Just easy. AC: Super, super simple stuff, and when you load tested, what was the total cost of all that? AW: Look, weve put out in the media like $, but the projected costs on Amazon were looking like under $. AC: Wow. AW: Thirty dollars. Yeah. AC: Pretty amazing. AW: The best part about the load testing budget actually is it wasnt automatic. They legitimately got people to go on to the site to test it. They didnt just have bots smashing at it. They manually did it and thats probably why it was so expensive. DW: Interesting. AC: Was there any official response to all this media coverage you guys got from the government? AW: Not really. In one of the articles, an ABS spokeswoman said they werent allowed to comment because of the investigation they had going on, so thats the best were going to get I think. DW: Are you worried about being deported? AW: No. Not yet. DW: Come to San Francisco. Join us. AW: Aw, man. DW: Youll be welcome here. AC: I was reading, I cant remember what the article was, but there were a few good comments underneath the article and the first one was, someone says that if the Stats Bureau is clearly totally incapable of building a computer system capable of handling this task, why would anybody believe that their rock-solid guarantees of its total security in handling their data? Like, if they cant put up a website that handles this much load, which isnt really that big of a deal, how can we know that theyre really competent in handling the data? I thought that was an interesting point. Of course, the other comment was that even more unfortunately, this highly publicized failure means e-voting and e-referenda which would be far cheaper and ultimately better will be delayed now, probably by years or maybe decades. I think thats such a great point. Unfortunately, because of this, everybodys going to have, in government at least, its highly likely that people are going to get more risk-adverse and theyre not going to want to build more elegant, simple solutions for people who need to comply with government regulations to do certain things: to take a census, to pay taxes or whatnot. For me, theres nothing worse in the world than going to a government website to do something. Like the IRS website of the United States, anything thats related to the Department of Motor Vehicles in California, that is just the worst experience and its been that way forever. Unfortunately, the downside of all this stuff is that theyre probably going to be more scared to embrace more online efforts and stuff in the future, which is just on the wrong side of history, unfortunately. AW: Yeah, no, absolutely. It does such. DW: How does it feel to have Werner Vogels tweeting about your guys story, the CTO of Amazon? AW: That was pretty crazy. When I saw that one, I was like, hey, wait, hang on. One of the guys who interviewed me said, This guys kind of a big deal. Hes actually going to come to Brisbane next month, Im pretty sure. We might try and have a sit down and just chat about what happened, I guess. I dont know, pretty cool. DW: He owes you one, man. This is like, put it on the front page. AW: It was all over Reddit, as well, which was really crazy. AC: Yeah, all right. I dont have any other questions other than this is a fantastic story. Austin, you rock. Youre clearly going to do lots of cool stuff. I hope that all the traction and interest thats come as a result of this works to your benefit and just great job, all around. AW: Thank you, man. Means a lot, man. Its good stuff. _Also published on Medium._",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dc4",
      "title": "'Introducing Serverless Partners: Meet Parallax'",
      "content": "Serverless architecture has opened a whole new world of possibilities for developers. Were seeing so many cool projects being built with the Serverless Framework. The open-source Framework allows you to easily build web, mobile and IoT applications with serverless architectures using AWS Lambda, Azure Functions, Google Cloud Functions and more. Startups to Fortune companies are using the Framework to build sophisticated event-driven systems. Were collaborating with some of these organizations to highlight their exceptional work through the new Serverless Partners Program. Serverless Partners are consultants and agencies who are verified experts in serverless application development with the Serverless Framework. Well share stories of how they use the Framework to work fast, manage costs and build for scale, in order to produce innovative products and solutions for their clients. Meet Parallax Meet Serverless Partner Parallax  an international digital marketing agency based in Leeds in the UK. Parallax builds websites, apps and software, in addition to crafting engaging content and optimizing SEO. They also work on all kinds of design, development and digital marketing. Parallax helps clients bring their ideas to life and tell their stories on the web and beyond. Find out more about Parallaxs work on their blog. Building An Infinitely Scalable Online Recording Campaign For David Guetta & UEFA Parallax worked on the huge marketing campaign for David Guettas new release  This Ones For You  the official anthem of the Union of European Football Associations (UEFA) European Championship finals. Working for UEFA in conjunction with sports group Perform, they created a revolutionary web app by utilizing in-house skills and a range of different technologies, including the Serverless Framework.  The Challenge A huge part of the marketing campaign surrounding David Guettas track This Ones For You revolved around fans collaborating on the songs actual production. Parallax was charged with building a web app that clearly explained the premise, allowed high fidelity voice recording and generated shareable content. Effectively, they needed to figure out a way of creating a virtual recording studio to enable million unique fans to sing along with Guetta  their voices would then be sorted and compiled for inclusion on the final song. The main challenge was creating the background architecture to handle the voice recording. UEFA required a stable platform that also looked the part and worked flawlessly time after time. On top of that, the site had to work seamlessly on all devices, operate in twelve languages and incorporate embedded video content. The Solution The Parallax team ultimately decided that writing a simple Lambda function and letting Amazon do all the heavy lifting seemed like the obvious choice. They settled on using Serverless and CloudFormation to orchestrate the entire platform in code, building a completely scalable architecture. Check out the Parallax Case Study for more on the specifics of their process and results. Spoiler Alert: It was a smashing success. _Are you a consultant or agency using the Serverless Framework to streamline your workflow and build innovative solutions for your clients? Contact partners@serverless.com for more info on becoming an official Serverless Partner._ _Also published on Medium._",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dc6",
      "title": "'Building a Facebook Messenger Chatbot with Serverless'",
      "content": "Chatbots. Maybe you've heard about them recently. Maybe you've even talked to one. But what are chatbots, why are they relevant, and how do you build one? In this post well answer those questions, plus show you how to develop your very own Serverless Facebook Messenger Chatbot. Well also explore why serverless architecture and the Serverless Framework are a great fit for this type of application. What are Chatbots Imagine that you could pull out your smartphone and send a quick Facebook message to your favorite shoe retailer. Something like What are the current shoe trends? Seconds later you receive a message with several pictures of trendy sneakers. Next you respond: Im more of a casual person and would like to go with black sneakers in size . _Ping._ You receive another instant message with the shoes you requested and a simple option to buy them. This might sound futuristic, but it's already happening in online retail stores. This is a chatbot. And it's been called the next big thing in customer technologies. Chatbots are a great way to enable users to instantly interact with companies in a very familiar format--direct messaging. Why use the Serverless Framework If you want to build your own chatbot, first you need the logic for your chatbot (the code), so that it can automatically respond to users who are sending it messages. Traditionally the next steps would be to setup the server; configure it to run as a webserver; install security updates; deploy the code and share the URL with the world. That _could_ be sufficient. But what if your chatbot goes viral and all the traffic causes your server to crash, rendering your bot unresponsive? What should you do? How do you scale it? And who will pay for your servers since your chatbot wont make any money yet? The solution? The Serverless Framework. Developing a Serverless Chatbot Here's how to build a Facebook Messenger chatbot with the Serverless Framework. Note: Well use Amazon Web Services (AWS) as our cloud provider of choice. We'll assume that youve installed and setup Serverless and your AWS credentials. Take a look at the Serverless docs if you need any help there. Getting started Well build a chatbot called Quotebot that will send us a quote if we send it a message. ; } bash serverless deploy ``` once again to deploy the Quotebot! Testing the Quotebot After all this hard work you could use some inspiration! Open up the Quotebot Page and click on Message to compose a new message. Type something and hit Enter as if you're interacting with a human being. Quotebot will reply with an inspirational quote from your array of quotes. But it wont end here. You could also use Quotebot from your smartphone's Messenger app. Just open the Facebook Messenger app, select the conversation with Quotebot and send a new message. Youll immediately get an awesome quote as a response. Quotebot source code Take a look at our Quotebot repository to see all the code we wrote during this blog post: https://github.com/pmuens/quotebot/. Conclusion Youve just created your very first Facebook Chatbot that will inspire you on demand with great quotes! Your chatbot can scale infinitely without you doing anything, and youll only be charged when people interact with it. This is just the beginning. Chatbots are becoming more and more popular, and the Serverless Framework is a great fit to architect an infinitely scalable Chatbot with very low operating costs. Want to learn more about the Serverless revolution? You can read more about Serverless and the Serverless Framework on our website, Medium or in our Forum.",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dc8",
      "title": "'Releasing Serverless Framework V. & Fundraising'",
      "content": "For over a year now, weve been building an application framework to help developers spend less time operating complex infrastructure and more time delivering results. To date, the Serverless Framework has garnered more than , stars on Github and hundreds of startups and enterprise users use it every week to deploy thousands of AWS Lambda functions. Today, were excited to announce that were bringing Serverless Framework V. out of beta. Were also announcing weve closed a seed round of funding worth $ million, led by Trinity Ventures. With these funds, well be able to substantially increase our efforts on the Framework and evolve its focus to help users deploy bigger, more powerful serverless architectures, across providers. Heres how to quickly get started with Serverless Framework V.: You can also look through our documentation or jump directly into our guide. For any questions please consult the Serverless Forum or open an issue on Github. Graduating From Beta In the beginning (Serverless Framework V.), the key AWS services that comprise the Serverless Architecture (e.g., AWS Lambda, API Gateway) required a lot of assistance to use. Today, they have all matured well and most of their functionality is supported by AWS CloudFormation, allowing users to perform safer deployments of their functions and required resources. In order for our users to take advantage of these new benefits, we started working on Serverless Framework V. Beta five months ago which relied entirely on AWS CloudFormation for deployments. This change allowed us to focus more on developer experience and solving new challenges within the serverless architecture. Since then, weve been building and testing Serverless Framework V. publicly. The feedback is unanimously positive, and this has led our decision to bring it out of beta. View the latest Changelog Comparison between V. and V. Open-Source This fundraising wouldnt have been possible without the power of open-source and the amazing community of developers who have submitted countless lines of code to make the Serverless Framework what it is. Therefore, our first order of business is simply to invest in more open source. The Serverless Framework was born as and will always be open-source. The majority of our new resources will be devoted to improving and expanding the Framework, engaging more closely with the community and improving our educational materials so people can better understand and realize the benefits of the serverless architecture. Additionally, we plan to invest in more Framework related open-source side projects. We will release more info on the next steps of the framework after the V. release. If you want to start contributing to Serverless check out our Contribution documentation. The Next Frontier The Serverless Movement came out of nowhere and has grown quickly to become one of the leading buzzwords of . What has also grown is the number of companies offering serverless-like compute services, also known as Functions-as-a-Service. These new entrants to the market are of interest because they represent new options we can present to our users. AWS Lambda is the dominant player in the world of serverless computing, and rightfully so considering its level of maturity. Our main goal is still to provide the greatest possible developer experience for AWS Lambda. However, our new secondary goal is to review the current options for serverless computing, determine which ones give developers the most value (and the best serverless experience), and support them within the Framework. Were working closely with the major providers of other serverless compute offerings, and we look forward to announcing a few soon. Thanks Again, Community We wouldnt have made it this far without our open-source community. The amount of effort the community contributes every day amazes us. Our next immediate priority is to start recognizing contributors more publicly and help promote them as the Serverless pioneers and experts they rightfully are. Well be announcing our new champion program shortly. Until then, take care and have fun building! - The Serverless Team ",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dca",
      "title": "Building A Serverless Screenshot Service with Lambda",
      "content": "A client recently requested a feature involving screenshots of random URLs. Now, there are several services out there that will do this for you. Most of these services have interesting REST APIs and pricing models, but I really wanted to develop something with Serverless. So I took this opportunity to check it out. This application will run on AWS Lambda. You can find all the source code mentioned in this repository. Quick installation ==================== If you just want to launch the service yourself, you can use this button which will setup everything for you in your AWS account through the magic of CloudFormation:  Give it a try!  This is a guest post from Sander van de Graaf. Sander is a freelance Cloud Solutions Architect specializing in AWS environments and large, high-volume applications. You can reach him on Twitter, LinkedIn or via email. Interested in writing for the Serverless blog? Find more info on contributing here.",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dcc",
      "title": "Introducing Serverless Partners: Meet Trek",
      "content": "Serverless architecture has opened a whole new world of possibilities for developers. Were seeing so many cool projects being built with the Serverless Framework. The open-source Framework allows you to easily build web, mobile and IoT applications with serverless architectures using AWS Lambda, Azure Functions, Google Cloud Functions and more. Startups to Fortune companies are using the Framework to build sophisticated event-driven systems. Were collaborating with some of these organizations to highlight their exceptional work through the new Serverless Partners Program. Serverless Partners are consultants and agencies who are verified experts in serverless application development with the Serverless Framework. Well share stories of how they use the Framework to work fast, manage costs and build for scale, in order to produce innovative products and solutions for their clients. Meet Trek Trek is an Indiana based consultancy that specializes in using the latest and best services around container-based and event-driven architectures alongside the rest of the AWS services to design, build, and support new systems for their customers. They also aim for massive scalability, high up-time, heavy automation, and remarkably low operating costs, much of which they accomplish through the use of AWS Lambda and the Serverless Framework. Serverless and GraphQL Announced at the Tokyo Game Show in September, DEKKI, described as a cross between Medium and sites like Mobafire and HearthPwn, promises to bring together the industrys favorite games and platforms to enable users to share strategies and opinions on the games they love. That said, conceiving of an intuitive, multi-lingual platform that is simple to use is one thing. Delivering a user experience that differentiates itself in the fast paced, ultra-competitive gaming industry is an entirely different story. PlayBrain, the company behind DEKKI, (described as a cross between Medium and sites like Mobafire and HearthPwn), partnered with Trek to build out a platform that enables users to share strategies and opinions on the games they love. The challenge put forward to Trek was simple, Build an elegant system that is infinitely scalable and low cost. Trek designed and built a serverless infrastructure, using the Serverless Framework, for PlayBrain that is centered around event-driven computing with AWS Lambda. The architecture in all cases relies on the highly performant GraphQL, Lambda, API Gateway, DynamoDB (GLAD) stack and CloudFront for edge caching of static content to speed up read times of the application content. The Serverless Framework allowed Trek and PlayBrain to work in parallel on the frontend ReactJS based editors as well as the backend API and rendering engine. Trek estimates that when compared to a project leveraging typical infrastructure (EC / Docker based), Serverless saves roughly %-% of typical infrastructure development time, allowing the Trek team to focus more on the core value. Check out the Trek Case Study for more on the specifics of their process and results. Spoiler Alert: Infrastructure costs were less than $/month! _Are you a consultant or agency using the Serverless Framework to streamline your workflow and build innovative solutions for your clients? Contact partners@serverless.com for more info on becoming an official Serverless Partner._",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dce",
      "title": "Building A Serverless Garden Monitoring System with Lambda",
      "content": "My Serverless Garden ==================== I have a problem with my garden. Or more accurately, I have a problem remembering to care for my garden. !Happy Plants My name is John McKim and I'm a software developer based in Brisbane, Australia. I work for A Cloud Guru on our Serverless learning platform. In February of this year (), I attended an AWS Meetup where Sam Kroonenburg spoke about A Cloud Guru. Sam told us how he built A Cloud Guru without a single server. I was inspired. After that Meetup I started learning as much as I could about Serverless. I started by converting an Express app to Serverless with the Serverless Framework. But that wasn't enough. I wanted to build a real project. To do that, I needed a problem. The Problem -- I love gardening, but I often forget to water my plants. I needed a way to monitor my plants, and more importantly, for them to tell me when they needed watering. !Happy Plants This was a great problem to solve. I needed to build a dashboard and notification system and connect it to my garden through an IoT service. It's an event-driven system which makes serverless architecture an ideal solution for this problem. The Project ==================== The architecture for the project is below: !Garden Aid Architecture I started the project as a single monolithic Serverless service. There are good reasons to do this. As the project grew, I broke it up into four services. IoT Service -- The IoT Service is the heart of this project. The IoT Service uses the AWS IoT Device Gateway and Rules Engine. The Device Gateway provides an endpoint for messages to and from devices. I use the Rules Engine for two tasks: - Storing raw messages in DynamoDB for the dashboard - Invoking the check moisture Lambda in the notifications service The Rules Engine connects the services with devices. It's the piece that makes Serverless a great solution for an IoT system. Notifications Service -- The Notifications Service sends messages to Slack when the soil moisture is too low. It has two Lambda functions in the service. The first Lambda function checks the moisture level and sends a SNS message if it's too low. The SNS message triggers the second Lambda function which sends a message to Slack. This design keeps the system highly cohesive and loosely coupled. The end result is notifications sent from devices appearing as messages in Slack. !Moisture Dashboard Web Backend -- The Web Backend provides a GraphQL API for the Web Client. The backend has just one API Gateway endpoint and Lambda function. The Lambda function processes GraphQL queries and responds with data from a DynamoDB table. A custom authorizer protects the GraphQL endpoint by verifying a JWT from Auth. This stack allowed me to create a useful backend with very few lines of code. Web Client -- The Web Client allows me to monitor my garden through a dashboard. I built the Web Client using React + Redux. The client polls the GraphQL API every seconds for moisture levels. This data is used to display a dashboard to users (me). !Moisture Dashboard What I Learned ==================== This project has been extremely valuable to me. I still forget to water my plants. But I have learned so much by building something to solve a real problem. It wasn't all smooth sailing. In fact, the project isn't finished (is software ever finished?). The AWS IoT Device Gateway requires clients to authenticate with mutual TLS . authentication. The device I bought does not support TLS . out of the box. So as of now (Oct ), I haven't actually gotten a soil moisture sensor working. Know your Services -- Knowing what cloud services exist and how they work is important. If I had known that AWS IoT requires TLS . I would have bought a different device. Knowing what different services are offered allows you to choose the best in class for your project. I chose Firebase hosting over S + CloudFront as it's far better suited for single page apps. Spending time researching services rather than just using what you know will benefit you in the long run. Selecting Boundaries -- Selecting boundaries for Microservices is hard. A good rule of thumb for Microservices is that each service should own its own data. Right now (Oct ), the DynamoDB table is in the IoT service and queried by the Web Backend. I took this approach as the Rules Engine is storing the data in DynamoDB. But this breaks the earlier rule. What I should have done is create the DynamoDB table and Rule that stores the data in the Web Backend. Automation -- Investing time in unit tests and CI/CD is always worth it. This project has been deployed to AWS and Firebase countless times. But it was rarely me that deployed the project. Travis CI has saved me a lot of time and caught errors before they were a problem. Even if you are creating a small side project, help yourself by automating everything. GraphQL -- Lastly, GraphQL is awesome. It's a great alternative to REST API's and well suited to Serverless systems. If you haven't had a look at GraphQL I strongly suggest that you do. The Serverless Framework has a boilerplate that can help you get started. What's next ==================== I recently (Sep ) started working for A Cloud Guru - a company that provides online on-demand training for engineers on AWS. I'm excited to be working for the company that inspired me to learn about Serverless in the first place. I now find myself very busy running a Serverless meetup, blogging and doing talks on Serverless. But I do hope to keep working on this project. I recently (Oct ) received some new devices and I hope to build the soil moisture sensor soon. If you would like to learn more about this project please go and read my blog posts: - Serverless Architectures&mdash; Building a Serverless system to solve a problem - GraphQL with the Serverless Framework &mdash; Building a dashboard for my garden - Slack Webhooks with the Serverless Framework &mdash; Building a notifications system for my garden - AWS IoT with the Serverless Framework &mdash; Building a monitoring system for my garden If you want see where this project goes next follow me on Medium or Twitter.",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dd0",
      "title": "Introducing Serverless Office Hours",
      "content": "At Serverless our community and contributors' input matters to us - a lot. Earlier this month we released V. of Serverless. Now that you've had some time to check it out, we want to know what you think. What questions have come up? What do you want to see next? Let us know in our first edition of Serverless Office Hours where we'll devote one hour to answering your questions and feedback. Join the Serverless CEO Austen Collins, CTO Florian Motlik and Developer Evangelist David Wells for a live chat about V. of the Serverless Framework. !office_hours____ How To Get Involved: . Install the latest version of Serverless: `npm install -g serverless`, if you haven't already checked it out. . Tweet @goserverless with your questions about V. of the Serverless Framework between now and AM PDT, Thur., Nov. . . Be sure to include the hashtag AskServerless. . Tune into YouTube Live Thur., Nov. , -AM PDT when the Serverless team will answer your questions. Can't join us live? No problem! We'll share a recording of Office Hours on YouTube, so send us your questions anyway. Start sending us your questions tagged with AskServerless and stay tuned for upcoming Serverless Office Hours topics and dates! You can join the ongoing discussion about Serverless on GitHub, Twitter, Gitter, the Serverless Forum or Facebook.",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dd2",
      "title": "Ways to Secure & Prevent Vulnerabilities in Serverless Applications using Snyk",
      "content": "Serverless is a powerful new approach that enables developers to focus on building features instead of having to focus on the underlying architecture. From a security perspective, it greatly reduces the risk of security issues due to unpatched servers. But it doesn't totally eliminate the risk. In serverless architectures vulnerable open source packages become the primary security risk. Open source software is increasingly consumed in the form of packaged code dependencies downloaded from repositories, such as npm, RubyGems, Maven, etc. The use of these packages continues to grow, and already the majority of code deployed in your app is most likey open source. The Snyk team approached serverless security with the following premises: The security vulnerabilities in these open source packages are typically known, and logged as GitHub issues. Many of these open source packages are downloaded millions of time each month, making exploits of their vulnerabilities highly reusable. Further, tracking these packages is difficult considering that any developer can add a dependency that includes lots of other indirect dependencies along with their security flaws. The new Serverless Snyk plugin was created to address these issues allowing you to ship securely and focus on building your app. The Serverless Snyk Plugin !Screenshot of the Serverless Snyk plugin in action The Serverless Snyk plugin helps to prevent vulnerable packages in your Serverless application, using Snyk.io. The plugin achieves this by focusing on four stages: find, fix, prevent and respond. Find With the Serverless Snyk plugin installed, each time you deploy the plugin will scan your dependencies and test them against Snyk's open-source vulnerability database. Serverless Snyk can either stop the deploy at this point (the default behavior), enabling you to address the issues, or continue on, simply noting the vulnerabilities for you to return to. Fix With Snyk GitHub integration a PR can be submitted to your repository with any updates or patches needed to secure your application. You can also fix the issues by installing and running `snyk wizard` locally. In either case, Snyk will create a `.snyk` policy file to help guide future Snyk commands. If the Serverless Snyk plugin sees that you have a policy file in place before your application is deployed it will apply any of the updates and patches you have specified by running `snyk protect`. Prevent Security is a continuous process. As your application continues to evolve the dependencies it uses may change. Snyk runs everytime you deploy to help identify and preemptively fix any new vulnerabilities. Respond You're given an API token when you sign up for Snyk. By including the API token in your Serverless project (using a `.env` file to ensure it's not mistakenly published), the Serverless Snyk plugin will take a snapshot of the current state of your dependencies and save it to your account. Whenever a new vulnerability is released that impacts your application, Snyk will notify you, and anyone else in your Snyk organization, by email or Slack so you can address the issue right away. Celebrate Security With the Serverless Snyk plugin in place, you can now let everyone know about your newly improved level of security by including a badge in your repository. Here's what the badge looks like right now for the Serverless Snyk plugin itself:  You can find more information about how to include the badge for your GitHub repository in the Snyk documentation. Summary Serverless is gaining momentum as an approach that enables developers to focus on building features instead of focusing on the underlying architecture. From a security standpoint, it reduces security issues due to unpatched servers. When paired with the new Serverless Snyk plugin, you can ensure your dependencies will also be secure  automating security so that you can focus on building your application. What other methods is your team using for security in the serverless world? Let us know in the comments down below.",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dd4",
      "title": "Lessons Learned on Building Awesome Developer Communities from CMX Summit",
      "content": "CMX Summit is an annual conference for community managers featuring top tier experts from companies like Google, Slack, and Salesforce. Speakers cover topics ranging from how to grow developer communities from scratch to member engagement to measuring ROI on community. Fortunately for me, the CMX Summit is hosted in San Francisco, where I live and work, making it possible to connect with the worlds top community managers right in my backyard. This year CMX divided the talks into two tracks - Developer Community & Support Community. I was stoked to see the focus on developer community. As an open source framework, community is one of our highest priorities at Serverless. I was especially looking forward to learning more about how Amir Shevat (Slack) and Adriana Cerundolo (Google) were engaging their communities. Here are a few takeaways on how to build a thriving developer community from my CMX experience. !pexels-photo- Focus on high value developers. The Pareto Principle states that % of activity comes from % of the population. So when youre determining how to move the needle in your community, it's important to focus on your top contributors. They provide the highest ROI. Build simplicity and predictability for a successful community. The natural progression of developer engagement in an open source community is: - Discovery - Onboarding has to be simple and developers need to have all the necessary info to get started. - Setup Toolchains - Also needs to be super easy, as in one click. - Develop Skills - Early beginners dont have the knowledge to be creative yet, so its important to show them whats possible. - Find Tasks - If people can't easily see (in the same place every time) what needs to be done, then theyll go and work on other projects. - Get Help - Make sure that forum and support questions are answered consistently to keep people coming back, and to make it obvious to users where to go for help. - Feel Appreciated - As developers become deeply embedded in your community, make sure that theyre recognized and appreciated. Each of these steps represents a potential stumbling block for turning developers into core members of your community. Figuring out how to remove all pain points is crucial to success. Developer Communities belong under Product, not Sales or Marketing. This is something to keep in mind as your community continues to grow. Leverage your community to make your product better and cultivate future users. But be aware of turning developers off by spamming them with marketing lingo or trying to sell them stuff. Empathy is king. It's easy to get excited about launching new programs and building new features that you think are cool. But it's important to listen to the community and hear what they care about. Communication is a two-way street. Measure, measure, and measure some more. The only way to truly understand the health and success of a community is by measuring key metrics over time. Determine which metrics you can track to see how dynamic your community is. Some key metrics that Gina Bianchi from Mightybell recommended tracking are monthly active users and monthly active user growth. Here at Serverless we define an active user as anybody who's run at least four commands in the last days. Make your community FUN! This was a common theme among every speaker. Most of us face fierce competition in our respective markets. Figure out how to make your community fun and engaging, and people will stay and attract more members, no matter how many more marketing dollars the other guy is spending. Conclusion These are just a few of the highlights from CMX Summit . It was so inspiring to hear how other companies are prioritizing community management. Weve already started incorporating these lessons into our strategy at Serverless. Well let you know how it goes. You can check out the CMX website for more helpful articles about community. Want to get involved in the Serverless Community? Join us on GitHub, the Serverless Forum, Gitter, Twitter or by writing a guest post for the blog. Were always open to feedback. Let us know what you think!",
      "__v": 0
    },
    {
      "_id": "64e08915b72e199dda603dd6",
      "title": "Better DevOps with AWS Lambda + API Gateway and the Serverless Framework",
      "content": "Hi, I'm Nick den Engelsman. Im a Full Stack Developer at BandLab who specializes in orchestrating and automating highly available infrastructures. In this post I'll share some lessons learned from optimizing DevOps and how that led my team and I to the Serverless Framework. At the start of , and for the previous five years, I worked as a Cloud Systems Engineer at a certified AWS Managed Service Program Partner in The Netherlands. Most of my tasks were to educate clients and migrate them to the AWS cloud. Their environments and applications needed to be highly available, scalable and fault tolerant. In that role automation was key. Clients only needed to worry about pushing code and in return we would worry about keeping their infrastructure up-to-date, secure, patched, autoscaled, and in general meeting SLAs. If you think about it, thats a lot of work! So streamlining our process was key. Optimizing DevOps -- Phase : -- In order to stay DRY we adopted AWS CloudFormation early on. This worked for a while, but we were copying CloudFormation JSON templates and other resources back-and-forth between projects (Not DRY). Phase : -- Since myself and a few others within the company had solid Ruby knowledge, we came up with the idea to create Ruby modules that would compile to CloudFormation JSON resources (similar to Pythons Troposphere). Again this worked for a while but also ended up not being as DRY as we wouldve liked! Phase : -- As time progressed, the company built-out the group of Ruby experts and we started an R&D team whose only focus was the automation and building of components that could be re-used between clients and projects. We matured our Ruby modules and made them more and more intelligent in order do most of the configuration for us. It became its own framework! Soooo, where does serverless come into play? While still at the Dutch company we managed to set up a solid framework to automatically deploy/update/patch highly available infrastructures, which left us with only one thing we couldnt change: Client applications! There was only one thing we couldnt change: Client applications! Every new client needed to be educated. And migrating them to AWS in general was a long process. For the most part, the refactoring of their application code was needed in order to become -factor-ready. AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume  there is no charge when your code is not running. When AWS first introduced Lambda at re:Invent in , I was a bit skeptical. Their example used S events. The triggering of image resizing with Lambda was awesome, but I didnt see our clients adopting this anytime soon. Internally we started sprinkling in custom Lambda-Backed Custom Resources within CloudFormation stacks to fill-up missing gaps. This was when I started to realize how powerful Lambda actually is. We were able to automate a lot of use-cases and rd party applications/providers with the use of AWS Lambda. !new_b Fast forward I had begun to work on a passion-project of my own known as Composr (later acquired by BandLab) which was fully EC - Ruby on Rails. I decided whilst funding my own project that it didnt make sense to spend money on resources which could only be used occasionally, such as for testing and staging purposes. As AWS API Gateway and AWS Lambda were maturing, I decided to invest my time in using these newer styles of computing, and prove that it is entirely possible to design complete applications without having to manage any servers. The Serverless Framework seemed like the perfect fit to accomplish this goal! Noticing that it used CloudFormation from the start also gave me the assurance that deployments would be straight-forward and battle tested. Today BandLab is where musicians and music fans from all over the world can come together to make, collaborate and share Music. The platform works on both iOS and Android. Everything users create is hosted in the cloud, so they can capture musical ideas wherever they are, then further develop them using BandLabs web-based, MIDI-enabled app. The Serverless Framework was easy to pick up within our team, even for developers without prior AWS knowledge. After migrating the first few existing microservices towards a serverless setup, we were able to handle more events and background tasks in less time, which in return led to happier users on our platform (oh yeah, and less costs). We came to the conclusion that we wanted to push even further with this technology, to try and go serverless first. The Serverless Framework has now become a core component within our organization. It allows us to define a standardized way of developing/deploying microservices, to release to market faster, and to keep our cost of ownership low. In general, the Serverless Framework makes our work and life just a little bit happier :) Today, BandLab is running most if its AWS infrastructure without any self-managed servers (just a few EC instances left). Since we run less servers, we have less people to worry about maintenance, monitoring, patching, scaling alarms and the like (happy developers). What's next Curious where the BandLab project is heading? Checkout our blog or follow us on Twitter.",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603dd8",
      "title": "State of the Serverless Community Survey Results",
      "content": "About months ago we shared a State of the Serverless Community survey. We wanted to find out more about how and why our community is adopting serverless architectures, what problems they're encountering, and how they feel about the future of serverless. The Responses We distributed the survey through our newsletter, on our readme, and on Twitter, and received a total of responses. The majority of respondents were from North America and Europe (% and % respectively), with the additional % spread across the rest of the world. Respondents' job titles varied considerably with about % identifying as developers (% backend, % frontend, % full stack). % were either an engineering manager or executive, while the remaining % filled various roles such as product manager, architect, and DevOps. Serverless architectures are definitely being adopted most rapidly by startups, with about half of respondents reporting they work at a startup. However, serverless architectures appear to be picking up traction at larger companies, as well, with % and % of respondents coming from SMBs and enterprise companies respectively. Use Cases Serverless architectures are still very much in the early days of their evolution, but weve seen indications that theyre already being utilized for mission-critical workloads and not just for hobby or side projects. % of respondents stated that theyre using serverless architectures for work, while % are using them for a side project, and % have experimented with them but arent actually using them on a project yet. The most common use case for serverless architecture was web server/API with % of respondents checking that box. Data processing came in at %, while internal tooling, IoT, and chatbots were all marked as use cases by over % of respondents. % mentioned a use case other than the ones we listed with the most common write-in being mobile backends. Providers & Tooling AWS was by far the most widely used serverless provider with Lambda being used by % of respondents. Azure Functions came in at %, Google Cloud Functions at %, and Webtask and OpenWhisk each at less than %. The significant lead here for Lambda doesnt come as a surprise since it's by far the most mature offering in the space. We expect this gap will narrow as other providers improve their products and more tools become available to support them. The Serverless Framework was far and away the most widely used framework among our respondents with % saying they use it to develop their serverless architectures. Apex was the second most common at %, while % said they didnt use any framework at all. When asked about how they monitor their serverless architectures, % of respondents said they use AWSs CloudWatch. Another % percent use New Relic, % use Data Dog, while Splunk, IOpipe, and Sumo Logic all had responses under %. Node.js was the most common operating language for serverless architectures (%). That's to be expected since its the first language that Lambda supported. % of respondents use Python, and % use Java. These findings are very much in line with the numbers that we see from users of the Serverless Framework (%/%/%). As providers start to support more languages and add more functionality to existing non-node.js languages, we expect to see more parity among languages. The Future One of our survey questions asked respondents to rate their level of optimism about the \"future of serverless architectures\" on a scale of -. The respondents were primarily early adopters, so bias could be an issue. But its also a group of users that understands serverless, and the advantages and disadvantages that come along with it. Overall the responses were very optimistic as you can see below. As we continue on our mission to build tooling that makes it easy to develop and deploy serverless architectures were making an ongoing effort to learn all we can about the serverless community. We look forward to sharing more of those insights with you in the future!",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603dda",
      "title": "How to build a serverless notification system on AWS",
      "content": "Real-time notifications are an important use case for modern apps. For example, you may need to notify your user that another post is available in their social feed or that someone else added a comment on one of their photos. But how do you do this serverless-ly? Typically, you would use WebSockets and have a dedicated server. You'd make a permanent link between the user and the server, and use the publish-subscribe pattern to share messages. The browser would subscribe to automatically receive new messages without needing a polling mechanism to constantly check for updates. But if you're going serverless, you don't have a dedicated server. Instead, you'll need a cloud service that will solve this problem for you providing scalability, high availability, and charging per messages and not per hour. The solution: AWS + the Serverless Framework In this post, I'm going to describe how I implemented a notification system for unauthenticated users with the Serverless Framework and AWS IoT for browsers. I know that \"Internet of Things\" sounds strange to be used in a website, but it supports WebSockets, is very easy to use, and unlike Amazon SNS (Simple Notification Service) it supports WebSockets. Win! Why IoT? I used IoT due to its simple messaging system. You create a \"topic\" and make users to subscribe to it. A message sent to this topic will be automatically shared with all subscribed users. A common use case for this is a chat system. If you want private messages, you just need to create private topics and restrict the access. Only one user will be subscribed to this topic and you can make your system (Lambda functions) to send updates to this topic to notify this specific user. Demo You can find the code on GitHub. Try it here: https://serverless-notifications.zanon.dev (open two browser tabs) Architecture I've used the following architecture in this demo. User makes a request to Route that is configured to reference a S bucket. S bucket provides the frontend code (HTML / CSS / JavaScript / images) and the IoT client code. After loading the frontend code, an Ajax request is done to the API Gateway to retrieve temporary keys. The API Gateway redirects the request to be handled by a Lambda function. The Lambda function connects to IAM to assume a role and create temporary AWS keys. Frontend code subscribe to IoT events using the temporary keys. Frontend This demo runs in a static site hosted on Amazon S. As I've used a Node.js module to connect with IoT, the index.html file adds a bundle.js that was processed with Browserify. I'll explain how it was done in the following sections. AWS IoT In this project I used the Node module AWS IoT SDK to connect to the IoT service. First, you need to create a \"device\" (client browser) by providing access keys and setting the IoT endpoint that is specific to your AWS account. I'll show later how you find those data. After providing those values, it will try to connect. The next step is to set callback functions to handle incoming events. You need to hook at least with message (receive messages) and connect (subscribe to a topic after successfully connected to IoT), but you can also handle the following events: reconnect, error, offline and close. To send messages, you use: `client.publish(iotTopic, message)`  In the project's folder you can find a folder named iot. Open it and run `npm install`, followed by `node make-bundle` to execute Browserify and export the bundle.js dependency that can run in the browser. Client-side code The client-side will use the IoT object that you've just created. It will be responsible by: ) Request Access Keys and the IoT endpoint address This request will be sent to API Gateway and handled by a Lambda function that will be configured later. ```javascript $('btn-keys').on('click', () => { $.ajax({ url: apiGatewayEndpoint, success: (res) => { addLog(`Endpoint: ${res.iotEndpoint}, Region: ${res.region}, AccessKey: ${res.accessKey}, SecretKey: ${res.secretKey}, SessionToken: ${res.sessionToken}`); iotKeys = res; // save the keys } }); }); javascript $('btn-connect').on('click', () => { const iotTopic = '/serverless/pubsub'; IoT.connect(iotTopic, iotKeys.iotEndpoint, iotKeys.region, iotKeys.accessKey, iotKeys.secretKey, iotKeys.sessionToken); }); javascript $('btn-send').on('click', () => { const msg = $('message').val(); IoT.send(msg); $('message').val(''); }); ``` Backend Now you need to create your backend. Using the Serverless Framework will make this task easier by deploying the API Gateway endpoint and your Lambda Function. The function will be responsible by creating temporary AWS keys. However, it needs a role to define what access those keys will provide. Create an IoT Role You can create this role using the IAM console or execute the index.js file that is inside the create-role folder to create one for you. This package uses the AWS SDK and requires a `npm install` before executing it. This role needs the following \"Trust Relationship\":  Note that you need to replace the string AWS_ACCOUNT with your account number. If you're using the code that I've provided it will automatically retrieve your account number using the STS service. The permissions will be set for IoT functions and for all resources, which means that the client will be able to subscribe to any IoT topic. You can restrict this access if you want.  Serverless Framework Now you'll create a Lambda function that will generate temporary keys (valid for hour) to connect to the IoT service. You're going to use the Serverless Framework to help here. If you don't have it installed yet, do so using:  The serverless.yml must add Lambda permissions for `iot:DescribeEndpoint` (to find your account endpoint) and `sts:AssumeRole` (to create temporary keys). I'm also creating a simple function named auth and excluding other folders that are inside of this project to avoid zipping them with my Lambda.  The Lambda function is pretty simple. I'm using the AWS SDK and making requests: iot.describeEndpoint(): find your account's IoT endpoint (you can hardcode this result, if you prefer) sts.getCallerIdentity(): get your AWS account ID that is needed to find the Role (you can also hardcode this) sts.assumeRole(): create temporary AWS keys that are allowed only to access the IoT service ```javascript 'use strict'; const AWS = require('aws-sdk'); const iot = new AWS.Iot(); const sts = new AWS.STS(); const roleName = 'serverless-notifications'; module.exports.auth = (event, context, callback) => { // get the endpoint address iot.describeEndpoint({ endpointType: 'iot:Data-ATS' }, (err, data) => { if (err) return callback(err); const iotEndpoint = data.endpointAddress; const region = 'us-east-'; // get the account id which will be used to assume a role sts.getCallerIdentity({}, (err, data) => { if (err) return callback(err); const params = { RoleArn: `arn:aws:iam::${data.Account}:role/${roleName}`, RoleSessionName: getRandomInt().toString() }; // assume role returns temporary keys sts.assumeRole(params, (err, data) => { if (err) return callback(err); const res = { statusCode: , headers: { 'Access-Control-Allow-Origin': '' }, body: JSON.stringify({ iotEndpoint: iotEndpoint, region: region, accessKey: data.Credentials.AccessKeyId, secretKey: data.Credentials.SecretAccessKey, sessionToken: data.Credentials.SessionToken }) } callback(null, res); }); }); }); }; ``` Now, deploy using the command `serverless deploy` and copy the API Gateway endpoint that the Serverless Framework you output in the command line. This address will be used in our frontend code. Pricing How much does it cost? Only $ per million messages (USA). It can be pretty cheap depending on your scale and traffic because you don't need to pay for a dedicated server. Official pricing page for IoT: https://aws.amazon.com/iot/pricing/ Improving this demo This sample considers that there is only one topic. It's ok if everyone is subscribed to the same channel. However, if you want to send private notifications to a specific user, you need to create a new topic per user. You can modify the Lambda function to achieve this restriction. When it calls assumeRole, add a Policy parameter that restricts the access to this specific user for a specific topic name. More about this parameter here. If you need to provide temporary keys for authenticated users (logged with Facebook, Twitter, OpenID, Custom, etc.), I suggest that you try Cognito directly instead of using API Gateway + Lambda. What more? I tried another experiment with this and created a demo for a serverless multiplayer game. If you want to develop an HTML game in a serverless architecture, you can use IoT to exchange messages between players and implement a cheap multiplayer game. The performance is good enough for dynamic games. You can see the demo here and the code on GitHub. You can try it using your desktop and phone to test the multiplayer feature. Conclusion IoT can also be used for real-time notifications in the browser. Notifications are a common use case in modern apps, and it's one more problem that you can solve with Serverless.",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603ddc",
      "title": "AWS re:Invent Recap - Serverless Christmas came early",
      "content": "Many of AWS' biggest releases tend to happen around their annual re:Invent conference, and was no exception. AWS announced lots of amazing stuff last week in Vegas. Here are some highlights of what we're most excited about as it relates to serverless architectures. Lambda@Edge What is it? - Allows JavaScript to be run inside a Lambda on any AWS edge location - Several options available that define which types of requests trigger your code to run Why its important? This allows for interesting use cases such as intelligent HTTP processing and modifying HTTP headers on the fly. Also has some interesting potential use cases for IoT. AWS X-Ray What is it? - Provides distributed tracing for distributed systems on AWS - Provides a visualization of your applications' components and allows for an end-to-end view of requests - The final missing monitoring piece for AWS Why is it important? Getting a detailed view of performance as well as debugging for microservice architectures built on AWS has always been a difficult problem. X-Ray appears to be a tool that will make this a lot easier. Its not available for Lambda yet, but when it is it will allow for a lot more peace-of-mind and encourage larger, more complex systems to be built on top of it. Step Functions What is it? - Visual workflow for designing and coordinating microservice oriented applications - Allows you to setup your functions as a series of steps, including automatic triggers, tracking, and retries Why is it important? We havent had a chance to dive in deep on the new service yet, but the focus seems to be on maintaining state across functions, as well as providing a visual workflow tool. Tracking state across Lambdas has always been a problem and if this solves that problem it will allow much more complex serverless architectures to be built on AWs. Greengrass What is it? - Embedded compute, messaging and data caching for connected devices - Allows for running Lambda functions on devices, online as well as offline - Brings the Lambda event driven story to devices - Makes the IoT story much more compelling - Runs online and offline - Makes device to device communication much easier and more interesting Why is it important? Greengrass is exciting because it brings Lambdas event driven compute power to devices, making the serverless story for IoT even more compelling. It promises to make offline compute as well as device-to-device communication much easier and more powerful. API Gateway in Marketplace What is it? - API Gateway endpoints can now be sold on the AWS Marketplace Why is it important? This is the first step towards allowing developers to sell their functions directly to customers. There arent a lot of examples to try out yet, but it will be very interesting to see what type of activity unfolds here. Dead Letter Queues for Lambda What is it? - Allows failed Lambda events to be automatically sent to an SQS queue or to SNS topic Why is it important? By default if a Lambda is triggered and fails it will automatically retry twice before discarding the event. Dead letter queues for Lambda mean you will never miss an event again. Failed events can be sent to SQS or SNS where they can be processed and debugged further, so you can actually find the cause of the problem and fix it. This feature makes event processing on Lambda much more reliable and will likely result in a more widespread use. C for Lambda What is it? - .NET Core . is now an officially supported runtime on Lambda Why its important? This announcement is especially interesting for organizations with a C competency (mostly enterprises) that want to take advantage of all of the value that Lambda has to offer. Take our survey on which announcements are the most exciting More re:Invent Lambda discussions in Serverless Office Hours Tune in for Serverless Office Hours, Thursday, Dec. , -AM PST, for a live discussion of the Lambda announcements at re:Invent and how they relate to the Serverless Framework. We'll also answer questions about the latest framework release - v..",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603dde",
      "title": "How to Create a Fast, Secure, and Scalable Open Source Blog with React + Markdown",
      "content": " Content marketing has been growing in popularity for years, and for good reason, it works. Creating blog content is a proven source of sustainable organic traffic, and a great way to drive qualified leads. Most companies look to WordPress, SquareSpace, or Medium for their company blogs. While these sites work for hosting blog content, they come with a number of downsides, especially if your target audience is developers. Problems with the typical company blog setup: - Barriers to contribute - Speed & security issues (cough cough WordPress) - Traffic cannibalization (Medium) - Limited ability to customize & reuse product UI - Poor authoring experience for developer-focused content This post talks about a different approach that, in my opinion, is better than the traditional blog setup. Let's explore the % Open Source, Markdown & React-powered blog that you're reading. The Serverless Blog Serverless.com's blog is % open source content driven by a GitHub repo and Markdown files. Blog content is fed into our site and rendered with our static website generator of choice Phenomic. Let's explore the benefits of our blog setup and how they address the problems outlined above. Easier to contribute and edit posts Well for starters, anyone with a GitHub account can submit blog post updates, typo fixes, and new content without needing a user account with our site. On every post, including this one, users have a one-click 'Edit this post' link that lets anyone submit updates to posts. This is great for getting streamlined contributions to our blog. P.S. We're always looking for fresh content. Drop us a line if you're interested in contributing. Better developer-focused posts If you've ever had to write a code heavy developer-focused blog post in WordPress, it's a rather painful experience. WYSIWYG editors are extremely proficient at mangling code snippets. On the flip side, writing in GitHub flavored Markdown makes writing and (more importantly) maintaining blog posts about code MUCH easier. Markdown FTW! Side note: If you're stuck with WordPress, I wrote a plugin to allow you to write in GitHub favored Markdown. Unified product experience We're using React as our frontend framework of choice for creating our serverless applications (like dashboard). This is a no-go with other blogging setups. If we decided to blog with WordPress, we'd need to maintain separate style/component libraries because WordPress can't render React components server-side. Keeping styles in sync in multiple platforms is painful and will eventually leads to an inconsistent user experience. Luckily, our site and blog are powered by Phenomic, so we can use all of the same React components we use in our products. Code reuse and brand consistency FTW! Scale out of the box Using a static site gives us raw scale out of the box unlike dynamic blogging platforms like WordPress. How our static site is built: `npm run build` uses React's server-side rendering capabilities to build all the static HTML files for the site That's it. The site is pre-rendered for all visitors If we get featured on the front page of the New York Times or Justin Bieber tweets about Serverless, we're sitting pretty because our site is statically served from the Netlify content delivery network. Speed like Whoa Because we're using Phenomic, which uses React + React Router under the hood, page fetching and transitions are lightning fast. Instead of hard page reloads when navigating around the serverless.com site, you get a single page app experience when navigating through site links. Each link clicked only fetches the required data from a `.json` file, instead of a full HTML page like other static site generators. Another out-of-the-box benefit with phenomic is optimistic page rendering. This is an example of how the loading state of a blog post looks while the post's lightweight `json` data is being fetched. As you can see, when navigating to any given page, the visitor gets instant feedback, so the perceived site performance feels snappy. React + Phenomic + Static Websites are the future Ultra scalable, fast, dynamic static websites are the future of frontend sites and I highly recommend checking out our site repo for how it's all hooked up! You can run our site locally by: `git clone git@github.com:serverless/site.git` `npm install` `npm start` If I haven't convinced you yet. Tweet at me @DavidWells and I will yell at you. I'm planning a series of posts illustrating how the site is built. For now, checkout the github repo.",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603de0",
      "title": "Talking re:Invent & the Latest Serverless Framework Features in the Dec. th Office Hours Live Video Recording",
      "content": "Serverless Office Hours is a monthly live chat with core members of the Serverless team. In the nd episode of Office Hours, Developer Evangelist David Wells and Core Framework Engineer Eslam Hefnawy reviewed Lambda announcements from AWS re:Invent and talked about new features in Severless Framework v. & v. (now with Environment Variables Support, Yay!). Want to know more about the recent AWS Lambda announcements and why they matter? Check out AWS re:Invent Recap - Serverless Christmas came early. David and Eslam also answered some questions from the community. @goserverless hi! Just hit Cloud Formation resources limit while deploying. Any workarounds for this issue? Thanks in advance.&mdash; Andrs Santibez (@asantibanez) December , And announced new Serverless projects, including: - Serverless Examples GitHub Repo - A collection of boilerplates and examples of serverless architectures built with the Serverless Framework and AWS Lambda. Add your project and share it with the world. - Serverless Plugins GitHub Repo - Serverless Plugins allow users to extend or overwrite the framework's core functionality. Many of these plugins are contributed by our amazing community members! - Serverless Dashboard Project - A desktop application providing a graphical user interface for the Serverless Framework, AWS Lambda, Google CloudFunctions and more. See more details and answers to community questions in the full recording:  Tune in for the next edition of Serverless Office Hours, Thursday, Jan. , -AM PST, coming to you live from our team retreat! Tweet @goserverless with your questions along with the hashtag askServerless to join the conversation.",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603de2",
      "title": "How Shifter Introduced Serverless Hosting to WordPress Using AWS, DynamoDB & the Serverless Framework",
      "content": " Shifter is a SaaS hosting product that turns any type of WordPress website or blog into a static site. In one click your site can be converted into static HTML files, moved to S and delivered through CloudFront. In this post the Shifter team will share more about our motivation for building this project along with a sneak peek at the backend. Introducing Microservices to WordPress WordPress currently serves about % of all sites on the web. People use it in many different ways - ranging from beginner blogs to big media websites to web applications. In our experience, WordPress is favored by many because its simple and powerful. The Shifter team consists of WordPress pros who contribute to the project actively. As AWS Advanced Technology Partners, we're also AWS experts. Through repeatedly creating hosting solutions for large-scale, high traffic websites, we realized an opportunity to introduce the concept of microservices into the context of WordPress. Solving the Scalability Issue Maintaining WordPress sites demands significantly more time and attention as the website gains popularity (and more traffic) or the number of websites you manage increases. In some cases, the amount of time spent maintaining the backend can exceed the work put into new content. This can cause major problems for WordPress users as their websites grow. Shifter addresses these issues by converting a dynamic WordPress site into static files. What if you want to make changes to your site? WordPress is there when you need it, on demand, running from a Docker container that only you can see. After youve made your changes Shifter goes to work generating those updates, delivering them across a global CDN, and the Docker container and WordPress backend will vanish again. Shifter brings the simplicity and benefits of serverless architecture to WordPress. Now, we'll share some of the architecture of the Shifter backend and a few lessons learned. A Glimpse of the Shifter Backend Shifter is built according to the microservices concept. The backend relies on highly available and durable services like S, Lambda, DynamoDB and API Gateway. !The overall outlook of Shifter Service Heres the architecture of the user console: !How the console architecture look like We chose to set up the console architecture with the Serverless Framework. It's a Single-Page Application. Here are some highlights: - HTML, CSS, Javascript files are hosted on S - User registration data is stored with Cognito User Pool - Dynamic process is handled by API Gateway, Lambda & DynamoDB The biggest advantage of using this architecture is that you dont need to maintain servers or manage user credentials, so you can concentrate on the code. Heres a snippet of the code what runs on Lambda:  Lambda functions are allocated to APIs one by one. The process is divided according to what HTTP method the API will be taken through. There are multiple APIs, which the Serverless Framework allows us to deploy at once. It's crucial (and awesome!) that we can deploy multiple APIs with one tool. Lessons Learned & Next Milestones Integration Testing Through this development process we've learned that it's vital to have automated tests in the Serverless Framework - not only unit tests, but also automated integration tests. For the microservices approach to work well, everything needs to talk to each other. We're currently trying to automate integration tests among the multiple different microservices. Powering Dynamic Features with Static Files Our next goal is to give Shifter the same capabilities as WordPress blogs, websites, and apps in traditional settings. Bringing new features such as SFTP, WP-Cron, and the ability to re-generate a single page are up next. We expect to face some obstacles here, and potentially solving them will be more difficult than solving them in traditional setups, such as LAMP. But we're up for the challenge and eager to share our learnings as we go. We'd love your feedback along the way. Try Shifter for free through the end of January and let us know what you think. Taking It to the Next Level with the Shifter Kickstarter The Shifter team recently launched a Kickstarter to help us reach our next development milestones. Check it out if you're interested in supporting our mission!",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603de4",
      "title": "'How To Write Your First Plugin For The Serverless Framework - Part '",
      "content": "Hi, I'm Anna Doubkova, a software engineer at Red Badger in London. If youre reading this post, you probably know some basics of the Serverless Framework, like deploying Lambdas and creating API endpoints. But maybe you've hit a block where you find doing something with the framework is either difficult, impossible, or simply too repetitive. You can ease your pain by using plugins. Some are already built for the most common problems (check them out in the official Serverless Plugins Github repo), but there are always some project-specific issues that plugins can help you resolve. Fortunately, writing a plugin for the Serverless Framework is easier than you might think. In this tutorial, youll learn by examples how to write your own plugins. Well start from very simple examples and build upon them to get all the way to writing useful plugins that could help you with your everyday deployments. !writing plugins for Create Your First Plugin The Serverless Framework is an incredibly well-built open source platform. It's nearly indefinitely extensible, and allows you to add new features with surprising ease. Lets see how you can add a plugin in the simplest way possible. Somewhere in a new directory run:  This command will create an `index.js`. When you open this file, you'll see a class with a constructor and a few simple functions. Let's have a look at it step by step. Plugin as a Class  Every serverless plugin is a class. This class gets instantiated with a `serverless` object and a bunch of `options`. We'll get to them in more detail in a little while, but for now it's enough to say that these will help you _do_ things in your plugin. Define Your Commands  The next thing to notice in the constructor is the definition of commands that your plugin introduces. In the boilerplate we initialised using the Serverless CLI helper, command `serverless welcome` is added. Usage - or Help In the `usage` section, you can specify a hint how the command should be used. This hint will appear when you run `serverless --help` in the list of commands and their descriptions. Lifecycle Events The crucial bit in any command is the `lifecycleEvents` array. Lifecycle events allow you to define the steps that are most likely to be taken while executing the command. For example, these are the lifecycle events for `serverless deploy` command: - cleanup - initialize - setupProviderConfiguration - createDeploymentArtifacts - compileFunctions - compileEvents - deploy From `cleanup` to `deploy`, we have a list of tasks we should fulfill on our way to deploying a service. We don't write any implementation yet, though. We're only describing the process in a very general way that doesn't include any implementation details. The advantage of this approach is that it doesn't tie us into one way of handling deployment (or executing any other command), which in turn helps us avoid vendor lock-in. As such, you can see the command definition as a guideline that you can use later on to write your code. This makes for more readable, self-documented code. Options - or Flags You might notice that in the plugin template we also have the `options` section alongside `usage` and `lifecycleEvents`.  The `options` section can be used to describe which flags can be used in CLI with your command. In this definition, the `--message` option is required, and has a shortcut `-m` so that you can equally write `serverless welcome --message \"Hello!\"` and `serverless welcome --m \"Hello!\"`. As with the command itself, we have a `usage` description which appears when asking for help in the CLI `serverless welcome --help`. Requirements for Defining a Command The snippet discussed above is defining lifecycle events, help description, and a flag. For the flag, we have again a help description section, information whether the flag is required and a flag shortcut. As you could guess, not all of them are required. A very minimalistic approach would be:  This will work in a very similar way to the definition above. However, it won't provide you with automatic checking that the required option is passed to the command, or any help information. From this perspective, I'd suggest spending the time with writing up `usage`, requirements and shortcuts. It will make it significantly easier for the users of your plugin to figure out how to actually use it. Hook Into Events Defining the commands and their lifecycle events is useful to describe what the plugin does. Hooks describe _how_ the plugin does it. You can find them right under commands in the constructor:  Hooks help us define the implementation of each step. In the code above, you can discern the command name `welcome` and the two lifecycle events we defined for it: `hello` and `world`. Hook `welcome:hello` defines what to do at step `hello` of command `welcome`; `before:welcome:hello` describes what to do before the first step. Similarly, `after:welcome:world` defines what to do after the last step. This gives us a very fine-grained control over definition of what the command does at each step, and enter specific actions before and after each step. Remember you don't have to define every step. Maybe in your particular implementation, you only care about `world` and not `hello`, and that's absolutely fine. Later on someone else can come with their plugin and define their own implementation of the `welcome` plugin based on the steps you specified in the command. It's an open-ended world, allowing (nearly) endless extensions. Implementation After having defined the command, its lifecycle events and how we hook into them, all that's left is the actual implementation. The details are obviously specific to the plugin you're writing, which platform you're targeting it at, etc. In the template, we're only logging out greetings and the message we've passed it: ```js class ServerlessPlugin { // ... beforeWelcome() { this.serverless.cli.log('Hello from Serverless!'); } welcomeUser() { this.serverless.cli.log('Your message:'); } displayHelloMessage() { this.serverless.cli.log(`${this.options.message}`); } afterHelloWorld() { this.serverless.cli.log('Please come again!'); } } ``` That, however, _might not be quite enough_ in a real world project. You'll learn how to write implementations of plugins, what the serverless object is all about, and how you can approach writing plugins in multiple ways - in How To Write Your First Serverless Plugin - Part .",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603de6",
      "title": "Scaling Email Marketing to Infinity & Beyond by Going Serverless",
      "content": "One day we found ourselves in a predicament: We were really good at email marketing, but our software sucked. We were spending way too much time trying to \"use\" the tools we had, and it was painful. At the same time, we were proud to say that we used AWS to create amazing platforms, products and services for others. Huge international companies relied on our NO-server concept and were extremely happy with the results. Years spent building eCommerce apps led us to start thinking about how to create a new tool that could improve communication flow between companies and their subscribers. Thus, the idea for MoonMail was born. We were already using the best performing email marketing tools for Shopify (Automation + Recover Checkouts), but we wanted to go a step beyond. We wanted to present something different to the eCommerce community: an email marketing platform with superior performance, plus infinite scalability. Overcoming Limitations by Building a Serverless Architecture We realized that just wasn't possible with the architecture we were using at the time since it relied too heavily on EC and Ruby on Rails. So, we decided to embark on a never-ending journey in perfecting the art and science of sending emails. In the meantime, we were playing with something new, called Lambda, with small Shopify apps and integrations. We also went into digital payments solutions, which we aptly named MONEI. The team was extremely fast in changing, deploying and updating the code. It was literally click away. No servers, and there were only incurred costs when an action was triggered, meaning that if the user didnt hit the function, there wouldnt be a cost. We read some serverless books, got involved with the OSS project to see if the project had solid roots, and after weeks our company went fully Serverless. We looked into different serverless approaches, but in the end decided to use the Serverless Framework because it relies on AWS, the infrastructure we were already familiar with. With Serverless Framework on our minds, we took a bold step toward a new approach based on totally decoupled microservices - better project organization and cost when it should be (i.e., only if they are used). If the microservices get any traction, they are almost infinitely scalable. How about that for creating and testing a minimum viable product? What Does This New Infrastructure Look Like? Let's have a look: Top Secret Front End Preview In the immortal words of wisdom: Simplify. Sometimes developers like to cram as many features as possible into an app. However, more often than not, we see that the most effective way is to have the right tools for the right job. No more, no less. With this in mind, we created MoonMails UI clean, simple and extremely easy to use. A Single Page Application MoonMails front-end consists of a Single Page Application, built in React and Redux, which is hosted in an AWS S Bucket and served through CloudFront with Route as DNS. Cloudfront provides the fastest and most reliable way to gain access to our application. Content is replicated in different regions, which significantly reduces loading times and also allows us to handle as many requests as needed. It obtains temporary AWS credentials for each user with our IAM SAML auth provider. This allows them to access the S bucket directly, which we use for file uploads in the app. The login is handled by Auth, so the Sign Up / Sign Up process is virtually frictionless. You are just clicks away from all the app awesomeness. !Moonmail serverless architecture MoonMail Serverless Infrastructure Back End Insights Event-Driven Architecture When it comes to MoonMails back-end architecture, we can say that its fully event driven. It doesnt use any traditional servers because it % relies on AWS Lambda, which handles the \"no server part\" for us. The Serverless Framework is the key to managing the lifecycle of the infrastructure. It covers us in the sense that we only have to worry about writing code and doing very little with operations. Its worth mentioning that MoonMail is composed of more than Lambda functions (and increasing) written in NodeJS with a microservices approach in mind. Functions & Microservices MoonMails simplest processing units are its functions. Functions are composed to create microservices, which are composed to build functionalities. So how do we deal with their growing numbers? Vast knowledge of business logic, which translates into: good separation of concerns, well defined microservices and boundaries, and last but not least, a good and reliable tool to manage the whole process. Thats why Serverless Framework is in our stack. Serverless Decoupled Concept of Functions & Events A Serverless decoupled concept of functions (the functional part) and events (what triggers the functions) is as good as it gets. We add REST Endpoints exposed by API Gateway or SNS/Kinesis integration with a simple configuration. This is very important to us since its easier to add or remove functions/services from our event driven \"Choreographies.\" Every action, from an Upload Recipient List to a Send Campaign triggers a number of microservices interactions, and everything is done asynchronously to avoid maximum direct microservices communication with SNS, SQS, Kinesis integration and/or data replication in an eventually consistent way. DynamoDb Since our data storage is DynamoDB, every record gets stored there. But we also store some redundant aggregated information in tables so reports can be created and presented to the user in near real time (with the help of Kinesis Streams and DynamoDB Streams). We even do some AI on the platform in the User Reputation System, which is handled in real time by our super complex AI program; we call it _Miguel_. The Serverless Framework As a result of using serverless technologies along with the Serverless Framework, the engineering team has become confident in the developing-release cycle. This is because we truly understand our business model, as mentioned previously, but also due to the reliability and simplicity provided by Serverless in configuration management, stage management and ecosystem. This gives us % automation capabilities to build our Development Environments in the same way we built our Production one so its been much easier to implement our Deployment Pipeline and Continuous Delivery system. The result is less cash spent on resources compared to conventional architectures. We couldnt ask for more. Totally in love. The Results What happened after developing this new infrastructure? SENDING BILLIONS OF EMAILS WITHOUT MAINTAINING SERVERS Our users (high techy guys) can send , emails per month for - drumroll please - absolutely free. . Zilch. Nada. (If they come with their AWS/SES account under their shoulder). Click here if you want to jump into the MoonMail jungle by using your own AWS/SES account: https://moonmail.io/amazon-ses-email-marketing Need more? Many more? We have you covered. Our Professional Plan offers an unlimited number of sent emails per month. For users in the \"low tech\" world, we offer a simple MoonMail account where we provision in real time SES endpoints for them. Our _Miguel_ here is really strict. If he detects any \"strange\" behaviour, he locks the user for the eternity of his children. Cick here if you want to check out this alternative MoonMail version: https://moonmail.io/ Soon after releasing MoonMail as an Open Source project on GitHub, some developers approached us asking us to help them in installing/deploying MoonMail in their own AWS account. We monetize there, too. Sometimes companies feel more comfortable with storing some really sensitive data within their AWS accounts. That's possible, too. Contribute to MoonMail MoonMail is the result of the combined efforts of many people. Thus, we would love for you to help us make MoonMail even better, greater, faster, and smarter! Developer? Contribute here: https://github.com/microapps/MoonMail User? Take it for a spin for FREE: https://moonmail.io Be sure to let us know what you think!",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603de8",
      "title": "Talking Serverless Framework Features & How To Use Them in a Webinar with Cloud Academy",
      "content": "Serverless CEO Austen Collins recently had the chance to connect with Cloud Academy to chat about new framework features, give an overview of basic application lifecycle management with Serverless and answer questions from the community. The webinar includes: - What the Serverless Framework does & how you can use it - New Lambda features - Use cases - A demo of how to get started creating & deploying Serverless services - Plus more! Check out the video below along with these other resources to help you get started with Serverless. - Serverless Examples Repo - A collection of boilerplates for pre-made Serverless services submitted by the Serverless team and community - Serverless Plugins Repo - Extend the framework with these plugins from the community - Serverless Docs - Dive deeper into how to use the framework by reading the Serverless Docs - Serverless Forum - A great place to ask questions and connect with other devs using the Serverless Framework Watch Here: ",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603dea",
      "title": "A Serverless Weatherclock To Monitor My Favorite Kiteboarding Spot At The Lake",
      "content": "Hi, I'm Douwe Homans. I'm a trained medical doctor, software engineer and entrepreneur in the Netherlands. I recently decided to turn an old weatherclock into an IoT project using a Particle Photon and the Serverless Framework. In this post I'll share how I did it. Background In the 's, my grandparents received a homemade weatherclock as a gift from a friend. The device had a clock-like display that indicated the current wind direction near their home. The sensor part was an actual mechanical wind vane that used a magnet and reed switches to determine the wind direction. They placed it several meters from their house on a high pole. It was connected with a multicore cable (a core for every wind direction) to the display inside. The display consisted of LEDs mounted in a black acrylic plate. The plate was framed in a circular piece of wood. After my grandparents moved to an apartment, we never found the space to put up the sensor. The clock hadn't been working since. Now A lot has changed since then. Now we have the Internet! I thought it would be nice to hook up the clock to the Internet so we could get the current wind direction (and speed) from the web. This would allow me to get rid of the mechanical wind vane, and I could also add windspeed to the display in addition to the direction. Getting Started It came down to steps: Connect the clock to the Internet Get the data to the clock Connect the clock to the Internet I decided to work with a Photon made by Particle.io. It's a microprocessor that automatically connects to the Particle Cloud once set up. The device can be programmed in C. I connected each LED, using a current limiting resistor, to a separate pin of the Photon. The cool thing about the Photon is that you can program it over the air from your browser. In your code you can define remote functions which you'll be able to call over the Internet once the code has been deployed to the device. (Particle Documentation). I exposed two of those functions: `setWindSpeed` and `setWindDir`. The first one takes the windspeed in Beaufort (a commonly used scale in the Netherlands), the second one the windDirection. The code on the Photon simply runs an infinite loop, similar to this:  You can see actual code on GitHub. The clock is pretty 'dumb'. It's not reaching out to the Internet to find, parse, and display data. It just displays the values for the windSpeed and windDir and those values get set by calling `setWindSpeed` and `setWindDir`. This helps to keep the code in the clock really simple and focused on one job. Gathering weather data and getting it to the clock is not the concern of the clock itself. So how do we get the data to the clock? Once the clock is connected to the Particle Cloud you can (with the right credentials) connect to it through the Particle Cloud and call the functions you exposed in the code (`setWindSpeed` and `setWindDir`). I can login to the Particle Cloud, find out the ID of our wind-clock, and just use the `particle` command line tool. `$ particle call ID_FROM_OUR_CLOCK setWindDir \"NNE\"` And that makes my clock's `N` and `NE` LED's light up indicating the wind is blowing from North-North-East. This shows me that the hardware is working, but of course I want the current wind direction for my favorite spot, pushed automically (and periodically) to the clock. So basically I need to run the following steps periodically. - Get data from the web - Push data to particle-cloud Serverless When I thought about how I wanted to run the code to update my clock I came to the following conclusion: I really don't care that much where my code runs. As long as I know it does AND I can tune in to see it still does :). I could spin up a server myself, I could use an EC instance, or Heroku, or a virtual server from a different vendor, or.... I don't want to spend time figuring out what is the best option. This is where Serverless made sense to me. I just write my code and configure which events triggers my code to run. The complete code is on GitHub. But it boils down to the following: A `handler.js` which exposes an `update` function which: Gets Weather data (from Dutch Weather Institute) Converts M/S to Beaufort Connects and pushes data to the Particle Cloud A `serverless.yml`:  That's it! I can run a `$ serverless deploy` to deploy my code and know that it runs every minutes. If I want to tune in I simply run `$ serverless logs -f update`. And just like that I can tell whether it's time to head out with my kiteboard.",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603dec",
      "title": "Announcing OpenWhisk Integration with the Serverless Framework",
      "content": "Today we're excited to announce OpenWhisk integration with the Serverless Framework! The official OpenWhisk Provider Plugin allows developers to build applications for the OpenWhisk cloud platform using the Serverless Framework. Special shout out to James Thomas (@thomasj) at IBM for his awesome contribution spearheading this effort. !openwhisk serverless integration OpenWhisk + The Serverless Framework The Serverless Framework enables developers to use a simple manifest file to define Serverless functions, connect them to event sources and declare cloud services needed by their application. The framework then deploys these Serverless applications to the cloud provider. Introducing OpenWhisk Support Multi-provider support was a goal we laid out following the Serverless Framework v release. With the OpenWhisk integration, developers using the framework can choose to deploy their Serverless apps to any OpenWhisk platform instance. Further, multi-provider support simplifies the process of moving applications between cloud providers and enables the development of multi-cloud Serverless apps. The Serverless workflow and developer experience is consistent across all providers. You don't need to learn custom commands or syntax for each platform. Resources for Getting Started Check out these resources to help you get started with OpenWhisk integration. Intro to the OpenWhisk Serverless Plugin Video Learn more about how to use the Serverless Framework with the new OpenWhisk provider plugin in this quick video. Serverless Docs OpenWhisk is now included in the Serverless Docs. You'll find a guide to building Serverless applications, CLI command reference, platform event support and an example application. Serverless Examples Repository Check out the Serverless Examples repository to see more sample applications. OpenWhisk examples include: how to build HTTP APIs, cron-based schedulers, chaining functions and more. Let Us Know What You Think For months we've been collaborating with the OpenWhisk team to ensure a great user experience and seamless integration. Let us know what you think. Community feedback is a driving force in the direction of the Serverless Framework. You can report bugs or request features by opening an issue in the Serverless-OpenWhisk repository. Join the conversation in the Serverless forum, chat room or this Slack channel for OpenWhisk. We're excited to hear your feedback. What's Next for OpenWhisk The roadmap includes support for non-Node.js runtimes, ensuring compatibility with popular third-party plugins and integrating new features from the platform.",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603dee",
      "title": "How to gain more visiblity into your Github Projects using Scope",
      "content": "Let's face it, large GitHub projects are hard to follow. They have tons of issues and PRs flooding your inbox and it's hard to sift through them on GitHub. We face this on a pretty regular basis here at Serverless, so we needed a solution. First, we tried GitHub projects, but the manual effort it took to keep the columns up to date was a bummer. Next, we did what any good engineer would do... we built our own solution using Serverless technology. Introducing Scope Scope is an open source status board driven by Serverless technology. It gives a customizable bird's eye view of your open source project. The application can be cloned down and deployed for your open source project in minutes. See how Deploy it as a stand-alone application, or embed it directly into your project's site. Run it for free under AWS's generous free tier. Why we built it We built this tool for our community to help keep people up to speed with what's happening with the serverless project, and to highlight places where we actively want feedback + collaboration. - Quickly sort and see high priority issues and pull requests - Call out which issues need attention from your community - Zoom into important aspects of your open source project Features - Customize the labels/columns to fit your project - Customizable styles - Driven by push based GitHub webhooks - Run as stand-alone app or embed on your project's site - Look mom! No servers! Data automatically updates when activity happens in your repository. Your status board will reflect the latest state of your project. Documentation Front End Documentation & Setup Back End Documentation & Setup Video Tutorials Contributing Want to contribute back to the project? Drop an issue or open up a PR. How it works A Lamba function sits waiting for a github webhook `POST` and saves the relevant information to be called by the UI. !cloudcraft - status board webhook listener The UI calls DynamoDB, avoiding heavy githubAPI calls/throttling issues and displays the issues based on your columns setup! !cloudcraft - status board ui Questions? Ping me @DavidWells",
      "__v": 0
    },
    {
      "_id": "64e08916b72e199dda603df0",
      "title": "'How To Write Your First Plugin For The Serverless Framework - Part '",
      "content": "Hi, I'm Anna Doubkova, a software engineer at Red Badger in London. In my previous post (How To Write Your First Plugin for the Serverless Framework - Part ), you learned what Serverless plugins are and how you can use them to hook into the Serverless Framework yourself. In this follow-up post, youll see how to write implementation of a plugin that could be used in real life. Extending The Serverless Framework Plugins extend functionality of the framework to tailor it for your use case. The framework is very flexible and allows you to take different approaches to implementing your logic. The main ways are: Writing a new command Extending an existing command to implement additional functionality Writing your own implementation of an existing command from scratch Writing A New Command Let's have a look at a practical example that will illustrate why you'd want to write a new command for the Serverless Framework. Imagine you have a microservice defined in your `serverless.yml` that contains a DynamoDB table. You can deploy the functions, add API Gateway endpoints, and create the table automatically by running `serverless deploy`. Easy! What if you want to copy data from production to dev table so that you can test your application with real data? You could export and import data from one table to another, but that'd be very tedious if done frequently. Instead, we'll write a plugin for it. Copy Data Plugin You can start by defining the command in a new class as shown in Part of this series. I imagine the command would have two steps, or lifecycle events - one for downloading the data and one for uploading it. It could look something like this:  When the command is defined, you attach lifecycle functions by defining them as hooks. Again, one function will handle downloading the data and the other one will upload it. Notice that we're binding `serverless` and `options` to these functions so that they can access their data.  Object-Oriented Programming (OOP) Approach If you'd rather not bind `serverless` and `options` to these functions, there's another more object-oriented approach.  Or if you're using babel and like cutting-edge ES features, you can use Class Properties Transform to use arrow functions and remove the need for binding completely.  However, I personally prefer a more functional approach. That's why we'll carry on in this tutorial with the two functions completely separate from the class itself - as per `Version `. Having defined the command, it's time to jump to the implementation. Downloading Data First, you want to download the data from a production database and save it somewhere so that you can upload it in the second step. I decided in this case to save the downloaded data to `serverless.variables` so that it can be easily accessed. ```js const downloadData = (serverless, options) => new Promise((resolve, reject) => { // function configuring aws-sdk and getting the DynamoDB client const dynamodb = getDynamoDB(serverless); const params = { TableName: 'users-production', }; dynamodb.scan(params, (error, result) => { if (error) { serverless.cli.log(`Error on downloading data! ${JSON.stringify(error)}`); return reject(error); } serverless.variables.copyData = result; serverless.cli.log(`Downloaded ${JSON.stringify(result.Items.length)} items`); return resolve(result); }); }); ``` As downloading is likely an asynchronous event, your function needs to return a promise. This way, the Serverless Framework will know to wait for this step to finish before starting the following one. Another handy thing to notice here is `cli.log()` function on the serverless object. It provides you with uniform message logs to the console. _Note:_ In your implementation, you can easily swap DynamoDB for another database (or even provider!) entirely - however, for now we'll stick to AWS - the most widely used provider. Uploading Data In the second step of the implementation, we simply need to take data from `serverless.variables.copyData` and upload it to the test/dev database. As far as I'm aware, we can only do that by uploading the data one by one: ```js const getPutPromise = (dynamodb, params, serverless) => new Promise((resolve, reject) => { dynamodb.putItem(params, (error) => { if (error) { return reject(error); } serverless.cli.log(`Uploaded: ${JSON.stringify(params)}`); return resolve(); }); }); const uploadData = (serverless, options) => new Promise((resolve, reject) => { // function configuring aws-sdk and getting the DynamoDB client const dynamodb = getDynamoDB(serverless); const uploads = []; serverless.variables.copyData.Items.forEach(data => { const params = { TableName: 'users-dev', Item: data }; uploads.push(getPutPromise(dynamodb, params, serverless)); }); Promise.all(uploads).then(() => { serverless.cli.log('Data uploaded successfully!'); resolve(); }).catch(error => { serverless.cli.log(`Data upload failed: ${JSON.stringify(error)}`); reject(error); }); }); ``` And we're done! Somehow, this doesn't quite feel satisfying. Why should this be a Serverless plugin when really we could write this easily as node or bash script? Why Create A Serverless Plugin? To see where the Serverless Framework helps us, we need to dig a bit deeper. Generally speaking, it contains whatever we specified in `serverless.yml`. To give a few practical examples related to our case: - region - default stage and custom variables - service resources If we look at the implementation of `getDynamoDB`, some of its benefits become immediately obvious:  Getting region from the `serverless` object makes our plugin more resilient. If we decide to deploy the service to another region, the plugin will still work. It also makes it useful outside of our particular service. We could use it across our codebase or even open source it. Well, nearly... Defining Stage Our plugin is so far really useful only if I have a `users` table deployed to `production` and `dev`. I might also want to use it for `customers`, moving data from `test` to `dev` for debugging and testing purposes. We've already used region to configure `aws-sdk`. For the others, we need to first consider what we might have defined in `serverless.yml`. One of the typical set-ups would be to have a default stage that can be optionally replaced by passing `--stage` flag to `sls`.  We are now sure we'll have a stage specified whenever we're running serverless commands. This allows us to swap `users-production` for a more generic formulation: ```js const downloadData = (serverless, options) => new Promise((resolve, reject) => { // function configuring aws-sdk and getting the DynamoDB client const dynamodb = getDynamoDB(serverless); const params = { TableName: `users-${serverless.service.custom.stage}`, }; //...scan }); ``` This code is equivalent to the original one if we run `serverless copy-data -s production`. Upload Stage Being able to choose which stage of the table to upload data to will be a bit more tricky. In the original example, we're using a flag already used by the service. We now want to introduce a new one - and that's done by defining `options`. This is very easy in serverless:  Now in the upload function, we update table name the same way we did in upload; this time however, getting target from the options. ```js const uploadData = (serverless, options) => new Promise((resolve, reject) => { // dynamodb... serverless.variables.copyData.Items.forEach(data => { const params = { TableName: `users-${options['target-stage']}`, Item: data }; uploads.push(getPutPromise(dynamodb, params, serverless)); }); // wait for promises... }); ``` Now the plugin can get data from `users` table in any stage and upload it to another one by running `sls copy-data -s production -t dev`. Which Table? To get the right table, you can again specify an option and swap `users` for `options.tableName`. However, there's another way to do it that illustrates the capabilities of the Serverless Framework. Let's say you've defined your DynamoDB as a resource in your `serverless.yml` in a following way:  Although the format of the table name is still `name-stage`, it won't work with our plugin because it expects a `user` table. Luckily, the format of the data can be defined in any way, so you only need to change the way you're getting the table name. You add a new option `--resource` that will point to the Resource name. Then you get the right table name from its Resource definition, and can replace the original stage with the target one:  Although this solution has its issues, it makes the plugin yet a bit more reusable and resilient. Ready Well done! You've just finished a Serverless Framework plugin that not only solves a real-world issue, but also can be easily packaged, published to npm and shared with others! There's yet much more to explore. What other things can we get from the `serverless` object? How do we hook into existing commands to extend them? How do we write a plugin that could be used, say, with both AWS and OpenWhisk? I'll leave these questions for you to answer with your own experiments. PS: To see the whole plugin together, including a service it works with, check my GitHub.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603df2",
      "title": "CICD for Serverless Part  - Mocha Endpoint Testing",
      "content": "By day, I'm a Technical Solutions Architect at Cisco where I teach Cisco's vast ecosystem of partners to present the value of CloudCenter (formerly CliQr). It's a Cloud Management Platform that uses abstraction on top of multiple IaaS APIs to enable a systems administrator to manage applications running on different clouds from a single pane of glass. But since March of last year, I've been using AWS Lambda on a personal project. I quickly became interested in what the good folks here at the Serverless Framework have been doing to similarly abstract details of different FaaS platforms to lower the learning curve and deployment overhead for developers. In this -part series I'll demonstrate how it's possible for emerging FaaS technology to coexist with modern programming techniques like test-driven development (TDD) and Continuous Integration/Continuous Delivery (CICD). I'll show you how to perform automated endpoint testing using Mocha (this post) and a AWS CodePipeline CICD workflow (next post). I'm kind of a web old timer. My first web application went into production in January of . It was written in CGI-BIN Perl, only worked with a specific version of Mosaic, and used `` tags to space the page elements because the `` tag wasn't part of the HTML specification yet. As a web veteran, the current state of FaaS reminds me of other technology waves in that it shows great promise, but needs to change some minds before it can experience widespread adoption. So when I met up with the Serverless Framework team in Las Vegas at their mixer during AWS re:Invent, among the things we talked about was the need for more complete examples. And how to demonstrate how this cutting edge technology can coexist with modern programming techniques like test-driven development (TDD) and Continuous Integration/Continuous Delivery (CICD). The team followed through with an excellent set of examples and I riffed off one of them to provide automated endpoint testing using Mocha (this post) and a AWS CodePipeline CICD workflow (next time). At a high level, the whole thing looks like this: !Serverless CICD Diagram Automating API Gateway Endpoint Testing Before the days of TDD there wasn't always a lot of agreement on when you were done with a particular coding task. So scope creep ruled the day. To prove to a wider set of developers that FaaS is ready for prime time, then, you need to be able to demonstrate that TDD is still possible with a more loosely related set of functions working together as opposed to a more monolithic set of application logic that is the norm now. As a first step, that's what I set out to do. Feel free to try this yourself with the Local Execution instructions on my GitHub repo, but the basic set up is as follows: Take the todos example (slightly altered) to deploy a set of Lambda functions behind AWS API Gateway Use the Node.js simplified HTTP request client to interact with API Gateway endpoints Let Mocha automate the testing of those endpoints What I appreciate most about the Serverless Framework is how much it lowers the learning curve on the deployment process involving multiple AWS services. Take a look at the serverless.yml file I used and compare that with the CloudFormation templates that get generated in your .serverless directory to see what I mean. It handles the mechanics for you nicely, allowing you to focus on adding your value. In my case, that meant writing four Mocha tests that utilize the request client to initiate transactions with the API endpoints and check both the HTTP return codes and payloads for different combinations. There's an argument to be made for also utilizing Mocha to test the Lambda services directly in absence of API Gateway or even the business logic inside the Lambda functions independently. But for the sake of simplicity I stuck with the API endpoints. When you deploy the service, set an environment variable containing the endpoint root, and then execute those tests, you get something like this:  This enables a team of developers all working on the same Serverless Framework project to deploy the service independently and run tests locally. Should those individuals then merge their code and perform pull requests on the master branch, those changes and tests could be integrated into a CICD workflow when bound for staging or production. Speaking of . . . Next Time: CICD with AWS CodePipeline Now that we have automated testing for our simple todos example, the next step would be to automate the whole toolchain in a CICD workflow. In my case, I chose to do what with the newly announced AWS CodePipeline, which will look at the master branch of a repo on GitHub, download it when there has been a change and then execute a set of steps that causes the service to be deployed and tested. We'll cover this in Part coming next week.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603df4",
      "title": "CICD for Serverless Part  - AWS CodePipeline Integration",
      "content": "My last post showed you how to use Mocha to automate endpoint testing for a service with multiple methods created and deployed using the Serverless Framework. It's possible for Test-Driven Development (TDD) to be practiced in the serverless world from a command line on a developer's local machine. But what if you have a team of developers that are constantly merging branches back into master and you want to set up automated testing and deployment using a Continuous Integration/Continuous Deployment (CICD) toolchain? Keep reading and you'll find out! Here we're still using the same Todo list example the folks at the Serverless Framework created as our codebase. But with some variations so that it more cleanly supports automated testing and the CICD toolchain used - AWS CodePipeline. At a high level the whole thing looks like this: !Serverless CICD Diagram Code Differences From The Original Todo In Part , I neglected to get into the details of what I had to change for the original Todo codebase to get it to function more cleanly for automated testing. Let's explore that here. First, two of the five methods in our service perform writes. Specifically create.js and update.js. The issue with automating the testing, especially for the create, is that the original version wasn't returning the UUID for the newly created Todo. That meant in order to verify that the write occurred correctly, testing code would have to do a list and scan for matching Todo content. The first change, then, is to return the entire JSON of the newly created Todo. For clarity, I kept the old code commented out, so the new lines - look like this:  For consistency's sake, the same was done for the update. Next, the original code hard-coded the DynamoDB table name in every method handler and again in `serverless.yml` for the creation of that table. It's possible to deploy multiple versions of your service on different branches with the same AWS account where one is your working copy if you use the local execution method on one while the other is based on a master branch that is executing in AWS CodePipeline. You just need to be a little more creative with the table naming mechanic. In the method handlers, in the constant set up to pass parameters to DynamoDB, you'll see a change similar to this one found in the create handler:  So now, the database table name gets pulled from the `TABLE_NAME` environment variable, which is getting set in the `serverless.yml` file based on the stage defined for the deployment:  I'm really liking the relatively new syntax for multiple `serverless.yml` variable references for a single evaluation, BTW. Creating the CodePipeline and Explaining the AWS CodeBuild buildspec.yml file I chose to use AWS CodePipeline since it was newly announced at AWS re:Invent in December. The CodePipeline Execution readme in my repo describes how you can set that up step-by-step. Future versions will automate this set up, but CodePipeline is new enough and the oAuth integration with GitHub wasn't straight forward to script. So for now I've got a lot of screenshots for a manual process instead. At the center of the automation is AWS CodeBuild and its `buildspec.yml` file. In our example, that file looks like this:  Here we've defined three of the standard phases that CodeBuild supports: install, build, and post_build. From steps performed in the Local Execution from last time, the commands for each phase should look familiar. The various dependencies are set up during install. During build, the Serverless Framework command line is used to deploy our service with a stage called \"cicd\" that shouldn't name clash with the default \"dev\" most likely used during Local Execution. The results are piped to deploy.out so that the endpoint name can be picked up by the post_build testing script that then runs the same Mocha tests as before. Results and Gotchas After you complete all the steps, you should be greeted with something similar to: !CodePipeline Goodness If you aren't, CodeBuild provides excellent detailed logging via CloudWatch - although it takes a couple of clicks to get there. The most likely causes of failure have to do with CloudFormation failing for one reason or another. I found that when that happens, an unfortunate side effect is that you have to manually delete the CloudFormation stack and possibly the DynamoDB table. Once over that hump, though, you can simply check changes into the branch you associated with your CodePipeline and all the automation kicks in to test and deploy your service!",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603df6",
      "title": "Azure Functions Support & Possibility",
      "content": "To the engineers and business leaders ducking out of political protests, excusing themselves from heated dinner table debates, rolling up their sleeves and getting back to work: Ready to actually take on the insurmountable challenges all around you? First you'll need courage. After that, you'll need really good tools. Fortunately, tools couldn't be better right now. The cloud providers are competing for your business and doubling down on investment in their platforms. The number of high-value services in database, storage, artificial intelligence technology and more is growing rapidly. Innovation in IaaS is now a daily occurrence. Meanwhile, getting started is easier than ever. If you want to adopt a cloud provider and utilize these new services immediately, put some code there, in the form of a Serverless Function. No doubt the cloud providers have recognized this, given there are now serverless compute offerings from AWS, Google, Azure, IBM and more. Serverless functions are a gateway drug into their platforms. However, a side effect has our attention... Stateless, zero-administration, pay-per-execution functions can exist in a single region, multiple regions as well as multiple providers, with minimal administration and no cost for idle. Developers can stash serverless functions across providers, enabling them to use more cloud services to solve more problems. This resulting possibility is what excites us at Serverless, Inc. It's a bit early to tell what the serverless multi-cloud architecture will look like, how it will work and whether it can solve the timeless concern of vendor lock-in (which is a complex problem). However, it's a concept we are heavily focused on and we'll be introducing more products this year to reduce lock-in and capitalize on all of the providers, together. That said, we're pleased to announce support for Azure Functions within the Serverless Framework. Our goal is to offer a uniform experience across serverless compute providers. So you can develop and deploy functions in a single fashion, regardless of their host. !https://s-us-west-.amazonaws.com/assets.site.serverless.com/blog/azure-functions.png The Azure Functions integration exists as a Serverless Framework plugin. Check out the README or the documentation to learn how to install and use it. Azure Functions offers a lot of great functionality, like binding directly to SaaS events (e.g., Github!). We hope you take advantage of all they have to offer. Sit tight. A lot more to come...",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603df8",
      "title": "Test-Driven Serverless Application Development",
      "content": "Quick Start to Test-Driven Development with Serverless Framework If you dont possess the magic that keeps your code working, the following instructions will help you start test-driven Serverless application development. What I like most about the Serverless Framework as a development tool is that it gathers together all of the cloud resources for a structured project. With the whole stack in the same project, it's convenient and easy to start writing tests. Usually for a new project I use the SC Serverless boilerplate. It's a good setup to begin with. But in this tutorial, I start with an existing example aws-node-simple-http-endpoint project, to show how easy it is to add Serverless testing plugin even to an existing project. Lets start by installing the service, changing the directory to the one that `sls install` command creates, and installing dependencies that service requires to run.  Then install the Mocha plugin with `npm install --save-dev serverless-mocha-plugin`. If you're more familiar with writing tests with Jest, you can use Jest plugin. The next step is to add the installed plugin to serverless.yml. This project doesn't have any plugins yet installed so the `plugins` key is also added.  Now, run `sls` and the output should include following new commands that the Mocha plugin adds to the Serverless Framework.  To create a test to an existing function, use the `create test` command with parameter `-f` or `--function`. In this example project, there is already a function called `currentTime`. To create a test stub for that, run `sls create test -f currentTime` and it should print out `Serverless: serverless-mocha-plugin: created test/currentTime.js` as a result. Next, invoke the test by running `sls invoke test` and the output should be something like this:  Now for the fun part  implementing the actual tests. The tests that the Serverless Mocha plugin creates are in the `test` directory, which is the default directory for Mocha tests. If you prefer to use a different directory you can create tests using `-p` or `--path` parameter when creating and invoking tests. Open the `tests/currentTime.js` to your code editor. There is a generated test that only tests that the response is not empty.  Replace that with the following one, which tests that statusCode is and that the response body contains a message that has the time. In real life, you may want to fake the date with Sinon.JS or similar so that you can test the response with predefined dates.  After invoking the tests again with `sls invoke test` command, the output should be:  The first test is now ready. Let's create some more! Now we have a function that returns time, so we need to know the date also. With `sls create function` command you are able to create a function and test case for it.  Then open `test/currentDate.js` to your editor and replace the default `implement tests here` test block with snippet:  When invoking tests with `sls invoke test` command, you should get the following error:  Next fix the function to match the test, open `date/handler.js` and replace the code with one that returns the date. ```js 'use strict'; module.exports.endpoint = (event, context, callback) => { const response = { statusCode: , body: JSON.stringify({ message: `Hello, the current date is ${new Date().toDateString()}.`, }), }; callback(null, response); }; ``` Once again, run the `sls invoke test` command and the result should be successful. In addition to test keeping your code functional, the benefit of using test instead of e.g. `sls invoke local` is that you can test the same handler function easily with different payloads. If you'd like a working example, here's the repository I used while making this tutorial.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603dfa",
      "title": "Advanced Plugin Development - Extending The Serverless Core Lifecycle",
      "content": "Introduction In Serverless .x you can easily write plugins to add additional commands that in turn define a lifecycle that can be hooked by other plugins. This works great as long as you initiate your plugin functionality by invoking it through the defined commands. But imagine, you've written a plugin (myplugin) that adds some functionality to the standard behavior of Serverless, i.e. the plugin does not offer any explicit commands, but only hooks into Serverless' core lifecycle events.  Your plugin is automatically invoked after the Serverless core deploy plugin left its deploy:deploy lifecycle. With this implementation you've implicitly created a dead end in the lifecycle dependencies, but why? Let me explain it: Everything works as expected as soon as `serverless deploy` is executed and its deploy:deploy lifecycle event is run. And because you hooked after that, your plugin is executed right after the deploy has been finished. So far, so good. But what if you want to expose hooks by yourself in that case? What if you want a plugin to be able to hook just before or just after your storeData() step, so that it can either add additional transformations or grab any work on the data after you've stored it? You just didn't offer any lifecycle events that can be hooked. That's why this is a dead end. To offer the best functionality for other plugin writers, the plugin should extend the Serverless core lifecycle and offer lifecycle events that can be hooked by others. That's what most people expect and what makes the plugin system valuable and usable. From a lifecycle event point of view, we'd expect that the following lifecycle events are available after adding your plugin to any Serverless service project:  Now any other plugin could hook before or after any of your plugin actions. That's exactly how it should work. Extending the Serverless Core Lifecycle The Serverless core implementation composes the lifecycle by inspecting the commands offered by any plugins. There is no direct way for a plugin to inject its own lifecycle events when triggered by a hook. Only a command invocation will start the plugin's defined lifecycle. The solution to this problem is the plugin manager that controls the lifecycle, runs commands and triggers the hooks. The plugin manager is available as property on the serverless object in every plugin, so we can use and access it from there. As lifecycles can only be started by invoking a command, and the plugin manager is able to run commands, we already have a feasible solution here. In short, we have to define an internal command, that defines our plugin lifecycle and that can be run by the plugin manager from within our hook implementation. Here's the step-by-step walkthrough. Adding Our Internal Command(s) We add the internal command to our plugin, although the plugin is only triggered by hooks.  The command only defines the lifecycle, not anything else. We don't want to let it be called from the user. Invoke the Command Within Our Hook / Enter Our Lifecycle In our hook we now use the plugin manager to enter our very own plugin lifecycle. Therefore we modify our hook's definition from above.  Multiple Descriptive Sub Lifecycles You may have noticed the it would be possible to define additional lifecycles besides the 'data' lifecycle. For larger plugins that will make the lifecycle model much more structured and transparent and implicitly adds more intuition to your exposed lifecycle events. Example:  A further advanced use case could be a combination of user command lifecycles and internal lifecycles. Your plugin could offer additional commands that are accessible by the user and can also be invoked within your internal hook chain. There are no limits to what you can do there. Prevent Invocation from the Outside (User) Serverless will show all defined commands in its help output and every shown command normally can also be executed. For our internal hook lifecycle both of these behaviors are issues, so we have to add a workaround - Serverless does not allow commands to not be exposed nor does it allow you to hide commands from the help output. We add a small description to our data command that will be shown on the help screen and a validate lifecycle event that we'll use to check if the invocation has been done from our hook implementation:  Additionally, we prevent the user from starting it via 'serverless myplugin data' and add a local invocation check to our hook implementation and the new validate event as follows:  Maybe the plugin manager can support some flag for internal commands in the future that prevents calling from the outside and display on the help screen. Then the trigger check can be removed completely. Conclusion Hopefully this approach will make your plugins more flexible and allow other plugin contributors to integrate easily with them.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603dfc",
      "title": "How To Schedule Posts for Static Site Generators (Jekyll, Hugo, Phenomic etc.)",
      "content": " Like many static sites we use Markdown + GitHub for all of our blog content. Having content under version control comes with some great benefits: - It's open - Anyone can submit/update content and fix typos via pull requests - Version control - Roll back & see the history of any given post - No CMS lock in - We can easily port to any static site generator - It's just simple - No user accounts to manage, no CMS software to upgrade, no plugins to install. All that said, there are some missing features when it comes to running your site or blog via a static site generator. Lacking the ability to schedule posts to publish at a specific time is a pain. Publishing content to our static site and blog has been a manual process. We had to physically be at our keyboards and click the \"merge\" button in GitHub. How antiquated... So I thought to myself: Introducing the Post Scheduler for Static Websites The post scheduler is a Serverless project that gives static site owners the ability to schedule posts (or other site content). It works with any static site setup (Jekyll, Hugo, Phenomic, Gatsby etc.) How much does it cost?: It's free and open source project. You can easily run under this under the generous free tier of AWS. Just clone it down, add in your repo details and `sls deploy` it into your AWS account. Before: Late night manual merges After: Sipping margaritas on the beach while posts are being published automatically. Show Me The (Demo) Watch the rest of the playlist on youtube How It Works A GitHub webhook fires when a pull request (aka new posts or site content) is updated. If the pull request comment has a comment matching `schedule(MM/DD/YYYY H:MM pm)` & the person is a collaborator on the project, the post/content gets scheduled for you. A serverless cron job runs every hour to check if a post is ready to be published. When the post is ready to be published, the cron function automatically merges the branch into `master` and your site, if you have CI/CD built in, will redeploy itself. To cancel scheduled posts, delete the scheduled comment and it will unschedule the branch. Github Webhook Architecture !cloudcraft - post scheduler webhook Cron Job Architecture !cloudcraft - post scheduler cron setup How to Install Clone down the repository and run `npm install` to install the dependencies Duplicate `config.prod.example.json` into a new file called `config.prod.json` and insert your Github username, API token, and webhook secret  - `serviceName` - name of the service that will appear in your AWS account - `region` - AWS region to deploy the functions and database in - `TIMEZONE` - Timezone the cron runs on. See `timezone.json` for available options - `CRON` - How often you want to check for scheduled posts? See the AWS cron docs or serverless `schedule` docs for more information. Default: every hour on the hour - `GITHUB_REPO` - The `owner/repoName` of your repository - `GITHUB_WEBHOOK_SECRET` - Any string you want. This gets plugged into your webhook settings - `GITHUB_API_TOKEN` - Personal access token. See below for additional info - `GITHUB_USERNAME` - Your github username. Used for requests to github Deploy the service with `serverless deploy`. If you need to setup Serverless, please see these install instructions. Take the POST endpoint returned from deploy and plug it into your repositories settings in github !image Add your GitHub webhook listener URL into the `Payload URL` and choose type `application/json` Plugin your `GITHUB_WEBHOOK_SECRET` defined in your config file Select which GitHub events will trigger your webhook Select Issue comments, these will be where you insert `schedule(MM/DD/YYYY H:MM pm)` comments in a given PR Submit a PR and give it a go! Contributions Welcome Have an idea on how we can improve the static site post scheduler? Leave us a comment below, submit a PR, or tweet @DavidWells Was this post scheduled? How did you guess it? This post was scheduled!",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603dfe",
      "title": "Build A Serverless Python Application with AWS + FaunaDB - The First Serverless Database",
      "content": "Join members of the Serverless team, and the author of this post, at the next Serverless Meetup at Fauna in San Francisco, March rd. Introduction to FaunaDB FaunaDB is the first truly serverless database. In this post, I'll demonstrate how to use the Serverless Framework to connect an AWS Lambda Python application with FaunaDB Serverless Cloud. (If you're interested in seeing the same for JavaScript, check out the companion post on FaunaDB's blog.) When I say serverless, I mean the Function-as-a-Service pattern. A serverless system scales dynamically by request and doesn't require any provisioning or capacity planning. For example, you can subscribe to FaunaDB in moments, and smoothly scale from prototype to runaway success. FaunaDB Serverless Cloud is a globally distributed database that doesn't require any provisioning. Capacity is metered and available on demand, so you only pay for what you use. Plus, you can always port your app to FaunaDB On-Premises in your own datacenter or private cloud, so there's no cloud infrastructure lock-in. Using the Serverless Framework Serverless offers a clean system for configuring, writing, and deploying serverless application code to different cloud infrastructure providers. Porting one of their storage examples from DynamoDB to FaunaDB was incredibly easy to accomplish. Looking through this code will show us how simple it is to set up a Serverless application with storage that's available in every region. The Python CRUD service is a simple REST API that allows the creation, updating and deletion of todo items as well as the list of all items. After we look through the code, Ill describe how youd go about adding a shared-list multi-user data model, where users can invite other members to read and update todo lists. The README file contains installation and setup instructions, and you can go here to gain instant access to FaunaDB. Once this configuration is running, you can play with deeper features of FaunaDB, such as querying a social graph. Take a look at this social graph tutorial. Defining Functions The first file to start reading any application that uses the Serverless Framework is serverless.yml. It defines the service functions and links to event handlers. readAll example Here we can see one of the function definitions:  This configuration means the `list` function in `todos/list.py` will be called when an HTTP GET is received at the `todos` path. If you look through the configuration youll see all the functions are linked to files in the todos directory, so we'll look at the implementation of `list.py`.  Most of `list.py` is concerned with managing HTTP, or module imports. Let's zoom in on the query.  In this case we run a query for all todos, using a FaunaDB secret passed via configuration in `serverless.yml`. FaunaDB uses HTTP for queries so you dont need to worry about sharing connections between modules or invocations. Your Turn Follow the README instructions to launch and run the service, then create a few todo items. Now youre ready to explore your data and experiment with queries in the FaunaDB dashboard. Open the dashboard via this sign up form. It will look something like this: !FaunaDB Dashboard Screenshot If you look closely at the screenshot you get a hint at FaunaDBs temporal capabilities which can power everything from social activity feeds, to auditing, to mobile sync. Taking It A Step Further In a production-worthy version of this application, the request would contain a list ID, and our query would validate that the list is visible to the user, before returning just the matching items. This security model is similar to collaboration apps you may be familiar with, and its supported natively by FaunaDB. Watch the Fauna blog for an updated Serverless application with a more mature multi-user data model using FaunaDBs security features.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603e00",
      "title": "Using Serverless Authentication Boilerplate with FaunaDB",
      "content": "Three cheers to our friends at Fauna as they announce FaunaDB Serverless Cloud, a globally consistent distributed database. This is a guest post from Chris Anderson, Director of Developer Experience at Fauna. A common serverless application architecture is to run an authentication service, which knows how to connect with OAuth identity providers like Facebook and Github, and exposes an authorizer Lambda that can control access to your functions. This makes authentication code reusable and cleanly separates it from your other functions. FaunaDB offers instance-level security, so you can model your applications data sharing patterns in the database. In this example we use the popular `serverless-authentication-boilerplate` to connect a FaunaDB app to Facebook login. Thanks to Eetu Tuomala for the help with API Gateway details! In the Serverless model, the authorizer supplies functions contained in the application with a FaunaDB connection secret that corresponds to the currently logged-in user. This way, there's no possibility of bugs at the application level impacting data integrity and security. There's no limit to the data security patterns you can model in FaunaDB. See our tutorials for social graph examples, or follow this space for a multi-user TodoMVC example. For now, the content service just looks up the current user in the database. These instructions for launching the Serverless Authentication Boilerplate with FaunaDB are based on the `serverless-authentication-boilerplate` README. This is not simplified example code, rather more like the first steps you'd take when creating a new real world application. The final result is not a cool demo, it's a useful auth service you can rely on. If you're looking for more basic usage of FaunaDB and Serverless, see our blog post about the FaunaDB Serverless CRUD example. There is also a Python version available. Installing Serverless Authentication The boilerplate ships with code for a few different identity backends. These steps walk you through installing the service and running it with FaunaDB. The FaunaDB example also integrates with the `test-token` example content service. So once you have it running you can look at that code to see how your application would use the database. If you haven't yet, `npm install -g serverless` and make sure your AWS environment variables are set. Run `serverless install --url https://github.com/laardee/serverless-authentication-boilerplate`, or clone or download the repository. Rename `authentication/example.env.yml` to `authentication/env.yml` and set environmental variables. Delete the `CacheTable` entry to avoid provisioning DynamoDB tables you won't be using. Sign up instantly and create a database in the FaunaDB dashboard. Configure `FAUNADB_SECRET` in `authentication/env.yml` with a server secret for your database. Uncomment `return faunaUser.saveUser(profile);` from `authentication/lib/storage/usersStorage.js`. Change the last line of `authentication/lib/storage/cacheStorage.js` to `exports = module.exports = faunaCache;` Change directory to `authentication` and run `npm install`. Run `STAGE=dev npm run setup:fauna` to create your FaunaDB schema. (optional) Change directory to `test-token` and run `serverless deploy` to deploy test-token service. Look here for the code to the test-token service and here for the code that uses FaunaDB as an authentication cache and user store. There's no need to configure the `test-token` service with database access, as the `authorize` function provides a database access secret that matches the current user. Each function invocation runs only with the privileges of the current user. In a future post we'll show how to model ownership of data instances, read and update control, and delegation of capabilities to other users. With FaunaDB you get multi-region cross-cloud replication of your data, with the option to run on-premise, avoiding vendor lock-in. You also get a functional relational query language and the ability to define complex indexes. There are temporal support for sync, audit and snapshot queries. And you never have to pre-provision, so you only pay for the database you use. Launch FaunaDB and you'll be storing data in moments.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603e02",
      "title": "One Chatbot, Two Millions Fans - How AbstractAI Reduced Back-End Costs By %",
      "content": "Serverless architecture has opened a whole new world of possibilities for developers. Were seeing so many cool projects being built with the Serverless Framework. The open-source Framework allows you to easily build web, mobile and IoT applications with serverless architectures using AWS Lambda, Azure Functions, OpenWhisk, Google Cloud Functions and more. Startups to Fortune companies are using the Framework to build sophisticated event-driven systems. Were collaborating with some of these organizations to highlight their exceptional work through the Serverless Partners Program. Serverless Partners are consultants and agencies who are verified experts in serverless application development with the Serverless Framework. Well share stories of how they use the Framework to work fast, manage costs and build for scale, in order to produce innovative products and solutions for their clients. Meet AbstractAI Los Angeles-based AbstractAI (Abstract) develops software and services to enable users to get the results they need faster. Abstract's areas of specialization include developing platform-agnostic apps, machine learning and AI, and bots. When working with bots - also referred to as Conversational Interfaces or CI - Abstract develops with less emphasis on conversation and more emphasis on CI's inherent lightweight platform. Million Fans, Bot Abstract partnered with LA startup Brainitch, - a company that offers personalized, -to- marketing to artists on Facebook Messenger and similar platforms - to create a bot to help promote a birthday bash for electronic music artist Laidback Luke (LBL). LBL wanted to build a bot that would ask users trivia questions, and if they got enough questions right, they'd receive an exclusive invite to his birthday party in NYC or Amsterdam. Reducing Backend Costs Abstract reduced the recurring costs associated with Laidback Luke's bank-end services nearly % by using the Serverless Framework. When the number of users spikes, the architecture provides the needed network resources, but the user isn't charged otherwise. Leveraging Amazon Lambda and the Serverless Framework allows you to only pay for the resources that you actually use. Another bonus - developing proficiency with Serverless Framework took only a week. The resources available within the Serverless Framework community showed the Abstract team how to create RESTful API's in less than ten minutes. Check out the AbstractAI Case Study for more on the specifics of their process and results. Are you a consultant or agency using the Serverless Framework to streamline your workflow and build innovative solutions for your clients? Contact partners@serverless.com for more info on becoming an official Serverless Partner.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603e04",
      "title": "New Event Sources and Other Updates in v.. of the OpenWhisk Plugin for the Serverless Framework",
      "content": "Multi-provider support was a goal we laid out following the Serverless Framework v release. Since then we've been working towards simplifying the process of moving applications between cloud providers to enable the development of multi-cloud Serverless apps. All while keeping the Serverless workflow and developer experience consistent across providers so you don't need to learn custom commands or syntax for each platform. OpenWhisk integration launched earlier this year. The official OpenWhisk provider plugin allows developers to build, deploy and manage apps running on the OpenWhisk platform using the Serverless Framework. Special shout out to James Thomas (@thomasj) at IBM for his awesome contribution spearheading this effort! OpenWhisk recently released v.. of the plugin, adding support for Cloudant DB and IBM Message Hub events, exporting Web Actions and local OpenWhisk deployments. Use the following command to upgrade the provider plugin to the latest version.  Due to an outstanding issue with provider plugins, the OpenWhisk provider must be installed as a global module. New Features Supported in v.. - IBM Message Hub Events - Functions can be bound to events from IBM Message Hub (Apache Kafka-as-a-Service) using a new event type (message_hub). Functions will be fired with the batch of messages received since the last invocation. Service credentials can be automatically read from an OpenWhisk package. Learn more in the docs. - IBM Cloudant DB Events - Functions can be bound to events from IBM Cloudant (CouchDB-as-a-Service) using a new event type (cloudant). Functions are invoked for each database modification surfaced through the CouchDB _changes feed. Service credentials can be automatically read from an OpenWhisk package. Learn more in the docs. - Export Web Actions - Functions can be turned into [web actions] which return HTTP content without use of an API Gateway. This feature is enabled by setting an annotation (`web-export`) in the configuration file. Learn more in the docs. - Support Local OpenWhisk Deployments - This plugin now supports targeting OpenWhisk instances without valid SSL certificates. Developers running personal instances of the platform often do not have a custom SSL certificate set up for their domain. See the milestone release on GitHub or the OpenWhisk blog for full details on the new features and bug fixes. Items planned for the next release are shown in the . milestone. Getting Started with OpenWhisk & Serverless Resources to help you get started developing apps with OpenWhisk and the Serverless Framework. - Getting Started with the Serverless Framework and OpenWhisk Video Tutorial - Serverless Apache OpenWhisk Provider Docs - OpenWhisk Examples in the Serverless Examples Repository Let us know if you have feature requests (or find bugs) by opening issues in the GitHub repository.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603e06",
      "title": "How To Use AWS Lambda & API Gateway to Send Shipment Tracking Updates via SMS with Shippo & Twilio",
      "content": "In this project, were going to receive a notification from a webhook about an a physical shipment in transit and trigger an SMS with the updated tracking information. We'll build an AWS Lambda function that will trigger whenever Shippo pushes an update about a shipment to our AWS API Gateway Endpoint. Inside of the Lambda function, were going to call Twilio to send an SMS update with our tracking info provided by Shippos webhook. Now, I know what youre thinking, this sounds pretty complicated and requires a lot of manual set up and repeated uploading of JavaScript files to AWS, but youd be wrong. Were going to use Serverless to do a lot of the heavy lifting on this for us, because Im all about writing less code to do more. You can find the full project repo at https://github.com/shipping-api/serverless-twilio-shippo Things you'll want before getting started with this tutorial: Twilio Account Shippo Account You can get Serverless by installing it globally on your machine using: `npm install -g serverless` Serverless provides a way to easily create a new service by just using their CLI as follows (you can omit the path if you don't want it to create a directory for you): `serverless create --template aws-nodejs --path twilio-shippo` Before you dig into creating your Lambda function, you'll want to setup a User in your AWS account for Serverless to have access for creating everything. They have a useful guide here that can walk you through getting your credentials setup. It's as simple as adding a user `serverless-admin` with `AdministratorAccess` and using the credentials with the following command: `serverless config credentials --provider aws --key ACCESS_KEY_ID --secret SECRET_ACCESS_KEY` Once you have the credentials setup you can start adding function dependencies at the top of the `handler.js` file that Serverless created:  From here, we want to create the endpoint that we'll be putting into Shippo's webhook interface for capturing all of our tracking updates. Every time Shippo detects a new update to the status of a tracking number that we have POSTed to them, Shippo will send out updates to our API endpoint that we give to them. By default, Serverless will create an exported function named `hello`, but we're going replace that with our own called `smsUpdates` (its also probably good to add a log so we can see this show up in CloudWatch):  We are creating a POST endpoint, since Shippo will be POSTing the tracking updates to us. We'll then parse the data to relay over to Twilio to send out our SMS messages. First, let's parse the body of the message that Shippo has sent to us. We'll set up a few variable to prevent repeating ourselves, and we'll add some logic in there to handle if there is no location provided with our tracking update.  Now that we have our logic built for handling the body of the response and safely handle when we don't get a location with our tracking status, we can dig into sending a formatted SMS using Twilio. The basic format for sending Twilio messages requires that we have a destination number (for sending our SMS to), our Twilio number that we're sending from, and a message to send (duh!). Here is what it looks like once we add sending our message:  You'll also notice that we create a `response` object for sending a response back, since Shippo expects a response when there is a successful receipt of a webhook post. We're also using `console.log()` to log all messages to CloudWatch, which is really helpful in debugging or seeing the history of webhook events. Now is a good time for us to tackle fixing up the serverless.yml file that will tell serverless how we want our lambda function configured and what AWS services it would use.  The above should get our function linked up to trigger when an AWS API Gateway endpoint `smsupdates` receives a POST. That should send off our tracking update to Twilio. If you want more details on configuring your Serverless service, checkout their docs here. Once you have your `serverless.yml` file setup, you can just use `serverless deploy` And your function should be uploaded to AWS with details about it logged out to the console. Next, navigate to https://app.goshippo.com/api and scroll down to Webhooks to click + Add Webhook. Since we had our route go to `smsupdates` we'll want to append that to our url so that the updates post to the right place. Look for these lines:  After pasting this into the URL field in Shippo, make sure that the dropdown under Event Type is set to tracking and click the green checkbox to save it. Now we can test the function by clicking on test on the far right. If everything goes well, you should receive an SMS with tracking information at the number you had in the `to` field of your Twilio sendMessage object. Now you can get SMS updates for all numbers that you post to Shippo automatically without having to provision any servers, and you only pay when you are receiving updates using Lambda and API Gateway with AWS. You could even take it a step further and include phone numbers for SMS updates in the `metadata` field when POSTing to Shippo and parse that out to dynamically send SMS updates to customers. You can find more information about Shippo and how to use their shipping API to improve your shipping experience at goshippo.com.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603e08",
      "title": "How the US Department of Defense is Streamlining Open Source Contributions with Serverless Code",
      "content": "Background Hi everyone, my name is Tom Bereknyei, and Im an engineer with the Defense Digital Service (DDS) at the Pentagon. We're an agency team of the U.S. Digital Service at the White House. Our mission is to bring private sector best practices, talent, and software into the Department of Defense (DoD). Its a tall order, but our small team of engineers, product managers, designers and bureaucracy hackers have made significant strides in tech modernization across a variety of projects and initiatives in the year since we were created. One such initiative is Code.mil. The Challenge The first phase of Code.mil was to work with the developer community in crafting a licensing strategy that was accessible and made the most sense to users. Our team at DDS (composed of myself, fellow engineer Brandon Bouier and general counsel Sharon Woods) took the feedback from over pull requests and hundreds of comments and decided upon using the Developer Certificate of Origin (DCO) as the mechanism for people to contribute while utilizing commonly accepted licenses selected for each project. My task: I wanted to simplify the DCO process for contributors while also maintaining the integrity of the contributions. Ideally, there would be an automated pass/fail check to make sure that all pull requests and commits had a DCO sign off tagged to it. GitHub doesnt currently provide a method of doing this (hint hint), so I began exploring other ways to incorporate this function into the user experience. The Solution I was interested in using Lambda - an Amazon Web Services (AWS) feature that runs on demand computing rather than running it on a server. However, I felt like I was reinventing the wheel by implementing this with terraform and deploying the Lambda. Serverlesss website led me to their Examples repository in GitHub, where I did a quick forking of their error code/error handling code and used it as the basis of our automated DCO bot. Combining this with some verification logic and the ability to update status and comments for a Pull Request made it pretty simple to put together. Debugging with `serverless logs -t` was much easier than messing with the logs in AWS, which shows that this is a tool developed by and for the convenience of developers. This was my first AWS Lambda project, first GitHub bot project, first Serverless project; overall it was an easy and pleasant experience. The bot was built in a few hours over a weekend while simultaneously learning a few interesting technologies and APIs. The Results This project was part of DDSs effort to release its first open source project to positive reception, and we're excited to release more projects from the wider DoD community. The DCO bot is an uncomplicated way to ensure private contributors are correctly attributed as theyre working to improve the software that supports services for citizens worldwide. Check out Code.mil to learn more about DoDs open source initiative and contribute to DoD projects.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603e0a",
      "title": "Building a REST API in Node.js with AWS Lambda, API Gateway, DynamoDB, and Serverless Framework",
      "content": "Serverless means different things depending on the context. It could mean using third party managed services like Firebase, or it could mean an event-driven architecture style. It could mean next generation compute service offered by cloud providers, or it could mean a framework to build Serverless applications. In this tutorial, you'll learn how to build a REST API following the Serverless approach using AWS Lambda, API Gateway, DynamoDB, and the Serverless Framework. AWS Lambda is the third compute service from Amazon. It's very different from the existing two compute services EC (Elastic Compute Cloud) and ECS (Elastic Container Service). AWS Lambda is an event-driven, serverless computing platform that executes your code in response to events. It manages the underlying infrastructure scaling it up or down to meet the event rate. You're only charged for the time your code is executed. AWS Lambda currently supports Java, Python, and Node.js language runtimes. Application: Lambda Coding Round Evaluator In my current organization, one of the interview rounds is a coding round. The candidate is emailed an assignment that he/she has to submit in a week's time. The assignment is then evaluated by an existing employee who makes the decision on whether the candidate passed or failed the round. I wanted to automate this process so that we can filter out unsuitable candidates without any human intervention. A task that can be automated should be automated. This is how the flow will work: Recruitment team submits candidate details to the system. System sends an email with assignment zip to the candidate based on candidate skills and experience. The zip contains the problem as well as a Gradle or Maven project. Candidate writes the code and submits the assignment using Maven or Gradle task like `gradle submitAssignment`. The task zips the source code of the candidate and submits it to the system. On receiving assignment, systems builds the project and run all test cases. . If the build fails, then candidate status is updated to failed in the system and recruitment team is notified. . If the build succeeds, then we find the test code coverage and if it's less than a certain threshold we mark the candidate status to failed and recruitment team is notified. If build succeeds and code coverage is above a certain threshold, then we run static analysis on the code to calculate the code quality score. If code quality score is below a specified threshold then candidate is marked failed and notification is sent to the recruitment team. Otherwise, the candidate passes the round and a human interviewer will now evaluate candidate assignment. In this tutorial, we will only build a REST API to store candidate details. Please refer to the guide to learn how to build the full application from scratch. Also, source code for the application is available on Github. Prerequisite To go through this tutorial you will need following: AWS account Node.js AWS CLI and configure it What is the Serverless Framework? The Serverless Framework makes it easy to build applications using AWS Lambda. It is multi-provider framework, which means you can use it to build Serverless applications using other providers as well. For AWS, Serverless relies on CloudFormation to do the provisioning. It also scaffolds the project structure and takes care of deploying functions. Getting Started with the Serverless Framework To install Serverless on your machine, run the below mentioned npm command.  This will install Serverless command-line on your machine. You can use `sls` alias instead of typing `serverless` as well. Now, we will build the application in a step by step manner. Step : Create a Node.js Serverless Project Navigate to a convenient location on your filesystem and create a directory `coding-round-evaluator`.  Once inside the `coding-round-evaluator` directory, we'll scaffold our first microservice for working with candidates. This will be responsible for saving candidate details, listing candidates, and fetching a single candidate details.  This will create a directory `candidate-service` with the following structure.  Let's look at each of these three files one by one. .npmignore: This file is used to tell npm which files should be kept outside of the package. handler.js: This declares your Lambda function. The created Lambda function returns a body with `Go Serverless v.! Your function executed successfully!` message. serverless.yml: This file declares configuration that Serverless Framework uses to create your service. serverless.yml file has three sections  provider, functions, and resources. . provider: This section declares configuration specific to a cloud provider. You can use it to specify name of the cloud provider, region, runtime etc. . functions: This section is used to specify all the functions that your service is composed off. A service can be composed of one or more functions. . resources: This section declares all the resources that your functions use. Resources are declared using AWS CloudFormation. Step : Create a REST Resource for Submitting Candidates Next, we'll update serverless.yml as shown below.  To deploy the function, execute `serverless deploy` command.   Now, POST operation of your service is available. You can use tools like cURL to make a POST request.   Step : Saving Data to DynamoDB Now that we are able to make HTTP POST request to our API let's update the code so that data can be saved to DynamoDB. We'll start by adding `iamRoleStatemements` to `serverless.yml`. This defines which actions are permissible.  Next, we'll create a resource that will create DynamoDB table as shown below.  Now, install a couple of node dependencies. These will be required by our code.  Update the `api/candidate.js` as shown below. ```javascript 'use strict'; const uuid = require('uuid'); const AWS = require('aws-sdk'); AWS.config.setPromisesDependency(require('bluebird')); const dynamoDb = new AWS.DynamoDB.DocumentClient(); module.exports.submit = (event, context, callback) => { const requestBody = JSON.parse(event.body); const fullname = requestBody.fullname; const email = requestBody.email; const experience = requestBody.experience; if (typeof fullname !== 'string' || typeof email !== 'string' || typeof experience !== 'number') { console.error('Validation Failed'); callback(new Error('Couldn\\'t submit candidate because of validation errors.')); return; } submitCandidateP(candidateInfo(fullname, email, experience)) .then(res => { callback(null, { statusCode: , body: JSON.stringify({ message: `Sucessfully submitted candidate with email ${email}`, candidateId: res.id }) }); }) .catch(err => { console.log(err); callback(null, { statusCode: , body: JSON.stringify({ message: `Unable to submit candidate with email ${email}` }) }) }); }; const submitCandidateP = candidate => { console.log('Submitting candidate'); const candidateInfo = { TableName: process.env.CANDIDATE_TABLE, Item: candidate, }; return dynamoDb.put(candidateInfo).promise() .then(res => candidate); }; const candidateInfo = (fullname, email, experience) => { const timestamp = new Date().getTime(); return { id: uuid.v(), fullname: fullname, email: email, experience: experience, submittedAt: timestamp, updatedAt: timestamp, }; }; shell $ serverless deploy -v bash $ curl -H \"Content-Type: application/json\" -X POST -d '{\"fullname\":\"Shekhar Gulati\",\"email\": \"shekhargulati@gmail.com\", \"experience\":}' https://ccffiraa.execute-api.us-east-.amazonaws.com/dev/candidates json { \"message\":\"Sucessfully submitted candidate with email shekhargulati@gmail.com\", \"candidateId\":\"fc-f-e-ed-bfff\" } yaml listCandidates: handler: api/candidate.list memorySize: description: List all candidates events: - http: path: candidates method: get ``` Create new function in the `api/candidate.js` as shown below.  Deploy the function again.  Once deployed you will be able to test the API using cURL. Step : Get Candidate Details by ID Define a new function in serverless.yml as shown below.  Define a new function in `api/candidate.js`  Now, you can test the API using cURL.  Working with Local DynamoDB Download the jar and run locally. Invoking Functions Locally and Remotely  Tailing the Logs  Conclusion In this part, you learned how to create a REST API with the Serverless Framework. To learn more read the guide.",
      "__v": 0
    },
    {
      "_id": "64e08917b72e199dda603e0c",
      "title": "Python EVE + MongoDB + Serverless + AWS = REST All Happiness",
      "content": " Hi, I'm Naresh Surisetty, an enthusiastic Python developer based in India. I'm especially interested in developing scalable apps in the cloud - a main reason I'm drawn to experimenting with serverless architecture. In this post I'll share a Serverless project using Python EVE and MongoDB. Get Started Progamming Installation We'll use pip to install EVE.  We'll create our MongoDB database using mLab. If you don't have an existing mLab account go ahead and create one as we'll be using mLab hosted database throughout the post. (There's an option for a free Sandbox account.) Make sure you create a database, as well. Creating a Basic API Create a project named eve-api-project. Navigate to eve-api-project folder and create a file called run.py. Copy the following code: run.py  Lets break down the run.py file. - Initially we've imported the EVE module  - MONGO_HOST, MONGO_PORT, MONGO_USERNAME, MONGO_PASSWORD and MONGO_DBNAME defines the variable for storing our MongoDB host, port, username, password and database name that we'll configure in our serverless.yml file as part of environment section. - api_settings dictionary defines all the configs and schemas for our API. You can use any name for defining your dictionary. For ease I've defined it as api_settings. - DOMAIN section under api_settings defines the endpoints and their respective schemas for API.  The above mentioned people schema defines EVE API to create an endpoint with name people and add the following structure for the API defined. We have two fields in the API named firstname and lastname. Different validations can be performed upon the schema such as minlength, maxlenth, type and many more that can be referred at EVE Allowed Validations. - Pass the api_settings as a parameter for settings in Eve. ` This registers all the configs and schemas defined under api_settings. Defining the Serverless File Create a file named serverless.yml under the same project directory. serverless.yml ```yamlex service: eve-api frameworkVersion: \">=.. ",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e0e",
      "title": "Using Serverless Technology to Bootstrap the Platform for a Billion Dollar Business Opportunity",
      "content": "Serverless architecture has opened a whole new world of possibilities for developers. Were seeing so many cool projects being built with the Serverless Framework. The open-source Framework allows you to easily build web, mobile and IoT applications with serverless architectures using AWS Lambda, Azure Functions, OpenWhisk, Google Cloud Functions and more. Startups to Fortune companies are using the Framework to build sophisticated event-driven systems. Were collaborating with some of these organizations to highlight their exceptional work through the Serverless Partners Program. Serverless Partners are consultants and agencies who are verified experts in serverless application development with the Serverless Framework. Well share stories of how they use the Framework to work fast, manage costs and build for scale, in order to produce innovative products and solutions for their clients. Meet SC Helsinki-based SC is a digital agency specializing in cloud solutions. The SC team works with enterprise clients to create cloud-native applications, APIs, and data solutions often leveraging Machine Learning. The team recently helped build the backend for the award winning Whim personal transportation app by MaaS Global. Using Serverless to Create an Innovative Transportation Solution MaaS Global (Mobility as a Service) is a startup revolutionizing personal transportation to be more efficient and eco-friendly. Their app, Whim, allows users to create an itinerary for a journey to a destination of their choice through a combination of public transport (like buses and trains), taxi service or even car rental. From the beginning, MaaS knew it didn't want to create its own maps, geodata or routing systems, instead aiming to tap into the existing systems in the field. They called on SC to help solve the technical challenge of making these systems work together reliably while serving potentially millions of concurrent users. Bootstrapping the Backend with AWS and Serverless Framework The system was founded on AWS, consisting of a REST API using Serverless Framework and Postgres data storage. With the exception of managed database services, the whole solution was based on a serverless architecture pattern with Lambda and API Gateway, allowing it to scale infinitely out-of-the-box. The first end-to-end version, consisting of the full flow of finding a route to getting a taxi ordered, was demoed to investors just four months after the start of the project. Since then the platform has launched in public beta in the Helsinki area, and it's currently expanding internationally to the United Kingdom. The Whim app, powered by the SC bootstrapped backend has received several awards, including the Helsinki Smart City Action Award and Nordic Smart Cities Award. Whim has also been featured in the international press, such as the Economist, the Guardian and Wired. Check out the SC Case Study for more on the specifics of their process and results. Are you a consultant or agency using the Serverless Framework to streamline your workflow and build innovative solutions for your clients? Contact partners@serverless.com for more info on becoming an official Serverless Partner.",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e10",
      "title": "Must Read Books for Becoming a Better Software Developer",
      "content": "At Serverless we're constantly educating ourselves to incorporate the latest best practices into our engineering processes so we can deliver the best Serverless toolings out there. This blog post lists useful software engineering books we've read and highly recommend. Note: This list is a living document. We'll update it periodically to reflect our recent learnings/recommendations. We are not associated with any authors or platforms we list here Clean Code: A Handbook of Agile Software Craftsmanship Clean Code by Robert C. Martin (aka Uncle Bob) is a classic book every software engineer and programmer should read. It teaches you how to write code in a way that's easy to read and understand. You don't have to follow every single technique, but even adopting some of them will ensure that your code \"reads like prose\". Remember that a majority of our time as developes is spent reading code rather than writing it. And to be honest:  is way easier to read than:  Get the book The Clean Coder: A Code of Conduct for Professional Programmers This is another great book from Robert C. Martin, the author of \"Clean Code\". The main topic of this book is how professional software developers should behave, incluing how they should communicate and work on projects or solve problems. Have you ever felt empowered and overjoyed after hitting a super tight deadline inspite of being overloaded with tasks in the current sprint? You and your manager should be proud, right? Read this book and think again... Get the book The Phoenix Project: A Novel about IT, DevOps, and Helping your Business Win Authored by Gene Kim, Kevin Behr, and George Spafford, this novel about IT operations is not the usual tech book. It's really something you can read before going to sleep without your head spinning with all the complex tech problems you've just read about. The Phoenix Project is a good read if you want to see how the \"wrong work\" will negatively impact the performance of your company. You'll learn the different types of work and see how a (fictional) doomed corporation transitioned from being a \"poor dog\" to a \"star\". Get the book The Pragmatic Programmer: From Journeyman to Master The Pragmatic Programmer by Andrew Hunt and David Thomas is another classic every software engineer should read. A true \"oldie but goldie\". You'll learn how to approach tackling different problems you might face during your professional career. This book is a little bit dated, but was revolutionary when it was publised in the s. Get the book Design Patterns: Elements of Reusable Object-Oriented Software Even with the rise of new programming paradigms (like Functional Programming), Object-Oriented thinking and designed software is definitely still around. Design Patterns by the infamous \"Gang of Four (GoF)\" - Erich Gamma, Richard Helm, Ralph Johnson, John Vlissides - with foreword by Grady Booch, is a great compilation of all relevant architectural patterns you can utilize to make your Object-Oriented software easier to maintain. Get the book The Imposter's Handbook: A Primer for Self-Taught Programmers Can you relate to the feeling that you just don't know enough? Perhaps someday you might be exposed as a \"fraud\" because you do your job, but compared to others you're still lacking some knowledge? This is called \"Impostor Syndrome\" and quite a common feeling in the tech industry where experienced coders are hired even if they don't have an academic degree. Some people even find themselves sitting next to a years old who was hired from college because of his coding experience. But The Imposter's Handbook by Rob Conery has got you covered. It fills your knowledge gaps and walks you through all the topics you'll also learn in a CS degree program. This will enhance your career, solidify your self-esteem and you can finally chit chat with the Ph.D. who's working in the Machine Learning division. Get the book Refactoring: Improving the Design of Existing Code Code gets messy over time. That's just a circumstance we cannot change. But what we can change is the complexity of our codebase through refactoring. The classic Refactoring by Martin Fowler and Kent Beck will show you how you can identify bloated code, and how you can work your way through the old, entangled codebase to a new shiny, refactored one. The books is old, but still a classic. The examples are Java heavy but can be applied to other codebases, as well. Get the book Is something missing? Know of a great book that's missing from this list? Great! Just open up a PR by clicking on the \"edit\" button above or add your favorite book in the comments below!",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e12",
      "title": "Enhanced Analytics & Monitoring for your Serverless Apps with the IOpipe Serverless Plugin - Now in Beta",
      "content": "At IOpipe, we enable users of AWS Lambda to monitor, analyze, and tune their serverless architectures. Getting started is a breeze with the iopipe wrapper library. The Serverless Framework is an indispensable tool for Lambda, OpenWhisk, and Azure functions-as-service development and deployment. Once you get the hang of launching functions with it, the sky's the limit. But when you have or functions, you might want to abstract your build pipeline further to include your favorite tooling + processes. There are plenty of powerful plugins already. So to enable even _easier_ integration with IOpipe, we made a plugin too. How Does it Work? The task of code modification shouldn't be taken lightly - and luckily we don't have to reinvent the wheel here. We can use an AST parser and builder to do the hard work for us. With the rise of varied JS build tools, AST libraries are certainly _en vogue_ and we have plenty of options. For this project we use jscodeshift that provides some nice sugar on top of recast. What are we trying to accomplish? Here's a super-simple function that outputs a message with a UUID.  Now say we need some extra metrics for our little function - how often it runs, when it errors, why it was slow. It's time for IOpipe. Here's the new code:  Nice and straightforward. The IOpipe library automatically records stats about the invocation and makes it ready for inspection in near real-time. How can we go from A => B automatically though? Putting Code In Your Code Let's fire up jscodeshift and parse that . First, we need to find the AST \"node\" that we want to wrap.  This simply looks for the statement  With our code represented as a tree, the original formatting won't make a lick of difference. As long as it runs the same, we will still find the statement we need.  Within the Serverless plugin lifecycle, we know the functions defined in the `serverless.yml` and what the handlers are named. With this info, we can be fairly confident that the right side of the statement will be a function. This is the function we need to wrap. We create a new AST node from raw text and insert the original function node into that.  Not bad! You can see the real thing here. Uploading the Result Serverless provides some great _hooks_ to work with and the ones we are after here are `before:deploy:createDeploymentArtifacts` `after:deploy:createDeploymentArtifacts` The plugin creates a temporary `.iopipe` folder, copies the source code, applies transformations, and tells Serverless to use the transformed code instead of the source code. After it's done deploying the `.iopipe` folder is deleted and we go on with our day. Thanks to Jonathan Goldwasser for inspiration from the Serverless Webpack Plugin. What Else? While we're at it, why not keep your IOpipe npm package up to date? Upgrading automatically provides the latest speed, developer experience, and feature enhancements. While the process of auto-upgrading the package could be a separate post, the main idea is to fire off a child process to run some npm commands. We  yarn at IOpipe, so we also support that. A simplified example:  And check out the real stuff. Final Thoughts The IOpipe Serverless Plugin is now available in Beta for you to try out. Let us know what you think!",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e14",
      "title": "Keeping Functions Warm - How To Fix AWS Lambda Cold Start Issues",
      "content": "Cold starts in AWS Lambda got you down? You've come to the right place. In this post, I'll briefly cover what cold starts are, and then show you some ways to reduce your cold start times. Read on! First: what is a cold start?  When running a serverless function, it will stay active (a.k.a., hot) as long as you're running it. Your container stays alive, ready and waiting for execution. After a period of inactivity, your cloud provider will drop the container, and your function will become inactive, (a.k.a., cold). A cold start happens when you execute an inactive function. The delay comes from your cloud provider provisioning your selected runtime container and then running your function. In a nutshell, this process will considerably increase your execution time. Understanding AWS cold starts When using AWS Lambda, provisioning of your function's container can take > seconds. That makes it impossible to guarantee ",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e16",
      "title": "Building a Web Store with GraphQL, Stripe, Mailgun and the Serverless Framework",
      "content": "The Serverless Framework has made it extremely easy to deploy business logic to scalable cloud infrastructure. The recent post Building a REST API by Shekhar Gulati goes into detail on how to expose business logic through a REST API. In this post we'll explore the benefits of GraphQL over REST, and build a feature rich webshop using Graphcool and the Serverless Framework. So, Why GraphQL? If you're already sold on GraphQL, you can skip this section.  RESTful APIs is a well-understood architecture for web and app backends. In a RESTful API, you expose domain models at individual URLs and enable the client to traverse the data model either by following links in the response, or by adhering to a pre-defined URL structure. The benefits of this approach are: - Separation of concerns - Browser + network caching - Mature tooling Because `Users` and `Posts` are separate entities, they're available under different URLs. The code responsible for returning users doesn't have to know anything about posts and vice versa. In fact, as the application grows it's common to move the code to separate microservices, and even separate development teams. The fact that resources are exposed on a canonical URL makes it really easy to use standard HTTP headers to enable caching in the browser and network layer. This is great in theory, but unfortunately this model is no longer a great fit for the rich web and mobile apps we are building today. Consider the canonical Facebook Feed. In a RESTful paradigm we would have at least endpoints: `/users`, `/feed`, `/posts`, and `/comments`. To fully render the first screen the app would have to: Query the `/feed` endpoint to retrieve the top items for the current user For each item, query `/posts` to retrieve the actual post For each post, query `/users` and `/comments` to retrieve more data required for the UI This pattern results in a waterfall of network requests where the response from one request leads to additional requests. Because of network latency - which is especially bad on mobile networks - the fully RESTful approach leads to poor user experience. To work around this, developers have had to break away from the clean RESTful architecture and allow multiple resource types to be returned in a single request. Many patterns have emerged for how to implement this, but they all sacrifice one or more of the original benefits of RESTful APIs. The innovation of GraphQL is to embrace the fact that a RESTful architecture no longer works while acknowledging that the three benefits listed below are worth striving for: - GraphQL enables strong separation of concerns in the backend by introducing the concept of independent resolvers and a batching DataLoader - Clients such as Relay and Apollo enable flexible and super fine grained client side caching - GraphQL is an open standard, allowing the community to build advanced tooling such as clients, editor plugins, code generators as well as the awesome in-browser GraphiQL query editor Nik Graf went into more detail in his recent webinar Serverless & GraphQL: A Love Story Anatomy of a Serverless Webshop Let's get to it! Our webshop example will have three types of data: A `Customer` can have many `Baskets` and a `Basket` can have many `Items`. In GraphQL a data model is described using IDL (Interface Definition Language):  Setting this up in the Graphcool schema editor will take less than minutes. Now that we have the data model nailed down, let's start implementing the core functionality of a webshop. Adding Items to Basket The first feature we want to implement is adding items to the basket. To write your first mutation, go to the Playground in the Graphcool Console. If you have not yet created an account you can perform the queries directly in this in-browser IDE. First, list all existing items:  This should return an empty list, so let's go ahead and add some items:  When you run the query again, you should get a response like this: To create a user with a basket containing the Mackbook Pro, you can write a nested mutation:  By now you should have a solid understanding of GraphQL queries and mutations. If this still feels a little foreign to you, I would recommend you spend some time trying out different queries in the playground before continuing. You can find more examples in the reference docs. Pay for Items in Basket So far we have only implemented pure data manipulation. Now it's time to add a serverless function to integrate with Stripe. Head over to stripe.com and create a free account. Make sure the account is in test mode, then go to the API menu and locate your `Test Secret Key`. You will need this when calling the Stripe API from your serverless function. In a real application you will use one of the Stripe native SDKs or checkout.js for websites to collect a customers credit card information and securely exchange it for a one-time token. During development you can use the test card number ` ` to generate a one-time token directly on the Stripe documentation page. Click Pay with Card to generate a token like this: `tok_AWBAMMAtIPOjmbuhze` When the front-end app has retrieved a one-time token from Stripe it can be associated to the basket using a GraphQL mutation:  Graphcool is an event-based platform that allows you to attach custom serverless functions at different stages of the request processing. To charge the customers credit card we will add a mutation callback for updates to the `Basket` type. In the UI it looks like this: When setting up the webhook we use a GraphQL query to specify the data requirements for our function. We retrieve information about the `Basket`, but also name and price of all `Items` as well as information about the `Customer`. When we've deployed our AWS Lambda function, we can return and enter the webhook URL. ```javascript const stripe = require('stripe')('STRIPE_SECRET_KEY') const Lokka = require('lokka').Lokka const Transport = require('lokka-transport-http').Transport const client = new Lokka({ transport: new Transport('https://api.graph.cool/simple/v/PROJECT_ID'), }) module.exports.handler = function(event, lambdaContext, callback) { const basket = JSON.parse(event.body).updatedNode stripe.charges.create({ amount: basket.items.reduce((a, b) => a.price + b.price), currency: 'eur', description: 'Purchased: ' + basket.items.reduce((a,b) => `${a.name}, ${b.name}`), source: basket.stripeToken, }, (err, charge) => { console.log('Charge went through!') client.mutate(`{ updateBasket(id: \"${basket.id}\" isPaid: true) { id } }`).then(() => callback(null, { statusCode: , body: 'success' })) } }) } yml service: serverless-webshop provider: name: aws runtime: nodejs. stage: dev region: eu-west- functions: stripeTokenAddedToBasketCallback: handler: functions/stripeTokenAddedToBasketCallback.handler events: - http: path: serverless-webshop/stripeTokenAddedToBasketCallback method: post ``` Running `serverless deploy` will start the deployment process and eventually return the URL where your function can be accessed:  Subscribe to Payments The next part of the webshop is a bit contrived, but it's a nice demonstration of how GraphQL subscriptions work.  Subscriptions are currently in the process of being incorporated into the official GraphQL spec, so this is all quite new. You can think of subscriptions as being notified of somebody else's mutation. For example you can subscribe to all changes to `Items` like this:  Go ahead and run this in the Playground. Then run a mutation in a different tab to create a few `Items`. If you only care about a subset of events, you can use the sophisticated filter API to narrow down the events that result in subscription triggers. For the Serverless Webshop we want to be notified only when a `Customer`'s credit card has been successfully charged:  Send Shipping Email to Customer The last feature is to send an email to the customer when her order has been shipped. Again we set up a mutation callback with the following payload:  Mailgun is a developer friendly mail provider that allows you to send . mails a month for free. Setting up an account takes just a few minutes, but be aware that you have to validate a credit card before you can start sending emails. The function for this integrations is simple as well: ```javascript const mailgun = require('mailgun-js')({ apiKey: 'MAILGIN_API_KEY', domain: 'MAILGUN_DOMAIN', }) module.exports.handler = function(event, lambdaContext, callback) { const basket = JSON.parse(event.body).updatedNode const mailData = { from: 'Serverless Webshop ', to: basket.user.email, subject: 'Your order is being delivered', text: 'Purchased: ' + basket.items.reduce((a,b) => `${a.name}, ${b.name}`), } mailgun.messages().send(mailData, (error, body) => { callback(null, { statusCode: , body: 'Mail sent to customer', }) }) } ``` Wrapping Up As we've seen, GraphQL provides a compelling solution to the challenges faced when developing a RESTful API. By letting the application developer specify the exact data requirements for a front-end component you can avoid the two most frequent offenders when it comes to app performance - overfetching and multiple roundtrips. In combination with AWS Lambda and the Serverless Framework, GraphQL is a great way to deploy scalable backends. Graphcool is the fastest way to get started and makes it really easy to integrate external services into your API.",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e18",
      "title": "Serverless Air  - The Serverless ecosystem now & where it's going with Jared Short",
      "content": "In this episode, Jared Short from Trek joins us to share his take on the serverless ecosystem. We get his take on where he thinks the serverless world is headed. We dive into current challenges in the space while discussing his AWSWishlist for and touch on why servers still have their place in the stack. He also goes into detail on Trek's auto scaling dynamoDB project and how it moves dynamoDB towards the set it and forget it end of the serverless spectrum. Enjoy the video Or listen to the audio Transcript David: Hey, everybody. Welcome to another installment of Serverless Air. Today I'm joined by a very special guest, Mr. Jared Short. He is the director of innovation over at Trek. Jared, welcome to Serverless Air. Glad to have you here. Jared: Hi there. Yeah, thanks for having me. I'm really excited to be here. And just be able to talk about Serverless and really get that knowledge out there for everyone. David: Yeah, for sure. So before we jump into it, can you give us a little bit of background about yourself? How did you start working in the industry? How did you start working in the industry? Jared: That's...it's a good question. It's a little bit of a long story but we'll make it short. So roughly...I don't know. , years ago some...probably more than now. I got into programming and I grew up in Indiana. So I went to programming and technology because I did a few years of fieldwork. I literally worked in a cornfield in Indiana. So I did...it was like the worst experience ever. And I liked video games and so I would play video games a lot and my parents told me, \"Don't do that. You're never going to make a living just playing video games or sitting on a computer.\" So I decided as anyone would do to prove my parents wrong. And so, I went through being a developer. I ran a small consulting agency for a little while. I did a short stint in software QA, quality assurance stuff and then had just a meeting with a few folks and I eventually ended up at Trek doing the Amazon Web Services stuff working in the Cloud. And then it just kind of progressed from there. From the Cloud, I just kept pushing technology further and further and seeing what I could get out of it and ended up in what we now call Serverless. And so that's.. David: Nice, nice. Jared: Yep. David: So yeah. So you're working atTrek. What...like Trek is one of or Serverless partner agencies. One of our favorites. Love you guys over there. Jared: We love you too. David: Yeah. So like what kind of projects are you guys working on over there? What kind of projects are you guys working on over there? Jared: Oh, man. A lot of stuff. There's been some larger Serverless projects and some smaller ones. And then it's not all Serverless, there's certainly still some containers here and there and some legacy infrastructure and... David: How dare you, sir? Jared: You know, it's funny. We always joke about this. Up until very recently like every single Serverless project we had ended up with at least one kind of server somewhere doing something. Just because there's some limitations of Serverless that maybe we could go over a little bit later. David: Yeah, yeah. Jared: But at some point, there's always...ended up being a server in our Serverless stuff which is quite funny. But we've done some fairly large projects. Some stuff doing tens of millions of hits plus a day and we've seen some really excellent results all the way up in the enterprise world down to just smaller projects and some of the really exciting stuff that we can definitely talk about later and talk...and kind of put in front of people is the idea of like Serverless Graph QL is...this is amazing combination of two really hot technologies. Everyone likes to talk about Serverless and Graph QL kind of apart from each other but we put them together and it's just been fantastic. David: Yeah. That's the pattern we're seeing all the time too and we're actually using that internally here. Jared: It's just fantastic combination and I can't say enough good about it. David: I don't know. I was against it at first because my JavaScript fatigue was hitting me really hard but I've come to the light and I do see the benefit in it. Jared: Are you guys using typescript at all yet? David: No. So we use flow on some stuff but yeah. Jared: We have done that as well. We actually...our first Serverless Graph QL project was with flow. David: Yes. Jared: But we've just seen slowly that typescript seems to be winning the war there a little bit. David: Interesting. Jared: But we'll see. David: Yeah, I'll just check it out again. Jared: Yeah. David: Cool. So being like in this industry for quite some time like what are some of the biggest like shifts and changes you've seen? What are some of the biggest shifts & changes you've seen in the space? Jared: Yeah. So I would say what's been quite interesting to us kind of seeing it from all over the board from startups to our internal stuff up to enterprise is that...and certainly there's some self-selection going on here but folks are coming to us and Docker was hot for like a year maybe. David: Yeah. Jared: But we've really seen that trend somewhat skipped over. The companies that would have been willing to go to Docker in the first place are now coming to us and asking about Serverless and so the whole container trend has kind of gotten jumped over for rather the managed platforms that underlying [inaudible ::] container type engines but they're more interested in just throwing their code at a platform and saying like, \"Just run this in response to some event.\" Right? So that very much has been skipped over the whole container...orchestrating your own containers has been skipped over which is really interesting. David: Yeah, why do you think that is? Just the...you know, I wanna build my app and not worry about the underlying pieces or... Jared: I think it's vastly more compelling to just say, \"Look, here's my code.\" And just run it in response to these particular events exactly when I need them at the scale that I need them at. I don't want to worry about the actual patching and management of my underlying containers or my underlying host systems and so that's something that's nontrivial. I know Coke does a presentation about this at the previous reinvent and we could probably link that in the resources or whatever. In the \"Do Billy do\", yeah. Coke talks about like in...I'm kind of working on a blog post about this where we really look at what's the actual cost of running a server, right? So, the server, the compute cost is the cheap part. The expensive part is the guy that has to jump in there and maintain it. Even if you're doing it with Ansible or things like that. There're still the overhead of actually managing the OS and that kinda...in all of your daemons running on the OS. Your antivirus if you're doing that, your log aggregation. And then if you just jump to Serverless and say, \"Look, I don't care about all of that. Here's my code. Run that.\" There's just this massive like weight lifted off of your ops team shoulders or your team shoulders where it just works and work being a relative term in some cases. But the underlying of it is that you just don't care about and that is a massive win in very tangible ways, right. The financial benefits from that can be massive. I think it's...Coke estimate something like $ of the cost of a server even if it's like a T medium. The T medium, if you run it for years in the US East one is probably...I think it's like less than $ a month so a year. It's like $ just for the maintenance cost and other things you have to run on that server. So it's a large factor of cost is just not [inaudible ::] cost. Whereas if you move to Serverless that's just all handled. David: Yeah, it's really like the lowest like total cost of ownership. Jared: Yeah. David: Yeah, and to me personally like digging into containers and having to orchestrate that stuff, it was just beyond kind of my skillset at the time when I jumped into Docker. So when I kind of saw this whole Serverless movement happening I was like, \"That's the thing that I want. I wanna build my apps and just have them work.\" Jared: Yeah. And I certainly wish...and there're some platforms I hear that kind of orchestrate it and then...I know we're gonna talk about Wish List stuff later. But I would love to be able to actually build my own container and hand its Lambda and say, \"Run my functions in this container.\" Right? I would love that capability but... David: Yeah, that would be cool. There are a lot of projects kind of cropping up running...like basically that function as Serverless provider in like Kubernetes or what have you. So it's kind of interesting to see what's happening over there. Jared: That said, I really don't want to manage the underlying post and that gets back to that problem. David: Indeed, indeed. Cool, cool. Yeah, so getting back to that Wish List then like what are some of the things...so for those of you that don't know there's AWS Wish List on Twitter that's really telling it like kind of the missing pieces that AWS...basically the missing features they should build. What are some of those for you? What would you like to see? And it doesn't have to be specifically about AWS. It could be about just the space in general. What is on your AWSWishList? Jared: Yeah. There's a few things and some of them are actually being worked on. One of them was actually fixed yesterday I believe or two days ago. So like just even just something as simple as tagging Lambda functions. So we do a lot of stuff at Trek around monitoring and figuring out how to monitor Serverless applications, right. So it changes to some extent. And I think everyone is still trying to figure out what is the actual best way to monitor and respond to Serverless event driven architecture type stuff, right. So if a function starts failing a whole bunch, jump in and start looking at that and it'd be nice for us to have tags rather than relying on like parsing function names or things like that. Because right now we just do some parsing and say, \"Well, if it has like service name - prod or service name - you know, production or something like that.\" Then we actually care if it's starts failing a whole bunch as opposed to service name - David's Dev. Like we don't care if that starts failing. Of course, it's gonna start failing, right? You know, we don't care if... David: Where are you guys seeing these tags with like...is this going through like a Cloud Watch or how do you guys...like parsing... Jared: Yeah, we actually use Data Dog and they pull in Cloud Watch and things like that. And we also have some other small, just monitoring script and things like that. But yeah. We...so there's actual tagging of Lambda functions which was announced which is great, right. So now we can just tag stuff with monitor. We have this convention. We just monitor through and we put that on resources and then we can just aggregate all of our resources by that monitor trough across, you know, dozens of clients. And it makes it much easier to filter out all of the noise from, you know, dozens and dozens of AWS accounts. David: Nice. Jared: See what else? X-ray is another thing coming from Lambda but I'm sure everyone's heard everyone else complain about it. It's really, really hard to debug Serverless stuff. David: Yeah. Jared: There was like a tweet recently where I had this massive system of all of these moving cups and wheels and everything. David: Yeah, I saw that on your Twitter. I love that gif. Jared: And it was quite interesting because actually previously to that day I was explaining to a new sales guy that we brought on. He's like, \"So why is like Serverless hard to debug?\" Because I had said that at one point and he pressed me on the question and I ended up with a picture that was like, \"So in a normal flow like with servers it's like...comes in from the public Internet through your load balancer to a server, maybe hits a database and then it comes right back out.\" It's very linear like in, out, in A fairly typical infrastructure whereas in the Serverless world it's like there's just like , arrows going every which direction and it's like you have no idea what went where and it's very hard to pull it out. So debugging and I think x-ray solves parts of that. I don't know if it's the whole story but that's still part of my Wish List. It's just better at debugging and that's been there since day one, right. The other interesting bits I brought up, being able to run your own containers and that's not just because I want different runtimes. I don't...not just because I want Ruby or I want other things but I would like to bake a slightly more solid container that has some of my dependencies just built-in, that has, you know, maybe a specific version of the AWS SDK or a specific version of internal tooling and SDKs and things like that. So I just...I can get it to be faster all the time so... David: That's a challenge right now if you're using any type of binary as well. You kinda have to compile it against the Lambda image. Jared: Yeah. David: So if you're using like Phantom JS or something like that. Jared: Yeah. David: Yeah. That's on my list. Jared: Yeah, so that's...I mean, that's part of it. I mean, the interesting part there is the thing that made that easier is the Amazon Linux Stocker Container that they released, the officially supported one. That does make that a little bit easier but certainly as... David: Have you guys done anything with binaries over at Trek? Are you using binaries in Lambda functions? Jared: Yes. Actually, we did some interesting stuff with image resizing. There's some native binding stuff. So image magic is kinda slow. There's some node packages that have native bindings to some specific like image manipulation libraries and so we do do some packaging with that. I actually talked about it at the Serverless Conf. Not the Serverless Conf. The Serverless Meetup in San Francisco. I talked a little bit about how we're doing some interesting image manipulation stuff there. David: Yeah. I think we recorded that as well. Jared: Yes. Link in the \"Do Billy do\". David: Awesome. Yeah, that's cool. So, yeah. You were talking all about like image resizing and what have you which is a really common use case that we see. Jared: Yes. David: What are some of your other like favorite use cases like for Serverless technology? What are some of your favorite use cases for Serverless technology? Jared: Oh, man. Well, see, image resizing, Graph QL, data processing, some analytics type stuff. Those are all kind of the normal target ones. Some other fun ones that I've seen are...and actually...kinda Athena kinda kills part of it was really this ad hoc querying of S data, right. So Athena came out and kind of kills part of that use case that we have for it. Because we could quickly aggregate over a whole bunch of S files and query data in kind of an ad hoc fashion out of it. That was in which case. David: So what would you use that for? Jared: Say you have a whole bunch of load balancer logs or Cloud front logs and we wanted to know how many s we got in the past week and what those sources were, right. Instead of having to pull it out of all those files onto a local machine and kinda graph through them we just have a Lambda function that we could go type in like a regular expression and run it and then it would just spit out the deluxe. And it would actually just fan out. So we did the fan out pattern, right, where one function would essentially list all of the items at a particular path in S and then each...and then fan it out so each of those items had a Lambda function run against it. And so, we could very quickly like do an ad hoc regular expression against, you know, hundreds of thousands of files and see...yeah, we had you know, 's and all of them came from the same IP address or something like that. David: Yeah, that's awesome. Yeah, the fan out pattern is really interesting. I don't see too much about it to be honest. Jared: We have a blog post on it. We actually do. That's like a year-old blog post too. David: But it...that's kind of one of the beauties of like kind of the Serverless space as well. It's like, you know, you could basically just spin up an infinite amount of functions like, you know, and you don't really have to worry about that piece so... Jared: And in combination with Graph QL, one of the really compelling things I like about it is that you can essentially have Lambda functions for each of your different like disparate data sources. So you might have one Lambda function that specifically is grabbing things from Dynamo BDB, another Lambda function that pulls from RDS. Another Lambda function that pulls from your rest API or something like that and so you have your central Graph QL end points that gets hit. And then inside of your resolvers, for those of you that know Graph QL. It is smart enough to resolve against those different data sources and says, you know, \"Well, I know if I'm fetching users. Those are coming from Dynamo DB, but if I'm fetching posts for each of those users or comments for those posts they're coming from RDS and maybe I'm fetching image information or something like that from like Cloud Aneri.\" That's more of a frontend thing but...and then we'll resolve that to a rest API or something like that, right. And so is that one Lambda function can now aggregate across three different services or Lambda functions and you can essentially get this burst of CPU capacity to respond dynamically to your queries as you have them. And say that model is insanely powerful. David: Nice. Is that what you guys typically use on all of your projects then? Jared: Not always. But it's been something that we've been trying to kind of feel our way around a little bit more. We've used it for a couple of things. The flipside of that, the alternative to that is that it gets harder to debug all of the sudden, right, because now my stuff isn't all on the one Lambda function. It's who knows where. So it gets back to that particular problem. You can put some logging hooks and things in place that try to make it easier but it certainly doesn't. Doesn't help the case. David: Have you guys seen any like additional latency with that pattern as well because it's like...I mean, if you're stitching together desperate data sources. Jared: Yeah. It certainly can add some latency. It hasn't been too bad but certainly like if you have a rest API or something that you don't necessarily even have control of it can hurt you pretty bad and then in that case you might wanna write timeouts or something on your own and just well, sorry. Like that data didn't come back or we gonna fail this query. But on the flipside of that though, there is the other alternative where you have this idea of...in Graph QL someone can just ask for things as deep as they want into the graph, right, where you can ask for all of a user's posts and then all of the users that commented on that post and then all of those users posts and you can just recourse down the graph essentially infinitely. And you can break a lot of things doing that, right. David: Yeah, yeah. Jared: There's some ways around it. Facebook has data loader, but one of the really cheap ways to handle that that we do is we just say like, \"This Lambda function's timeout is five seconds. If it doesn't return in five seconds, something is very wrong and very broken or you're trying to do something you shouldn't and we're just going to fail.\" David: Right, right. Jared: Right, so we just time it out. And there you go. You took care of % of the bad things that could happen in a really like deep nested graph attack. David: Yeah, we notice a little bit of latency on one of our Graph QL end points. And like we basically like implemented data loader to kind of cache it up. Jared: Yep. David: But yeah. Interesting, interesting, yeah. I still...I don't know. A lot of things that I do are smaller and I'm like, \"Just a couple of rest end points and I'm done.\" But it's definitely powerful especially from the developer experience in the Graph QL world just, you know, using graphical like their UI tool to like explore what data you can get back. Jared: Yeah. David: Very cool. Jared: From the Graph QL end point I will tell you what we started doing internally for some of our tooling and things like that and to enable people that aren't necessarily...whose jobs aren't day-to-day no JS developers, right, which is a fair amount of people at Trek at this point is there's a few services out there that essentially offer like Graph QL as a service. And recently graph.cool came out and is probably the most exciting tool that I've seen since really Serverless framework probably. David: Wow. Bold claim. Jared: It's a bold claim but the...I think those guys who genuinely understood the power of Serverless compute so it's Graph QL with a really fancy like schema editor and relationships and they generate the full Graph QL schema and mutations in filtering and you can say, you know, \"Give me this particular object and all of the relationships to this object.\" A brilliant permission system. But more importantly and I think what we're gonna see from more companies moving forward and this is something I really wanna talk about, something that I think is important in companies even if they aren't doing Serverless themselves should realize is that they are adding this extensibility to their product that is very tightly coupled with this idea of functions as a service or Serverless compute whereas they have these mutations that come to their system. So Graph QL mutations is this idea of changing data, right, So it's like a post request or something like that in a rest API or a put or a patch or something like that. So these mutations come in changing data or adding data or deleting data. And they offer this capability that says that when the particular type of node or something is changed, it triggers an event or an execution of a code that you could be hosting in your own systems or platform, right, So you write a Lambda function that says, \"When a new user is created, I want to post the slack notification to my own system and I want to create a record in Sales Force and I want to send an email back and that extensibility in their platform is perfect, right. So [inaudible ::] has very similar type thing where they use their own system's webtask.io to run kind of this sandboxed code. And I think that's the new web hook, right. So that's...it's the new web hook and I think companies are slowly starting to realize that and I think that's gonna be critical. I think in the next two years we're gonna see some things around kind of this code execution in response to events. Yeah, maybe the extendible end point. David: I agree. It's like the evolution of web hooks that just offers almost like infinite flexibility to your system and it almost like out of the box...like if you build your product in this way, where you can trigger these events and have users write whatever custom code against that, it's like you're out-of-the-box. You're building like a platform, right? Jared: Yeah. David: Which is what most successful companies have done but yeah. It's... Jared: And I think...and to that point...so I'm % in on AWS, Trek is % AWS. To that point, I think Azure has actually done something here really, really well that I don't think many people realize or have seen. They have something called the logic app service which is like literally something like Zapier or [inaudible ::] except it's like their own implementation of it but it's kinda built with this core idea of events rather than this just web hooks. It's like event-triggering type service and they have stitched together a bunch of the different services. Really brilliant service. I think I have a tweet of it where I was like, \"This is the first thing I've seen that Azure did that...\" I was like, \"Why doesn't AWS have this?\" Like, \"I want this.\" So yeah, and I think that just paints an interesting picture where the space is getting more competitive. Lambda was there first, AWS was there first. They have the lion share obviously but Google computer functions is catching up I think. They're still beta, I believe but...and then Azure is doing a lot of efforts, putting a lot of effort into their systems there. So it's interesting and exciting and I think it certainly says that this is the way forward. People have realized that this is the future. David: Yeah, yeah. And not only that. It's like just the competition is what keeps, you know, that AWS Wish List like rolling, right? Like if there was no, you know, no competitor you'd be like, \"This thing is good enough, you know, there's nothing else. Pay us money.\" But yeah. That competition is amazing to see. And yeah, a lot of the providers are doing some really interesting stuff and we're working on kinda building out our kinda fleet of examples and what have you with a bunch of them. And like you mentioned Graph Cool. We're working with them on some stuff as well. Can't talk about it yet but it should be pretty, pretty interesting how we tie that in but cool. I wanted to talk to you about...so I called you out on a previous episode. You shared an auto scaling...you guys wrote a post on Dynamo DB auto scaling. And I was like, \"Why isn't this open source? Release it to the world, Jared.\" So yeah. So like give us a little bit of background on that project like what does it do and, you know, how did it start. DynamoDB auto scaling? How did that project come about? Jared: Yeah. Yeah, so we did open source it, we got called out and we made good on it. [inaudible ::]. But yeah. So we actually had a need. It was one of our first...it was actually our very first large implementation of Serverless and we leveraged Dynamo DB and the issue was that it was in a space, enterprise space where people come in in the morning and towards midafternoon are actually using the system very, very heavily and then at night there's like no usage to the point of, you know, millions and millions of hits over like something like % that hits are over like a two-hour period in their system. So it's literally like just...every day massive spikes. Like the typical auto scaling scenario, you see for EC the AWS is always like, \"Oh, if you use auto scaling you'll save billions of dollars.\" Right? We're like, \"Okay. what do we gonna do? Go on a Dynamo and like have...like just create a task or just API like every day we'll just increase and then decrease over the hours but then what happens if it's a holiday? What happens if there's like an unexpected spike?\" Right? Dynamo has no capability of handling that. So we essentially said, \"Well, we can look at like Cloud Watch metrics or we could look at metrics so why not just create a Lambda function that looks at those metrics and kind of intelligently decides if we should start scaling up or if it's safe to scale down?\" So that's basically exactly what we did. It has a small API in front of it to make it easier. So if you have other systems or people that need to communicate with their API and change their own tables they can. But really the basic concept is behind...it's just a small scheduled Lambda function that just does some basic polling and just polls and says, \"Hey, is this table reaching like a certain threshold of capacity and if so we're gonna scale up, and if it's below a certain threshold of capacity, we're going to scale down.\" And it reacts pretty quickly and we've seen it actually work quite well, where after we get to like % of capacity of a table and we just say, \"All right. Like we're gonna scale up. Maybe you can set how much you...the scale up factors are.\" David: Yeah. Jared: And then you just keep scaling up. The interesting part is, scale-ups are...you can do as many as you want during a day. Scale up doesn't matter. But you can only scale down four times in any given day. Right? So... David: And that's per table, right? Jared: That is per table yes. Yeah, so we just kinda take a fairly blunt force approach where we just have one Lambda function that checks four times a day if our capacity is below our threshold and then scale down and then we have another Lambda function that actually...I think you can configure it. I think we just do it like every five minutes or something, it might be every minute. I'm not sure. Where it checks and just scales up as needed. But it's worked really, really well. It saves a lot of money and then also, right, it kinda tells you how much reserved the system doesn't but you can kinda look at where your auto scaling starts balancing out and then just by reserved capacity is also another money saver for Dynamo DB. If you're at the point that you're auto-scaling Dynamo tables, look at reserved capacity as well. David: Right, right. Yeah, it's an awesome project, it's...I can't remember who said this, it might have actually been you. Someone was mentioning that like the Serverless world is on this spectrum where there's things that like you don't care about, it scales for you. Like you put your code there, it runs and then there's things that fall kind of in the middle there like towards the Serverless world like Dynamo, where you're not really concerned with the machine, the software etc. But you do have to manage that scaling, so like you guys kind of took that and like moved it more towards the, you know, totally Serverless space. Jared: Yeah, so I guess the way I've always explained Serverless to people and the concept of Serverless is that it's really only Serverless if I don't...if there is a painless way to scale my services on demand, right. I should never have to over provision capacity and pay for that over provision and there should be a path to painlessly scaling. That's what Serverless is. I don't care if there's servers, I don't care if I have to pay somebody else to do it, right. As long as it's just a painless method where...we had a client once that said to us, \"If it's just a button and money, I don't care. It's easy like...\" That's the easiest problem in the world to solve. I can throw money at stuff and it just works, right? That's the scenario that I like to be in. If it's just a button or checkbox and money, I'm in. Right? David: Well, that's business. Jared: Yeah. David: So yeah. So you just mentioned like you don't care if there is a server, like you mentioned earlier, you know, like in, you know, some of your Serverless projects you guys still end up using a server for something. What are you guys using that server for? You peaked my interest. What are you guys still using servers for? Jared: Yeah, so in one of the projects it's because we needed a binary. We needed to be able to accept and return binary traffic. So images. David: Okay. Jared: In the first days...and you still can't do this through API Gateway very effectively. We needed to be able to accept and return images because we are integrating in some legacy platform stuff to the new completely Serverless system we had built and there was no method for returning images through like API Gateway or anything like that. So the systems essentially were uploading images and then asking for like resized images back, right? Typical system, but they were doing it in such a way that it was kinda like this request, response model and there was just no way to do that in API Gateway at that point so we had to change it so that the request...we had to essentially put it like Engine X proxy. Actually, I think we did like Engine X in node or something like that. That was this kinda black box system to our actual Serverless API so like the request comes in, we on the back end of that use the API and then return back what this legacy system expected. So that was one reason we did it. We've done some...there's some Web Socket requirements, right. Everybody knows this. You can [inaudible ::] or whatever to get around it but it's not elegant. David: Yeah. Jared: So just Web Sockets and persistent connections. You can't do that. David: Yeah, we have a blog post on how to use like Web Sockets in the IOT. I haven't done it personally yet but yeah. Jared: Yeah, we did it. We've done it and we have some folks actually looking at it right now and are using it for some really, really clever use cases but IOT is actually not super cheap either. It's a fairly expensive solution. Especially if you're just trying to leverage it for Web Sockets. It actually ends...it's like $ for a million messages. And you can get much cheaper other providers out there as well. So IOT, right? You're also paying for the cost of all of these other kinds of systems where if you're just using it for Web Sockets, it's this massive, massive solution to a very simple problem. Yeah, so there's that...I can't think of any other really big use cases we've had for servers and our systems. There's always, you know, the pulling in... David: Long running process? Jared: Yeah. Long-running processes that aren't embarrassingly parallel or aren't easy to chop up into little segments and throw in the Lambda functions, things like that. David: Cool. Awesome. Well, thanks for clarifying why you use servers. It was weighing on my soul a little [inaudible ::] Jared: I mean, interestingly in most cases it has been because like, well, there's like legacy pieces of this system that just have to somehow communicate to the new system. David: Yeah. Jared: So it's never been a game stopper, right? And yeah. David: I think...I mean, that's actually like what we're seeing a lot is, you know, it's not like when you're adopting this like Serverless approach, you can't...a lot of companies can't just throw everything away, right? And start Greenfield. It's like they'll adopt it kind of piecemeal and basically have kind of like a hybrid system where they might have to get a container fleet and, you know, functions running as well kind of in parallel so... Jared: Yeah, one of the ways we always bring it up with clients or a lot time we'll have folks come to us and say, \"We've heard about Serverless, we've done some research but we don't really know how or where to get started.\" Right? And so, one of the things we'll always ask kind of after just initial chatter and discovery is we'll say, \"What is the biggest like problem area you have right now in your systems, right? What doesn't scale and is a bottleneck? What are your ops guys spending disproportionate amount of time maintaining or fixing?\" Right? And if we can look at that part of the system and say...we can enter and look at it and say, \"Yeah, we could...this fits Serverless fairly well if we would break it out.\" And then we can go back to them and say, \"If we can just make all of these problems like magically go away, right, is that a benefit to your company in terms of, you know...the ops guys don't have that problem anymore. You don't have the scaling issues, you don't have all of this maintenance. Basically, a magic wand that goes away, right? Is that worth it to you for, you know, for being able to invest in other systems at that point and you almost never get to know. They're just like, \"Well, yeah.\" David: Shut up and take my money. Jared: Yeah, yeah. Checkbox and money, yes. There are... David: Awesome, awesome. Jared: So yeah. So people looking at Serverless are trying to bring their company into the Serverless world. Like look at your systems and say like, \"Okay, if we can take this specific chunk, this thing that causes a disproportionate amount of pain and transition at the Serverless, right? How much better does our world get if we do that?\" And you can also look at it as a marketing, internal marketing and say like, \"Look, we did it for this and now we can do it for this.\" And we've had people become champions of Serverless and kind of even of Trek inside of larger companies, simply because there's like this massive success with this one pain point and then they're like, \"If we could do it across all this other stuff, it becomes this...kind of this epidemic of Serverless.\" Right? Like everyone's just like, \"Okay, now we have to do it.\" It just spreads. David: I like that, \"The epidemic of Serverless.\" That's gonna be our new T-shirt. Awesome. So looking into the future, where do you see things, you know, heading? Jared: Oh, man. This is a good question and is something that obviously, no one's sure about but... David: We'll get through this in a year. Jared: Well, certainly like I mentioned before, I think events are the new web hook. But I think on top of that and certainly you and I, David, have kind of talked about this before and I know this idea is kinda floating out there. But the unified event log I think is going to be...I think is going to be huge. David: Yeah. Jared: And we could probably...we can spend some time talking about that I think certainly where there's this idea that you can model essentially all of the activity or events in a domain or business in a structured fashion and pipe it all to this unified event log, this block of events that have all occurred and then your various systems can feed off of those event logs, filtering for what they need and reacting. David: And reacting. Yeah, yeah. Jared: Yeah. And then in terms of reacting, they can actually feed events right back to the same log. And so, other folks or systems can react off the reactions, right? And that I think is going to be part of the future. I think someone's going to really build something exciting in that space, whether its services or just unified event log providers, and then more importantly, I don't know if it's more important but also there's just interesting world that that enables where you can replay your entire history of your business against a new system and so your system comes online with full contextual knowledge of all the history of your business, right. And there's just these really powerful things. You can essentially in kind of like this...the stock market example, you can replay your algorithm against a chunk of your history and make predictions and see if they actually hold true against another slightly further head chunk of history. David: Right, right. It's this idea of having like that immutable data that you can just replay over and over again or like even just from like the development side, having that...your like full data in development environment that you can replay etc. Jared: Right. David: Yeah. Jared: Or the idea of...if I'm a service provider, I can say, \" Just throw your event log at me.\" And if data is structured properly, right, the integration point for new services is I just say, \"Just feed your unified log to me and I will...my service will execute operations just from your event log.\" So if I'm just the provider, you give me that and that is the integration point between our two surfaces and now I can provide, you know, massive value to you for a little integration cost. David: Indeed. Yeah, there's a really good book I highly recommend for everybody watching. It's called, \"I heart blogs.\" It's from O'Reilly but it was written by the guy that...I can't remember his name right now but he basically like invented Kafka at LinkedIn. And he was basically talking about like why they did it and the use cases that fell out of it, like touching all the different orgs inside of LinkedIn. Jared: Yeah. David: Really interesting stuff. But yeah. Jared: Yeah. I guess to throw the ball at you little bit here, what do you see in the future? David: I mean, you stole all my answers but yeah. It's really...I see...I mean, all these are really good soundbites like, you know, events are the new web hooks. I totally agree. I think more companies will start exposing events and like giving people the ability to react to them either like in their product itself, or I could see that happening, you know, with let's say a Serverless framework but yeah. Just like this idea that everything is an event and you can react to it I and kind of chain these kinds of event driven systems together or just, you know...it's happening now but as like tooling matures and what have you, I can see just being able to build stuff much faster but yeah. Jared: So what's after Serverless? What's after functions as a service? Is it like...I know one thing though that comes up occasionally is this like composability of functions, right. So I can just pull down from a library of pre-created or pre-provisioned functions, Standard Lib kinda does this. David: Right. Jared: I've seen that. Is that the next iteration on top of it? David: Yeah, there's a couple of people there are doing this. I don't know. I've always been in the camp of like I build my own functions or I'll use libraries and like incorporate those into my functions but there are a lot of common use cases where I could absolutely see either pulling down functions that are already written that are for a specific use case or tying into already provision functions where you don't even have to like worry about it. There's, you know...obviously when you do stuff like that, there's more...your system is even more distributed and you're relying on more third-parties so...I don't know. Like things need to be solved in that space for I think that to take off but I could definitely see that happening. Jared: Sure. David: And again, this unified like event log, it's more...I mean, again like going back to the idea of immutability, it's like we're seeing this kind of...like the concept's not new by any means but we're seeing it being popularized by things like React and Redux and what have you, and databases like Atomic where it's like instead of updating data in place which is what we used to have to do for technical reasons, it's like now there's no like limit on how much data you can store so having that immutable log of information that you can react to and replay and do, you know...have other people react and replay to it like...I think that's just gonna be a total game changer. Jared: Yeah, I guess, not necessarily Serverless but one interesting thing that comes up there that I think will get more common like you said. So Cockroach DB has this feature where you can issue a query and get a response for a given time. So you say, \"I want to know what this query looks like for yesterday.\" Right? And you query it and it has the full history of data. And so, you can say, \"Give me that the answer to this query for yesterday or for last year or for right now, the current time and it will actually give you kind of like the snapshot of data as of the given timestamp that you've asked for it. David: How nice. Jared: So that's something that I think will be...I think will become more common. I'm not sure if Fauna...I think Fauna might have something similar. I think they like store the history of data as well. We'll have to look at that. David: Interesting. Cool, man, yeah. I don't know. It's definitely an exciting space. But yeah. One final question. I wanted to get your take on like, what are your favorite resources to learn about this stuff and like keep up-to-date on what's happening? What are your favorite resources to learn about serverless stuff? Jared: Yeah, so there's the awesome Serverless page [inaudible ::] That's a good one to kind of look over. And then some other...I guess I probably have somewhat of a problem here but I just go to YouTube and type in Serverless and just sort by most recent and I'll just kinda...I'll do that and then there's just...the Serverless blog is a good one and then the Serverless getter on occasion is a good one and kinda just to follow that and get a baseline for what people are talking about to some extent about Serverless and then the Serverless Conf actually tends to be a great resource as well. Even just watching the videos online you can kind of see. Even in like the three or four month increments or whatever that it ends up being. Maybe it's been like what...I think November, December, January. Something like five months since the last Serverless Conf. Those...it's interesting to watch those videos progress and those talks progress where kind of the first Serverless Conf in New York City was like, \"What is this thing?\" And then there was Tokyo which was still kind of a little bit about, \"What is this thing?\" And then London was like, \"All right. We kinda know what it is now. Now here's how to orchestrate it or how to do it.\" So I'm really excited to see what Austin is gonna be about. David: Yeah, you're gonna be there right? Jared: I will be, I will be talking, yes. David: Yeah, we'll be there as well. I think we've got like six or seven people going but yeah. Jared: Yeah, so I mean...and there's...is there actually...I don't...is there a Serverless like...there's a Graph QL weekly. Is there like a Serverless weekly type thing yet? I don't actually know if there is. David: I don't know, that's a good idea. Oh, we actually were talking about doing something like that on the blog like a recap of what's happening in the industry... Jared: Yeah. David: Every week so... Jared: That's something we need. Because I'm tired of wasting time just going through YouTube and...I just want somebody to tell me what I should know. David: All right, we're on it, we're on it. All right, awesome. Jared, where can people find you online? Where can people find you online? Jared: Actually, @ShortJared on Twitter and GitHub and jared short a.t. trek.com is my email address and I'm more than happy to talk with anyone about basically anything. This stuff's exciting and the more people we have in it, the better. David: Yeah, totally, totally. And if you, you know...anything peaked your interest on this interview that you want Jared to open source just hound him and, you know, they'll do it eventually so... Jared: Eventually. David: Yeah, thanks so much for coming on, Jared. It was a pleasure. Jared: Yes, thanks for having me. I'm excited to see what the future holds for all of us.",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e1a",
      "title": "Serverless Database Wish List - What's Missing Today",
      "content": "The rise of serverless infrastructure hugely simplified the process of deploying code into production. It's no longer necessary to worry about scaling, capacity planning or maintenance. Furthermore, serverless enables developers to build globally distributed services with ease. You can now deploy code to numerous data centers around the world, providing low-latency access to your services for many customers with just a few commands. There's a problem though. Existing cloud databases just aren't a good fit for serverless applications. The number of cloud database offerings continues to grow - from hosted open source databases (Compose, Amazon RDS, Google SQL to proprietary NoSQL solutions from major cloud vendors (DynamoDB, DocumentDB, Datastore). But these solutions were created for applications that are largely: - Running continuously for many days - Running from a single geographic location - Running from a fixed set of servers Going Global Cloud providers are launching new regions at an incredible pace. This is attracting customers from different parts of the world. In , AWS launched new regions. In , Google Cloud plans to launch regions. Microsoft Azure has regions in total. So why does most of the software we build still run from a single location? First and foremost, cost. Having the same set of hosts, load balancers and other stateless cloud resources in multiple regions is expensive. It also adds a significant layer of operational complexity. Function-as-a-Service (FaaS) providers, like AWS Lambda, are trying to solve this issue. Through FaaS providers, you deploy an application and only pay for the resources that are needed to run your business. The challenge of data replication and synchronization is another reason for the prevalence of single region applications. Distributing database across geographic regions is a challenging technical task. It requires knowledge and expertise in distributed consensus algorithms and CAP theorem. Many of the most popular open-source DB engines weren't built with that in mind. The remaining solutions - even if capable of geographic distribution (like Cassandra) - require significant operational overhead. Consistency is another factor. Ideally, the serverless database should guarantee strong consistency which might be required by some, if not most, of the use cases. Its worth mentioning that companies like Twitter or Google have built such databases for their internal use. The landscape looks more promising for databases from cloud vendors. Googles Datastore supports multi-regional active-active replication which isn't limited to nearby geographic areas on the same continent. Azure DocumentDB provides decent global distribution features with configurable different consistency levels. But like other cloud NoSQL solutions, it comes up short in other aspects. Pricing The micro-billing pricing model is one of the main attractions to serverless application development. This model means you don't pay for unused resources because infrastructure automatically scales to meet current demand. And, its super cheap. AWS Lambda charges per single function invocation. Databases, on the other hand, dont provide the same level of granularity when it comes to pricing. You still have to pick an instance type for your database, pay for all unused resources and manage capacity planning. In the best case scenario, you pay for read/write units (per hundred) and separately for storage (per GB). The pay-per-request pricing model doesn't fit in the case of a database since the storage always generates costs. Costs should scale with usage. This setup would charge per operation, plus per storage with the highest possible level of granularity (e.g.: total costs = ops single op cost + records record cost). This pricing model makes calculating operational costs a no-brainer. No Maintenance Manual intervention for operations like data replication and synchronization, adding a new region, removing a region, scaling underlying resources should be abstracted. And it shouldn't cause downtime. Maintenance windows need to become a blast from the past. Eventually developers might not even be responsible for selecting geographic regions manually. Databases could provide low-latency access from different parts of the world - either by replicating data to all possible datacenters by default, or by detecting where requests come from and automatically replicating data to the closest data center. (Kind of like how CDN works) Relational model is useful. SQL not so much. There are tons of use cases for columnar, document-oriented or graph databases, but the truth is that most consumer-facing applications use the relational model (with transactions and joins) because it prevents you from having inconsistent and duplicated data. Also, it's generally simpler for developers to manage. We got used to query relational databases with SQL, a language with some flaws. It doesnt fit the object-oriented programming model. Its too low level and doesnt have first-class support for hierarchical data (other than JOIN statement). There is definitely room for improvement in this area and GraphQL looks like a viable option that might be used as a query language in modern database systems. So what's available now? Recently launched FaunaDB seems to be the most ideal database for serverless. Its a cloud, globally distributed database with strong consistency guarantees. Its definitely worth checking out, especially because of their pricing model and query language. Im planning to write a review on FaunaDB from FaaS perspective. Stay tuned! Shout out to Nik Graf and David Wells for their review and feedback!",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e1c",
      "title": "Real World Security for your Serverless Apps with FaunaDB [Video]",
      "content": "Serverless Meetups are a gathering place for anyone interested in building web, mobile and IoT apps with serverless or event-driven architectures using the Serverless Framework or other tools. During a recent Meetup at Fauna, their Director of Developer Experience Fauna, Chris Anderson, shared his experience building a real world app with FaunaDB and the Serverless Framework. Check out the video: You can read more about Chris' project in this blog post - Using Serverless Authentification Boilerplate with FaunaDB Transcript Chris: I'm Director of Developer Experience at FaunaDB, Chris Anderson. I don't have a slide about me, but you can find me on Twitter, @jchris. And I've been working with Fauna for a few months now, focusing on the developer experience and like really, just getting up to my eyeballs as a user of the database and recently, as a user of the Serverless Framework, putting them together to write some application code, sort of just to see how apps are written in . My background, I've done various iterations of that at different database companies and open source projects and stuff over the years, so I really enjoy seeing what a new platform can bring to the table. And today we're gonna talk about my adventure making the popular TodoMVC Example App into a real application. So who here has messed around with TodoMVC in any capacity, ever? So the way I end up there is when it's time to start a new project and have to figure out what it is that JavaScript developers have adopted this year as the framework, I go TodoMVC and figure out which one is the least bad. So I spent a month maybe, a few weeks looking around, like somebody somewhere has got to have written one of these that talks through a database and cares which user you're logged in as. Nobody nowhere has done that. The best I could find was this Reddit thread that says if you decide to do a to-do list app again, please give it a back-end with user auth and persistent storage using a database. So I did that. We'll look at it in a little bit. Kind of what's going on here, what I've done is essentially, taken TodoMVC from a front-end demo to a full stack application that, while it's still got a demo level of features, the implementation of this stuff is the same way you would implement it for a production app. So what are we gonna do today? First, I'll tell you why FaunaDB is the first serverless database. Then we'll get into the application architecture and the code and talk about the components to making this sort of production-style TodoMVC. And then we'll go and do a short demo at the end and, if we have time, maybe I'll talk a little bit about some best practices stuff that came up for me when I was doing this. So FaunaDB, it's a serverless database and we'll have two slides on this. This is the first slide. These are kind of the table stakes. The second slide is the stuff that FaunaDB does that competition hasn't really gotten to yet. So obviously, you're serverless developers, your stuff runs in the cloud. I don't really need to get into this particular bullet point, except for to say that currently we're running in five data centers around the world and...AWS and adding Google Compute and Azure sort of as we speak. So you'll be able to not only deploy your data everywhere, but also do dynamic site selection and only pay for the points of presence that you're interested in. And you roll those changes out on the fly, so if you add a new customer base in Southeast Asia, you can just turn that data center on. The data model that we use in FaunaDB is friendly to JSON. It allows you to have rich, nested data structures. So it's not tabular. You don't have to worry about being constrained by the schema like you had to in the past with relational databases. But that doesn't mean we're not relational. So the objects that are stored in FaunaDB can have references to other objects and they're lightweight. You do it all the time. You could have constraints and unique indexes and all those features that you wanna have as an application developer. I guess this is a little bit of a history lesson and it hasn't been that long. We're all old enough to remember the beginning like, I don't know, what, years ago? You just used a relational database. And when your app got too big for it, or too much scale for that, it's just too bad, like you had Fail Whale. But then, there was, \"No, we gotta do something other than that,\" and so then NoSQL came around and you started to throw away features because you thought you had to in order to scale. So what we're doing with FaunaDB is bringing those features back, but it scales. So you've got this database that will take you from one user to millions of users and you can do your joins and all your good stuff all the way through. So yeah, if your database can understand the application now again, it's kind of retro but it's also futuristic and we're gonna go through the code and talk about, sort of, what it means in to have an application-aware, security-aware database. Just to round out the set of important features, you need to be able to build streams and have triggers fire and have your Lambdas around when things happen in the database. So we have a temporal data model that allows you to ask, for a given instance or for an index query, what did it look like at a previous snapshot or what changes have happened since that previous snapshot? So you can play a stream or build an elastic search index or something off of it in a reliable way. So now, what makes Fauna really different? This object-level security that I talked a little bit about means that you can model your business rules in the database. Your database can know who was the author of that blog post instead of just having a lambda know that. And it becomes a lot harder to write a bug that gives people access to data they're not supposed to be messing with when your database is security-aware. I wrote a whole blog post on the second point, escaping the provisioning trap. So I guess the question... I assume folks here are using DynamoDB. Who has ever set your DynamoDB through put either too high or spent a bunch of money you didn't need to or too low and had your app fall on its face when it got popular? So that's happened before. We don't have that. That's not one of the options. So instead you just pay for the actual traffic that runs. The hierarchical multi-tendency is useful for a whole bunch of things. The way I think about it is it means that your programs can provision things as complicated as themselves, like you wanna have somebody sign up for their hotel booking service and so you spin up a new database for the new hotel chain or maybe a new database for each hotel in the chain and it's all in the same hierarchical multi-tendency, so the billing is really simple for you to do. I didn't mention this anywhere, but the billing figures... You know what we charge you, all those headers are on every response, so you can bill through to your customers, too. And the client is stateless, it's HTTP, it means that you're not doing collection pooling and then throwing out the connection pool or whatever when your lambda runs. Kind of a small one, but I go to Google and people are having all kinds of problems with this, so it's good that it's not a problem. So let's talk about the object-level security as we dig into TodoMVC. So there's four main components to the application. The content service. Back in the day, this was the thing in the rails directory that does all your business rules and knows about recipes or blog posts or authors or whatever your domain model is. So your content service is typically your app and it may be more than one lambda. It may be one lambda. It may be some lambdas talking to lots of different back-ends. It could be arbitrarily complex. And in our case, the content service is pretty simple and we'll dig into it lambda by lambda. There's also, in the architecture, this reasonable authentication service that's based on the Serverless authentication boilerplate, which is linked from the Serverless read-me. It's probably one of the more popular Lego bricks in making a Serverless app. And before I submitted my full request to that project, it just used like, a dumb key value store to keep track of who you were and hand out authorization tokens to you. But then your content service had to have like lots of permissions on the database and then be trusted to only act on behalf of the user that the authentication service said was there. So, instead, what we do is the authentication service actually gives the content service just the credentials that it needs to operate on the database as the current user. So you can't write these app bugs that corrupt the database in terms of permissions. Then I pulled this out because it's its own moving piece in terms of how we talk about it, but part of the authentication service is this custom authorizer. And all it does is play a role in getting this FaunaDB access token from the authorization header to the content service. It essentially runs as a proxy inside of API Gateway before your content service runs. It all lives over here in this Serverless project, but the custom authorizer gets called at run-time when your content service does. And then the front-end which is where all the integration happens, so the front-end goes and hits the Facebook login endpoint and sends that information back to the Serverless lambdas that get you logged into FaunaDB. And that hands the authorization token back to the browser and it runs the queries. So there's definitely some session glue that happens in the browser. So I'll go through these. This isn't all the lambdas in the content service, but it's enough to kind of show you the different things that we're doing in there. So I'll talk through them and then we'll go look at the code, because I wanted to show you the code in my editor, not put codes onto the screen. It's more fun to see it in the editor. So you create a Todo item. You type something into the UI and hit enter and it does an HTTP post to the Serverless endpoint and what we're gonna do is write that to the database, but before we write it in, we're gonna tag it with a current user and we're gonna grant that user the ability to read and update the item. So we actually apply the access control rules on the way in. We write the item with the permissions of who's allowed to mess with it. Then when we read all the Todos, we used to have a naive query that says give me all of the ToDos and it's actually the access control system that says, \"Well, no, I'm actually just gonna give you your Todos.\" So you can see this is a security-naive code. It doesn't care about any of that policy stuff and it still does the right thing. Well, with update, you already know what you're gonna do, so I'll just show you how we swap fields out on an object so you get a sense of how the database works. And then toggling all is a bulk operation. It's a lot like a read all but instead of just finding all of my Todos, it loads them up and then switches done to not done or vice versa. So we'll take a look at those. We are creating a Todo. And I'll skip some of the noise and just go to the FaunaDB part. What we've got here is, we're grabbing this little atom of query language stuff. It's kinda neat. So this doesn't execute in terms of going to the server and figuring out who this reference is and getting them and selecting that field off of it. Instead, the return value of all this is just some abstract syntax tree of the query. And so now we just have a little query item in this me variable. So we're gonna say the Todo.user is me and we're also gonna say the permissions involve me. And, essentially, that's just taking that query snippet and dropping it in all those places. So when the query runs, the server runs all this code, rather Fauna runs all this code and doesn't know that I used to have it decomposed into a variable. So that's one of the fun things about having essentially a query builder as your SDK because then you can use these little query built things and reuse them. Your question? Person: Yeah, are other query builders in other languages? Chris: Yeah, so we've got most languages already covered and they're all on the website. We develop all our own drivers. We have some community folks who are interested in developing drivers and when they get to a certain point we're probably gonna adopt them. Person: So you can use this with...like, on the server as well? Chris: Yeah. And this is running in the lambda. Person: Okay. Chris: You could write this and have it be in the browser. Our whole dashboard is just the browser talking directly to FaunaDB. So that's an option, too. Question? Person : So where do you put in the region here? Chris: So it's nice that we don't have to worry about it. I'll show you my whole server list to YAML. Hey, it looks like there's nothing in there about FaunaDB. The closest there is to anything about FaunaDB is the authorizer ARN. And so you set up this authorizer and that's what passes the fauna secret back to the, you know, to instantiate the client. I hit in the client for event logic over here in utils, but all it's doing is parsing out that policy document. Person : So, as a user, I still have to go to FaunaDB and provision something? Chris: As a user, what I had to do for this crowd service we're looking at now, that works already done, so the only time I had to provision something was when I was setting up the authentication service and then in the environment here nobody memorized that. So that's the secret that has the ability to delete all the Todos and all the users, right? And so you had to go create the database with the users and Todos lived in and then create a database secret for that database and hand it here. If we have time at the end, I'll talk a little bit about how I managed the schema in this stuff, but that's also the secret that's gonna run the setup schema code. Person : So no region? Chris: Yeah, all the regions... So, in the future, right now we run all our data with the same availability which is all the availability, but in the future we're gonna have dynamic site selection where you say like, \"I wanna be US East and US West and that's all I need.\" Or, \"I also need Australia,\" or something. Person : Yeah, but I love that I don't have to think about region. Chris: Yeah. You get to if you want, but you don't have to. Person : Right. So my next question is, what's the response time? Chris: It's fast enough. I mean like, I'm gonna say we are beating the competition at benchmarks. Person : Is that Dynamo? Chris: Dynamo, Cassandra, in general. And core team can probably talk to it better, but the consistency model that we used is oriented towards batch throughput, and so you end up with just having...being able to process lots of data and slowing everything down when there's contention. So yeah, let's go look at this read-all because you get a sense of what can happen on the server. So what I'm doing is I'm looping. First, I load all the Todos. In this case, they all fit in one page. So production code, I need to hit all the pages of this. But I'm loading all the Todos and...or rather, loading all the references out of the index, looping over it and then, for each one in the loop I'm going and fetching the actual instance. And so the result set is just a list of all the instances. And we'll see that loop construction again. Not here, here's a simple update, but we'll see an update inside a loop after this. So, in the simple update, we see all our data is in this top level data field because there's other top level fields you don't need to worry about but sometimes you might want. And this is just allowing the user to change the completed state and the title of their Todo. We're not allowing them to change who the author is in this code. And the reason that I wanted to start with the content service, just because then you get to see these Fauna queries without all the complicated authentication requirements and stuff around it. And so this is about as complex of a query as you'll see today. And it's the same queries you saw before. We're grabbing all the Todos and we're looping over them. But then instead of just returning them, we're loading it and then making the completed field be the opposite of what it used to be. So if it was true, it's false, and vice versa. And that's the only field we changed in this update. So we just go touch all the Todos and flip the bit on there, completed. So that's all the code in the content service that I want to take a look at. Are there any other code questions? Person : So the query that you're building there, is that... Like, should you be building a representation of the query and it's actually sent to Fauna, it doesn't map in there? Chris: That's right. All this looping happens on the server and you have loops and branches and all that good stuff and it all runs inside the transaction. Person : I see. Person : And I'm just looking at the data and internal...? Chris: Yeah, all your user data lives under here because this could be time-stamped or something, like there's some top level fields that aren't user controlled, so you might want to index or update. I guess you can't update them, but you might want to select them. Everything lives in the data name space. Person : So you provide that data or that envelope just out of the box? Chris: Yeah, so the time-stamp is built in out of the box. We don't do auto-increment but we do... What's the name of it for the ID service? Person : Snowflake. Chris: We do a snowflake-style ID service and then there's metadata saying what the class of an instance is so we can use that to look up the ACL rules and stuff. Person : So in Dynamo, you have to set all this up, too? Chris: Then the time-stamp will automatically... You might not want to use our machine time-stamps for your time-stamps, but we use our time-stamps for the internal temporality tracking and all that. So I found this great blog post about basic app architecture and stole the image to sort of show the difference between the status quo and what we're doing in this app with Fauna. And the main difference, so you've got the browser, it talks to all these lambdas via the API Gateway. Let's not worry about what order they go in. But in the status quo, the user service talks to a dumb database that doesn't know anything about your application rules, and the content service talks to whatever, right? And in the app that we've got, we've got both the content service and the user service talking to the same database so that they can share these business rules and whatnot. So has anyone here written a Serverless app that uses that authentication boilerplate? It's probably the easiest way to add login to your app. The thing that took the longest was going onto Facebook and getting a Facebook app ID. But this is how it works, this is what your code's gonna do. We can be the code for a minute. So you go to the app, you click login, you get sent to this sign-in lambda. The sign-in lambda has to look up some environment stuff in lambda-land like whether or not this app wants you to sign in with Facebook or Google or whatever. And assuming it's Facebook, it redirects you to Facebook which does its thing and decides who you are and redirects you back to this callback lambda. And so the callback lambda then has your Facebook user ID, essentially. And what we do is we find or create a user that's in FaunaDB. And by a user, that's not like some special Fauna class, it's just like, I happened to call it class user, I could have called it people. Any instance in Fauna can have credentials and access control, so you can actually have different classes of things that can log into your app if you want. So, in our case, we happen to have a class called user and it has some credentials, and so we're able to get a secret for it. And the callback then packages that secret up as an authorization header and that goes back to the browser. So now the browser is equipped to talk to the content service. So when you go talk to the content service, you're gonna make an API request, like, \"Give me all my Todos.\" And the content service goes to this authorizer. Forgive me, the arrows had to be like this because I just couldn't make sense of it any other way, but the authorizer is really a wrapper around the content service. So the first thing that happens when you call the content service is the authorizer unpacks your authorization header and makes sure that the content service has the unpacked version of it or rejects the call altogether if it's been expired or something like that, or the roles don't match up. And so now the content service has this user...this FaunaDB that corresponds to the current user and it can do all its database work against FaunaDB with only the credentials that it should have and no super user powers at all and then sends that response back to the browser and everybody's happy. Except for I have a bug right now in my particular setup where, for whatever reason, this thing that happens when you get logged out because you were inactive for long enough, it's just happening all the time for me. I need to get to the bottom of this. And so it refreshes, and what that means is that the browser goes and tries to do what we just saw, but instead it gets access denied and the content service is like, \"Oh, I got access denied,\" and the browser's like... It's not that bad because it has this refresh joke in that it got back early on in the process that it's just been sitting on for a time like this. And it sends the refresh joke into this refresh lambda, which is nice because the refresh lambda doesn't have an authorizer in front of it. It doesn't have any other functionality except for converting tokens to headers. So the refresh lambda goes and looks up the user by the token and gives a new secret to the user back and resets that authorization header and now we're back here, making API requests. Person : So the authorizer applies the policy of which functions you guys..the policy of which data you can access? Chris: That's right. And in my particular app, there's all these authorizer features that correspond to stuff you can and can't do in API Gateway and whatever. Mostly, it didn't matter for me. Mostly, it was the data stuff. So I'm doing the simple thing, but there are a ton of complex features and you can make the authorizer grant different things to people with different Amazon roles and all that stuff. But at the end of the day, in this system, you're gonna either attach a FaunaDB secret to that policy or not and do the database work with that policy. Person : Okay. Person : So how do I configure lambda functions to listen to FaunaDB events or downstream stuff? So the user is signed up and now I want to send them a... How do I do that? Chris: Right now you're gonna have to do that the ugly way. We're working on web hooks, essentially, which would be the way to have Fauna light up a lambda instead of having something listening to Fauna light up a lambda. Person : Cool. Person : Apparently, you used lambda streaming and triggers, they used to be called triggers... Chris: So let's see if it still works. Should be no maintenance, right? So I'm all logged out and this is the application and this is where we keep the embarrassing stuff. And I'll log in and you'll see... If you're fast about paying attention to the stuff going down here in the corner, you'll even see that says sign in, that corresponds to my sign in lambda, it'll blink callback just for a second, there will be a Facebook URL that pops up. So I'm gonna click and I'm gonna define the word width. So we're waiting. Things are slower than they ought to be for a whole host of reasons, I'm on the slow Wi-Fi right now, stuff like that. Unless we actually hit time out, then it's gonna succeed eventually. So here we go. Here are the Todos. I can make a new one, you can see that it's running that refresh loop in the background because of my bug that I need to fix. So I'm not gonna fix that tonight, but what's essentially happening is that it's making kind of four requests for every one that it should be making, which makes it seem slower than it really is. But once I get to the bottom of that, what I love about Serverless, coming from a different background, is the maintenance costs. Once you've got the bugs down to zero, you can walk away and never look back. And, to me, that's really exciting, because if I still had all the cool stuff that I had deployed over the course of my career, I would have a really neat portfolio, but instead all I have is memories. Person : So when you can send it out, you don't care which site is making callback to the conference server, doing that all navigation stuff again, but then it waits for confirmation from the conference server or adding it to the UI? Chris: Yeah, I'm just doing the simplest UI thing, so basically every time I change the database I do a full query and repaint. But, yeah, it really should like, be way much faster if it wasn't churning through Facebook on every request and stuff. Maybe some Serverless folks can help me debug this core's caching thing. And, yeah, you're welcome to try it yourself. All this is open source. I'll have the links at the end for you to go download the projects and deploy it yourself. There's a blog post on serverless.com that'll get you up and configured with a running Serverless authentication boiler plate and FaunaDB with an even simpler content service, just a little test content service, but if you walk through that blog post, you'll be ready to write a real app on FaunaDB that has all your data access control stuff happening. So I did two different things, how I managed my schemas as an application developer building this stuff. On the authentication service, because the authentication service is already configured with a powerful secret, I just made a lambda that doesn't have any API gateway access configured. And so you have the lambda invoke it and it sets up the schema for you, creates the indexes, that kind of stuff. I think that's good, but then I also had this crud service that doesn't have a privileged secret, right? It doesn't have any secret. And so, instead, I just had a privileged secret on my workstation and I hit the cloud and set up the schema that way. And I thought that was kinda sketchy, like I guess I wasn't used to thinking like that at first, but then after I did them both, I feel like they're both equally valid. If you have a big org, you'd wanna know which workstations you trust for that sort of thing, but that's a different problem. So yeah, it's nice to have options. It's one of the nice things about the cloud is that it's not gonna yell at you about where your firewalls are and stuff. So if you wanna try this stuff out, you can go ahead and get up and running and have a database with data that you created in it in a minute and a half on fauna.com, maybe faster, because last time I timed it we were still making you come up with a password. And then the crud example is Serverless crud. It's been around forever as example code talking to other back-ends. I wrote an example that talked to FaunaDB using a privileged secret, using the boring old school patterns and then I rewrote it again for this using the database access control pattern. And then this... That's our fork of it, you might actually, when you get there click and go to the original fork because our PR has been merged and improved since then. So yeah, that's all I've got for today except for QA. Thanks. Person : The pricing of FaunaDB? Chris: It's supposed to be very easy to reason about. So if points... And you get points for a dollar. No, you get points for a penny and a point is roughly what it costs to run a simple query. Person : Is there a free tier, free points? Chris: You get a sign up pack with some points, so we basically don't start trying to email you to get you into the billing system until you've used all those up. Person : $ worth of points? Chris: Yeah. Person : Which is a million points. Chris: It's enough to let you develop for a few months or run in production for a few minutes. Person : So DynamoDB is like cents for each read and write throughput and it charges you regardless of whether or not you used it of course, not the old archaic model that's going away soon. This is like paper execution. Chris: Yes, paper execution for sure. If you're like extremely highly utilized, that's when you may want to move to our on premise, because if you're running full bore all the time, paper and the machines that you're running on, you'll save money. The off-premise licensing is by Core. So yeah, probably talk to Evan more about that if you're interested. Any other questions? Great, well thank you.",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e1e",
      "title": "Introducing Serverless (Cron)icle - News from the Serverless Ecosystem",
      "content": "The open source Serverless Framework is backed by a super smart and extensive community of developers (we've passed , stars on GitHub!) Besides contributing code, our community is passionate about sharing their knowledge and expertise. We're going to start sharing a roundup of some of these community links to help you keep up with news in the serverless ecosystem. Check out this week's list below: What Serverless and the Internet of Things Can Learn from Each Other \"Both are seeing massive interest from developer communities and enterprise, eager to implement solutions, and both need to learn how to support developers and systems integrators to adopt a fast moving technology with a rapid pace of innovation that means there are new things to learn every other week.\" Read more about what lessons the two fields can exchange on The New Stack here. By Mark Boyd. My AWS Wish List for A serverless developer at A Cloud Guru (the masterminds behind ServerlessConf), John outlines his wish list for Lambda, API Gateway, DynamoDB, CloudFormation and more. Check it out on the A Cloud Guru blog here. By John McKim. Virtual Panel - ServerlessConf Debriefing In this conversation, Krish Subramanian from Rishidot Research talks with Erica Windisch, Benjamin Ball, Matt Weagle, Pete Johnson, Lee Calcote and Kevin McGrath about their major takeaways from ServerlessConf Austin. Watch the video here. From Rishidot Research. Serverless: How We Bootstrapped Our Startup by Skipping Half the Work Learn how the founders of the Moo.do task management app were able to bootstrap their startup by going serverless. Read about their process and lessons learned in this Entrepreneur article here. By Jay Meistrich. Mastering Chaos - A Netflix Guide to Microservices In this video from QCon San Francisco former Netflix engineering lead Josh Evans explains the anatomy of a microservice, and the challenges and benefits of distributed systems. He goes on to explore the cultural, architectural, and operational methods needed to master microservices. Lots of themes that are still relevant today. You can watch the video here. From Josh Evans.",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e20",
      "title": "The Multi-Provider Future of Serverless Application Development [Video]",
      "content": "Serverless Meetups are a gathering place for anyone interested in building web, mobile and IoT apps with serverless or event-driven architectures using the Serverless Framework or other tools. Writing a serverless function is easy. But deploying that same function to multiple cloud providers can seem impossible. The serverless community in SF recently gathered for a discussion on how developers are overcoming provider lock-in, creating portability and freedom in their Serverless projects. Serverless, Inc. Product Manager Brian Neisler gave a talk covering some of the main challenges behind multi-provider support for serverless functions and what the Serverless Framework team is doing to solve them. Watch the video Transcript Brian: Thank you, everyone, for coming out. These Serverless Meetups are always a really good time, and you learn a lot of really cool, new technologies. So hopefully I can shed a little bit of light on something that's neat and interesting. Definitely, if you have any questions afterwards, feel free to come up and chat with me or find me in the back. So I'm going to be talking tonight about multi-provider Serverless and some of the challenges associated with multi-provider - specifically around writing multi-provider functions and what this means. What this opens the door for. This is really our vision of where we see both Serverless and multi-provider support going. So first, a little bit about me. Casey already mentioned this, but I'm Product Manager over at Serverless. There's my contact info. Feel free to take it down. Reach out to me if you've got ideas, questions, you just want to talk about Serverless in general. Always interested in meeting new people. Alright. So Serverless. The serverless movement kind of speaks for itself. It's really taking off. Lots of clear interest, especially since . Weve seen a massive uptick, definitely a wave of new technology emerging. So the question is, what is this all about? I think some people definitely have this kind of core question of, \"Hey, what's going on over here? It sounds interesting.\" Hopefully, I can shed a little light there. Some people are calling it the next step in cloud computing. It's the natural evolution. We went through virtual machines and Platform-as-a-Service (PaaS), containers. Now, we're kind of touching on this next thing, which is serverless. But I think it's a little bit more than that. What it really is, is this idea from the developer's perspective that, \"I just want to create without restriction.\" I think we all feel this way. That we want to be able to introduce new things into the world and not be prohibited by boundaries, not be prohibited by just the general idea of having to think about, \"How do I get this out to as many people as possible?\" It should just naturally happen. I think we're all, especially in this room, feeling that that's the way that development should occur. So in a lot of ways, we're kind of moving past scale. We're heading into this era of optimization, which is an interesting one. Because instead of thinking about, \"How am I just going to support as many people as possible?\" now we're really thinking, \"How do I optimize for specific use cases? How do I get my business logic, my data, as close to the use case as possible?\" Sometimes that's the end user, but then it also might be data processing or any other type of backend-style applications. So this is a kind of basic example of what I call the \"optimization problem.\" Right? You have your end user, your function, and then the data and the third-party services that it needs to interact with. There's all these things that we want to be able to optimize for; we want to be able to optimize for capacity, latency, region. Sometimes whenever you're dealing with certain restrictions, those types of things - the provider, cost, security - these are all things that we have to take into account when we think about optimizing where our business logic lives, where is it running. So I think the general value proposition that we're talking about for serverless and kind of where serverless as a whole is going is this idea that you can offer a developer the lowest total cost of ownership. We can take away the problems of scale and optimization, and security, all the things that are usually really difficult to think about, and allow them to be able to have highest output. Really give them that ability to be able to innovate, have productivity, be able to create new things. Again, that's, you know, what we all want. But we want more. We also want freedom of choice. We want the ability to be able to determine where this thing is going to run, who am I going to be depending upon, and not to be potentially locked into specific providers. So that's kind of the theme of the talk tonight, about how do we kind of remove some of those restrictions and give us that freedom of choice to allow us to be able to move between multiple layers, or multiple different providers, and effectively run our code where we want to. But before we dive into that, for those of you who might be newer to serverless, let's do a quick little recap here. What is serverless? So this is the definition we like to use with serverless, though the servers exist, yes, there are servers there, the developer does not have to think about them. That's kind of the way we've been thinking about this conceptual serverless concept. Again, removing that concern and need to have to think about these things, and instead offloading that to these larger providers and other systems, to be able to handle some of these complex problems. So serverless qualities, these are the things we like to associate with serverless in general and the general concept of serverless. So microservice, server administration, event-driven, built on top of powerful platforms, and a real key one here is pay-per-execution. This is something that is huge for developers, this ability to be able to only pay whenever your code is actually running. That opens so many doors for people to be able to build new and very interesting things that originally weren't even feasible. So I think this is a real key kind of component to the quality of what serverless offers. General, just quick serverless architecture, I think everyone is pretty familiar with this. But again, the general concept of Lambda is this idea of infinite scale very quickly, and that it's on demand. So you're only paying for what's executed, handles bring it up and bring you down, all very, very fast. Serverless compute providers, so there are a lot of obviously big companies in this game. We're talking about, obviously, AWS. But Microsoft, Google, IBM, these are all major providers that are entering into this space and giving us more options. So that being said, the question that we oftentimes get is, \"What about lock-in?\" How do we kind of overcome this problem of being locked in to a particular provider? So it's an interesting question, and I think it's definitely a valid concern. It's one that we hear a lot whenever we're going and talking to customers, and just talking to people in the industry in general. But I want to make a clear distinction here. I think something that's missed when we talk about this is that there's too different types of lock-in that we're referring to here. We're talking about compute lock-in, which is this idea of, \"Where's my function actually going to execute? Is it being locked in to a particular provider, and then incompatible with other ones so that I would have to actually make code changes in order to be able to move it to a new location?\" And service lock-in. Service lock-in, giving it a little bit clearer of a definition here, but that's the idea of being dependent upon a third-party service. Utilizing a third-party service within your function creates a form of lock-in. Right? You are now dependent upon that service. So there's two different types there. So compute, lock-in, kind of inability to potentially take one function and move it to another provider, and service lock-in. Again, you may have a function that does work appropriately across multiple providers. But again, they all make use of that same service, and so therefore, they are dependent upon that particular service. So there are different problems. Some of these problems we can solve. Some of them we can't. It's just kind of the way of things. So this one specifically is about solving compute lock-in. How do we get to this point where I can write a function and easily deploy it to any potential provider? So our solution here, we can solve compute lock-in by creating what we call \"provider-agnostic functions.\" It's the idea that I'm writing a function that is in a format that can work inside of any provider. So you can take that single function, move it over to another provider, and it will work in the exact way that you would expect it to. But we're still bound to services in this area. This doesn't solve the service lock-in problem. It is just a solution for the idea of compute lock-in. So provider-agnostic functions. There's a few problems associated with provider-agnostic functions. James Thomas from IBM gave this really great talk called \"Taming Dragons.\" He referred to each of these problems as basically a dragon to be tamed. As I think it's a pretty apt description, there are some definitely, rather complex problems here to try and deal with. But I highly recommend checking out this talk. He did an excellent job of kind of breaking down these problems. I'll walk through the core basics of these things here, but I definitely can't do his talk justice. So some of these problems can be fixed. Interfaces problems, so this is the basic idea that functional interfaces are different across providers. Each provider expects a different function format, and this is kind of like one core problem to being able to take a function, and then cleanly and easily be able to move it over to another provider. So the interface solution, this is part of what the Serverless team has been working on is this idea of we want to be able to effectively wrap or provide the kind of interface point for your function. So that we can have this kind of universal function format that you can then convert from the provider into this format. That would give you a way then of being able to take a single function and deploy it to any provider, and that kind of conversion layer would do what's necessary to be able to bring in the parameters appropriately, and to be able to handle the response appropriately that comes out of that function, both in a SyncEnd and asynchronous way. This is kind of a little sneak peek at some of the general concepts of what we're working on currently on the team. So there's also an events problem. So this is the basic idea that event shapes and available events are different across providers. So there's a little bit of two different problems here. The general idea of event shapes being different is that an HTTP event that you get inside of Azure might look a little bit different than what you get inside of AWS or what Google gives you, etc., and the idea that each provider has different offerings. Right? So therefore, there are different events associated with those different offerings. I would argue that that's a little bit similar to the service problem. If we have customer events that are specific to a particular service, there's not necessarily a clear solution for that. But for the common ones, we can offer a common event shape. So here, we're talking about again this idea that, through this wrapping layer, we can take in the inbound events, identify the types, and correctly convert them into a shape that you would expect. This way, whenever you're dealing with that event, you can write your code in one way and it will work appropriately across all providers. Some problems, though, are harder or don't have appropriate solutions. So one of these is the runtimes problem, for instance. This is an idea that providers offer different languages and different versions of those languages, and this is something that is definitely more difficult to overcome. But there are some partial solutions. Right? We have a few tricks in the bag that you can use to kind of overcome these particular issues, especially things like transpilation, shimmying. I was talking with Nick Graf [SP], who was in the audience earlier today, about whether or not we could shim up or down, depending upon if you wanted to run a different version of Node, or a different version of whatever language you needed. So there are some kind of interesting, conceptual ideas there, but again they're not really full solutions. But I think the providers are definitely keenly interested in this area, and are trying to move forward, keeping their environments up to date, keeping the language versions up to date, offering new languages, etc. So this is a problem that is likely to get a lot better on its own. The SDKs problem, so this is a fundamental concept that the providers' offerings can be accessed through their SDKs. Right? We have this ability to be able to make use of the providers' functionality through the SDKs that they give us. But these SDKs are different on a per provider basis, obviously, because they have different offerings, plus the idea that they just provided you their own code. So in that scenario, there, again, a couple of partial solutions. So for the interfaces issue, for the common types of functionality, like function invocation, if you wanted to be able to invoke a function within the provider, we can offer an SDK that would allow you to be able to correctly invoke against the correct SDK. So it can kind of handle that conversion. But again, different offerings, this is really the same problem as service lock-in. We're depending upon a third-party service or a managed service in this scenario that the provider is offering. So again, that's kind of the same problem in this scenario. So the question is, what do we do about service lock-in? I would make the argument that third-party services are here to stay. They're not going anywhere. In fact, if you think about the kind of momentum associated with just the general ecosystem altogether, it's more likely that more services are going to be introduced. Things are being created kind of faster than ever. I mean, the whole point of serverless is to make it easier to write more services. So we're only going to see more and more of these things get created. In fact, I think we're going to see much more sophisticated services being created. The fact that we were able to remove some of these hard problems opens the door for people to be able to build much more complicated things. These complicated things are clearly things that we will depend upon, and we want to be able to make use of these things. We get some questions about this idea of, \"Well, can we abstract? Can we find the common overlap, and then abstract those types of things, downward?\" I don't necessarily know that this is really a solution. You're kind of attuned to the idea then that you're really just getting the lowest common denominator of functionality, and that's not necessarily something that I think we all want. Then, oftentimes, the devil is really in the details there. We're talking about little things that we can do to either optimize, or little bits of functionality that we want to be able to take advantage of and that we wouldn't be able to through abstraction. So we don't really see that potentially as a solution. But what we can do, we can look at solving this optimization problem. We can try to make it very easy to be able to inject your functionality and to make use of these third-party services, and bring your functions to that correct point of optimization. We can focus on, again, looking at capacity and region, latency, cost, security, provider, all of these things, and kind of finding that sweet spot of getting your function at the perfect point between your end user and the things that it needs to actually be able to operate on. A lot of those third-party services kind of fall under that umbrella. What's interesting about this problem is that, especially going back to the idea that you're only paying for whenever you're actually running, is that some of these problems actually get very easy to solve. We can, in all reality, just deploy everywhere. We can take our function and put it across all providers, and all regions, and not have to worry, actually, about the fact that it's been deployed everywhere. Normally, that would've been a nightmare and extremely costly. But nowadays, this is actually a real approach. So this kind of opens the door for us to really be able to say, \"Hey, here's how we can effectively handle all regions, remove the kind of concerns, and make regions be something that's agnostic, and handle multiple providers, etc.\" So that your function and your kind of core functionality can get as close as possible to the services that it's making use of. I think that's where we, in general, can really see these things going. So that's my general presentation. These are some hard problems. If you want to help us solve these problems, we are hiring. Like we actively need really good developers, and we have a lot of really good developers on board. But it takes an army to solve these problems. I think, these are all ones, again, going back to the idea that we just want to be able to create and we want to be able to do it without restriction, and I think that's really what we're out to do, trying to make that a reality. Any questions? Anything I can help shed some light on? Person : I'm curious about the runtime problem. Where you might have different providers offering different versions of languages, or libraries. What do you think about using containers as a layer of abstraction there? Brian: Yeah. I mean, when you're talking about running Lambda functions within kind of a compute provider, clearly containers, I think, is an option for if you want to run your own container system. Right? But it doesn't really open that same door, at least not unless I'm missing something. But it doesn't really open that same door of being able to easily just hand over that code to a provider and say, \"Let them run with it.\" So that's why the shims, I think, are an interesting approach. Clearly, I think, there's obviously some great work going on in the containers area, which kind of solves that problem. But a lot of these problems are very provider-specific and they kind of have those systems locked up. So it becomes a much more harder problem to solve, since it's no longer within the developer's control. You're giving that control over. Person : I think the difference with Lambda, why Lambda got so much attention is just it spins up so quickly. You can't do that with containers. They're heavy. Lambda spins up fast enough to handle HTTP requests and return a response. That's one of the big benefits of using Lambda. Runtime, I wouldn't say it's that bad, actually. I mean, all the runtimes are pretty consistent. The difference is, maybe if you use Lambda, the AWS SDK is there. Brian: Right. Person : But you're probably not going to be using AWS SDK on Google when you provision a Google Cloud Functions. So I don't think it's a big problem. Me, personally, I love the value of containers. I get it. Affordability, having that perfect consistency. I just don't want to think about that, actually. Brian: Yeah. Right. Person : I don't want to manage the lifecycle of containers, personally. I just don't want to care about that stuff. I want to Runtime as a Service, have pretty strong consistency. You know, if you're using Node.js . on AWS Lambda, Node.js . on Cloud Functions is going to be pretty consistent give or take an SDK or something like that. Brian: Yeah. I think one of the interesting spaces there is this idea of, right now, clearly, we see a pretty wide division in terms of the languages that are being supported by different providers. But one of the kind of cool aspects of becoming provider-agnostic is that you can actually make use of multiple providers at the same time, today, to handle those different languages for you. Right? So by being able to write these kind of provider-agnostic functions, you can then have this bulk of functionality, which you can easily deploy to a different provider in order to give you that language that you need. This opens up, I think a lot of doors. Person : Yeah. Brian: So again, it's that idea of, if we can get away from having to be concerned with, \"Which provider is going to run my compute and where is it going to land?\" instead, just like, \"Hey, I want to run on C.\" Like, \"Okay. Go find a place to run C or F, great.\" They're like, \"This is supported over here,\" and you can literally just go and it would figure out where to put it for you. I think that's part of the vision that we see coming down the road. Person : When we made the Serverless Framework, it was amazing how many people from the container community immediately came over. We didn't know this was going to be that appealing to them. But the reasons are kind of timeless. Like they just want to manage less. They don't want to think about containers anymore, or have any cognitive overhead related to containers. So what the future looks like to us, I think, it's a lot of serverless functions. It's containers for certain use cases. Functions-as-a-Service do have limitations still, like a five-minute maximum duration and language limitations, and stuff like that. But it's incredible, these qualities that Brian put out there: micro-services, event-driven, pay-per-execution, zero administration. Amidst these big platforms, we have all these other services we can work with compute. It's the lowest total cost of ownership, a.k.a. so convenient, that people are using them increasingly for every use case under the sun. It's that convenience that brings all users and use cases to the table. So in terms of the future, I think containers will be a part of that, I think a lot of serverless functions. More serverless functions than anybody anticipates, too, because all these companies, we see these serverless teams growing in each company, and these teams are like two people, two developers, and they're provisioning hundreds of functions. Right? Because once they provision a function, they're done. They don't have to operate it anymore and really worry about it, or pay for it until it gets spun, crazy productive. So I think the future is kind of like containers are part of that, lots of serverless functions, more than anyone kind of anticipates right now, no APIs. It's a mesh, you know, of architecture. Brian: Any other questions? Oh, go ahead. Person : So I like the idea of solving this lock-in problem. I have been dealing with it a bit for databases. So when you provide the rack you are actually [inaudible ::] your own locking in that. Right? Or maybe not. [inaudible ::]. Brian: Yeah. That's a fair question. So part of what we're trying to do to avoid that being the case is that, our wrapper we are open-sourcing. So this is something that you can take in open source, and you can literally go wrap your own code, and then just put it wherever you need to. So it's not meant to be, clearly, a form of lock-in. The other thing that we're really trying to do is to also bring you down to kind of the root fundamentals. It's this idea that a function at the end of the day is arguments, and it's return type. Right? If we can get the behavior of these functions to behave in exactly that way, then I think we can start to kind of stop having these very specific function signatures, and instead being able to use them in kind of any way that we desire. So I think there's a lot of things there that we're pushing to the open source community, because we understand, clearly, that's a concern. Person : Lock-in is such a weird thing. Like it's natural. Right? As humans, we're locked into the Earth. The Earth is our platform. I'm locked into oxygen. The thing we talk about a lot is we hear people characterize this as lock-in and stuff, and we understand it's about concern, especially, for the enterprise, of course. It's not a different field to be able to diversify. That's the a huge problem. Person : The open source might not be the ultimate solution for the lock-in problem. Because the open source tool, whoever want to use it for free, at the same time, they might not have the capability that you guys do. So eventually, you still have to get a service, some kind of service lock-in behind this open source. So it's still a different layer again. It's a chicken-egg problem. Brian: Of course. Person : I'm not saying you're not doing a good job. But it's kind of like a philosophy issue. When I was trying to deal with lock-in of the current situation, you're always ready to... It's just how you say it. Person : Yeah. I mean, what we have seen at Fauna, it's like right now, if you use something, you've invested in it. In particular, at the data tier you choose to get locked into proprietary software and the specific infrastructure provider, which doesn't make sense. Like there's no reason... They have the software like Google Spanner... is not actually specialized to the hardware. So at Fauna for example, Fauna is a service, you can also [inaudible ::]. You can choose which cloud providers you want your database has to be provisioned in. So you can pass directly through your functions to the regions you would be in [inaudible ::]. Person : I see you are improving user experience by... Instead of saying, \"You're not getting [inaudible ::] lock-in computing,\" you're just improving the user experience. That kind of explains what [inaudible ::]. Person : Like unless you use physical machines that you go in and can build yourself, I guess, you can't ever get out of lock-in to something. Person : You can increase development choice by an enormous amount by removing these artificial [inaudible ::]... Person : But eventually, it's a user experience or performance you're exposing to the user or you're enhancing over time. Person : Yeah. Like Amazon doesn't own JavaScript. That shouldn't be the only place you can write node. They don't have disks either or keep a database there. Why [inaudible ::] be made inside the infrastructure? Person : The user experience is one way of characterizing it. I characterize it as possibility. Because the interesting thing about the serverless function is zero administration and pay-per-execution. What you get out of that is you can start putting functions everywhere, across multiple providers. I mean, why not? To replicate it across regions, just like Brian was saying, deploy everywhere across multiple providers. What you get out of that is these big cloud providers are offering so many other high-value services, machine learning services, storage services. The easiest way to adopt a cloud provider now to gain access to those other services is just put a function there. It's not going to cost you anything until it runs. Put functions everywhere. Why not? So we talk about it in the context of lock-in and we try to chip away at that problem. I don't know if it can ever be solved, but we chip away at it. But I like to refer to it now as just possibility. As a developer, I just want to use every single tool under the sun to solve my damn problem. I don't want anyone telling me, \"You can only use the tools in the Amazon platform,\" and stuff like that. No, no, no. Let me just put a code wherever I want, and from that code, I can access all these great services and stuff. We get this regular request like, \"Oh, we love Lambda. It's amazing. But we really want to use Google's machine learning stuff, because they're doing the best at that right now. We wish we had a great alternative to DynamoDB. Does anyone know an awesome service [inaudible ::]?\" Person : One more question about hiring for Serverless. Do you have remote employee, or can you talk a little bit about... Brian: Yes. Person : Yeah, we do. If you want to come speak with myself, Brian, Austin, or Nick after this, we're happy to give you some more information. Thank you. Person : We build a company like we build software, everything, product strategy, values, all that stuff. Regardless of role or location, you can have an impact by just submitting a PR in the company repo. You can change the whole company overnight. Person : Thank you, Brian. Did you have any last? Brian: No. If there are no other questions. Thank you.",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e22",
      "title": "Serverless (Cron)icle  - News from the Serverless Community",
      "content": "The open source Serverless Framework is backed by a super smart and extensive community of developers (we've passed , stars on GitHub!) Besides contributing code, our community is passionate about sharing their knowledge and expertise. We're going to start sharing a roundup of some of these community links to help you keep up with news in the serverless ecosystem. Check out this week's list below: Perspective on multi-cloud (part of ) Multi-cloud is a hot topic in serverless these days. In this series Heptio CEO Craig McLuckie shares his perspective on cloud providers, decoupled architectures, provider lock-in, and the tension that exists between private and public cloud deployments. Read more on the Heptio blog here. By Craig McLuckie. Perspective on multi-cloud (part of )  On premises plus/vs public cloud In part of this series, Heptio CEO Craig McLuckie takes a deep dive into the characteristics of on-premises deployments vs. public cloud deployments. Continue reading the series on the Heptio blog here. By Craig McLuckie. Taming Dragons - aka Multi-Provider Serverless Apps James Thomas, from IBM, talks about the challenges related to building multi-provider serverless apps and how he's working to overcome them. Check out the video from his talk at a recent AWS Meetup in the UK here. By James Thomas. How to Build a Serverless, SEO-Friendly React Blog Learn how to build a serverless, CMS-powered blog using React, ButterCMS, and built-in prerendering through Netlify in thi tutorial. See the full tutorial as well as links to the code on GitHub on The New Stack here. By Roger Jin. Announcing the new webpack CLI Webpack recently updated its CLI for an improved user experience. Their goal was to make it easier for developers, especially beginners, to get started. See more details and the roadmap on the webpack blog here. By Evan Stensberg",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e24",
      "title": "Getting Started with the Serverless Framework [Video Tutorial]",
      "content": "Learn the basics of getting started with the Serverless Framework in this brief tutorial series from Serverless lead front-end engineer Nik Graf. Learn how to setup the Serverless CLI and credentials, deploy your first function and attach HTTP endpoints. Watch Videos New to the Serverless Framework? Check out the docs for more info and then get started here.",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e26",
      "title": "How To Build A Serverless REST API with AWS Lambda & DynamoDB [Video Tutorial]",
      "content": "Learn how to build a serverless REST API using AWS Lambda, Serverless Framework and DynamoDB in this tutorial series from Serverless lead front-end engineer Nik Graf. This is one of the most popular use cases for the Serverless Framework. Watch Videos New to the Serverless Framework? Check out the docs for more info and then get started here.",
      "__v": 0
    },
    {
      "_id": "64e08918b72e199dda603e28",
      "title": "DevOps Use Cases With Serverless Variables",
      "content": "The Serverless Framework has a powerful built-in variable system that helps secure your sensitive data and can be used to keep even the most complex configuration simple and manageable. We have pretty comprehensive coverage of all of the features of the variable system in our documentation, but when combined together in creative ways these features enable some extremely powerful workflows and real world use cases. In this article well explore some ideas on how you can use these features to better automate your serverless operations. Dynamic Configuration You can use the variable system to dynamically generate configuration data for your config files with the help of JS scripts, which is normally not supported by YAML/JSON. Some examples include generating random numbers, dates, doing mathematical computation, or fetching remote data. Lets look at two of these examples.. Generating Random ID Lets say you want to create a bucket with a unique id attached to the name. It wouldnt be possible to generate that random id inside of serverless.yml without the magic of programming. You can utilize the variable system for this use case like this:  And in vars.js  Fetching Remote Data You can go even further with JS scripts by working with promises and async operations, which is supported out of the box by the Framework. That means you can fetch data needed for your configuration from any remote source you want. For example, if you need to use your Github first name as the function name, you can fetch that data from the Github API using a vars.js file like this:  Then in serverless.yml you can do something like this:  The use cases of this feature are limitless! You can fetch global config from DynamoDB, get sensitive data from KMS, or interact with your own API. Whatever is needed by your application. Predeployment Processing A super creative usage of the variable system is to use it without having any actual variables at all! One use case of that is doing some pre-deployment processing as a build script, maybe to prepare or build certain AWS resources that your service depends on or make some changes to your config files. This works out of the box because processing variables happens at the beginning on Serverless initialization. Youll just need to write your script and reference it wherever needed, most likely in the `custom` property of your `serverless.yml`. Lets take a look at an example. Say you want to control which AWS account to use based on certain logical conditions. You can make use of the variable system to setup the correct value for AWS_PROFILE environment variable for you before deployment. All you have to do is just reference a JS variables file that contain that logic. So in serverless.yml  Then in vars.js, you can write your logic:  This way, vars.js could determine which AWS_Profile to use. Organizing Your Serverless Configuration If youve been using Serverless in production for a while now, most likely youve realized how complex your serverless.yml can become. Many of the configuration options could actually be reused, not only within your services, but throughout all your services. As the framework matured, we started to notice patterns in how users are organizing their config file with the help of the variable system. Here are some common structures: Custom Resources Files Real world serverless applications require the use of many AWS resources that youll need to set up in the `resources` section of `serverless.yml`. Those resources themselves have many configurations on their own and they will most likely reference other variables. To keep `serverless.yml` from getting bloated, many users keep all their AWS resources in a single `resources.json` file, and reference that file in `serverless.yml` like this:  You can also have an outputs.json file and reference it the same way:  Then, inside of your resources.json & outputs.json, you can still reference other variables as required by your resources. Function Config Files Some users decide to follow a monolithic architecture when working the Serverless Framework. That means they have a single service, containing all their functions, for their entire project. As the number of functions grows, it become very difficult to manage configuration. Variables allow you to keep each function config in a separate file, for example sitting next to the handler in the service directory structure. Here is an example of what a serverless.yml might look like in this case:  And the directory structure would look something like this  Central Variables File If you make extensive use of the variable system, and define a lot of variables for your project, you can keep them all in a single vars.json, vars.yml or vars.js file. Keeping them in a JS file is the most powerful option because it ensures that you can use logic to dynamically manipulate your exported variable values before theyre populated in serverless.yml. So youd have a vars.js file that looks something like this:  And then reference this file in `serverless.yml` like this:  This way, if you want to make changes to your variables, youll only have to do it once in this central vars.js file. Final Thoughts We just touched the surface on what the variable system could do for your application. There are really no limits with what you can do thanks to the support for JS scripts. We hope this article gave you some inspiration on how to use the variable system in even more creative ways as needed by your application.",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e2a",
      "title": "Tools For Building Production Serverless Architectures with Lambda",
      "content": "The Serverless Framework offers one of the fastest and cheapest ways to produce scalable and modern backend applications. However, building these event based systems differs quite a lot from traditional applications both by development workflow and production infrastructure. The main difficulty of developing these systems is the inability to test code locally and also the difficulty of orchestrating large number of functions to work together. It is also complicated to monitor the functions and have an overview of how each service is operating. Here are some of the tools that help to maintain function based services and improve the development worflow. Dashbird Dashbird is a service made to monitor, debug and improve lambda functions. It gives developers a real-time overview of all lambda executions and detects errored invocations in them. This allows users to easily to monitor traffic and ensure service quality. !Main dashboard _Overview dashboard_ The service also stores details and logs about every code execution which is perfect for later debugging. !Lambda dashboard _Lambda dashboard_ In addition Dashbird offers powerful tools to process function logs, like searching and live tailing. Setup Setting up Dashbird takes about minutes and requires no code changes. You can read more about setup here. Dashbird is currently free of charge! TL;DR - instant overview - error detection - duration and memory statistics - minute setup with no code changes - Dashbird is currently free of charge SumoLogic SumoLogic is a machine data analytics service for log management and time series metrics. With SumoLogic, developers can construct meaningful dashboards to monitor specific parts of the system that are especially important. SumoLogic is also lightning-fast for searching over large amounts of data. !Custom dashboard for monitoring integrations _SumoLogic dashboard to monitor integrations_ Setup To set up SumoLogic, you have to subscribe a lambda to CloudWatch log groups posts data to SumoLogic via a HTTP endpoint. A more detailed instruction can be found here. _Be mindful that CloudWatch currently allows only one subscription per log group, meaning that no room if left for other subscriptions._ TL;DR - custom dashboards - log search - subscription to CloudWatch required - $/mo for professional, has freemium Sentry Sentry is a real-time error tracking service. With Sentry, developers get notified instantly when errors occur in live environments. This is crucial for reacting quickly and ensuring customer satisfaction. Along with every report, sentry gathers stack-traces for faster and better debugging. Sentry also analyzes the impact of each release, so it's easy to later see which release introduced which bugs. TL;DR - real-time error reporting - version statistics - implementation required - event based pricing, has freemium Offline Serverless plugins Serverless has a lot of useful plugins to test code locally before deploying to a remote environment. This helps developers save time of unnecessary deploys. Here are some of the plugins to use: - Emulate AWS Lambdas and API Gateway locally - Emulate DynamoDB locally - Fetch logs to terminal Conlusion Working with serverless systems is great when you find the right tools that work for you. With these tools, you can fix errors faster and be more connected to your serverless applications. _PS! Write in the comments if you think anything is missing!_",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e2c",
      "title": "How Reuters Replaced WebSockets with Amazon Cognito and SQS",
      "content": "The advantages of a serverless architecture are, at this point, not really a matter of debate. The question for every application or component becomes, How can I avoid having to manage servers? Sometimes you come across a roadblock: Perhaps you need a GPU; it takes seconds just to load a machine learning model; maybe your task takes longer than the seconds Amazon gives you for a Lambda process and you can't figure out how to chop it up. The excuses never end. Perhaps you want to push events into a browser or app through a WebSocket to create something similar to a chat or email application. You could use Nginx and Redis to create topics and have applications subscribe to them via a push stream; however, that means managing some long-running processes and servers. You can fake it and pound your backend once a second, butBut Amazon SQS and Cognito offer an easier way. Each user session can be paired with a Cognito identity and an SQS queue meaning applications can use SQS long-polling to receive events in real-time. At Reuters, we use this in production to support messaging in event-driven web applications and have open-sourced the underlying Serverless stack. Architecture To push an event stream to a web browser, we need to tie each session to an SQS Queue URL. The client creates a unique session ID, supplies credentials to the application, and receives: a dedicated SQS URL; Cognito access credentials that grant them permission to poll the queue; and an AES encryption key that allows them to decrypt messages received from the queue. A diagram of the steps from the user's perspective is shown below. The first step is creation of a session. In our case, this includes an Account ID identifying a collection of Users; a User ID identifying a particular person; and a Session ID identifying a particular application connection or browser session. The client calls an application endpoint with their identity and session information; then, the application validates their credentials against the claimed identity. In our case, authentication is handled by a separate single signon process, and we use an unauthenticated Cognito pool. This could also be managed by Cognito federated identities without needing an intermediate system; but, in our case the session creation endpoint requires IAM credentials only available to our application backend. Using federated identities would allow authentication with Facebook, Google, or through a custom developer-authenticated mechanism. Once the session is created, the browser client has: a set of Cognito AWS credentials that are good for around one hour; an SQS Queue URL uniquely associated with their session; and an AES private key. The client then starts making requests to the SQS Queue using long-polling. The messaging stack can receive events from Kinesis or from an API Gateway method. The events can be directed at a specific session ID, at all sessions for a User, at all Users in an Account, or can be broadcast to all users. A Lambda function handles insertion of the messages into all of the appropriate SQS Queues. In our case, several systems can create events while access to the message dispatching endpoints is managed by IAM. End-to-end message delivery time, from Kinesis to the Browser, is around seconds, and with long-polling makes reasonably economical use of AWS resources. Messaging Demo To demonstrate the solution, we'll implement a simple shell script to listen for messages and then discuss the internal workings of the stack. First, we launch a version of the SQS-Browser-Events serverless project. We also set a TTL on the session table to facilitate cleanup; which requires at least version .. of the AWS CLI, as the feature was only released in February of . This setting can't yet be managed through CloudFormation templates.  An API Gateway method provides a session creation function that will return a set of Cognito AWS credentials an SQS Queue URL and an AES encryption key that will be needed to decrypt any messages. The Cognito credentials can then be used to retrieve messages from the queue using Long Polling. Generally, the session management API Gateway would be protected and users would not call it directly; but, instead be mediated by a component that provides authentication of the user. The API Gateway uses IAM for authorization, which means any requests need to be signed. To facilitate making authenticated API gateway calls, we use a script provided in the package, and it will use whatever credentials are provided by the environment. To create a session for UserID , AccountID , with the given session ID:  The response should be something similar to this:  The expires field indicates the expiration time of the credentials. You can open two shells and in one of them poll the SQS queue using the provided credentials. In this case, we use the long-polling option and loop forever. There's a long visibility timeout, as we aren't deleting the messages from the queue, so they'll otherwise pop back up quickly. We also use a helper script ./scripts/decript-message to use the session's AES key to decript the message.  In the other shell, you can generate messages for the user, to all users in the account, and broadcast a message to everyone:  In the first shell, you'll receive the following messages:  The stack offers a few other features, including retrieving the history of user messages, read-receipting messages, and posting read-receipts to SQS. It also can accept messages from a Kinesis stream. Cognito Sessions and Message Security The Cognito ID created for a user session only has access to SQS Queues. Generally, one would restrict access to a specific resource through a policy that references the Cognito ID. For example, the condition below, when added to a policy as a Condition element, would restrict access to a specific user pool and user ID.  These are known as policy condition keys. When you create an SQS queue, you can attach a custom policy to the queue using the Policy queue attribute. Theoretically one could create an SQS Queue and only grant access to a specific Cognito identity. However, SQS currently supports a very small set of condition keys. Unfortunately, the Cognito keys are not among them. We've pointed out this deficiency to AWS, and explained its usefulness in helping craft serverless event-driven applications. Since a little piece of me dies every time I have to manage another server, we decided to work around it. The only solution that works at an arbitrarily high scale is to grant message read and delete access to all queues based on the name prefix of the queue. The queue names are generated using a random bit value generated by the Linux kernel using environmental entropy /dev/urandom, which is suitable for generating unguessable and secure Queue URLs. Another random bits are generated for an AES key that is used to encrypt and decrypt all messages. This key is returned to the caller when creating a session and stored in the DynamoDB session table. A random counter initialization value is used for the AES stream cipher and returned along with the encrypted message. The decrypt-message script in the project provides an example Python implementation of decryption and a Javascript version is provided in the project's documentation. Any set of Cognito credentials from the pool can be used to read and delete messages from any queue. The Cognito GetId method is also public, so anyone with knowledge of the Cognito endpoint can generate ID's. However, to access a Queue, one would need to know the random Queue name and URL. If they know the Queue URL, they can delete messages, but can't decrypt them without the private key. Whether this provides sufficient security depends on the nature of the data being handled. We hope however that AWS will at some point provide the tools necessary to secure a queue to a specific Cognito identity. Access to the user message API Gateway endpoints is secured via IAM, meaning users can't directly call them. In our case, access is mediated through a separate service that checks the user's credentials. Handling High Message Volumes Dispatching messages to individual users is relatively easy to scale if they are injected through the API Gateway. Messages that have to be broadcast to all users, or to accounts with a large number of users, can become more difficult. Each message requires an AWS API call to SQS to insert into the queue, and a call to DynamoDB to add it to the user's message history. Because these Lambda functions mostly wait on HTTPS requests to AWS and make light use of CPU resources, we use low memory allocations of MB or MB, making operation as economical as possible. Multi-threading the operations, the dispatcher Lambda can only get off a few thousand API calls before timing out. If you need to broadcast a breaking news event to , users, a single Lambda invocation will not suffice. To achieve sufficient scale, the dispatcher only retrieves the list of recipients, and then splits the recipients into batches and dispatches these batches to separate Lambda functions that handle SQS message insertion and user history insertion, invoking them asynchronously. We have tested this up to a few hundred thousand users. This issue is unique to broadcast messages, and scaling beyond a few hundred thousand users is probably best done by splitting the broadcast message into account partitions. For messages inserted through the API Gateway, scaling is relatively easy, as the API Gateway lambdas are unlimited in number, and the only element that needs to scale out is DynamoDB. For messages inserted via Kinesis, processing the stream can be a bottleneck, as only one Lambda process will run for each shard. We therefore have to clear the stream messages as quickly as possible, and it's advantageous to keep the Kinesis Lambda reader as light as possible and asynchronously dispatch the actual queue insertion operations to a separate Lambda which is not subject to scale limitations. Session Cleanup All of the queues and Cognito identities need to be removed at some point. The entries in the session table are all given a ttl, and DynamoDB will automatically delete them after some idle period. Another Lambda function is connected to the table update stream, looks for queues and Cognito ID's that are no longer in use, and deletes them. Performance and Cost In production we typically see a delivery delay for Kinesis injected messages of about seconds before they appear in the user's browser or app. This involves several serial operations: a Kinesis API call, delivery of the event to the dispatcher Lambda, a DynamoDB query to retrieve the recipient Queue list, invocation of the SQS queue insertion Lambda, a call to the SQS API to insert a message, and a call from the Browser to the SQS recieve-message endpoint using long-polling. Periodically longer delivery delays can happen because of Lambda startup times. A graph below shows the average event delay over a seven day period, for the th, th, and th percentiles in delivery time. The average is just over seconds and rarely will see to seconds. !average message delivery times Faster mechanisms can surely be implemented using fixed servers; however, so long as seconds is an acceptable delay, the advantages in terms of scale and maintenance are significant. As for cost, we roughly estimate each message consumes around Megabyte-seconds of Lambda time (Lambda functions are billed in terms of Megabytes consumed and time taken, in ms chunks), which is the equivalent of around ms of a MB Lambda process. Ignoring the cost of DynamoDB, which you'd have to pay for with any similar application, we can calculate the effective cost of the serverless stack. We'll assume users, generating messages per day each, at ,, messages per day. SQS costs are $. per API call, with users polling every seconds, and API calls per message (adding, reading, deleting), so the cost per day is $. (/ + ) = $./day. For the Lambda functions, with an equivalent ms of a MB Lambda taken per message, at ,, messages per day, the cost is $. = $./day. This gives a total cost of around $ per day per users. For , users and an aggregate messages per second, this comes to $/day, or around $k per year. That sort of capacity you could probably handle with or well designed mid-range EC servers using Nginx and Redis, and only be out $k or $k per year. So the operational costs are higher once you reach significant scale. You probably over-provision fixed servers to accommodate load, even with auto-scaling. Given the approximate x markup of Lambda functions over EC instances, if your servers are running at % load or less, you would probably save money going serverless. In this example, the economics of serverless are a bit more expensive due to the high call volume to the SQS API coupled with the relatively short second long-polling timeout, making cost increases closer to a factor of . However, operational simplicity, time-to-market, and development costs can all make the choice favorable. The low upfront costs are also beneficial. The big bills don't come in until the product is successful, and once it's successful, it's easier to find the money. Even once you do reach usage levels where economics might drive you to fixed servers, there are advantages to not worrying about provisioning. Products are seldom static and new features and changes can impact capacity. Not having to worry about the impact of new features on resource demands is liberating. Conclusion Going completely serverless is still not a trivial task for all types of applications and support by Amazon is thin in areas. In this case, we were able to expand the boundaries of serverless architectures by using SQS and Cognito to push events to a browser or app. Using AWS to manage scaling, we eliminate the need for provisioning fixed servers. Up to a certain point, in our case a few tens of thousands of users, it's also an economical solution. Even at larger scale, the benefits in operational and architectural simplicity may still outweigh the higher AWS bills. There are still some gaps in features: DynamoDB TTLs can't yet be managed via the serverless CloudFormation templates while SQS Queues don't support Cognito condition keys. Amazon is constantly improving their offering and hopefully will expend more effort into supporting the serverless community. Tooling is also improving rapidly. Setting up connections between DynamoDB and Kinesis streams became much easier with the .. Serverless release. For some types of applications it requires rethinking the architecture, and there are some trade-offs. That being said,we generally see a bright future for serverless architectures and have found advantages in them for applications that might at first glance not be a natural fit.",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e2e",
      "title": "Why we switched from docker to serverless",
      "content": "Here at ShoutOUT, we've been happily operating on Amazon Web Services for years. But there is always room for improvement. Luckily, we have the kind of team that doesnt hesitate to embrace new technologies that will improve the platformand thats the very reason why we decided to try serverless architecture. Background Before I move into explaining our own serverless architecture, I'd like to share some info on the services and infrastructure we previously had, and why we decided to switch to serverless. Originally using Docker + ECS Previously, we ran a couple of services built with Node.js on top of the Express Framework. This served our ShoutOUT dashboard and integrations with several third party services. These services were deployed on Docker containers inside Amazon's Elastic Container Service (ECS). The reason for using Docker, was the level of support and overall convenience it provided for running microservices. We could self-contain the services and handle scaling individually. Additionally, Docker helped ensure that features and fixes would seamlessly ship between our development and production teams. In short, AWS + ECS = a solid platform from which to run and manage docker containers. Within these processes, services ran smoothly with high availability and resiliency. But then the number of services increased However, when the number of services and complexity of each service increased, it became obvious that we needed a way to expand the computational capacity. The interesting (or not so interesting) fact is that the traffic we get for these services is very unpredictable. For example, during the Holiday season, we get much more traffic than normal through the dashboard. If an integrated third party application starts sending more traffic for some reason, or if a customer runs a Facebook campaign integrated with their ShoutOUT account, we also experience traffic spikes. So there is no defined pattern for spikes and idle times. But when it happens, the service should be available and capable of handling the increasing workloadespecially for third party integrations in which data is being synchronized. We could have scaled our ECS environment by adding more container instances and multiple service containers. And we did try that at first, but there was a hurdle. We were running a SaaS business, making cost a critical factor. This solution was not appealing. The following diagram shows an overview of our previous deployment setup. Making the switch to a serverless architecture Around this time, we started to hear more and more serverless success stories that illustrated exactly what we had been looking for as a SaaS solutions provider. Being able to remove all the scaling concerns was a big relief, especially when combined with the added advantage of paying only for what we use. So, without any further ado, we started migrating our services to the serverless stack. Luckily, we had built our services with Node.js (which was was fully supported by AWS Lambda), making it a small effort to combine them with Lambda functions. By this time, the Serverless Framework was in a very early stage and we had no clue that it even existed. Prior to integration with the Serverless Framework, we had to do a lot of manual configuration on API Gateway and Lambda; deploying multiple services this way was really painful. Thankfully, shortly after this, we found this awesome Framework! The Serverless Framework saved us a lot of time, and streamlined the development to deploy process. We fully embraced it and it solved a big pain for us. The Serverless Framework has a great mechanism to provision and update resources required to run serverless apps. It's command line interface makes it super easy to deploy new versions to production, with the option to easily roll back if anything goes wrong. This eliminated all the manual configurations we had to do within our AWS account. The added advantage is that we can deploy the same services to different regions or different AWS accounts via a single command in CLI. The Results Around % of the backend services we had were successfully migrated to a serverless stack, and we were able to reduce a considerable amount of cost this way. The following diagram shows an overview our current setup after successful integration with the Serverless Framework: Apart from cost reduction, we were also able to gain a lot of other advantages through this migration. No need to worry about scaling since Amazon takes care of it nicely High availability of our backend services Resiliency since each execution is contained and isolated, and thus has no impact on other executions Easily accessible logs from cloud watch ensures traceability Since integration, we've taken a serverless first approach; all new services are built in a serverless fashion unless there is an obvious reason not to go serverless. This has helped us dramatically shorten our release cycles, which, as a startup and a SaaS provider, has been hugely beneficial. One last note Finally, I would like to point out one other thing. Cloud technologies and platforms, especially serverless architectures, evolve and improve at a very fast pace. As solutions providers, we need to have our systems prepared to embrace and cope with these new technologies. We should always adapt our systems to them, or we will end up with legacy systems that dont reap the benefits discussed above. I suggest you go back and look at how far you are lagging behind new technologies to see whether you can improve what you are doing to ensure you can keep up in an ever-changing industry.",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e30",
      "title": "Event-driven processing with Serverless and DynamoDB streams",
      "content": "In the last few years, we've seen an explosion in developers using event streams to power interesting applications. Many companies are turning to Kafka, a persistent, distributed log developed at LinkedIn that is now a top-level Apache project. But Kafka operations aren't free -- you'll need to manage a cluster of brokers as well as a highly-available Zookeeper ensemble. Configuring, monitoring, and repairing this infrastructure can distract you from the value you want to deliver to your customers. In this post, I'll show you how you can use DynamoDB streams to decouple your application, giving you the benefits of an event stream without the maintenance burden. I'll walk though why you might want to use an event-based architecture. In a follow-up post, I'll talk about the challenges of an event-driven architecture and some patterns you can use to build event-driven services. User Signup with DynamoDB You have a great idea for an application. Like many apps, you need a way for users to sign up. An increasingly popular architecture is to create a REST API using API Gateway, Lambda, and DynamoDB. With the Serverless Framework, your create user function might look like:  For brevity, I've left out the logic to actually save the user to DynamoDB. In the sample above, the function parses the user data from the request body and attempts to save it to the database. If it fails, it returns an to the user, while a success returns a . Pretty basic. Adding in User Search with Algolia So now you've implemented user signup, and your app is growing like crazy. Many of your users are requesting the ability to search for other users so they can connect with their friends that are also using your app. DynamoDB is a poor fit for implementing search, so you look to other options. The classic choices are Solr or Elasticsearch, but these both require setting up and managing your own infrastructure. In keeping with the serverless theme, you decide to try Algolia, a fully-managed search SAAS offering. To make your users available to be searched in Algolia, you'll need to index your users as they are created. We can add this into our prior user creation Lambda function as follows:  Great! Now users are indexed in Algolia and can be searched from the frontend. However, there are some problems with this approach: - Added Latency: A user signing up for the first time now needs to wait for two separate calls on the backend before receiving a response. - Partial Failures: How should the function react in the event of a partial failure -- the write to DynamoDB succeeds, but the index call to Algolia fails? Right now, we're logging the failed index call and moving along, but that creates drift between our source-of-truth user data and our search index. - Poor separation of concerns: We're now doing two things that could be separate within the same function. If a change is needed to either part, a redeploy of the user creation function is needed, which could introduce regressions in the other part. Further, we'll need to add index-related functionality to other user endpoints as well, such as update user and delete user endpoints. A Better Way: Event-driven functions with DynamoDB Streams To overcome these issues, we're going to use the Streams feature of DynamoDB. A DynamoDB Stream is like a changelog of your DynamoDB table -- every time an Item is created, updated, or deleted, a record is written to the DynamoDB stream. The stream has two interesting features. First, it is ordered by time, so older records appear before newer records. Second, it is persistent, as it retains the last hours of changes to your DynamoDB table. This is a useful attribute compared to an ephemeral pub-sub mechanism like SNS, as you can reprocess records that occurred in the recent past. DynamoDB streams are charged based on the number of read requests, so there's no cost to setting them up when you set up a DynamoDB table. When you set up a DynamoDB stream, you'll need to set the Stream View Type. This specifies what data about the changed Item will be included with each Record in the stream. The Stream View Types are: - KEYS_ONLY: The record includes _only_ the partition key and sort key (if applicable) for a changed Item. If you use this, you would need to make a query to DynamoDB with the keys if you needed additional, non-key attributes about the Item. - NEW\\_AND\\_OLD_IMAGES: This record will include both the full Item as it looks after the operation and the full Item as it looked before the operation. While this includes the most information, it comes at a cost. You're charged for DynamoDB streams based on read requests, and each read request can return a max of MB of data. If you're sending the full old and new Items, you may have to make more requests. This is particularly true when you have Items with lots of fields or with a very large field, such as a set with many elements or a binary representation of an image. - NEW_IMAGE: This record shows the full Item after it was modified by the operation. - OLD_IMAGE:: This record shows the full Item before it was modified. Choose the option that best fits your use case, but I find the `NEW_IMAGE` view type is often the most useful as I'm usually concerned with the Item as it currently exists. Event-driven Indexing The real power from DynamoDB Streams comes when you integrate them with Lambda. You can set Streams to trigger Lambda functions, which can then act on records in the Stream. Let's return to our example to see why this is a powerful pattern. Recall that we had set up a user creation endpoint that both saved the user to DynamoDB and indexed the user in Algolia for searching. However, this introduced problems such as increased latency, partial failures, and poor separation of concerns. Let's separate these two operations to help reduce these issues. First, we rollback our user creation function to the original version under the `User Signup with DynamoDB` heading. Second, we create a new function that will be triggered by the stream from our DynamoDB table. The `event` argument that is passed to our Lambda handler function will be a dictionary with roughly the following shape:  In the example above, there are two records from a stream. The first shows the creation of an Item whose partition key `userId` had a value of ``. We can tell this Item was created from the record as the `eventName` is `INSERT`. In the second record, the same Item is affected but with an `eventName` of `MODIFY`. If you examine the two `NewImage` objects closely, you'll see that the second one changes the `fullname` attribute from `Bob Smith` to `Robert Smith`. Because I've chosen the `NEW_IMAGE` view type, I received the full Item as it exists after the given operation. Now that I have the DynamoDB stream set up, I can set up an Algolia indexing function that will be triggered from the stream. The example code below shows how we would handle the stream records in a Lambda function:  This function iterates over the records from the stream. If the record is inserting a new Item or modifying an existing item, it pulls out the relevant attributes and makes a call to index the user. If the record is deleting a user, it makes a call to delete the user from the index. Think about how this helps with our problems before. This indexing operation is outside the request-response cycle of the user creation endpoint, so we aren't adding latency to the user sign up experience. Further, we don't have to think about how to handle a partial failure in the user creation endpoint where the user was created but we couldn't index in Algolia. Now our two functions can fail independently and handle them in ways specific to their use case. Finally, we've better separated our actions so that a change to our indexing strategy doesn't need to require a redeploy of all user creation and modification functions. Multiple Reactions to the same event: Adding users to your CRM Let's look at one more reason this pattern is so powerful. Imagine your Marketing team is hungry for user information so they can target them with special promotions or other marketing material. They are begging you to copy new users into their customer relationship management (CRM) platform as they sign up. Without an event-driven architecture, you might be leery of this -- the API request to the CRM would need to happen within the user creation flow. This use case is even further separated from production than the user search index example and is unlikely to be allowed. The Marketing crew will likely need to ask for a developer to write a batch job to run at off hours to extract users from the production database and pump them into the CRM. It doesn't need to be this way. Thanks to the DynamoDB stream you previously set up, your Marketing team could hook into the same stream of user events in a safe way that doesn't impact production. They can even enrich the users by calling an identification API like Clearbit or FullContact without worrying about latency to the end user. This function might look like:  This is completely separate from production and from the other Lambda functions triggered by your stream. If your Marketing team decides to change their CRM, they can update this function and redeploy without worrying about its effect elsewhere. Further, by removing the need to hook up the internal plumbing to get access to what's happening in production, you've reduced the time to create something of value from these user events. If someone on your team wants to drop a message in a Slack room every time you get a new user, the event stream is already hooked up and waiting for them -- she just needs to write the code to pipe the users into Slack. You could send welcome emails to new users or store counts of these events in a database for internal analytics. The possibilities are endless. Conclusion In this post, we've seen how an event-driven system with DynamoDB streams and Lambda functions are a powerful pattern for reacting to what's happening in your application. In a follow-up post, I'll cover some patterns you can use when working with DynamoDB streams.",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e32",
      "title": "Serverless (Cron)icle  - News from the Serverless Community",
      "content": "The open source Serverless Framework is backed by a super smart and extensive community of developers (we've passed , stars on GitHub). Besides contributing code, our community is passionate about sharing their knowledge and expertise. Here's a roundup of some of these articles, videos, and posts to help you keep up with the news in the serverless ecosystem. The evolution of scalable microservices A look at microservices, not as a tool to scale the organization, development and release process (even though it's one of the main reasons for adopting microservices), but from an architecture and design perspective, and put it in its true context: distributed systems. In particular, we will discuss how to leverage Events-first Domain Driven Design and Reactive principles to build scalable microservices, working our way through the evolution of a scalable microservices-based system. - By Jonas Bonr Kicking The Tires On This Serverless Thing I ended up with a mess that cost more money than I wanted it to. It wasn't easy to figure out ... I had better luck with Firebase. I had a fun time and built something interesting. At least I think it is. I had to approach the development process in a completely different way than what I was used to... but I like that kind of thing. I know others don't. The big thing to me, however, is that I was able to build something that I would truly use. In fact I'm using parts of it now in production (more on that in a future post). - By Rob Conery Serving Million Requests for $/Month, or: How We Reduced Our Hosting Costs by Two Orders of Magnitude When I joined Postlight as an engineer last year, my first task was a big one: Rewrite the Readability Parser API ... ... Finally, we focused on cost, and the answer here was simple. To drastically reduce our costs, we chose a serverless architecture running on AWS Lambda and API Gateway, built and deployed using the Serverless framework. - By Adam Pash Service Discovery as a Service is the missing serverless lynchpin Changing a functions dependent resources after deployment is a critical step towards feature parity with traditional architectures. When we talk about serverless architectures, we often talk about the natural fit between serverless and microservices. Were already partitioning code into individually-deployed functions  and the current focus on RPC-based microservices will eventually change into event-driven microservices as serverless-native architectures become more mature. - By Ben Kehoe Writing IAM Policies CAREfully This isnt a tutorial, just a conceptual framework thats helped me write better IAM policies. Its extra useful when an app needs a group of services like DynamoDB, S, and Kinesis. The method is called CARE, and its an acronym based on the four parts of an IAM policy statement. - By Ryan Scott Brown",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e34",
      "title": "Announcing Emit",
      "content": " Why The serverless movement is all about productivity, speed, and results. These are the values that make serverless powerful and that the community has rallied around. As the creators and maintainers of the Serverless Framework, weve enjoyed a front-row seat to watch the serverless movement grow and evolve. What started as a small group of passionate, forward-thinking developers scratching their own itch has evolved in to Fortune companies rethinking the way that they build and architect their applications in order to empower their development teams to get to market faster. One theme that weve seen a growing amount of interest in is the event-driven nature of serverless architectures. While event-driven architectures are certainly nothing new, the emergence of serverless compute has made them far more powerful, relevant, and exciting. The growing interest and excitement around this theme is what prompted us to create Emit, a conference focused on event-driven architectures. What Emit will be a single-day, single-track event with technical thought leaders in the space sharing knowledge, experiences, and predictions around event-driven architectures. It will be small, intimate, and more like a summit than a traditional technology conference (i.e no sponsor booths). There will be ample networking opportunities as well as a closing reception during which discussion, debate, and free-flowing ideas will be encouraged. When/Where Emit will take place on August th in San Francisco at The Pearl. The Pearl is a beautiful space with a large auditorium for the speakers and panel, a mezzanine which will serve as a lounge for networking and relaxing, as well as a large rooftop where lunch and the closing reception will be held. Speakers We have an extremely exciting group of speakers lined up, made up of leading thinkers in the serverless and event-driven space, including technology leaders from organizations such as Nordstrom, CapitalOne, Uber, Coca-Cola, IBM, Google, Microsoft, and more. We'll be adding more speakers over the coming weeks but an up to date list can be found on emitconference.com. Attending Emit will have a small, curated, and relevant crowd made up mostly of technical thought leaders, architects, strategic decision makers, and analysts. Tickets are $ and available by invite only. If you wish to attend please reserve a spot on the conference site. There is an attendance cap of , we apologize in advance to anyone who would like to attend but does not get an invite. Professionally produced videos of all talks and panels will be made available to the public after the conference. Sponsors We have a great group of sponsors, who are passionate about serverless and event-driven architectures, helping us make Emit a world-class event. Our initial sponsors include AWS, A Cloud Guru, and Trek with others being announced soon on emitconference.com.",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e36",
      "title": "AWS Lambda Power Tuning with AWS Step Functions",
      "content": "During the last few months, I realized that most developers using serverless technologies have to rely on blind choices or manual tuning to optimize their Lambda Functions. In this article I will present: the data I collected, the open-source project I created to solve this problem, and the design ideas that guided me. AWS Lambda optimizations, let's go data-driven I launched this poll a few weeks ago, and I managed to collect almost responses thanks to the community who helped me share it. It turned out that more than % of the responders just go for the same RAM configuration all the time (I'm still wondering why they didn't call it \"power\"!). I guess they got tired of measuring different configurations against aleatory performance results? Apparently, the other half is still trying to optimize each Lambda Function manually. Well, unless we consider very special use cases, this manual process requires you to waste a lot of time, and it often comes with very subjective interpretations of statistical relevance. This is why I decided to work on a side project that would help everyone solve this problem in a language agnostic and deterministic way. But first, let's dive deeper into the problem and a few possible solutions. What can make your functions slow? If you are developing small self-contained functions that do only one thing, there are not too many factors that can slow them down. Some functions just involve plain computation and get their job done in a few milliseconds, they are usually easy to test, debug, and optimize (unless you need weird machine learning stuff, as we do at Cloud Academy). On the other hand, most functions are meant to interact with other functions and APIs, as a sort of glue code between services. Here is a brief list of the reasons why your functions may slow down: AWS SDK calls: everytime you invoke an AWS API using the official SDK - for example, to read data from S or DynamoDB, or to publish a new SNS message. These calls are usually pretty fast because they are executed locally within AWS's datacenters. You may try to perform bulk read/write operations and further optimize those services configuration, whenever possible. External HTTP calls: By invoking external APIs, you might incur significant network latency unless the API is also hosted on AWS and provides regional endpoints. You may try to execute multiple calls in parallel and avoid serial execution, whenever possible (this is trivial in Node.js, but it might become a bit tricky to handle in Python and other languages). Intense computation: Complex algorithms might take a while to converge, especially if you use libraries that require loading a dataset into memory. For example: Natural Language Processing or Machine Learning models that need to manipulate and normalize textual data, invert matrices, process multimedia files, etc. Cold starts: These occur whenever you update your code,when your Lambda container gets cold, or even just when AWS decides to swap containers around. Unfortunately, you don't have much control on this, but luckily it happens every once in awhile, and we can keep this in mind when evaluating our Functions performance. How to achieve objectiveness and repeatability? My goal was finding an objective way to analyze the performance of any given Lambda Function, and then make informed decisions about the corresponding power configuration based on its priority level in the system. Such a tool should be cheap and fast to execute, and it should provide repeatable results, taking into consideration the fluctuant trend of CPU, network, external resources, cold starts, etc. Here is the complete list of requirements I gathered initially: Speed: similarly to unit tests, you should be able to evaluate the impact of code changes as quickly as possible, which means the tool should run within seconds (not minutes or hours) Cost: evaluating performance should be cheap enough to be executed automatically and as often as needed. In some critical and high-throughput scenarios, spending up to $ could be more than acceptable since the resulting power optimization might generate considerable savings Complexity: the tool should be easy to deploy, understand, extend, and visualize Flexibility: you may want to use the same tool for different functions, and provide different configurations or optimization strategies Concreteness: the tool should provide insights based on a real AWS environment, without limiting assumptions or mocks Statistical relevance: the results should be relevant and robust to random fluctuations of measurement tools and external services Multi-language support: the tool should be language agnostic and provide the same level of accuracy and relevance for Node.js, Java, Python, Ruby, Go, etc. I easily solved the complexity issue by using the Serverless Framework for deploying and provisioning all the resources I needed. Since I wanted everything to run in a real AWS environment and generate statistically relevant results, I quickly realized I had to actually execute the Lambda Function provided as input, as opposed to computing a performance estimate based on code static analysis (which wouldn't be easy to achieve across languages). To be fast and cost-effective, I thought the tool itself would need to run on AWS Lambda, and it'd need to invoke the input function thousands of times in parallel (at least a few hundreds for each of the available power configurations). Why AWS Step Functions? Given the problems and requirements described above, I figured I'd need some orchestration logic to invoke the input Lambda Function with all those different configurations, retrieve the resulting logs, crunch some per-configuration statistics, and then aggregate everything together to take a final decision. This sounded like the perfect scenario for AWS Step Functions since I wanted all of this to happen in parallel, map the results into the corresponding performance/cost metrics, and finally reduce it to a single value (i.e. the optimal power configuration). Therefore I went on and designed a multi-branch state machine on paper, where each parallel branch would correspond to a power value. Here is when I found out that AWS Step Functions does not allow you to generate states dynamically, and you can't update the state machine structure via API either. I didn't give up, so I started building a simple command that would take a list of power configuration to evaluate as input to generate the state machine JSON code. One more problem: every branch would need to run the same Lambda Function in parallel with different power configurations. Therefore I decided that I'd need an initialization step that would create a Function Version/Alias for each configuration. Since I didn't like the idea of leaving a mess after the state machine execution, I also added a cleaning step at the end. Once the state machine contains the correct number of branches, you can still tune a few parameters at runtime: lambdaARN: the AWS Lambda ARN of the Function you want to optimize num: the number of invocations to execute for each branch (recommended between and ) payload (optional): the static payload to be passed as the input of each invocation enableParallel (false by default): if true, it will enable parallel executions at the branch level (Note: enabling this may cause invocation throttling, depending on the value of num and your account soft limit) Here is a screenshot of a sample execution, generated with only three power values: How-to and Use Cases You can find detailed instructions to get started on the repository. Basically, you `sls install` the service, `npm install` its dependencies, `npm run generate` the state machine, and `sls deploy` to AWS. The current optimization strategy is based on cost alone. The state machine will return the power configuration which provides the cheapest per-invocation cost. Of course, we could implement more strategies in the future. For now, you should keep in mind that if your input function executes in less than ms or ms, MB will always be the optimal RAM configuration. This happens because that's the cheapest configuration and it's more or less always optimal for very trivial functions. The real performance evaluation makes sense for those functions that perform intensive computational tasks or time-consuming HTTP/SDK calls. For example, imagine a Lambda Function that updates a DynamoDB record and then sends a new SNS message. Your code may look like the following: ```js const AWS = require(\"aws-sdk\"); const DDB = new AWS.DynamoDB.DocumentClient(); const sns = new AWS.SNS(); exports.handler = (event, context, callback) => { const ID = event.ID; return Promise.resolve() .then(() => updateRecord(ID)) .then(() => sendNotification(ID)) .then(data => callback(null, 'OK') .catch(err => callback(err)); }; function updateRecord(ID) { const params = { TableName: \"MyTable\", Key: {ID: ID}, UpdateExpression: \"set Active = :a\", ExpressionAttributeValues: {\":a\": true} }; return DDB.update(params).promise(); } function sendNotification(ID) { const params = { Message: `Record ${ID} has been activated`, Subject: 'New Active Record', TopicArn: process.env.TopicArn }; return sns.publish(params).promise(); } js { \"lambdaARN\": \"arn:aws:lambda:{REGION}:{ACCOUNT_ID}:function:HelloWorld\", \"payload\": {\"ID\": \"YourID\"}, \"num\": , \"enableParallel\": true } ``` Also, remember that the total number of parallel invocations will depend on the enableParallel parameter and how many branches you have generated. Assuming that you have generated only branches (out of the available values), the state machine may launch up to parallel invocations ( branches that invoke the same input function times in parallel). If you need to test more power configurations, or if you'd like to achieve better statistical relevance, I'd suggest raising the soft limit of your account to at least concurrent invocations and use \"num\": . Final Thoughts I believe performance and cost optimization are still quite an open issue in the serverless world. Not only is it hard to estimate costs, but it's even more so without an objective way of measuring and optimizing your code. I hope this tool will help you take more data-driven decisions, and save some time while optimizing your Lambda Functions. Let me know what you think about my AWS Lambda Power Tuning project, and feel free to contribute on Github.",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e38",
      "title": "Steps To Faster Serverless Development",
      "content": " One of the biggest pain points we hear from developers moving into the serverless world, is the slower feedback loop while developing. In this post, I'm going to walk through some of the ways I speed up building and deploying live serverless services. The serverless framework has a ton of hidden gems to speed up your dev cycle. Lets explore! Deploy changes faster with `sls deploy function` Many users aren't aware of the `serverless deploy function` command and instead use `serverless deploy` each time their code has changed. What is the difference? `sls deploy` re-deploys the entire stack through cloud formation and can be noticeably slooowww. On the flip side, `sls deploy function` only zips up the code (& any dependencies) and updates the lambda function only. This is much much faster than waiting for an entire stack update. So, when developing you can use `sls deploy function -f myFuncName` for speedier code changes in your live AWS account. One note: If you make any changes to `serverless.yml` config, like changing endpoint paths or updating custom resources, etc that will require a full `sls deploy` to update the entire stack. If you have only make changes to your function code, `sls deploy function -f myFuncName` will work just fine and be much snappier.  Tail your live service logs Debugging remote code in the serverless world can be tricky. Luckily `console.log` and lambda functions go together like peanut butter and jelly. One of my favorite serverless framework commands is the `sls logs` command. It will pull the logs from your remote function directly into the terminal. This is handy for debugging errors or inspecting what the `event` or `context` args contain. It's even more useful when you tail the logs and get a live update as you are pinging your live functions.  I will pop open a new terminal window and run `sls logs -f funcName -t` and then ping my lambda function with Postman (or my UI) and live debug the function. Having terminal windows open and combining `sls deploy function -f funcName` for quicker code changes and watching the live logs with `sls logs -f funcName -t` is super easy to do and speeds up my feedback loop. Offline Emulation Now you might be asking, what about offline emulation? It's absolutely a way to speed up dev cycles without having to re-deploy anything. With the serverless offline plugin you can speed up local dev is by emulating AWS lambda and API Gateway locally when developing your Serverless project. Install the plugin  Then add the `plugins` key in `serverless.yml`  Now you should have the `serverless offline` commands available when running `serverless -help` Serverless has a lot of useful plugins to test code locally before deploying to a remote environment. This helps developers save time of unnecessary deploys. Additional emulation resources: - Emulate AWS Lambdas and API Gateway locally - Emulate DynamoDB locally - Local Stack _PS! How do you speed up your dev flow?!_ Image credit",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e3a",
      "title": "Serverless (Cron)icle  - News from the Serverless Community",
      "content": "The open source Serverless Framework is backed by a super smart and extensive community of developers (we've passed , stars on GitHub). Besides contributing code, our community is passionate about sharing their knowledge and expertise. Here's a roundup of some of these articles, videos, and posts to help you keep up with the news in the serverless ecosystem. Serverless Event Sourcing at Nordstrom A look at the award-winning reference architecture for a retail store, Hello Retail, by the team at Nordstrom. Hello Retail is a proof-of-concept Serverless architecture for a retail store. The team at Nordstrom built the project to experiment with Serverless Event Sourcing. Nordstrom is an early adopter of Serverless architectures. The team has built Serverless microservices for a number of production systems. The use cases for Serverless include API backends and stream processing. - By John McKim An essential guide to the serverless ecosystem Serverless computing is gaining momentum in the IT industry, and for good reason. Despite being a relatively new capability, it has captured the imagination of many technologists with the promise of delivering such benefits as shortened time to market, improved operational and security practices, and a revolutionary economic model based on pay-as-you-go pricing. - By Rafal Gancarz A vision for loosely-coupled and high-performance serverless architecture When we talk about serverless architectures, we often talk about the natural fit between serverless and microservices. Were already partitioning code into individually-deployed functions  and the current focus on RPC-based microservices will eventually change into event-driven microservices as serverless-native architectures become more mature. - By Ben Kehoe Serverless: The pros and cons of building a company without infrastructure In my talk, I tried to address what Serverless is and why it is becoming such a big thing, also exploring what I believe are the pros and cons of this new paradigm. - By Luciano Mammino Coding for a serverless future Looking down the road at zero-infrastructure development. I consider myself really lucky that for all the progress weve made in the last few decades, were still in the nascent stages of our industry. Theres no other line of work I know of where the landscape changes so rapidly, and so dramatically, as in technology. - By Jay Rodgers",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e3c",
      "title": "Challenges and patterns for building event-driven architectures",
      "content": "Challenges with the Event-Driven Architecture In my previous post, I talked about how you can use DynamoDB Streams to power an event-driven architecture. While this architecture has a number of benefits, it also has some \"gotchas\" to look out for. As you go down this road, you need to be aware of a few challenges with these patterns. Changing Event Schemas In our user creation example from the last post, we've been saving the user's first name and last name together in a single `fullname` field. Perhaps our developers later decide they'd rather have those as two separate fields, `firstname` and `lastname`. They update the user creation function and deploy. Everything is fine -- for them. But downstream, everything is breaking. Look at the code for the Algolia indexing function -- it implicitly assumes that the incoming Item will have a `fullname` field. When it goes to grab that field on a new Item, it will get a `KeyError` exception. How do we handle these issues? There's no real silver bullet, but there are a few ways to address this both from the producer and consumer sides. As a producer, focus on being a polite producer. Treat your event schemas just like you would treat your REST API responses. See if you can make your events backward-compatible, in the sense of not removing or redefining existing fields. In the example above, perhaps the new event would write `firstname`, `lastname`, and `fullname`. This could give your downstream consumers time to switch to the new event format. If this is impossible or infeasible, you could notify your downstream consumers. The AWS CLI has a command for listing event source mappings, which shows which Lambda functions are triggered by a given DynamoDB stream. If you're a producer that's changing your Item structure, give a heads up to the owners of consuming functions. As a consumer of streams, focus on being a resilient consumer. Consider the assumptions you're making in your function and how you should respond if those assumptions aren't satisfied. We'll discuss different failure handling strategies below, but you shouldn't just rely on producers to handle this for you. How to handle failure Failure handing is a second challenge to consider with Lambda functions that are triggered by DynamoDB streams. Before we talk about this, we should dig a little deeper into how Lambda functions are invoked by DynamoDB streams. When you create an event source mapping from a DynamoDB stream to a Lambda function, AWS has a process that occasionally polls the stream for new records. If there are new records, AWS will invoke your subscribed function with those records. The AWS process will keep track of your function's position in the DynamoDB stream. If your Lambda function returns successfully, the process will retain that information and update your position in the stream when polling for new records. If your Lambda function does not return successfully, it will not update your position and will re-poll the stream from your previous position. There are a few key takeaways here: - You receive records in batches. There can be as few as or as many as records in a batch. - You may not alter or delete a record in the stream. You may only react to the information in the record. - Each subscriber maintains its own position in the stream. Thus, a slower subscriber may be reading older records than a faster subscriber. The batch notion is worth highlighting separately. Your Lambda function can only fail or succeed on an entire batch of records, rather than on an individual message. If you raise a failure on a batch of records because of an issue with a single record, know that you will reprocess that same batch. Take care to implement your record handler in an idempotent way -- you wouldn't want to send a user multiple \"Welcome!\" emails due to the failure of a different user. With this background, let's think about how we should address failure. Let's start with something simple, like a Lambda function that posts data about new user signups to Slack. This isn't a mission-critical operation, so you can afford to be more lax about errors. When processing a batch of records, you could wrap it in a simple try/catch block that catches any errors, logs them to Cloudwatch, and returns successfully. For the occasional error that happens, that user isn't posted to Slack, but it's not a big deal. This function will likely stay up to date with the most recent records in the DynamoDB stream because of this strategy. With our Algolia indexing function, we want to be less cavalier. If we're failing to index users as they sign up or modify their details, our search index will be stale and provide a poor experience for users. There are two ways you can handle this. First, you can simply raise an exception and fail loudly if _any_ record in the batch fails. This will cause your Lambda to be invoked again with the same batch of records. If this is a transient error, such as a temporary blip in service from Algolia, this should be fixed on the next invocation of your Lambda and processing will continue as normal. It's more complicated if this is _not_ a transient error, such as in the previous section where the event contract was changed. In that case, your Lambda will continue to be invoked with the same batch of messages. Each failure will indicate that your position in the DynamoDB stream should not be updated, and you will be stuck at that position until you either update your code to handle the failure case or the record is purged from the stream hours after it was added. This hard error pattern can be a good one, particularly for critical applications where you don't want to gloss over unexpected errors. You can set up a Cloudwatch Alarm to notify you if the number of errors for your function is too high over a given time period or if the Iterator Age of your DynamoDB stream is too high, indicating that you're falling behind in processing. You can investigate the cause of the error, make the necessary fix, and redeploy your function to handle the new record schema and continue your indexing as usual. Between the \"soft failure\" mode of logging and moving on, and the \"hard failure\" mode of stopping everything on an error, I like a third option that allows us to retain the structured record in a programmatically-accessible way, while still continuing to process events. To do this, we create an SQS queue for storing failed messages. When an unexpected exception is raised, we capture the failure message and store it in the SQS queue along with the record. An example implementation looks like:  Our main handler function is very short and simple. Each record is passed through a `handle_record` function, which contains our actual business logic. If any unexpected exception is raised, the record and exception are passed to a `handle_failed_record` function, which is shown below:  Pretty straightforward -- it takes in the failed record and the exception and writes them in a message to an SQS queue. When using this pattern, it helps to think of operating on individual records, rather than a batch of records. All of your business logic is contained on `handle_record`, which operates on a single record. This is useful when reprocessing failed records, as you can reuse the same logic. Imagine the unexpected error in your function was due to a bug in your logic that only affected a subset of records. You can fix the logic and redeploy, but you still need to process the records that failed in the interim. Since your `handle_record` function operates on a single record, you can just read records from the queue and send them through that same entry point:  This is a simple reprocessing script that you can run locally or invoke with a reprocessing Lambda. It reads messages from the SQS queue and parses out the `record` object, which is the same as the `record` input from a batch of records from the DynamoDB stream. This record is passed into the updated `handle_record` function and the queue message is deleted if the operation is successful. This pattern isn't perfect, but I've found it to be a nice compromise between the two extremes of the failure spectrum when processing streams with Lambda. Concurrency Limits Finally, let's talk about concurrency limits with DynamoDB streams. The big benefit of streams is the _independence_ of the consumers -- the Algolia indexing operations are completely separate from the process that updates the marketing team's CRM. The development team that manages the user search index doesn't even need to know about the marketing team's needs or existence, and vice versa. However, it's not quite accurate to say that consumers are completely independent. DynamoDB streams are similar to Kinesis streams under the hood. These streams throttle reads in two ways: throughput and read requests. For throughput, you may read MB per second from a single shard. For read requests, Kinesis streams have a limit of read requests per second on a single shard. For DynamoDB streams, these limits are even more strict -- AWS recommends to have no more than consumers reading from a DynamoDB stream shard. If you had more than consumers, as in our example from Part I of this blog post, you'll experience throttling. To me, the read request limits are a defect of the Kinesis and DynamoDB streams. If you are hitting throughput limits on your streams, you can increase the number of shards as the MB limit is on a per-shard basis. However, there's no similar scaling mechanism if you want to increase the number of read requests. Every consumer needs to read from every shard, so increasing the number of shards does not help you scale out consumers. The entire notion of an immutable log like Kinesis or Kafka is to allow for multiple independent consumers (check out Jay Krep's excellent book, I Heart Logs, for a better understanding of immutable logs). With the current read request limits in Kinesis and DynamoDB streams, the number of consumers is severely constrained. Conclusion In this post, we discussed some implementation details and some gotchas to watch out for when using stream-based Lambda invocations. Now it's your turn -- tell us what you build with event-driven architectures!",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e3e",
      "title": "Serverless (Cron)icle  - News from the Serverless Community",
      "content": "The open source Serverless Framework is backed by a super smart and extensive community of developers (we've passed , stars on GitHub). Besides contributing code, our community is passionate about sharing their knowledge and expertise. Here's a roundup of some of these articles, videos, and posts to help you keep up with the news in the serverless ecosystem. What is Serverless all about? Serverless. The new hype buzzword is taking over the development universe, promising big savings in infrastructure for applications and less deployment headaches to developers. In an agile world, shipping scalable software with budget constraints has become a big puzzle: one that Serverless may solve. - By Jonatas Baldin The Need for Asynchronous FaaS Call Chains in Serverless Systems Functions as a Service (FaaS) provide massive gains in efficiency since users are no longer billed for idle time. FaaS is billed based on execution time and allocated resources  whether you use those resources or not. So time spent waiting is money wasted  and synchronous invocation of other functions means double billing. - By Ben Kehoe Developing serverless backends with OpenWhisk and API Gateway integrations As actions can be seen as flexible and independently deployable microservices they are perfectly suited to build up entirely serverless microservice backends that expose functions via APIs. In this context, APIs form the digital glue that links services, applications, sensors, and mobile devices to create compelling customer experiences and helps businesses to tap into new market opportunities. They allow you to bring new digital services to market, open revenue channels and exceed customer expectations. - By Andreas Nauerz Let serverless solve the technology problems you dont have This is a story about how serverless can help you do the things you already know you should be doing but arent. The thing is you probably know all of this. Most of the people in your organization probably know all of this. So why arent you spending more of your time solving your business problems? - By Phillip Manwaring Customizing The Serverless Framework With Plugins So you already know the Serverless Framework - its handy for converting a pretty simple config format into a pretty complex CloudFormation template. When youre deploying, it builds that template and uploads your code for you. But if this isnt your first serverless application, you might have special requirements. Ive had needs like custom metrics, performing extra security/preflight checks, or removing the auto-created resources (such as the Cognito User Pool) from the template and subbing in my own. - By Ryan S. Brown",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e40",
      "title": "Anatomy of a Serverless Application",
      "content": "We've all been new to serverless before. In this post, I'll walk you through how to get up and running on your first application. Let's cut through the docs, shall we? This application will be a backend email service that can be called over HTTP from a simple frontend like `curl`. You will learn how to: Setup the development environment Create an application project Create a serverless service using a boilerplate template Run and test the service locally Deploy the service Run the service via the public HTTP endpoint Perform basic validation and error handling Getting Started I had been following serverless technologies for a while, and skimmed over the provider documentation and examples. It was really helpful to know the lay of the land and what was available out there. AWS Lambda's getting started documentation was helpful but overwhelming, and it was tedious to use the AWS Console. I wanted to use my own development workflow - code using my favorite editor, build using an easy to use toolchain, do a test/debug cycle, and finally deploy. I had to make some choices before I started development: Programming language: NodeJS (my familiarity, all serverless platforms support it) Platform: AWS Lambda (most popular and mature, lots of supporting services) Toolset: Serverless Framework (opensource, repo with K+ stars, actively maintained, week release cadence) Setup The initial setup was straightforward: Install NodeJS: download or using package manager Install the Serverless Framework: `npm install -g serverless` Setup an AWS account Sign up for AWS Create an IAM User Install and setup AWS CLI The AWS setup is necessary so that we can deploy our serverless application on AWS Lambda. After creating the AWS IAM user, we'll have to configure the credentials to give access to the Serverless Framework, for creating and managing resources on our behalf. You can use either of the options to configure the credentials: `aws configure` or `serverless config credentials` Creating the Project A little bit of planning on the project structure makes a lot of difference in visualizing the different parts of the system. We will build an application named `postman`, with a simple frontend using `curl` and a backend serverless email service, to send out emails to users over HTTP. Here is an example that works for me: ```bash |-- postman |-- README.md |-- frontend `-- services ``` where, `frontend` folder holds the frontend application `services` folder holds the serverless service(s) A structure like this provides clear separation for the non-serverless and serverless code for the overall application. The way the non-serverless portion of the application is written is totally your choice. In this post, I will primarily focus on the serverless portion of the application inside the `services` folder. We will not create a frontend application, so we don't need the `frontend` folder. Creating the Email Service Let's create an email service that will send out emails to users with some text. We will use Mailgun as our email service provider, and shoot for making the email service generic enough to be reused across other applications. Starting With a Boilerplate Template The Serverless Framework comes with boilerplate templates that make it really quick to get started. In our case, since we are using AWS as our provider and NodeJS as our language of choice, we will start with:  which creates the service for us:  Note: We have been passing in data via the `-p` flag in our previous examples but you can also pass in data using the `--data` flag. Leaving an exception unhandled is not acceptable, so let's refactor the code to return a proper HTTP response.  Now, when we deploy the function and call it, we get a better response.  This concludes the development of the application. Summary We explored the path to creating a serverless application from scratch, starting with a boiler plate template. We then customized the code, tested the code locally, and deployed it to AWS Lambda. Finally, we accessed the public function via the HTTP endpoint, and also looked at some error conditions and use cases. At the end of it all, we created a fully functional serverless backed email service that sent out emails via the Mailgun email service provider.",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e42",
      "title": "Event-driven Influencers - A Cloud Guru",
      "content": "Were getting geared up for Emitour conference on event-driven architectures. Emit has some of the most prominent speakers and sponsors in the field, and in this five-part Influencers series were highlighting their contributions. Want to know more about where we think event-driven architectures will be in two years? Join us on August . Background There is a lot of value on the table for enterprises who go serverless, but the problem is: its a rapidly-changing space. Very few terms are lined out in concrete, and even the most established experts are still debating about things like what an event really is. A Cloud Guru saw early on that there was a disparity in what people could do in the cloud, and what people knew they could do in the cloud. They stepped up to fill in those educational gaps, with a hard focus on serverless computing and the event-driven mindset it facilitates. For a company who wants to reap the benefits of event-driven architecture, or for a developer who wants to expand their reach, A Cloud Guru does a great job of keeping it relevant. They continually cull out the latest industry best practices and create new courses based on what developers really need to know. Looking to the future Look: this is the nature of the web. Nothing stays the same. As a developer, it will always be necessary to learn new languages and frameworks. As a company, it will always be necessary to update your digital strategy in order to stay competitive. Education is your weapon. A Cloud Guru keeps their eyes on the pulse of what is driving the industry, and updates their course materials in stride. As deeper knowledge becomes necessary, they provide certification courses that keep development teams up to date and drive value. Every change has its bellweather. A Cloud Guru is telling you exactly where to look.",
      "__v": 0
    },
    {
      "_id": "64e08919b72e199dda603e44",
      "title": "Serverless (Cron)icle  - News from the Serverless Community",
      "content": "The open source Serverless Framework is backed by a super smart and extensive community of developers (we've passed , stars on GitHub). Besides contributing code, our community is passionate about sharing their knowledge and expertise. Here's a roundup of some of these articles, videos, and posts to help you keep up with the news in the serverless ecosystem. BinaryAlert: Real-time Serverless Malware Detection Airbnb is proud to announce the open-source release of BinaryAlert: a serverless, real-time framework for detecting malicious files. BinaryAlert can efficiently analyze millions of files a day with a configurable set of YARA rules and will trigger an alert as soon as anything malicious is discovered! Organizations can deploy BinaryAlert to their private AWS account in a matter of minutes, allowing them to analyze internal files and documents within the confines of their own environment. - By Austin Byers Three Startups Using IBM OpenWhisk Serverless to Transform Their Industries Serverless computing, which many are hailing as the next era of cloud computing, relieves many of these hassles by abstracting away infrastructure, running code and scaling on-demand. For developers, serverless platforms with a strong cognitive stack, such as IBM Bluemix OpenWhisk, gives them unprecedented access to powerful services such as Watson APIs, the Watson IoT Platform and weather intelligence. - By Jason McGee How long does AWS Lambda keep your idle functions around before a cold start? Using AWS Step Function to find the longest time your AWS Lambda function can idle before the resources are reclaimed. - By Yan Cui Developer Experience Lessons Operating a Serverless-like Platform At Netflix The Netflix API is based on a dynamic scripting platform that handles thousands of changes per day. This platform allows our client developers to create a customized API experience on over a thousand device types by executing server side adapter code in response to HTTP requests. Developers are only responsible for the adapter code they write; they do not have to worry about infrastructure concerns related to server management and operations. To these developers, the scripting platform in effect, provides an experience similar to that offered by serverless or FaaS platforms. - By Vasanth Asokan, Ludovic Galibert and Sangeeta Narayanan Is it possible to host Facebook on AWS? Facebook has been around since . In the years since, the company, now one of the five US tech giants, has moved from a single server running in a dorm room to seven purpose-built data centres dotted around the globe. Its likely there are more planned for coming years as Facebook expects its user count of .BN to continue growing. Recent news of Snaps $BN + $BN deals with Google Cloud Platform and AWS (Amazon Web Services) got us wondering whether its possible to run the behemoth that is Facebook on AWS. Now remember, were not asking if Facebook should host on AWS - were just asking if its possible. - By SQLizer",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e46",
      "title": "Event-driven Influencers - AWS Lambda",
      "content": "Were getting geared up for Emitour conference on event-driven architectures. Emit has some of the most prominent speakers and sponsors in the field, and in this five-part Influencers series were highlighting their contributions. Want to know more about where we think event-driven architectures will be in two years? Join us on August . Background Type event-driven architecture into your search bar, and youll get relevant results all the way back to . Weve known for centuries (in internet years) that event-driven architectures are powerful. So why is it just now becoming an industry buzzword? Until , building out microservices was really intricate, and almost no one had the resources to throw at it. And then Amazon Web Services launched AWS Lambda: a brand new event-driven compute service. Lambda gave everyone a way to be event-driven. The flood gates opened. Lambda has fundamentally amplified the way companies can grow their digital business. Teams are able to create apps that rapidly, automatically scalewith no need to maintain any servers internally. Code runs in the cloud in response to events, and the compute resources used are billed on a pay-per-execution basis. No server capacity is wasted. Products iterate and improve faster because developers dont need to request provisioning; there is less overhead involved in testing and pushing new code. Lambda has accelerated the rate of change. Looking to the future Serverless computing is already shifting the ways we think about structuring digital products. With traditional architectures, we used to think proactively (read: anxiously), and run constant checks for new information. Do we have a new user yet? No? How about now? Okay, send a welcome email. Lambdas event-driven model encourages us to sit back and react to user events as they occur. Someone just created a new account? Your Lambda code is always listening, and will send them a welcome email the second the event is triggered. No more checking. Enterprises are piling on board to enhance their competitive edge. Coca-cola, for instance, is using Lambda for managing cross-promotional activities and for digitally claiming bottle cap rewards. Lambda is heavily integrated with a wide range of AWS services, and will continue to expand its breadth. Much like the way smartphones shifted development focus from single, text-driven websites to modular, gesture-driven applications, Lambda is pushing digital product development forward into event-driven design. Its hard to even predict what types of services event-driven architectures will allow us to create two years from now, or how they will shift our perceptions of what applications on the internet should be able to do. This is the next iteration of the web, and its only picking up speed.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e48",
      "title": "Event-driven Influencers - Trek",
      "content": "Were getting geared up for Emitour conference on event-driven architectures. Emit has some of the most prominent speakers and sponsors in the field, and in this five-part Influencers series were highlighting their contributions. Want to know more about where we think event-driven architectures will be in two years? Join us on August . Background Jared Short is a self-proclaimed purveyor of the bleeding edge. But hey. Weve known that for a while. Hes the Director of Innovation at Trek, who coincidentally is the first Serverless Partner company we ever accepted into the program. They have been active contributors to our GitHub and maintain a fantastic blog. At Trek, they saw immediately that building serverless apps was a way to take on minimal development overhead, and still produce a fine-tuned piece of software that would infinitely scale in production. They have created apps for clients (from small business to enterprise) that do tens of millions of hits per dayall built on serverless infrastructure. So, they have a lot of proof to go off of when they say that going serverless has cut their infrastructure development time in half. Looking to the Future When we interviewed Jared in May, he was telling us about the way companies are skipping over the container trend and moving straight into serverless, because event-driven applications are a breath of fresh air. A developer can say, Look, heres my codejust run it in response to these particular events, exactly when I need them, at the scale I need them at. Serverless computing makes it make more sense to focus on action and reaction. When companies think in terms of events, they can design more along the lines of: Okay, when a user clicks delete account, we want to get a notification to a specific Slack channel and we want to trigger this specific email survey to them. As for where serverless will be in a few more years? An event-driven mindset means Trek can continue to release better, more reactive apps. As they saw early on, actions carry the value.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e4a",
      "title": "Things to Know Before Migrating An Existing Service to Serverless",
      "content": "Last year, my company decided to make the plunge. We were going to go Serverless! Except...most of the resources about serverless architectures are about how to start from scratch, not how to migrate existing services over. We spent eight months figuring it out along the way and, for all of you serverless hopefuls, we made a cheat sheet. These are the steps that worked for us: Identify the problems Train the existing team Create a Proof of Concept to verify that the problem is solved Optimize the solution to take advantage of the cloud Automate your Continuous Integration/Continuous Deployment pipeline Automate your testing Identify the problems What problems will serverless solve that your current solution does not? We (for example) wanted serverless to help us () lower operational costs, and () give us an easier way to replace a bunch of legacy systems with a small team. At my company, we decided that since AWS Lambda can be used as a glue between different AWS managed services, it was the best option for us. Train the team After you have defined the problems, it is easier to define (more or less) what set of technologies can help you. Your developers are the ones who already know your business logic and service requirements better than anyone else. So instead of heavily outsourcing, I'd recommend that if no one on your existing team is familiar with serverless, it is time to train them. We encouraged our developers to go to meetups, conferences and to spend time (even work hours) tinkering with new technologies. In addition to that, we did hire a Serverless consultant to show us how to think in an event-driven manner and make sure we were following best practices. In my opinion, one of the reasons our project was so successful was the weekly workshop meetings we held. They lasted - hours per week, and we used that time for discussing new serverless-related topics, solving issues and implementing solutions. We all got to learn a lot from each other, and workshop was very inspiring. Create a Proof of Concept and validate your hypothesis After you have identified the key problems to solve, and after your team has a better understanding of the tools available, it's time to create Proof of Concept. By now, your team should have an idea of what to do nexta \"hypothesis\". A Proof of Concept will help the team validate their hypothesis. Remember: a Proof of Concept is not production code. Use it to focus on solving your problem and get rid of it. Your Proof of Concept should go to the trash after validating your hypothesis. In our project, we developed five Proofs of Concept. Recall that our problems were: () to replace legacy systems and () reduce operational costs. Our hypotheses were as follows: To replace legacy systems: - AWS Cognito will replace the existing authentication system - DynamoDB will replace the existing Riak database as our NoSQL database - AWS Lambda + S + Elastic Transcoding will replace our existing transcoding process - AWS Lambda + API Gateway + S will replace our existing image resizing and provide better caching mechanisms To reduce high operational costs: - A workflow using Step functions + S + AWS Lambdas will replace our existing EC instances By implementing these Proofs of Concept, my team ended up validating all of our hypotheses. Optimize the solution to take advantage of the cloud This step is critically in line with the previous step. When thinking about your Proofs of Concept and new architecture, it's important to take full advantage of the cloud. As in: dont just grab your instances and decompose them into AWS Lambdas and API Gateways using a DynamoBD; try to think about how to take advantage of cloud-managed services, like queues and caches. Also remember that by migrating everything to Serverless, you are transforming the architecture of your system into an event-driven architecture. In an event-driven architecture, events move around your system and all the services are decoupled from each other. AWS has a lot of services that can be used to manage events communication, like queues and streams. S is a great place to store your events. DynamoDB Streams can be used to let other services know that there was a change in your DynamoDB database. Automate your Continuous Integration/Continuous Deployment pipeline Since people can disagree on what exactly a microservice is, I call a microservice a collection of AWS Lambdas and other resources related to some very specific domain, like authentication, application management or transcoding. Serverless architectures involve lots of resources being deployed into different environments in this way. When you have several moving parts, one way to make things simpler is to automate everything you can. On my team, we used Serverless Framework to organize our projects and to automate microservice deployment into different environments. We wanted to define all our infrastructure configuration as code. The Serverless Framework helped us to do that, as all the resources needed in the microservice can be defined using Cloud Formation notation in the Serverless YAML file. We used Jenkins Continuous Integration server to take care of running the Serverless Framework deployment in three different environments (development, staging, and production). For each environment, we used a different AWS account. We wanted to have three accounts so we could take advantage of managed resources soft limits as much as possible, and also to keep the different environments isolated. Automate your testing Testing is undervalued by a lot of teams. In a Serverless and event-driven architecture, the complexity of the code moves to the architecture itself. Because of this, testing at all levels is critical for peace of mind. Test, test, test. Also, test. Unit tests will help you to make sure that your business logic works. For your managed services, you should write integration tests against them. It's important to define clear interfaces between your services and the managed services so that you can have control of what is going in and out of your service. Dont forget about End-to-end Testing. In an event-driven architecture, the services are decoupled from each other; as a result, it can be hard to know how the events are moving around the architecture. Conclusion Migrating an existing service to Serverless was work, true-but honestly, it was also fun. The two most important things to have in mind when migrating one (or all) of your existing services to serverless are: - optimize your architecture to take advantage of the cloud - remember that your new architecture is an event-driven one Best of luck, and see you on the other side.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e4c",
      "title": "Serverless (Cron)icle  - News from the Serverless Community",
      "content": "The open source Serverless Framework is backed by a super smart and extensive community of developers (we've passed , stars on GitHub). Besides contributing code, our community is passionate about sharing their knowledge and expertise. Here's a roundup of some of these articles, videos, and posts to help you keep up with the news in the serverless ecosystem. Serverless Web apps without client-side Javascript When most people talk about Serverless architecture, theyre normally talking about REST APIs interacting with a javascript or mobile client, but its possible to use Serverless to process HTTP requests from Web browsers (including HTTP form posts) and respond with HTML. This is really powerful, because it allows you to build Web applications which dont require a Web server to operate and dont require javascript at the client-side. - By Adrian Hesketh Im here to tell you the truth, the good, the bad and the ugly of AWS X-Ray and Lambda AWS announced general availability of AWS Lambda support for AWS X-Ray back in May . While there is plenty of value using AWS X-Ray with Lambda, the current limitations restricts its usefulness for complex systems. - By Yan Cui Using Apache Kafka as a Scalable, Event-Driven Backbone for Service Architectures While Apache Kafka is a messaging system of sorts, its quite different from typical brokers. It comes with both pros and cons and, like any technology we choose, this shapes the design of the systems we write. - By Ben Stopford Predictions for the direction of serverless platforms I want to focus on a couple of things relevant to the evolution of Serverless as a platform and the resulting commercial ecosystem, namely the importance of Open Service Broker and a bet on OpenWhisk. - By Gareth Rushgrove Exploring Serverless with Python, StepFunctions, and Web Front-end Serverless slack invite and community signup project. Serverless Python project boilerplate. Story of my serverless exploration. Tips, tricks, links. - By Dmitri Zimine",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e4e",
      "title": "How to Handle your Python packaging in Lambda with Serverless plugins",
      "content": "I use AWS Lambda for almost all of my projects these days-from Flask apps and Slack bots to cron jobs and monitoring tools. I love how cheap and easy it is to deploy something valuable. Python is my go-to language, but handling Python packages in Lambda can be tricky. Many important packages need to compile C extensions, like psycopg for Postgres access, or numpy, scipy, pandas, or sklearn for numerical analysis. If you compile these on a Mac or Windows system, you'll get an error when your Lambda tries to load them. The import path also requires finesse. You can install your dependencies directly into your top-level directory, but that clutters up your workspace. If you install them into a subdirectory like `deps/` or `vendored/`, you have to mess with your `sys.path` at the beginning of your function. But there is a much better way. In this post, I'll show you a how, by using the `serverless-python-requirements` plugin for the Serverless Framework. Initial Setup Let's get our environment ready. If you have Node and NPM installed, install the Serverless Framework globally with:  You'll also need to configure your environment with AWS credentials. Note: if you need a refresher on how to install the Framework or get AWS credentials, check out the Prerequisites portion on the top of our Quick Start Guide. Creating your service locally For this quick demo, we'll deploy a Lambda function that uses the popular NumPy package. We can create a service from a template. I'm going to use Python , but this works with Python as well.  This will create a Serverless Python template project at the given path (`numpy-test/`) with a service name of `numpy-test`. You'll need to change into that directory and create a virtual environment for developing locally. (Note: further reading here about how and why to use virtual environments with Python.)  Let's set up the function we want to deploy. Edit the contents of `handler.py` so that it contains the following:  This is a super simple function using an example from the NumPy Quick Start. When working with Lambda, you'll need to define a function that accepts two arguments: `event`, and `context`. You can read more at AWS about the Lambda Function Handler for Python. Notice the last two lines of the file, which give us a way to quickly test the function locally. If we run `python handler.py`, it will run our `main()` function. Let's give it a shot:  Ah, we haven't installed `numpy` in our virtual environment yet. Let's do that now, and save the package versions of our environment to a `requirements.txt` file:  If we run our command locally now, we'll see the output we want:  Perfect. Deploying your service Our function is working locally, and it's ready for us to deploy to Lambda. Edit the `serverless.yml` file to look like the following:  This is a basic service called `numpy-test`. It will deploy a single Python . function named `numpy` to AWS, and the entry point for the `numpy` function is the `main` function in the `handler.py` module. Our last step before deploying is to add the `serverless-python-requirements` plugin. Create a package.json file for saving your node dependencies. Accept the defaults, then install the plugin:  To configure our `serverless.yml` file to use the plugin, we'll add the following lines in our `serverless.yml`:  _Note: a previous version of this post set `dockerizePip: true` instead of `dockerizePip: non-linux`. You'll need `serverless-python-requirements` v.. or higher for this option._ But, Docker packaging is essential if you need to build native packages that are part of your dependencies like Psycopg, NumPy, Pandas, etc. The `plugins` section registers the plugin with the Framework. In the `custom` section, we tell the plugin to use Docker when installing packages with pip. It will use a Docker container that's similar to the Lambda environment so the compiled extensions will be compatible. You will need Docker installed for this to work. The plugin works by hooking into the Framework on a deploy command. Before your package is zipped, it uses Docker to install the packages listed in your `requirements.txt` file and save them to a `.requirements/` directory. It then symlinks the contents of `.requirements/` into your top-level directory so that Python imports work as expected. After the deploy is finished, it cleans up the symlinks to keep your directory clean.  Great. Let's invoke our `numpy` function and read the logs:  And there it is. You've got NumPy in your Lambda! Be sure to check out the repo for additional functionality, including automatic compression of libraries before deploying, which can be a huge help with the larger numerical libraries in Python. Many thanks to the United Income team and Daniel Schep in particular for creating the `serverless-python-requirements` package. If you want to work on serverless full-time, check out United Income. They use a % serverless architecture for everything from serving up their web application to running millions of financial simulations, and they are always looking for talented engineers to join their growing team in Washington, DC.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e50",
      "title": "Event-driven Influencers - Microsoft Azure",
      "content": "Were getting geared up for Emitour conference on event-driven architectures. Emit has some of the most prominent speakers and sponsors in the field, and in this five-part Influencers series were highlighting their contributions. Want to know more about where we think event-driven architectures will be in two years? Join us on August . Background We spend a lot of our time talking to engineering teams. One thing thats been made clear to usfor organizations across the board, digital products are monopolizing revenue focus. Inside Silicon Valley or not, every company is a tech company. Every company is looking to enhance its digital portfolio. When Azure Functions busted onto the scene, they had a goal in mind: to be the serverless infrastructure for enterprise. They moved quickly to support a flexible range of deployment options and languages (C, JavaScript, F, Python, Batch, PHP, PowerShell); still the most of any serverless provider. They even let you try out Azure functions without making an account, so theres little in the way of getting started. And it makes sense. Having a lot of support and options means that when companies like Fujifilm decide to build new serverless apps, they can do so with minimal friction. Serverless architectures cut operational costs to a fifth of traditional (serverful) architectures. They shorten development test cycles and scale immediately with demand. Companies want to make the switch, but they also want it to be easy. Looking to the future When we talked with Jared Short a few months ago, he said that Azure was doing something really, really well that not a lot of people had seen yet. He was referring to their Logic Apps service. Logic Apps is built around the idea of events, triggers and workflows. Think something like Zapier or IFTTT, except for cloud services. High levelwhen you think about building microservices, there are a lot of moving parts to manage. Logic Apps lets you stitch them all together much more easily. It gives you a central place to build and manage all of your event-driven services. As Jared put best: I want this. What this shows is that serverless infrastructure is blooming. From its barebones start, providers are taking broad and detailed strokes to fill out the space of whats possible. Azure is thinking forward to the unknown unknowns. What will developers need next year? How will their work environments change? How can we be one step ahead?",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e52",
      "title": "Event-driven Influencers - Accenture",
      "content": "Were getting geared up for Emitour conference on event-driven architectures. Emit has some of the most prominent speakers and sponsors in the field, and in this five-part Influencers series were highlighting their contributions. Want to know more about where we think event-driven architectures will be in two years? Join us on August . Background Change tends to sneak by unnoticed at first. And when it happens fast, as it does in technical fields, that moment of noticing becomes a sudden reckoning. What have we missed? For how long? What does this thing even do? Accenture does change better than almost anyone. They anticipate it by studying whats new out there, daily, and asking themselves: What could we do with this? Theyve built cloud applications for everything from healthcare to finance to telecomservices that do things like monitor insurance claims and detect fraud. Their approach to building new technology is to place themselves in the problem space of an entire industry. From there, they think from the ground up about what those companies need to be able to do that they currently cant, and what it is they are already doing that they could be doing more efficiently. Looking to the future Its an all-too common conception that serverless is a convenient tool for small, disruptive organizations to use in their early stages. But this conception overlooks everything that large enterprises have to gain: much quicker feature release cycles (a.k.a., competitive edge), and slashed infrastructure costs. Accenture sees that too, and they are building ways for more established industries to take advantage. One result that were already seeing, is that enterprise companies are skipping over the container phase entirely. They are moving straight from their legacy architecture into serverless architectures. Our prediction? The strength of serverless technology will pierce through the enterprise sector much faster than most people expect. Except, of course, those who are already vigilant for this change, and who are on the ground, right now, implementing it.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e54",
      "title": "Serverless Ops  - Using CloudWatch Metrics & Alarms with Serverless Functions",
      "content": "For many users, the biggest benefit of serverless is how _managed_ it is -- developers and designers don't need to waste their time updating system packages or monitoring CPU usage. They can work on what they do best while the cloud provider handles most of the operations work. You can't avoid operations entirely though. In this post, we'll talk about the basics of monitoring your Lambda functions with CloudWatch metrics. This is the first post in a series of the basics of serverless operations. Basic CloudWatch metrics CloudWatch helps you by monitoring certain metrics for all of your Lambda functions automatically. These metrics include: - Invocations: The number of times your function is invoked; - Errors: The number of times your function fails with an error, due to timeouts, memory issues, unhandled exceptions, or other issues; - Throttles: The number of times your function is throttled. AWS limits the concurrent number of executions across all your functions. If you exceed that, your function will be throttled and won't be allowed to run. - Duration: How long your function runs. For every serverless service I run, I care about Errors and Throttles. I want to know if my code is failing for any reason, whether errors in my code or too many concurrent Lambda invocations in my account. To monitor Errors and Throttles, I use the serverless-plugin-aws-alerts plugin from the folks at A Cloud Guru. It makes it easy to set up alerts for your services. To use it in your Serverless service, first install the plugin in your Serverless service:  Then add it to your `serverless.yml`:  This setup adds alerts to all of our functions in our service when deployed to the `production` stage. For every one minute period where I have + errors or throttles, I'll get at email to `name@domain.com`. (Make sure to change the email to your email.) Advanced Usage: Custom CloudWatch Metrics In addition to Lambda's out-of-the-box CloudWatch metrics, you can also create your own custom metrics. Imagine you have a Lambda function that is processing records from a Kinesis stream. Your Lambda function will receive a batch of multiple records. Because of this batch, your visibility into what is happening is limited in a few ways: You can't tell by the Invocations metric how many records were processed; and If there's a record with unexpected input, you may want to know about it without throwing a hard error. Throwing an exception in a batch of Kinesis records will end up retrying the entire batch of records. We can handle both of these problems with CloudWatch custom metrics. Using the AWS SDK, you can make a `put_metric_data()` call with a CloudWatch client. The example below is in Python, but the APIs are similar for other languages:  In this example, we're storing a metric named `KinesisRecordsSeen` that stores the number of Kinesis records in each Lambda invocation batch. We're storing the metric in the `AWS/Lambda` namespace with a `FunctionName` dimension to segregate metrics from one another, so I could have a `KinesisRecordsSeen` metric for each of my different functions. I can easily set up alerts on my custom metrics as well. Let's say I want an email alert whenever I see more than Kinesis records in minutes:  A caveat here is that this will add latency to your overall Lambda execution as you will be waiting on the API call to CloudWatch. If you'd like, you could avoid this by using CloudWatch log Metric Filters to create metrics from your logs instead -- more detail here. As your Serverless application gets more serious, you will want to track metrics more closely using a tool like DataDog, IOPipe, or Honeycomb. But for quick and easy monitoring, it's hard to go wrong with CloudWatch and the serverless-plugin-aws-alerts plugin. Next post in the Serverless Ops series: - Serverless Ops : CloudWatch Logs and Centralized Logging with AWS Lambda",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e56",
      "title": "Use These  AWS Learning Resources to Master the Cloud",
      "content": "From a newbie's perspective, there is a lot happening inside of AWS. At the time of writing this post, there are over different products in the AWS admin console. SEVENTY-FOUR. Where to start? In this post, I'll highlight resources I wish I knew existed when diving into the world of AWS. (in no particular order) Amazon Web Services in Plain English  Alfred is one of my favorite productivity apps. This Alfred plugin is super handy for having the AWS docs literally right at your fingertips. I use this all the time when I am working with a particular AWS sdk and can't remember all the methods avialable to me.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e58",
      "title": "Serverless Experts You Should Follow on Twitter",
      "content": "This list was ordered more or less by a random number generator, so dont take these as ranked recommendations. Theyre all equally worth an add. Mark Russinovich (@markrussinovich) Mark is the CTO of Azure, so suffice to say hes an insider. Hes (obviously) always the first to post about updates from Azure, but what we love most about his feed is that he re-tweets the best stuff from all over. It isnt all product all the timethere are also server jokes.  Now, is this everyone to follow on Twitter? Definitely not. But we hope it'll give you a good start.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e5a",
      "title": "Event Gateway - The Missing Piece of Serverless Architectures",
      "content": " Background The innovation of serverless compute (FaaS) was huge. It dramatically decreased operational complexity and allowed developers to perform compute more easily than ever. Then entered the Serverless Framework, which offered an application experience of functions and events around serverless computing. This is now widely known as serverless architecture. Despite being relatively new, serverless architectures have proven themselves well. Serverless teams consistently exhibit shortened time to market, increased developer productivity and reduced operational overhead. But there has been a missing piece. Developers have been locked into a single cloud provider, unable to perform service communication between various services. They have been left without a good way to perform service discovery across different teams and applications. That is exactly why we made Event Gateway. Introducing: The Serverless Event Gateway The Event Gateway is an open-source communication fabric for serverless architectures. It combines both API gateway and pub/sub functionality into a single experience. Inside the Event Gateway, all data is considered to be an event. This lets developers react to data flows of all their applications in a centralized way, with serverless compute. This is powerful; when developers can manage those data flows from a single place, they can take events from one provider and trigger functions on another provider. Serverless architectures become truly cross-cloud. Features The Serverless Event Gateway is the missing piece of serverless architectures. Cross-cloud Businesses do not want to be limited by where they can access their data. With Event Gateway, any of your events can have multiple subscribers from any other cloud service. Lambda can talk to Azure can talk to OpenWhisk. This makes businesses completely flexible. Building an events-first experience that exists cross-cloud and on-premise protects you from lock-in, while also keeping you open for whatever else the future may bring. Open Source The Event Gateway is open-source and platform agnostic. Use it to create the cohesive nervous system of your digital business. Run it on all the major cloud providers, on-premise or in a hybrid architecture. Unify events from all over your system. Even teams who are working on separate applications can easily share resources that shave time and overhead. Tightly integrates with Serverless Framework The Event Gateway ties right into the Serverless Framework and is available for developers to use locally today. Get started Use the Event Gateway to start taking full advantage of the serverless cloud. Serverless architectures just got their missing backbone. The Event Gateway is currently in beta, and is available to use locally via the Serverless Framework. To check out the code, see the repo here and walk through the example app.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e5c",
      "title": "The Definitive Guide to Serverless Architectures",
      "content": " for your Github profile. License This work is licensed under a Creative Commons Attribution-ShareAlike . International License.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e5e",
      "title": "How Serverless Defined Its Team Values",
      "content": "As a solo founder with a lot on his plate, it would have been so easy for our CEO, Austen Collins, to let values and culture take a back seat. But he didnt. Austen knew that having a strong mission was the key to attracting top talent, and from day one he was determined to have a clearly-defined culture that resonated with people. It isn't as straightforward as you'd think to define such a lofty thing as culture, though. It took us a few tries to get it right. Heres how the serverless team worked to bring it all together. Dive below the iceberg Fun fact: when you see icebergs floating in the water, youre really only seeing the top %. The other % is hidden beneath the surface of the water. This cultural iceberg metaphor is one we use a lot when discussing values amongst our team. Organizational culture represents itself with language, artifacts and traditions (the visible piece of the iceberg)but those things are only a manifestation of the core values that team members intuitively feel under the surface. When you clearly define what lies under the surface of your team, it means that everyone can swiftly move towards the same end goal. Recruiting gets easier, as prospective team members easily get a sense of whether they will flourish in your environment. Existing team members have a much easier time prioritizing projects and feeling confident enough to initiate new projects that will propel the company forward. For a practical example of how having a strongly-defined team culture has already helped us day-to-day, one of the lessons weve learned at Serverless is that team collaboration is a core value for us. When candidates do their onsite interviews, we make sure that their project includes collaborating with other team members. In a nutshell? It makes it easier to get what your organization needs when you know exactly what your organization needs. Bottom-up approach: round Defining culture at Serverless has been an ongoing, participative journey. Our first attempt was at our January team retreat, where we tried a bottom-up approach. Each team member got a set of sticky notes, where they wrote out first their own personal values, and then their aspirational company values. We posted each note on whiteboards and clustered them into groups as a team. At the end of the exercise, our list looked like this: - Camaraderie - Collaboration - Results-oriented - Accountability - Bold & Ambitious - Open Source - Keep It Simple It seemed great! ...at the time. We quickly realized there were weaknesses with the initial list. Bottom-up approach: round When we were back at the office trying to synthesize everything after the retreat, we found it challenging to make our values fit into neat, actionable boxes. We didnt know how to apply them to the actual behaviors we wanted to see. So we iterated. We took our fuzzy list and teased it into a series of condensed points. What emerged were the final value statements that we all felt we would be empowered to follow: Serverless is a group project It takes the whole team working together to meet Serverless bold and ambitious goals. We help each other and strive to remove any roadblocks that slow our teammates down. We hold each other accountable and communicate expectations clearly through asynchronous communication (Slack, Github, Email) and ask for help when we need it. Keep our eyes on the prize Its easy to get distracted when pioneering technology in a new space. We keep the team successful by setting clear goals that start at the company level and work their way down to each individual team member. We focus on results and celebrate success as a team. Blaze the trail As the first mover in the serverless space, our team is bold and ambitious. We are always looking for ways to embrace new technology of all stripes and make it accessible to our community. Open source forever The open source community is an integral strategy to our success. We will never forget where we came from and we pledge to always give back. Simplicity is king Its easy to overly complicate language, code, and projects. We strive to create the most simple and elegant solutions. When communicating we ask, How can I say this in five words instead of ten? Work with freedom and responsibility Be a good teammate, honor your commitments, and strive to exceed all expectations. However you manage to do that is up to you. On to round Spoiler alert: the v of our bottom-up approach still wasnt quite what we needed. Our goal was to make our values tell a story that would be digestibleboth on our website and to existing team members. In practice, they kind of fell flat. They didnt give prospective team members a realistic understanding of what it was like to work at Serverless, or provide clear guidance about company expectations to existing team members. We received feedback from our team that these condensed values were difficult to relate to. We also went through some painful hiring experiences. We brought on really talented individuals who, in theory were going to be rock stars, but in practice our culture just was not a good fit for them and they struggled before leaving after a few months. Trying a top-down approach We decided to take a different approach during our July team retreat. This time we abandoned our bottom-up approach and went top-down. (Dont worry, tons of sticky notes were still involved.) Each team member wrote - things (respectively) that they: love about Serverless, hate about Serverless, and wish we did differently. We found that several core values fell out in the process. Where we are today Weve found ourselves with another list. This time, a list of goals. We are actively working to weave these values more tightly into our day-to-day: - Empathy - Trust - Respect - Grit - Learning culture - Constructive feedback We are putting them into our our culture handbook and creating tests for each of these in our interview process. This is certainly our best system so far, but we dont doubt that well soon be humbled. Where were headed Our culture process, much like our software, is open to the community. We put our culture handbook in GitHub; it is a living document that, as you can see from this post, is constantly improving. Feel free to follow along as we continue to flesh it out.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e60",
      "title": "Key takeaways for the future of serverless - Emit  recap",
      "content": "Last Thursday, we held Emit Conf here in San Francisco. It was a tight-knit gathering of thought leaders and early pioneers in the serverless space. We saw some killer presentations and had even better conversations, with topics ranging from theorizing about system limits to practical real-world use cases of serverless architectures. In case you couldnt make it out, we compiled some of the most important takeaways and themes. Event unification Serverless architectures provide a lot of incentives to go event-driven, so its not surprising that we got several talks on this topic. In an event-driven world, all data are represented as events. Storing these events and making them useful is paramount. Rob Gruhl, Senior Manager of the Serverless Platform Team at Nordstrom, talked about the unified event log approach that Nordstrom takes to record all application state changes. This approach, often known as an event-sourced architecture, results in a lot of events, which can potentially be put to use by developers deploying serverless functions. Robs talk did a great job of surfacing the potential of event-sourced architecture, and also talked about some hard, unsolved problems within it that his team has been working on. See the slides. Austen Collins, CEO of Serverless Inc., unveiled a new open-source project that is also focused on event unification. The Event Gateway is designed to be the central piece of a serverless architecture. It collects all events in a system and exposes them to developers so they can be easily routed to functions. While many developers currently use systems like Kafka to accomplish this, the Event Gateway includes additional features, such as an API gateway, specifically designed to make it play nicely with a serverless architecture. See the slides.  Stay tuned We have more Emit tidbits coming soon, including speaker videos. Keep your eyes on @EmitConf and @goserverless to know as soon as they are released.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e62",
      "title": "Kubeless brings functions as a service to Kubernetes & on-prem",
      "content": "One of the primary goals of the serverless framework is to provide a platform-agnostic cloud experience for developers. We want it to be easy for people to write code once and have the freedom to choose where that code runs. That is why we are super excited to announce a brand new serverless provider Kubeless to expand where your functions can run. Introducing Kubeless Made by the the fine folks over at Bitnami, Kubeless is a Kubernetes-native way to deploy and manage your serverless functions via the serverless framework. Kubeless lets you deploy small bits of code without having to worry about the underlying infrastructure. It leverages Kubernetes resources to provide auto-scaling, API routing, monitoring, troubleshooting and more. Checkout the Kubeless Docs This integrations brings functions and events into your Kubernetes cluster. functions and events are logically grouped together in services. Let's explore those two main concepts and how they pertain to Kubeless. Functions A Function is an independent unit of deployment, like a microservice. It's merely code, deployed in the cloud, that is most often written to perform a single job such as: - Saving a user to the database - Processing a file in a database - Performing a scheduled task (To be added in newer versions) The framework is designed to help you easily develop and deploy Functions into your Kubernetes cluster. Events Anything that triggers a Kubeless Function to execute is regarded by the Framework as an Event. Here are some examples of events: - An API Gateway HTTP endpoint (e.g., for a REST API) - A Kafka queue message (e.g., a message) - A scheduled timer (e.g., run every minutes) (To be added in newer versions) See list of supported events Services Functions and events are grouped together in services and configured with a `serverless.yml` file. `serverless.yml` is where you define your Functions and the Events that trigger them. Continue reading for an example at the end of this post. Note: serverless services are not to be confused with Kubernetes services Serverless + Kubeless for on-prem functions Not a day goes by without a user of the framework shooting us an email asking about on-prem support. It's a widely requested feature and Kubeless delivers on-premise to the enterprise. We are very excited to see what people start building with it. Getting Started with Serverless & Kubeless Make sure you have the serverless framework installed on your machine.  Then create a new service with the `sls create` command.  Kubeless runs on Kubernetes, therefor you need a working Kubernetes cluster in order to run it. See the guide on installing Kubeless in your Kubernetes cluster to finish setup. Project Structure Below is a quick overview of the `kubeless-python` template for an example hello world python service running on Kubeless. For an in-depth overview of kubeless, see the provider docs.  The `serverless.yml` defines what functions will be deployed and what events they respond to.  The simple python code returns a simple JSON response.  Deployment After hooking up kubeless to your Kubernetes cluster you can deploy your functions with:  The Serverless Framework translates all syntax in serverless.yml to the Function object API calls to provision your Functions and Events. For each function in your serverless.yml file, Kubeless will create an Kubernetes Function object and for each HTTP event, it will create a Kubernetes service. Links and Resources Heres what you need to get started with the Kubeless plug-in now: Start here in our docs. - GitHub repo - Kubeless Example - Kubeless Docs - Kubeless Homepage If you have questions or comments about the integration, we'd love to hear from you in the comments below or over on the serverless forums.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e64",
      "title": "Run Serverless Functions at half the cost with Spotinst & Serverless Framework",
      "content": "We've seen it time and time again: companies get tremendous cost savings when they port over into the serverless realm. Some real world examples: - How I cut my AWS bill by % by going serverless - Costs a small fraction (~%) of the traditional cloud approach - AWS Lambda: million API calls for $ What if I told you...it just got even cheaper? Serverless & Spotinst team up Introducing the latest serverless provider integration, Spotinst. Spotinst Functions enables users to deploy multi-cloud functions at Spot Prices. By leveraging spot instances, you could save to % over standard serverless FaaS pricing. Spotinst supported runtimes Spotinst currently supports Node (, , & .), Java, Python, Ruby, Go as function runtimes. Cross-cloud deployments Spotinst functions can be deployed simultaneously in AWS, Azure and Google Cloud Platform. This ability to deploy across clouds improves SLA and offers higher function availability. If one cloud provider goes down (or pricing is cheaper elsewhere), Spotinst handles this for you under the hood. Edge location support Reduce data transfer costs with edge location support. Unlike other edge FaaS solutions, Spotinst Functions doesnt have limitations on external api calls or execution time limits. They are currently supporting different locations around the US and + locations in APAC, SEA & Europe. Function Analytics - Invocation count & latency metrics, errors & HTTP response codes analytics - Geo Location analytics - Easy log debugging Pricing Example (Compute Only): Lets see how pricing stacks up with an example. With , Requests per Minute (,,, Monthly) with MB of RAM & Getting started Heres what you need to get started with the Spotinst plug-in now: Start here in our docs. - GitHub repo - Spotinst Docs - Spotinst Functions Homepage - Ping us on @goserverless or @spotinst Or continue reading for step by step instructions on how to set up and deploy your first service on Spotinst. Pre-requisites: Make sure you have the latest version of serverless installed on your machine.  Sign up for Spotinst Account  or via the `sls info` command in your terminal. Multi-Cloud & Multi-region functions Congrats you are now leveraging spot instance prices and running your functions across multiple clouds. If you have questions or comments about the integration, we'd love to hear from you in the comments below or over on the serverless forums.",
      "__v": 0
    },
    {
      "_id": "64e0891ab72e199dda603e66",
      "title": "Scaling the Resistance - a Zero-maintenance Donations Platform with Serverless and AWS",
      "content": "Hey folks, I'm Victor Stone, technology lead for MovementVote.org. The business of \"funding the resistance\" is a volatile one. There are big spikes of donations after major events (Charlottesville, Harvey, etc.), followed by relatively quiet periods. And those traffic spikes can be huge. As a non-profit running on volunteerism and a shoestring budget, it's a lot to manage, and we wondered where to turn. Atomic services (like AWS) were both a flexible blessing and a rigid curse. Sometimes they practically require a PhD in configuration files. When we tried out the Serverless Framework, all that power and flexibility suddenly seemed within reach. Serverless meant we could have a pay-as-you-go, zero-maintanence site that we could build, deploy and host in a single environment. We were pumped to try it out. Background Gamechanger Labs has grown up over the last few decades. We started as a scrappy little non-profit relying on part-time contractors and volunteer love; now we've become a concierge platform for major donors that moves millions of dollars into the hands of nearly different grassroots organizations. The rise in attention was overwhelming for our tiny, free-tier EC / Nodejs-React donations site. Too much energy was going to things like chasing memory leaks in expressjs and (frankly) keeping the site from crashing. Foreground Taking a step back, it was obvious that the website didn't really do any computing at all. It was a single-page app with five static pages. There were also a small set of services (e.g. contacting staff and managing donations plans) that had very little to do with the webpages itself. The site's content lived on a completely separate hosting service. We used that as a headless Wordpress installation where the admins could edit the site via plugins and the Wordpress app. That WP installation emitted a JSON RPC API, which was then fed to React components that populated the website. As thin as our React app was, it wasn't thin enough to save it from being overwhelmed. Because the AWS universe is highly componentized, we could migrate the server side functions and our completely static website in an S bucket - one piece at a time. There was a period when I was wedging AWS services into our environment, and that worked perfectly. The goal was to move concierge's server side functionality (running on EC) over into Lambda. It was critical for that move to be completely transparent to the React code in the browser. The mantra of the project became \"this should be easier.\" In fact, I hit a wall when it came to Lambda. The service itself was pretty easy to understand conceptually, but one look behind the scenes of what the Serverless Framework is actually doing and it's obvious there are many s of man hours involved in creating a seamless development environment. We didn't have the resources to invest anything close to that. Server -> Serverless The server functions were moved into a new Git project called bellman. Each API gets their own `serverless.yml` configuration.  Some of our services are CRUD, some are just straight up RPC APIs. For all cases I created a base class (LambdaFunc) to encapsulate and normalize some of the more arcane AWS structures and methods. It was stunning to see how much of the previous code was http server plumbing versus functionality. When I started migrating from the Expressjs environment to Lambda, the code shrunk an order of magnitude and focused % on app functionality. Here's how I encapsulated the 'contact' form hander:  Exporting the method was encapsulated in a property of the base class:  For CRUD APIs I created a RESTService class that encapsulated boilerplate DynamoDB access to the point where I didn't even have to code the methods. By passing the name of the DynamoDB table in the environment:  the default REST code reduced to almost none (this is the entire code for the CRUD interface for managing users):  Client SDK It's important to encapsulate the API for browser consumers, so that when you install the `bellman` package it is very OOP/API-like. The API Gateway has a feature to generate a client SDK for your remote Lambda function. I generated that SDK, but it looked very much like a generic catch-all. Here's a hint: if the process to generate the client is % automated, it will feel like that to the caller. My old friend and mentor used to call this condition \"implementation-bubble-up-itis.\" I chose to create my own system that generated a client SDK. My system was mostly automated but needed some manual guidance. First, I created a client side helper class for all cases. Next, a specialized derivation for REST/CRUD APIs. With these in place, I could expose a CRUD interface with less than lines of code. For custom APIs, I had to manually create the end point class with the shape of the API:  The last piece to the client SDK is a rather funky build-time script that ) extracts the API endpoints for both stages and ) creates a `module.exports` that encapsulates each endpoint into an instance of the `Client` class:  This then allows for a 'natural' interface in the browser:  Static Site Having moved our server code into Lambda, it was now possible to serve our website from a static S bucket. What I learned in this migration, however, is that AWS CodeBuild deploys to a directory off of the root of your bucket, but the Web hosting feature of S wants your `index.html` file to be at the root. CodeBuild also leaves the files encrypted, but the Web hoster wants the files to be public and unencrypted. I couldn't figure out a way to handle that with AWS services. So I wrote a couple of lines of script and pushed it into our bellman API project. It gets triggered from the AWS CloudWatch event after the CodeBuild project is finalized. Conclusion On paper, this setup seems like it will be a huge time and cost savings for us. I suppose we'll come to that when we've traversed another event that inspires folks to open their checkbooks! Either way, none of this, not one piece of it, would have been possible without a tool like Serverless to glide us into the future.",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e68",
      "title": "Serverless Ops  - CloudWatch Logs and Centralized Logging with AWS Lambda",
      "content": "In our last ops post, we set up simple alarms to monitor your Lambda functions with CloudWatch metrics and alarms. This gives you a baseline understanding of what is happening in your functions. But metrics can only take you so far. When errors are firing and alarms are triggering, you need visibility into how and why your functions are failing. Enter an old friend of every developer: the log. Logging lets you drop status updates from your code and provide additional detail around errors. Inspecting logs in a serverless world uses some different patterns -- no SSH-ing onto a production box and grep-ing through text files for you. In this post, we'll talk about the basic logging mechanisms with AWS Lambda and dive into some advanced practices for understanding your functions. The Basics: Logging to CloudWatch First, let's start with a walkthrough of how logging works with AWS Lambda. We'll create a Serverless service to test logging. I like to use Python, but the mechanics are similar with Javascript or the other supported languages. To follow along, you'll need to install and configure the Serverless Framework. Check the instructions here if you haven't done that yet. First, let's create a simple service to demonstrate logging:  In the logs, we can see two logging statements. The first is from the print statement with just the bare message passed in. The second is from the logger and includes formatting such as a timestamp, the RequestId, the log level, and the log message. Either method will work. Both will be captured by CloudWatch without adding significant latency to your functions. This is similar for other runtimes. For example, in Node, use `console.log()` for logging to CloudWatch. Viewing your logs If you're actively developing a function, the fastest way to view your logs is with the Serverless Framework itself. As shown above, you can invoke a function and get logs for that specific invocation:  Or you can use the `sls logs` command to get logs for many invocations:  Pro tip: Open a different window in your terminal and run `sls logs -f --tail`. The `--tail` flag will continuously poll your function's log group, so you can stream in logs as you invoke your function with different input. If you're looking to search a larger portion of your logs, you can use the CloudWatch Console. Make sure you're in the right region for your function, then click on \"Logs\": This will show all of your Log Groups in the region. Use the search box to filter to the function you want. By default, the group will be named `/aws/lambda/--` Click on your Log Group. You will see a list of Log Streams listed. Each function \"container\" that spins up for your function will get its own Log Stream, but they will all feed into the same Log Group. If you want to look at logs across all instances of your function, click \"Search Log Group\". You'll see a screen like the following: This has all of your function's logs. In addition to your log statements, it contains Lambda output such as when a request has started, when a request has ended and a report of the resources used by that request invocation. The Lambda logs usually aren't useful for debugging, so I filter them out at the top with `-\"RequestId: \"`, which means \"remove all logs that have the string \"RequestId: \" in them. Once you've done that, it's easier to browse your logs for the information you want. Advanced Usage: Centralized Logging The terminal and CloudWatch console are fine for small-scale debugging purposes, but they quickly break down. If you're looking through high volumes of log messages or correlating errors across multiple Lambda functions, you'll be pulling your hair out with the CloudWatch console. At this point, you should move to a log aggregator. There are a number of options out there -- Splunk, SumoLogic, Loggly, LogDNA, etc. The particular choice doesn't matter, as long as it has a way to ship logs in via HTTP. AWS allows you to invoke a Lambda function whenever a particular Log Group receives a new log. This means you can have a log forwarding Lambda function whose sole purpose is to take CloudWatch logs and send them to your central aggregator for inspection and debugging. To get started, you should first deploy a Serverless service with your log forwarding function. Your `serverless.yml` should be similar to:  Your service should have a `handler.js` file which takes a CloudWatch log input and forwards it to your aggregator service. If you need examples, some aggregator services have built these functions for you: - Sumologic log forwarder - Splunk log forwarder blueprints Once you have your handler, run `sls deploy` to put your forwarding function into production. Then, get the ARN of your Lambda using `sls info -v`:  In the `StackOutput` section, there's a `ForwarderLambdaFunctionQualifiedArn`. Copy this, and remove the `:` at the very end. The number is the version, which you shouldn't worry about. Now that you have a log forwarding function, you can subscribe that function to the CloudWatch Log Groups of your other services. The easiest way to do so is with the serverless-log-forwarding plugin from Amplify Education. To use it, go to your Serverless service whose logs you would like to forward. Then, install the `serverless-log-forwarding` plugin:  Then, add the forwarding configuration to your `serverless.yml`:  Use the ARN from your log forwarding function previously as the `destinationARN`. The `filterPattern` is optional, but I use it to filter out the internal Lambda logs as shown in the CloudWatch console walkthrough. Once you `sls deploy` your function, your CloudWatch Log Groups will be wired up to send to your forwarding function to be shipped to your log aggregator! Additional Reading Yan Cui recently did an excellent series on managing CloudWatch logs with Lambda. He goes further in depth with logging, including using correlation Ids to trace requests across function boundaries. Yan is an excellent resource on Lambda in general, having managed some large Lambda-backed deployments at Yubl. His series on Yubl's road to Serverless architecture is well worth reading. Other posts in the Serverless Ops series: - Serverless Ops : CloudWatch Metrics and Alarms",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e6a",
      "title": "How to set up a custom domain name for Lambda & API Gateway with Serverless",
      "content": "With Serverless, it's easier than ever to deploy production-ready API endpoints. However, using AWS API Gateway results in odd hostnames for your endpoints. Further, these hostnames will change if you remove and redeploy your service, which can cause problems for existing clients. In this guide, I'll show you how to map a custom domain name to your endpoints. This post is the first in a two-part series. Check out the next post to configure multiple Serverless services on the same domain name for maximum microservice awesomeness. Before you start To get started, you'll need the Serverless Framework installed. You should also have your desired domain name registered through AWS. Read the documentation on that here. Getting a certificate for your domain The steps below walk through setting up a certificate for your domain. If you already have a certificate issued, skip to the next section. API Gateway requests must be served over HTTPS, so you need to get an SSL/TLS certificate. You may manually upload your certificate to Amazon, but I find it easier to use AWS Certificate Manager to handle my certificates. Best of all, it's free! To set up the certificate: First, make sure you have the domain name in your Registered Domains in Route . If you have a domain that's registered with a different registrar, you can transfer registration to Route . If you don't have a domain yet, you can purchase one through Route . Once you have your domain, request a new certificate with the AWS Certificate Manager. Note that you'll need to be in region us-east-. This is the only region that works with API Gateway. Add the domain name you want, then hit Review and Request. After you confirm, it will say that a confirmation email has been sent to the registered owner of the domain to confirm the certificate. At this point, the certificate will be in a \"Pending validation\" status. The registered owner of your domain will get a confirmation email from AWS. Click the link in the email to confirm issuance of the certificate. Once you do that, the certificate will change to an \"Issued\" status. Your certificate is ready to go! Move on to the next step to create a custom domain in API Gateway. Create your serverless backend Before you go any further, you should have a Serverless service with at least one function that has an HTTP event trigger. If you don't have that, you can use the code below. This example is in Python, but any runtime will work. In a clean directory, add a `handler.py` file with the following contents:  We've created two simple functions, `hello` and `goodbye`, to demonstrate how to write HTTP handlers in Serverless. Now, let's connect them with a Serverless service. Create a `serverless.yml` file with the following contents:  This `serverless.yml` file configures the functions to respond to HTTP requests. It says that the `hello` function will be triggered on the `/hello` path of your API Gateway, while the `goodbye` function will be triggered on the `/goodbye` path. Run `sls deploy` to send your function to production:  Once the deploy is finished, you will see the Service Information output. This includes the API Gateway domain where you can trigger your functions. In the example above, my `hello` function is available at `https://aanavk.execute-api.us-east-.amazonaws.com/dev/hello`. I can visit that in my browser: and I get my `Hello, world!` response. If I change to the `/goodbye` endpoint, I'll get the `Goodbye, world!` response. It's nice how easy this is to get a production API endpoint, but this still isn't ideal. My domain is impossible to remember (`aanavk.execute-api.us-east-.amazonaws.com`). Plus, if I ever remove my service and then redeploy, I'll get a new random domain. Finally, the path is odd as well -- `/dev/hello` includes my stage as well as my actual page. I'd rather have a cleaner path. This shows the need for using a custom domain. Create a custom domain in API Gateway By this point, you should have an issued certificate and a Serverless service with an HTTP event configured. Now you need to create a custom domain in API Gateway that you can use with your deployed gateways. The easiest way to do this with Serverless is with the serverless-domain-manager plugin. Big thanks to the people at Amplify Education for developing this plugin. To use the plugin, first make sure you have a `package.json` file in your service. Run `npm init -y` to generate one. Then, you'll need to install the plugin in your service:  Then, configure it into your `serverless.yml`:  Make sure you replace the `domainName` value with the domain name that you've configured your certificate for. If you're using a certificate that doesn't exactly match your domain name, such as a wildcard certificate, you'll need to specify the certificate name with a `certificateName` property under `customDomain`. Once this is ready, you can create your custom domain with a single command:  As the output notes, it can take up to minutes for your domain to be ready. This is how long it takes AWS to provision a CloudFront distribution. In my experience, it generally takes - minutes. Once your domain name is ready, run `sls deploy` again to redeploy your service:  At the end of the `Service Information` block, you'll also get a `Serverless Domain Manager Summary` that shows the domain name associated with your domain. Now you can visit that domain in your browser with the cleaner path that you've assigned to your functions: Voila! You have a much cleaner URL for your endpoints. If you want to put multiple services on the same domain, be sure to check out the follow up post!",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e6c",
      "title": "Writing an Event-driven Serverless Application with Full Local Development Experience",
      "content": "In my previous post on Anatomy of a Serverless Application, I lay the foundation for building a very simple application with an email service using the Serverless Framework, deployed to AWS Lambda. In this post, we will build `mailman`, an event-driven serverless application. The application has a simple frontend using `curl` that calls into a couple of backend services: a users service and an email service. The post will highlight event-driven application development with focus on full local development experience. We will look at services emitting, subscribing and reacting to events in a seamless manner using the Serverless Application Platform. You will learn how to: - Setup the development environment - Create an application project - Create a users service - Create an email service - Write an event-driven application - Run the serverless services locally - Run the full application locally Getting Started We will look at an important aspect of writing serverless applications, i.e. local development support. We touched upon the fact that Serverless Framework helps with local testing of a serverless service by using `sls invoke`. But, the real productivity gain is the ability to write a serverless, event-driven application with full local development support, with provider emulation and running it without the need to deploy it to a cloud provider. My choices: - Programming language: NodeJS - The Serverless Application Platform: It consists of - Serverless Framework (v.. or higher) - Serverless Event Gateway, the central hub of event communication - Serverless Emulator, the local serverless provider emulator - Serverless Functions Development Kit (FDK), to enhance developer experience for writing severless applications Setup Let's install and setup the toolsets required for development. Install NodeJS (v..): download or using package manager Install the Serverless Framework (v.. or higher): `npm install -g serverless` No need for setting up any cloud provider account. YEAH! Note: You might wonder what happened to the Event Gateway and the Emulator setup. That's where the Serverless Framework makes it all come together. Keep reading. With Serverless Framework already installed, we will see how to use the Event Gateway and the Emulator in order to provide a centralized event hub and a much needed local development environment. The newly added `serverless run` command downloads and runs the necessary components. Before we use the `serverless run` command, we need to first login to the Serverless Application Platform using the `serverless login` command:  Event-driven Serverless Application Let's look at three core capabilities that we need to write, test and run a serverless event-driven application: an event-driven workflow, programmatic access, and full local development experience. Event-driven Workflow The tenet of an event-driven application is that the components interact with each other asynchronously via events. The components are not aware of each other and rely on a central event communication hub for relaying events across the application. The Event Gateway serves as the central event communication fabric for serverless applications. It acts as the broker for all event communication and allows services to publish and subscribe to events. Additionally, it acts as an API Gateway for all HTTP communication. The Event Gateway uses a special `http` event-type to recognize standard HTTP requests. It standardizes both pub/sub and HTTP communication to be represented as events, combining them into a single experience. Services can send custom events and the Event Gateway wraps them up in a standard event envelope passing the payload along as-is. Events from well-known SaaS providers will be recognized as first-class event-types in the near future. \"Everything as events\" is the mantra. Programmatic Access To write event-driven serverless applications, its individual services need to programmatically take advantage of all the goodness of the Event Gateway features. The Serverless SDK hands that capability to the developers. The developers have access to the Event Gateway API via code for registering & invoking functions, subscribing to and emitting events, and configuration. Local Development Support One of the biggest challenges with developing serverless applications is to run and test the application locally. Since serverless applications are hosted on the cloud, it makes it very tedious to debug, test and develop code in an iterative manner. Although all cloud providers and the Serverless Framework allows invoking one function at a time locally, the developer wishes to run the full application locally on their machine. The Serverless Emulator emulates different cloud provider FaaS offerings (currently AWS Lambda or Google Cloud Functions) on your local machine in an offline-focused manner. It provides that missing piece of tooling that makes application development with serverless so productive and exciting. It enables deploying and invoking serverless functions without the requirement of setting up and deploying your serverless application to the cloud provider. Seamless Developer Experience To empower the developer with a great development experience, the Serverless Framework brings it all together in a very simple and intuitive interface: the `serverless run` command. The `serverless run` command detects and installs or spins up the latest version of the Event Gateway and the Emulator. It provides a unified visual interface for the event-driven workflow for the application as they happen. It provides an intuitive and decoupled interface for the services of an application to communicate with. In the sections that follow, we will use the example application to demonstrate these killer features. Creating the Application A basic project structure shown below will work: ```bash |-- mailman |-- README.md |-- frontend `-- services ``` where, - `frontend` folder holds the frontend - `services` folder holds the serverless service(s) Let's create a couple of the backend services and then we will explore the local experience when we run the application. The way the non-serverless frontend portion of the application is written is totally your choice. In this post, I will primarily focus on the serverless portion of the application inside the `services` folder, and use a basic `curl` script to simulate the frontend. Goals The application has a few business goals: - register a user via an HTTP POST API call from the frontend - on registration, send a welcome email to the user The application has a few technical goals as well: - run and test the application locally without deploying to the cloud - have loosely-coupled services interact with the application - visualize the event-driven workflow as it happens Building the Services We will not get into the details of creating the services, but we will explore the code and dig deep into the functionality that is offered. Also, the guts of the services are mocked up, but the intention of the example is to showcase the event-driven nature of the services, so we will just focus on that. Building the Users Service The workflow of the `users` service is: - expose an HTTP endpoint for user registration, - register a user, and finally - emit an event `user.registered` Let's look at the functions section of the `serverless.yml`:  Nothing different than what you are used to. Behind the scenes Actually, there are a couple of things that the Serverless Application Platform does behind the scenes. The `serverless.yml` is parsed, and the `register` function from the `users` service is registered with the Event Gateway. Next, a subscription is created for the `http` event and the `register` function. We will visualize this when we run the service and talk about it in the next section. Now, let's look at the handler code:  In this portion of the code, we use the Serverless FDK to initialize the Event Gateway.  This portion shows the `register` function, that first checks if an email was passed in the event body, and then registers the user. Next, on successful registration of the user, it emits an event `user.registered` using the FDK, passing in the user data. Building the Email Service The workflow of the `email` service is: - send a welcome email to the registered user, and - emit an event `email.sent` Let's look at the functions section of the `serverless.yml`:  Behind the scenes There are a couple of things that the Serverless Application Platform does behind the scenes. The `serverless.yml` is parsed, and the `sendWelcomeEmail` function from the `email` service is registered with the Event Gateway. Next, a subscription is created for the `user.registered` event and the `sendWelcomeEmail` function. We will visualize this when we run the service and talk about it in the next section. Now, let's look at the handler code, specifically the `sendWelcomeEmail` function:  This portion shows the `sendWelcomeEmail` function, that would send out a welcome email to the user that is passed in via the event data. Next, it emits an event `email.sent` using the FDK, passing in the event data. Running the Application Now that we understand what the application is meant to do, and having looked at the code, let's run the application. Everything will run locally on our machine. We will run one service at a time, emit and respond to events, visualize the workflow and then call the services via `curl` simulating an application user interface. The Local Development Experience The first time you run any service using `serverless run`, you will get a slightly different experience. Let's run the users service, and look at the visualization on the terminal:  The required components of the Serverless Application Platform, namely the Event Gateway and the Emulator, are downloaded and installed locally on your machine unless already installed. Also, note that the individual components advertise their API endpoints. Then, these components stay in the background listening for events and ready for action. I want to point out that this is how the local development experience starts for a developer. It is all inclusive, seamless and is just there for you. Running the Users Service With the local development setup done, we are ready to run our `users` service.  Let's break it up and discuss what just happened. Event Gateway receives the `http` event `POST http://localhost:/users`  The `http` event triggered the `register` function from the `users` service. The `register` function emitted an `user.registered` event, which was received by the Event Gateway. You can see the data payload for the event as well.  The `user.registered` event triggered the `sendWelcomeEmail` function from the `email` service. Then, the `register` function which had started async in Step , finishes.  Then the `email.sent` event emitted by `sendWelcomeEmail` function is received by the Event Gateway. Then, the `sendWelcomeEmail` function which had started async in Step , finishes.  That concludes the discussion about the application. Hopefully, you followed the workflow I laid out and can see how easy it is to write an event-driven serverless application that can be tested and run locally. The terminal visualization during development is crucial to debugging and tracing async event-driven applications, and can greatly help speed up application development cycles. Summary The serverless event-driven application that we created was limited to a couple of services to keep it simple and easy to follow. We have a reference example application that showcases many other event-driven use cases utilizing a variety of services, spanning multiple cloud providers.",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e6e",
      "title": "How to use multiple runtimes in a single serverless microservice",
      "content": "As a developer on the cloud, there are many tools at your disposal. The Serverless Framework supports an array of runtimes to enable you to use different languages in your Serverless application. To manage these related codebases, you might choose to divide your applications functions into a number of different services. However, if you prefer to deploy a single service for all your functions, regardless of what language they are written in, the Serverless Framework empowers you to do just that. Lets consider a small application that uses two runtimes and provides two functions. This example will use Python and Node targeting AWS, but the concepts will be broadly applicable in other circumstances. The full the project files can be found here Well create an application that has an endpoint that reports the current system timestamp, and a web controller that displays the time in the browser. The configuration will look largely similar to a single-runtime application. We specify the name of the service and the target provider in our `serverless.yml`:  Note that I omitted the usual declaration of runtime inside the provider section. If you specify it here, it will serve as a fallback for any functions that do not have a runtime specified individually. Here I specify a function that will render the webpage markup:  This web controller is a Python module, so I specify the `python.` runtime. The handler field points to the module located in my project at `web/handler.py` and names the function `hello` as the handler for received events. Heres what the implementation looks like:  The other function is a Node-backed endpoint that reports a timestamp:  Again, this looks the same as in a single-runtime service, with the exception that it specifies the runtime `nodejs.` alongside the function declaration. The module for this function is located at `api/handler.js`, and exports a function named `timestamp`. It is not necessary to move files of different languages to separate folders, but depending on complexity and build procedure, you may find it useful. The function responds with the millisecond timestamp:  Deploying the service with `serverless deploy` tells us the URL of the page at `/greet`.  Accessing the page shows the greeting message and tells us the date. !Browser view To try it out, download the project files and run serverless deploy from the directory that contains `serverless.yml`.",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e70",
      "title": "Understanding and Controlling AWS Lambda Costs",
      "content": "AWS Lambda is the archetype of a class of cloud computing products called serverless functions-as-a-service, or FaaS. Others in this product class include Google Cloud Functions and Azure Functions, both of which share the same billing model as Lambda, but with different rates and service limits. This post looks at the serverless FaaS billing model in general, but focuses on AWS Lambdas current pricing (as of September ). Its goal is to point out some observations that may help in reducing or preventing unexpectedly large AWS Lambda bills. Many of these may also be relevant to other products that share a similar billing model. Lambda Functions A Lambda function is a piece of application software that runs in a short-lived container to service a single request or event. Although the use of the term function can suggest that the code must consist of a single function, Lambda functions are regular processes that can also, for example, spawn child processes. They must conform to a specified interface, but can otherwise contain arbitrary code. Each Lambda function can be sized by setting the maximum memory size (GB) parameter in the Lambda Console or using the API. This value also affects the CPU shares allocated to the function when it runs, but in a manner that is not currently disclosed by AWS. Lambda also allows limiting the maximum function execution time (seconds) for a function, to prevent runaway or hanging functions from driving up cost. Since Lambda functions run only when a request must be serviced, they only incur charges when used. The general pricing model adopted by serverless FaaS providers is based on two numbers per function invocation: Maximum memory size (GB): note that this is not the actual memory used by the function, but rather the maximum memory size parameter in the Lambda functions configuration. If you reduce your functions memory usage, but do not adjust this configuration parameter accordingly, you will not see a cost reduction from the reduced memory usage. Function execution time (seconds): the actual amount of clock (wall) time that the function invocation took to run. Note that if a Lambda function makes an outgoing network call and sits idle waiting for the result, the time spent idle is still counted in the functions execution time, i.e., this is not a measure of CPU usage. For each function invocation, these two values are multiplied together to produce a number with the unit GB-sec. After allowing for a monthly allowance of free GB-sec from the free tier, the billable compute cost is the total GB-sec across all function invocations, multiplied by a fixed GB-sec rate. This fixed rate is currently the following for three popular FaaS services. !Cost summary of various serverless FaaS services in US Dollars, as of September . Free tier GB-sec is per month. Since GB-sec is a composite unit, it can be unintuitive to reason about. Focusing on AWS Lambdas current pricing, the following chart shows the cost of executing , invocations of a Lambda function that executes for a varying amount of time, broken down by three different maximum memory sizes. Note that each invocation may have to start a language runtime and load third-party libraries before getting to user code, all of which add to the function's billable runtime for each invocation. !Current Lambda function pricing It can be helpful to remember that a GB-sec represents neither a gigabyte nor a second, and is the first composite unit to be used for commercial cloud product pricing in recent memory. Spreadsheet-inclined users may want to look at the data in this public, read-only Google Sheet. Observations Costs are multiplicative in function memory size and execution time. Suppose that a Lambda function uses MB of memory and executes in slightly less than milliseconds. After a code change, the function now needs milliseconds to run (double), and MB of memory (double). The total compute cost increases times. If the memory requirement is instead tripled to MB, the total cost increases times. If both the memory requirement and execution time are tripled, the total cost increases times (spreadsheet). The impact of multiplicative costs may not be intuitive in that small changes to either function memory size or execution time can cause large changes in the total billable cost. Processing delays can be expensive. Suppose that a MB Lambda function executes in slightly less than milliseconds, with a hard limit of seconds. As part of its processing, the function calls an external service over HTTPS and waits for a response before ending. Suppose that network congestion or an external service degradation adds a spike of seconds to each network call. For the duration of the latency spike, the extra seconds of Lambda running time will increase cost times, from \\$. per k requests to \\$. (spreadsheet). The free tier can run out quickly. Suppose that a service must support a sustained request rate of , requests per hour (sustained requests per second). At this rate, the free tier of , GB-sec per month will run out in approximately hour for the following function sizes (spreadsheet): MB, executing for seconds per invocation MB, executing for seconds per invocation MB, executing for seconds per invocation Spot pricing may sometimes be cheaper than Lambda pricing. For some types of sustained workloads, using a reliable, distributed work queue in conjunction with preemptible, spot priced instances can allow fine tuning of the price/performance ratio, while still allowing a function-based, event-driven application architecture. As a concrete example of the observation on spot pricing, consider the following numbers, drawn from the Baresoil Image Resizing Benchmark: A fixed-sized cluster of c.xlarge spot instances can resize at least , images per hour for a total cost of \\$. per hour, using a median of second of processing time per image (code and data). This can be taken as a lower bound on the performance of an EC spot fleet where work is distributed evenly across nodes. The exact same code and workload on Lambda costs \\$. to run using a MB function memory size (the smallest possible for this task), which is % higher than the spot fleet. With the largest MB function size, the Lambda cost is % higher than the spot fleet (spreadsheet). Processing times ranged from ms to ms depending on the Lambda function size, as reported by Lambda's \"billable duration\" metric (raw data). On the other hand, if you are currently processing , images per hour using spot priced queue workers, you may be able to process the same actual workload in minutes rather than an hour for just a % increase in costs, using Lambdas default concurrency limit of . Summary Monitor and adjust maximum memory size and execution time parameters. Since the cost incurred by a Lambda function invocation is multiplicative in its execution time and memory size, increasing or reducing both by even a small amount can have an unexpectedly large impact on billable cost. Monitoring actual function execution time and memory usage allows these parameters to be set closer to their required value, which also limits the cost impact of runaway or hanging functions. Note that this does (and should) cut into the \"no operations\" myth surrounding serverless services. Avoid high maximum execution time. A common engineering instinct is to build in a safety buffer beyond minimum requirements. If a Lambda function normally runs in ms, it may be tempting to set the maximum execution time parameter to something large, like seconds. However, consider the additional cost incurred by Lambda functions that are waiting on flaky external services, versus having the function terminate earlier and return an error to its caller. Consider EC spot pricing for queue-driven workloads. If traffic is predictable and sustained, an application architecture based around a reliable queue and spot priced queue worker instances may be cheaper than Lambda functions. This may change if Lambda starts supporting spot priced Lambda functions in the future, such as is the case with SpotInst. Expect a price war soon. The serverless FaaS model is already heading towards a degree of organic standardization when it comes to features and interfaces, helped by frameworks like the Serverless Framework. Once serverless FaaS becomes a commodity cloud service, providers may only have room to compete on pricing. We can already see that Google has chosen different rates than Amazon and Microsoft, opting for a lower GB-sec rate but a higher per-request rate. In the future, some things we could conceivably expect are native spot (preemptible) pricing for serverless FaaS, and a lower slope to the cost curves shown in the earlier chart.",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e72",
      "title": "How to manage your AWS Step Functions with Serverless",
      "content": " - New to Step Functions? Check out this Ultimate Guide to AWS Step Functions When diving into the Functions as a Service (FaaS) world, a question that often pops up is: There are a number of ways to manage state with backend data stores, `tmp` directories & building this logic into your existing lambda functions but there is a simpler alternative provided by AWS: Step Functions. Step Functions allows you to control complex workflows using Lambda functions without the underlying application managing and orchestrating the state. In essence, it's a state machine to help with complex workflows and aims at keeping your lambda functions free of this additional logic. Serverless + Step Functions A couple months ago, I created the Serverless Step Functions plugin to deploy and manage Step Functions and a bunch of composed Lambda functions via the Serverless Framework. In this post, I will share the functionality and usage of the plugin, and a workflow for your development. So let's get down to business! Install Before getting started, you need to install the plugin. This is hosted on the Serverless Plugins registry, so you can install this via the plugin install command which is introduced since v... Please run the following command in your service, then the plugin will be added automatically in plugins array in your `serverless.yml` file.  If you run `serverless --help` command and you can see an explanation of subcommands for the plugin like `serverless invoke stepf, installing is successful. Getting Started Define AWS state language To define a workflow with Step Functions, you need write a structured language called Amazon States Language, which can be defined within `definition` section with yaml format in your `serverless.yml`. I recommend using in combination with Serverless AWS Pseudo Parameters since it makes it easy to set up in `Resource` section in serverless.yml. The following is an example which is a simplest state machine definition, which is composed of a single lambda function.  Event You can define events to invoke your Step Functions. Currently, `http` and `scheduled` events have been supported. The configuration syntax is similar to the Lambda events provided by the framework core. Heres how to define those events:  Use triggered Lambda events If you want to use events other than `http` and `scheduled`, you can create a Lambda function which only run your statemachine Using the AWS SDK, you can trigger your step functions like: ```javascript 'use strict'; const AWS = require('aws-sdk'); const stepfunctions = new AWS.StepFunctions(); module.exports.start = (event, context, callback) => { const stateMachineArn = process.env.statemachine_arn; const params = { stateMachineArn } return stepfunctions.startExecution(params).promise().then(() => { callback(null, `Your statemachine ${stateMachineArn} executed successfully`); }).catch(error => { callback(error.message); }); }; ``` Then, you set up the Lambda will be triggered by events what you want. `startExecution` API requires a statemachine ARN so you can pass that via environment variables system. Heres serverless.yml sample which a triggered statemachine by S event.  Create a sample application Lets consider a small step application that starts EC and write the result on S bucket. First, we will create a Lambda function that only starts an EC instance, to which will be passed instanceId via API Body request parameter. ```javascript 'use strict'; const AWS = require('aws-sdk'); module.exports.startEC = (event, context, callback) => { const ec = new AWS.EC(); const params = { InstanceIds: [ event.instanceId ] } return ec.startInstances(params).promise().then(() => { callback(null, `Your ${event.instanceId} instance started successfully`); }).catch(error => { callback(error.message); }); }; javascript 'use strict'; const AWS = require('aws-sdk'); module.exports.writeS = (event, context, callback) => { const s = new AWS.S(); const params = { Bucket: 'sls-logs-bukect', Key: 'success!!' } return s.putObject(params).promise().then(() => { callback(null, `a log writed successfully`); }).catch(error => { callback(error.message); }); }; ``` In the end, describe your serverless.yml looks like, and deploy with `serverless deploy`.  If you can see the API Gateway endpoint on your console, it means to deploy successfully  Send a CURL request to your live endpoint:  You should see that specified EC will be started and a log will be written to S Bucket. Summary The Serverless Step Functions plugin makes it easier to manage and deploy your Step Functions. If you have any comments or feedback, please create a new issue or send a Pull Request. I always welcome them!! One more thing, tutorial on how to use the plugin has been coverd on FOOBAR youtube channel. You can also learn it there. Thanks @maviuy for making the great video! ",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e74",
      "title": "Webtasks brings node , no cold starts, and streamlined on-boarding to Serverless",
      "content": "When we set out to build the Serverless Framework, we wanted to streamline how developers build things and how they interact with cloud providers. We strive to get developers to the \"aha\" moment of the serverless experience as soon a possible. This moment where their eyes light up & they see the world of possibilities that functions-as-a-service world opens to them. We work day and night (thanks team Europe), towards streamlining the developer experience but still one of biggest hurdles we have is getting new developers setup with a functions provider. It's not impossible but it sure ain't easy. Until now. Welcome Auth Webtasks as a the newest deployment targets and functions provider to the Serverless Framework . Auth Webtasks Born out of Auth rules, Webtasks allows users to write & deploy nodeJS code without having to think about the underlying server. Using the serverless webtasks integration allows framework users to write their serverless services using the standard `serverless.yml` configuration and deploy functions into Auth Webtasks. How is Webtasks different? Setup takes about a minute and seconds Setup takes about a minute and seconds. Watch the video above. Node support Node runtime support. This allows users to use the latest and greatest JS today without needing to transpile their code. Persistent storage There is also small persisted state that you can re-use within functions. This is currently limited to a k json doc. No cold starts That's right, no cold starts (drops mic). If you have something that needs to be super snappy, say a backend API, I'd suggest running that through a Webtasks function to avoid cold start latency. One thing to note: There is a soft limit of request per second on the Auth Webtasks free tier. Some Webtasks Use Cases Inside your Webtask functions you have full access to the npm ecosystem to pull in your favorite modules to get the job done. Here are some common use cases for Webtask functions: - setting up webhook listeners - running chat bots & slack automation - glue code & data transformation - backend apis for static sites - handling site forms - github automation - payment processing with stripe - ...(use your imagination) Getting Started It's incredibly easy to get started with Webtasks. You can be up and running in minute (or : minutes) as seen here: We will be setting up Webtasks with the `sls create` service command. You can also install the serverless-webtasks plugin in an existing service to deploy your functions to Webtasks. Pre-requisites Make sure you have nodeJS installed on your machine and also the Serverless Framework  Create Your Webtask service  Install the Webtask plugin Inside the `my-new-webtask-service` directory run:  Config your Webtasks account  Then enter your email or phone number and verify the code. Thats it. You are setup and ready to deploy live code. Deploy your Webtasks service Inside the `my-new-webtask-service` directory run:  This will package your code and deploy it into the Webtasks cloud. The CLI will return your live function endpoint for you to use in your app.  Resources and links - Serverless + Webtasks Docs - serverless-webtasks plugin repo - Webtasks Main Docs",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e76",
      "title": "How to deploy multiple micro-services under one API domain with Serverless",
      "content": "In this post, I'll show you how to put multiple Serverless services on the same domain name. This is the most requested issue in the Serverless repo right now but is a tricky feature to implement directly within the Framework. Using the power of Serverless and the serverless-domain-manager plugin, we can use API Gateway's base path mappings to handle this. Follow the instructions below to deploy your two services to the same domain. _Addendum: Many users asked about how to deploy to different domains based on the stage, e.g. `staging-api.mycompany.com` when deploying to staging. I've added a section on Working with multiple stages below._ If you already have your own services set up and just want the simple instructions, skip to the TL;DR section below. Getting Started To get started, you'll need the Serverless Framework installed. You should also have your desired domain name registered through AWS. Read the documentation on that here. You should also register a certificate for that domain through the AWS Certificate Manager. If you need help with that, read the _Getting a certificate for your domain_ section of my previous post on using API Gateway with Serverless. Deploying your two services Before we dive it, let's discuss exactly what we're trying to build. Imagine you have an e-commerce store which is a single-page application that consumes a backend REST API. Your REST API is hosted at `api.mycompany.com`, and you have two services: `users` and `products`. You would like all `users` requests to be accessed at `api.mycompany.com/users` while all `products` requests would be accessed at `api.mycompany.com/products`. Further, you would like to separate these two services so they could be deployed independently -- changes to a products endpoint wouldn't require a redeploy of all users functions as well. For this example, do the following: First, create a new directory for your application:  Then, create a directory for your `users` service:  In your `users-service` directory, add the following `serverless.yml` file:  Then add the following as `handler.py`:  This is a super simple service with a single endpoint (`/hello`) that will return the name of the service. To test it, deploy the service:  Copy and paste the endpoint into your browser, and you should see the following message: As you can see, the URL isn't very friendly. We'll fix that during this walkthrough. To finish our setup, let's make a products service as well. Move up a level in your directory structure, then copy the users-service into a products-service directory:  Then edit the name of the `service` in your products-service `serverless.yml`:  Run `sls deploy` to deploy the products-service, and make sure it's working in your browser: Again, it's an ugly URL, which we're going to change soon. Adding your services to your custom domain Now that we have our two services set up, let's add them to a custom domain. You should still be in your `products-service`. Let's install the serverless-domain-manager plugin:  Then add the configuration to your `serverless.yml`:  We've added two sections to the `serverless.yml`. First, we registered the `serverless-domain-manager` in the `plugins` block. Then, we configured the plugin via the `customDomain` section of the `custom` block. Note the `basePath` attribute that we're configuring. This will be prefixed to every route in our `products-service`. Thus, our route that is registered as `/hello` will actually be located at `products/hello`. If you haven't previously registered this domain with API Gateway, you'll need to register it:  As the output notes, it can take up to minutes to provision this in AWS. This is a one-time setup cost. Once your domain is set up, deploy your service with `sls deploy`. Once the deploy is done, your endpoint will be available at `api.mycompany.com/products/hello`: That's a much cleaner URL! Let's do the same with our users service. Change into that directory:  and follow the same steps as above. Install the `serverless-domain-manager` plugin:  and add the config to your `serverless.yml`:  Note that the `basePath` in this one is `users`, which will be prefixed to all routes in the users-service. You don't need to run `serverless create_domain` again. Because you created the domain already, it is available for any services that want to use it. Run `sls deploy` to deploy the users service, then check it in your browser: That's it! Now you easily separate your functions into services while still keeping them on the same domain. You're not limited to two services on this domain -- as you add additional services, just use a new `basePath` to add it to your domain. Working with multiple stages When working with Serverless services, you'll often have multiple stages for your project. In this section, I'll show you how to set up your project to deploy to custom domains for different stages. To set the scene, let's imagine we have three stages: - `prod`, which is accessible at `api.mycompany.com`; - `staging`, which is accessible at `staging-api.mycompany.com`; and - `dev`, which is accessible at `dev-api.mycompany.com`. The first thing you need to do is get certificates for each of these domains in Amazon Certificate Manager. Please read the _Getting a certificate for your domain_ section of my previous post on using API Gateway with Serverless. Once you've done that, use the following config for your `custom` block in `serverless.yml`:  Pay particular attention to this line: `domainName: ${self:custom.domains.${self:custom.stage}}\"` We're using the Serverless Framework's powerful variable system to infer the domain name based on the stage. I've set up my three stages in the `domains` block of the `custom` section. This will use my given stage to determine which domain to use. Once this is set up, you'll need to create your custom domain _for each of your stages_. This is a one-time setup step. If you use the stages I gave above, you would run:  Once your domains are set up, you can deploy to your proper stages! Use `sls deploy --stage prod` to deploy to `api.mycompany.com` and the other stages to deploy to their respective domains. TL;DR If you already have multiple services set up and are looking to add them to the same domain, follow these steps. Before you begin, you'll need to get a certificate for your domain with the AWS Certificate Manager and register your domain with API Gateway. To do that, follow the steps in my previous post on using a custom domain with API Gateway and Serverless. Stop after the step that says `sls create_domain`. In each service, install the serverless-domain-manager plugin:  Then, add the following configuration to your `serverless.yml`:  Make sure you change the `domainName` value to the domain name you want to use. Change the `basePath` value to the prefix you want for your routes in that service. For example, if you want your routes to start with `/products/`, the `basePath` value should be `products`. Then, run `sls deploy` to get your service deployed to your custom domain with a base path!",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e78",
      "title": "Serverless monitoring - the good, the bad and the ugly",
      "content": " Not so long ago, a job requirement pushed me into the world of FaaS, and I was thrilled. I had dreams of abstraction -- eliminating all that tedious work no developer likes doing. \"We are not operations engineers!\" I exclaimed proudly. \"We should not need to dabble in the dark arts of the Linux Shell.\" But little did I know how wrong I was. We humans are creatures of habit, and one of my habits as an AWS user is checking the AWS Console religiously. It was my central place to monitor everything I needed to know about my servers' health. Now comes the difficult question: How does monitoring work when using AWS Lambda and Serverless? Monitoring All applications have metrics we, as developers, need to monitor. This is crucial: downtime and slow apps can create some pretty grumpy customers. Trust me, I get angry phone calls and rage mail every once in a while. So how can you avoid getting yelled at by customers? Track your errors and monitor your software! Implement a good notification system that lets you know when and where an error occurred. Make sure to have good and easy to view logs of all errors, warnings and other crucial data your application creates. Be responsible for the software you write. Because it is our legacy as developers. We have made an oath, to be creators of awesome stuff! But user experience is only one side of the performance metrics. The second crucial metric is the measure of computational resources. How much resources is the app consuming. If it is too much you need to scale down your servers, otherwise, if the app is capping all resources you may consider larger servers or more of them. _Note: I recently came across an awesome article on this topic by none other than the CTO of RisingStack, Peter Marton. He explained in detail how to do monitoring right. I urge you to take a peek, it will change your view on monitoring forever._ Overhead? Excuse me...? Can I have some monitoring, please? But, without being a burden on my application. We're lucky that, in , this is a given. Monitoring software has become so advanced that in today's world of programming the overhead is minimal. The sun was not shining so bright back in the day. Monitoring applications was followed by a known fact that it would impact your app's performance significantly. How does this translate to Serverless? The Serverless revolution has been gaining strength for the past few years. I see no reason for it to stop. The hype is real. Developers are starting to view the Function as a Service architecture as a savior, something that makes it possible to scale applications automatically and serve only as many users as needed. The pay-as-you-go method cuts costs drastically and makes it possible for startups to create awesome software for a fraction of the cost. But, wait a minute. What else needs to be cut for that to become a possibility? A couple of things come to mind. The overview of your code performance and tracking errors are first. Silent failures as well. How do you monitor the performance of a server that is not a server? Schrdinger's server? Okay, now my head hurts. This paradox needs a new perspective. Monitoring Serverless is a new beast in itself. Traditional methods will not work. A new mindset is in order.  Instead of telling our functions to send along additional data with every invocation, why not just collect their residual data? This is a cool idea! It's a known fact all AWS Lambda functions send their logs to AWS CloudWatch. Serverless is unforgiving Unlike in traditional applications, you don't have full overview of every part of your system. Not to mention how hard it is to test Serverless. You have to push code to AWS to see if it's working or spend an eternity on setting up emulators on your local machine. The process is incredibly tedious. Not to start with adding third-party services to your app. It creates overhead and additional costs. Try attaching monitoring services to every single Lambda function. That's never going to scale well! Let's imagine a scenario of monitoring a simple function on AWS Lambda. The purpose is to test the function and check the verbosity of the logs on CloudWatch. After hitting the endpoint with Postman a couple of times I'm assured it works fine. !postman Opening up CloudWatch I can see the logs clearly. All the function invocations are listed. !cloudwatch The logs are extensive, the only issue is I can't seem to make any sense of them. I can see the functions we're invoked, but not much else. Error messages for failing functions are not verbose enough, so they often go unnoticed. I'm also having a hard time finding functions that timed out. I also tried logging through the command line. It shows possible errors a bit better, but still, not good enough to have peace of mind.   Not to mention the tiresome nature of having to push code to AWS every time you'd want to try out something new. Thankfully, all is not lost. Making my life less miserable What if I didn't need to push code to AWS every time I wanted to test something? All heroes don't wear capes. Like a knight in shining armor, Serverless Offline comes barging in to save the day! At least now I can test all my code locally before pushing it to AWS. That's a relief. Setting it up is surprisingly easy. Installing one npm module and adding a few lines to the serverless service's serverless.yml and voila, API Gateway emulated locally to run Lambda functions. Switching to the directory where I created the sample function and service, I just ran the following command in a terminal:  After installing serverless offline I just referenced it in the serverless.yml configuration:  Back in my terminal running Serverless offline is as easy as just typing:  That's it, a local development simulation of API Gateway and Lambda is up and running! The logs are still bad though... I still can't get over the fact how bland the logs are. Not to mention the lack of error reporting. I took me a good while to find failing functions in the logs. Imagine the nightmare of tracking them in a large scale production application. This issue is what bothers me to most. The lack of overview. It's like swimming in the dark. I don't have the slightest clue what's down there. What did I do? I went hunting. There has to be something out there on the web that can help me out. I was looking for a way to simulate the monitoring and logging of a server. I thought maybe there's a way to create a broader perspective over the whole serverless system. What I found blew me away, in a good way. A bunch of tools exist that parse and analyze logs from all functions in a system on the account level. Now that's cool. I decided to try out Dashbird because it's free and seems promising. They're not asking for a credit card either, making it a \"why not try it out\" situation. They say it only takes minutes to hook up with your AWS account and be ready to go, but hey. I'm a skeptic. I timed myself. The onboarding process was very straightforward. You just add a new policy and role on your AWS account, hook it to your Dashbird account and that's it. They even have a great getting started tutorial. If you want to know, the timer stopped at minutes. I'm impressed. However, I'm much more impressed with Dashbird. I can finally see what's going on. !dashbird dashboard Errors are highlighted, and I can see the overall health of my system. I feel great all of a sudden. It also tracks the cost so I don't blow the budget. Even function tailing in real-time is included. Now that's just cool. !dashbird per function errors With this watching my back I'd be comfortable with using Serverless for any large-scale application. The word relief comes to mind. Final thoughts Whoa... This has been an emotional roller-coaster. Starting out as a skeptic about the ability to monitor and track large-scale Serverless apps, I've turned into a believer. It all boils down to the developer mindset. It takes a while to switch from the mental image of a server to FaaS. Serverless is an incredible piece of technology, and I can only see a bright future if we keep pushing the borders with awesome tools like Serverless Offline, Dashbird, CloudWatch, and many others. I urge you to check out the tools I used above, as they have been of great help to me. Hope you guys and girls enjoyed reading this as much as I enjoyed writing it. Until next time, be curious and have fun. Do you think this tutorial will be of help to someone? Do not hesitate to share. If you liked it, let me know in the comments below. Tools: - Serverless offline - Dashbird - CloudWatch Resources: - https://hackernoon.com/node-js-monitoring-done-right-ecbbff - https://blog.risingstack.com/monitoring-nodejs-applications-nodejs-at-scale/ - https://en.wikipedia.org/wiki/Application_performance_management - https://medium.com/dashbird/is-your-serverless-as-good-as-you-think-it-is-baadbde",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e7a",
      "title": "What's new for Serverless plugins?",
      "content": "Introduction It's been quite a while since my last post about plugins, way back in the ancient days of Serverless .. The Framework is in its 's now and so grown up...(sniffle). Things have changed since then. It's a new age, filled with new features. Your dreams of being a plugin author have never been easier to achieve. Let's do this. Table of contents - Command aliases - Command delegates - Enhanced logging Command aliases Before Every plugin installs a unique set of commands. Since the command lifecycle and the hookable lifecycle are rooted at the command names, the commands cannot be changed. E.g. the deploy function lifecycle is built from these events: `deploy:function:initialize`, `deploy:function:packageFunction` and `deploy:function:deploy`. From a UX perspective, wouldn't it would be more natural to access the function deployment as subcommand of a `function` command (i.e. `serverless function deploy`)? But alas, if you rename the command to `function:deploy` it also changes the command hooks, and any plugins depending on these hooks cease to function. Whoops! Now The solution? Command aliases. Commands can now have specified alternatives (aliases), and you can use those alises to access the command from the CLI. Invoking a command alias is internally no different than invoking the original command--it will start the original command's lifecycle and run through all known lifecycle events. So, any hooked plugin will work exactly the same as with the original command. Aliases are simply added to any command definition. You can even specify multiple aliases! However, aliases cannot overwrite existing commands and Serverless will error accordingly if you try to do so. Here is an example of the aforementioned `deploy function` command (code from `./lib/plugins/deploy/deploy.js`):  This sample makes the `deploy function` command available as `function deploy`. As you can see, aliases implicitly create hierarchies when needed (here: a virtual `function` command level). In general, the alias command position is not limited and can also be a _new_ subcommand of any existing command. `serverless help` will also reflect existing aliases, in addition to printing both the aliased command and the original command description. Command delegates (lifecycle termination) Before Sometimes, depending on the options given, you'll need to delegate execution to a different command. E.g., our example from above, where `serverless deploy --function=XXXX` now executes the `deploy function` command instead of `deploy`. But how is this done? As you might remember, there is `pluginManager.spawn()` that starts a command from within a lifecycle event. It returns after execution so that the current lifecycle (command) is continued afterwards.  It is important to remember that a terminating spawn will not execute any subsequent lifecycles of the current command. I recommend that you do not spawn with termination when you depend on any subsequent event in the current lifecycle to do some cleanups. The deploy command delegates right after the validation step. Enhanced logging A much-requested feature was improved logging, especially for plugin authors. There have been some changes that allow us to let us debug our plugins with fewer headaches. Stacktraces on plugin crashes When the `SLS_DEBUG` environment variable is set, Serverless now prints the stacktraces of the crash (even if it happened within a plugin), instead of just telling us that the plugin could not be loaded. Plugin loading and command registration With `SLS_DEBUG` the plugin manager will now output any commands as well as aliases that are registered during the plugin loading. This allows us to debug crashes during plugin initialization, and shows us the exact location where commands and aliases have clashes. Log of spawned and invoked commands With `SLS_DEBUG`, all commands that are started via invoke or spawn are now logged. This allows you to see if you spawn commands correctly or if there are any plugins that change the event chain in an unexpected way. Grand finale That's it, team. Now you have no excuses--go author the next big plug-in!",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e7c",
      "title": "things I learned designing developer-centric tools at Serverless",
      "content": "I joined the Serverless family about a month ago (in startup months, so make that in enterprise company months). Lets just say the ramp-up curve has been...intense. Ive built complex developer tools before, but never for open source, and never with this many moving parts. But hey, when you gain knowledge fast, might as well share it around, right? Here are things to keep in mind when it comes to designing dev tools in this new (to me, anyway) context. Engineers need two screens for a reason Code wrangling comes with a full posse of devious collaborators. Youve got: - multiple terminal windows for writing code and accessing file structures - additional browser windows for viewing live changes - GitHub and Waffle (or if you prefer, waffle) for project tracking and asynchronous comms - plus a couple extra windows for those reams of documentation (try using AWS without docs, certainly not pretty) Once you add in Google Drive, Slack and break-time Twitter, a savvy developer needs at least screens, if not . So what does that mean for slick dev tool design? Enable rapid context switching between services. Let people deploy some code (switch), check that it loaded correctly in the browser (switch), open up a metrics dashboard to see why its not grabbing http requests (switch), rant about it on Twitter (switch), coffee break. Spending hours in the depths of Atom cranking out code is the exception these days, not the norm, and designers of developer tools mustnt forget it. Balance the CLI and UI Theres an art to ensuring that information presented in the UI is complementary to what the user is accomplishing in the CLI. If you dont get the balance right, your tool can end up being a workflow impediment rather than a benefit. Lets walk through an example I dealt with recently: someone signing in to Serverless. If the user has logged in before, its probably best for them to type  into the CLI and bamtheyre in. But what if the user is logging in for the first time? You could definitely handle this flow with in-line CLI prompts, but this ends up being clunky and can take ages. By funneling the user through a UI, youll have way more control over making it a great experience and getting them to the Ah HA! moment as quickly as possible. Another thing to never forget as a dev-tools designer: in the CLI, an engineer will use shortcuts for everything. No need to take the scenic route to what theyre looking for; using shortcuts is like snapping your fingers outside a hotel lobby and being teleported straight to the comfy sofa in Room where youre watching Fixer Upper on HGTV. Magical. From a design perspective, this means that as long as the core commands still work, the entire structure of the application can change and engineers might not even notice or care. All they need to know is that a finger snap puts them on their comfy sofa. Of course, a UI takes a wholly different approach, providing progressive context and waypoints to craft a user journey. Continuing with the hotel example: users start in the lobby, walk past the concierge, see the signs for the elevators, smell fresh-baked cookies wafting from the continental breakfast... All these cues help users orient themselves. Remove or move too many of them and the overall experience can be ruined. Tl;dr: be careful about changing the UI. The CLI doesnt matter as much. Open source vs. focus One of the things that drew me into Serverless from day one was the passionate developer community. We wouldnt be where we are today without all those issues and PRs. Having said that, being open source adds an interesting aspect to the product development process: where a typical engineering planning meeting might involve going through maybe a couple handfuls of active issues and open PRs - we have dozens. And being responsible product owners, we make sure to review and reply to all of them, all of which takes time. The (positive) flip side is that many suggestions end up actually informing the direction of our product. Honestlywe frequently discuss user-submitted issues that uncover things we either hadnt planned for or hadnt realized were so important. It makes for some pretty lively development to say the least :) Design @ Serverless As a semi-technical non-developer, Ive embraced the daily ins and outs of how our own team works. CLI? For breakfast, lunch and dinner now, thanks. Im keeping all this in mind as the team works hard to bring you the next generation of serverless tools, and in the meantime Id love to hear from other people solving similar design challenges. Stay tuned!",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e7e",
      "title": "Deploy a REST API using Serverless, Express and Node.js",
      "content": "We're seeing more and more people using Serverless to deploy web applications. The benefits are hugelightning-fast deployments, automatic scaling, and pay-per-execution pricing. But moving to serverless has a learning curve as well. You need to learn the intricacies of the platform you're using, including low-level details like format of the request input and the required shape of the response output. This can get in the way and slow your development process. Today, I come with good news: your existing web framework tooling will work seamlessly with Serverless. In this post, I'll show you how to use the popular Node web framework Express.js to deploy a Serverless REST API. This means you can use your existing code + the vast Express.js ecosystem while still getting all the benefits of Serverless ! Below is a step-by-step walkthrough of creating a new Serverless service using Express.js. We will: - Deploy a simple API endpoint - Add a DynamoDB table and two endpoints to create and retrieve a User object - Set up path-specific routing for more granular metrics and monitoring - Configure your environment for local development for a faster development experience. If you already have an Express application that you want to convert to Serverless, skip to the Converting an existing Express application section below. Getting Started To get started, you'll need the Serverless Framework installed. You'll also need your environment configured with AWS credentials. Creating and deploying a single endpoint Let's start with something easydeploying a single endpoint. First, create a new directory with a `package.json` file:  Then, let's install a few dependencies. We'll install the `express` framework, as well as the `serverless-http`:  The `serverless-http` package is a handy piece of middleware that handles the interface between your Node.js application and the specifics of API Gateway. Huge thanks to Doug Moscrop for developing it. With our libraries installed, let's create an `index.js` file that has our application code:  This is a very simple application that returns `\"Hello World!\"` when a request comes in on the root path `/`. It's straight out of the Express documentation with two small additions. First, we imported the `serverless-http` package at the top. Second, we exported a `handler` function which is our application wrapped in the `serverless` package. To get this application deployed, let's create a `serverless.yml` in our working directory:  This is a pretty basic configuration. We've created one function, `app`, which uses the exported handler from our `index.js` file. Finally, it's configured with some HTTP triggers. We've used a very broad path matching so that all requests on this domain are routed to this function. All of the HTTP routing logic will be done inside the Express application. Now, deploy your function:  After a minute, the console will show your `endpoints` in the `Service Information` section. Navigate to that route in your browser: Your application is live! Adding a DynamoDB table with REST-like endpoints It's fun to get a simple endpoint live, but it's not very valuable. Often, your application will need to persist some sort of state to be useful. Let's add a DynamoDB table as our backing store. For this simple example, let's say we're storing Users in a database. We want to store them by `userId`, which is a unique identifier for a particular user. First, we'll need to configure our `serverless.yml` to provision the table. This involves three parts: Provisioning the table in the `resources` section; Adding the proper IAM permissions; and Passing the table name as an environment variable so our functions can use it. Change your `serverless.yml` to look as follows:  We provisioned the table in the `resources` section using CloudFormation syntax. We also added IAM permissions for our functions under the `iamRoleStatements` portion of the `provider` block. Finally, we passed the table name as the environment variable `USERS_TABLE` in the `environment` portion of the `provider` block. Now, let's update our application to use the table. We'll implement two endpoints: `POST /user` to create a new user, and `GET /user/{userId}` to get information on a particular user. First, install the `aws-sdk` and `body-parser`, which is used for parsing the body of HTTP requests:  Then update your `index.js` as follows:  In addition to base \"Hello World\" endpoint, we now have two new endpoints: - `GET /users/:userId` for getting a User - `POST /users` for creating a new User Let's deploy the service and test it out!  We'll use `curl` for these examples. Set the `BASE_DOMAIN` variable to your unique domain and base path so it's easier to reuse:  Then, let's create a user:  Nice! We've created a new user! Now, let's retrieve the user with the GET /users/:userId` endpoint:  Perfect! This isn't a full-fledged REST API, and you'll want to add things like error handling, authentication, and additional business logic. This does give a framework in which you can work to set up those things. Path-specific routing Let's take another look at our function configuration in `serverless.yml`:  We're forwarding all traffic on the domain to our application and letting Express handle the entirety of the routing logic. There is a benefit to thisI don't have to manually string up all my routes and functions. I can also limit the impact of cold-starts on lightly-used routes. However, we also lose some of the benefits of the serverless architecture. I can isolate my bits of logic into separate functions and get a decent look at my application from standard metrics. If each route is handled by a different Lambda function, then I can see: - How many times each route is invoked - How many errors I have for each route - How long each route takes (and how much money I could save if I made that route faster) Luckily, you can still get these things if you want them! You can configure your `serverless.yml` so that different routes are routed to different instances of your function. Each function instance will have the same code, but they'll be segmented for metrics purposes:  Now, all requests to `GET /users/:userId` will be handled by the `getUser` instance of your application, and all requests to `POST /users/` will be handled by the `createUser` instance. For any other requests, they'll be handled by the main `app` instance of your function. Again, none of this is required, and it's a bit of an overweight solution since each specific endpoint will include the full application code for your other endpoints. However, it's a good balance between speed of development by using the tools you're used to along with the per-endpoint granularity that serverless application patterns provide. Local development configuration with Serverless offline plugin When developing an application, it's nice to rapidly iterate by developing and testing locally rather than doing a full deploy between changes. In this section, I'll show you how to configure your environment for local development. First, let's use the `serverless-offline` plugin. This plugin helps to emulate the API Gateway environment for local development. Install the `serverless-offline` plugin:  Then add the plugin to your `serverless.yml`:  Then, start the serverless-offline server:  Then navigate to your root page on `localhost:` in your browser: It works! If you make a change in your `index.js` file, it will be updated the next time you hit your endpoint. This rapidly improves development time. While this works easily for a stateless endpoint like \"Hello World!\", it's a little trickier for our `/users` endpoints that interact with a database. Luckily, there's a plugin for doing local development with a local DynamoDB emulator! We'll use the `serverless-dynamodb-local` plugin for this. First, let's install the plugin:  Then, let's add the plugin to our `serverless.yml`. Note that it must come before the `serverless-offline` plugin. We'll also add some config in the `custom` block so that it locally creates our tables defined in the `resources` block:  Then, run a command to install DynamoDB local:  Finally, we need to make some small changes to our application code. When instantiating our DynamoDB client, we'll add in some special configuration if we're in a local, offline environment. The `serverless-offline` plugin sets an environment variable of `IS_OFFLINE` to `true`, so we'll use that to handle our config. Change the beginning of `index.js` to the following:  Now, our DocumentClient constructor is configured to use DynamoDB local if we're running locally or uses the default options if running in Lambda. Let's see it if works. Start up your offline server again:  Let's run our `curl` command from earlier to hit our local endpoint and create a user:  And then retrieve the user:  It works just like it did on Lambda! This local setup can really speed up your workflow while still allowing you to emulate a close approximation of the Lambda environment. Converting an existing Express application If you already have an existing Express application, it's very easy to convert to a Serverless-friendly application. Do the following steps: Install the `serverless-http` package -- `npm install --save serverless-http` Add the `serverless-http` configuration to your Express application. Youll need to import the serverless-http library at the top of your file: `const serverless = require('serverless-http');` then export your wrapped application: `module.exports.handler = serverless(app);.` For reference, an example application might look like this: ``` app.js const serverless = require('serverless-http'); ",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e80",
      "title": "Your definitive guide to ServerlessConf  in NYC",
      "content": "This was written before ServerlessConf in NYC. If you're looking for the ServerlessConf Recap, head on over here. ServerlessConf is a -track conference. I.e., youll have to make some tough choices. Heres what we plan on prioritizing. This is the Serverless team guide to getting the most out of ServerlessConf. Most anticipated talks Day :-: - theatre I tips for running a serverless business... number will blow your mind! by Sam Kroonenburg :-: - theatre I Harmonizing Serverless and Traditional Applications by Ryan Scott Brown :-: - theatre I Serverless Design Patterns by Tim Wagner, Yochay Kiriaty & Peter Sbarski :-: - theatre II The Best Practices and Hard Lessons Learned of Serverless Applications by Chris Munns :-: - theatre I Event-driven Architectures: Are We Ready for the Paradigm Shift? by Ben Kehoe Day :-: - theatre I Why the Fuss About Serverless? by Simon Wardley :-: - theatre I Unicorns Poop Too: A Field Guide To Serverless Observability by Charity Majors :-: - theatre I Going serverless at a bank by Sander van de Graaf :-: - theatre I Global Resiliency when going Serverless by Jared Short :-: - theatre I The CNCF (Cloud Native Computing Foundation) point of view on Serverless by Daniel Krook :-: - theatre I Data Layer in the Serverless World by Alex DeBrie Swag guide In ServerlessConf Austin, A Cloud Guru was custom-printing shirts and Google Cloud was giving out Firebase hot sauce. We fully expect the swag game will be on point. But how do you optimize your booth-hopping for maximum value and appeal? Heres where we think youre likely to get the best goods: - A Cloud Guru - Spotinst - AWS - CapitalOne (we heard tell of mini drones at the last AWS re:Invent) - Google Cloud - Serverless (of course) All we can divulge in advance is: the Serverless booth will have not only some slick threads, but an even slicker Nintendo Switch giveaway. Whos who: find these people and pick their brains Take a break from donuts and t-shirts to chat with the best serverless minds in the biz. Swing by Treks booth and throw them some implementation curveballs. Absorb anecdotes and truth pills from Ben Kehoe at iRobot. Try to get future product secrets out of the developers AWS, Azure, Google and IBM are sending. Serverless.com is having an open office hours, which you can sign up for in advance. Bring it on. We are so ready to nerd out with you.",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e82",
      "title": "Serverless (FaaS) vs. Containers - when to pick which?",
      "content": "Contrary to popular thought, Serverless (FaaS) and Containers (Container Orchestration) have some pretty important things in common. You want a modern, future-proof architecture? They both have it. You want to build that slick architecture while also leveraging the latest innovations in distributed systems and large-scale application development? Yep, they both have that too. It makes it hard to decide which one is best for you. But friend, you deserve to know. So we're taking off the gloves and laying it all on the line. What are the commonalities and distinctions? What are the advantages and disadvantages of each? It's serverless computing vs containerization, right now. Read on. How did we get here? Before we jump right into the details, lets cover some very important history. Physical servers We used to build our own infrastructure in the form of physical servers. We set up those machines, deployed our code on them, scaled them and maintained them. The whole thing was a manual process, and pretty slow to boot. Server clusters and VMs Using a single physical server for one application was a waste of resources. So, we evolved our infrastructure thinking and combined multiple physical servers into a cluster. We used those so-called 'virtual machines' to run multiple applications in isolation on top of this infrastructure. Deployment and management got way faster and easier. However, server administration was still necessary and largely very manual. Entering the cloud (IaaS) Setting up and operating your own datacenter came with new operational challenges; cloud computing began to tackle those issues. Why not rent your servers and operational services individually, for a monthly fee? This approach made it way easier to scale up or down, and let teams move faster. PaaS While cloud environments made it convenient to build large-scale applications, they still came saddled with the downsides of manual administration: \"Are the latest security fixes installed?\" \"When should we scale down/up?\" \"How many more servers do we need?\" Wouldnt it be great if all those administrative hassles were taken off of our plates, and we could simply focus on applications and business value? Yep! That's what some other folks started thinking, too. In corner : Containers Wouldnt it be nice if one could pack the application, with alllllll its dependencies, into a dedicated box and run it anywhere? No matter what software dependencies the host system has installed, or where and what the host system actually is? Thats the idea of containerization. Create a container which has all the required dependencies pre-installed, put your application code inside of it and run it everywhere the container runtime is installed. No more devs saying: \"Well, it works on my machine!\" Containerization gained attention when it came to light that Google used such technologies to power some of their services (such as Gmail or Maps). Using containers was initially pretty cumbersome, however; it required deep knowledge about Linux kernel internals and making home-grown scripts to put an application in a container and run it on a host machine. Then Dotcloud (a PaaS startup from San Francisco) announced a new tool called Docker at Pycon US . Docker was an easy to use CLI tool which made it possible to manage software containers easily. Dotcloud then pivoted to become Docker, and Google worked on an OpenSource implementation of the \"Borg\" container orchestration service, which is called Kubernetes. More and more enterprises adopted containers, and standards around this new technology got defined. Nowadays, nearly every cloud provider offers a way to host containerized applications on their infrastructure. Advantages of containers - Control and Flexibility - Vendor-agnostic - Easier migration path - Portability Disadvantages of containers - Administrative work (e.g. apply security fixes for containers) - Scaling is slower - Running costs - Hard to get started - More manual intervention In corner : Serverless compute (FaaS) About a year later, AWS introduced the first serverless compute service ever: AWS Lambda. The most basic premise of a serverless setup is that the whole application--all its business logic--is implemented as _functions_ and _events_. Here's the full break-down. Applications get split up into different functionalities (or services), which are in turn triggered by events. You upload your function code and attach an event source to it. Thats basically it. The cloud provider takes care of the rest and ensures that your functions will always be available and usable, no matter what. When serverless compute was first introduced in , the workloads were pretty limited and focused around smaller jobs such as image/data manipulation. But then AWS introduced the API Gateway as an event source for Lambda functions. That changed everything. It became possible to create whole APIs that were powered by serverless compute. More and more AWS services integrated with the Lambda compute offering, making it possible to build even larger, more complex, fully serverless applications. But what is a serverless application, exactly? In sum, an architecture is serverless if it has these characteristics: - Event-driven workflow (\"If X then Y\") - Pay-per-execution - Zero administration - Auto-scaling - Short-lived, stateless functions Advantages of serverless - Zero administration - Pay-per-execution - Zero cost for idle time - Auto-scaling - Faster time-to-market - Microservice nature > Clear codebase separation - Significantly reduced administration and maintenance burden Disadvantages of serverless - No standardization (though the CNCF is working on this) - \"Black box\" environment - Vendor lock-in - Cold starts - Complex apps can be hard to build When to choose what? Now it's time for the big question: Truthfully, it depends. When to choose containerization Containers are great if you need the flexibility to install and use software with specific version requirements. With containers, you can choose the underlying operating system and have full control of the installed programming language and runtime version. It's even possible to operate containers with different software stacks throughout a large container fleet--especially interesting if you need to migrate an old, legacy system into a containerized environment. As an added bonus, many tools for managing large-scale container set-ups (like Kubernetes) come with all the best practices already baked in. This flexibility does come with an operational price tag, though. Containers still require a lot of maintenance and set-up. For maximum benefit, you'll need to split up your monolithic application into separate microservices, which in turn need to be rolled out as individual groups of containers. That means you'll need tooling that allows all those containers to talk to each other. You'll also need to do the grunt work of keeping their operating systems current with regular security fixes and other updates. While you can configure the container orchestration platform to automatically handle traffic fluctuations for you (a.k.a, self-healing and auto-scaling), the process of detecting those traffic pattern changes and spinning the containers up or down won't be instantaneous. A complete shutdown where no container-related infrastructure is running at all (e.g. when there's no traffic) will also not be possible. There will always be runtime costs. When to choose serverless In that vein, serverless is great if you need traffic pattern changes to be automatically detected and handled instantly. The application is even completely shut down if there's no traffic at all. With serverless applications, you pay only for the resources you use; no usage, no costs. The serverless developer doesn't have to care about administrating underlying infrastructure; they just need to care about the code and the business value to end users. Iteration can be more rapid, as code can be shipped faster, without set-up or provisioning. In fact, because the underlying infrastructure is abstracted, the developer may not even know what it looks like. They won't really need to. But currently, there are some limitations with vendor support and ecosystem lock-in. Programming languages and runtimes are limited to whichever the provider supports (though there are some workarounds (or \"shims\") available to overcome those restrictions). Event sources (which trigger all your functions) are usually services that the specific cloud provider offers. Reasoning about all the individual pieces of the application stack becomes harder when the infrastructure and the code are so separate. Serverless is a bit more new, and its tools still have room to evolve. That's what we're actively working on here at Serverless.com, anyway. Final verdict? Of course, this will be an oversimplification. The real world is always more complex. But your rule of thumb? Choose containers and container orchestrators when you need flexibility, or when you need to migrate legacy services. Choose serverless when you need speed of development, automatic scaling and significantly lowered runtime costs. Related articles: - Why we switched from Docker to Serverless",
      "__v": 0
    },
    {
      "_id": "64e0891bb72e199dda603e84",
      "title": "How to plan a team retreat",
      "content": "So about this extensive country list in the Serverless.com footer: Yeah. We decided early on to embrace having a largely remote team. The reason was pragmatic: as a small company, wed get access to bright minds while avoiding the intense competition (and requisite salaries) for talent in the Bay Area. At this point, half our team works from another corner of the world. Its been a fantastic challenge. Ever tried to find a good time to schedule all-hands meetings when your team spans time zones? The most important thing we learned is that, in smaller companies, decisions get made at the lunch table. Our remote team members were missing out on daily small talk that turned out to be critical. Asynchronous comms over Slack and Github werent quite enough. So we did what seemed like the most reasonable thing: we brought everyone, the entire remote & local team members, together for a retreat. And howd it go? It was so successful that we now do them twice a year. Our team chemistry got way better, and weve been able to collect tons of candid feedback about company culture and process that we wouldnt have gotten otherwise. If you have a largely remote team, frankly we really recommend it. But before you do, we've learned some important lessons about getting the best return on investment. So read on and learn from our mistakes. Distribute goals beforehand Our goals inform our work sessions, team outings and even downtime. Setting them in advance means we can send out the agenda early enough for team members to prepare for each session. If youre used to OKRs, then youre probably familiar with how to set a high-level Objective and achieve it with Key Results stepping stones, but here are some concrete examples from our past retreats: At the end of , some changes in leadership had left several distributed team members feeling out of the loop. Objective: get everybody re-aligned. Key Results: define our team values together, and do team building activities to establish trust and comaraderie. It really helped, and we started making swifter progress toward some aggressive product goals. But that swiftness exposed some serious issues with our product process. That became the goal of our next retreat: improve the way we do product. There were several days of intense conversations, but at the end of the week we had solved a lot of pain points and implemented a framework loosely based on human-centered design. Survey & track success Of course, we wouldn't know how successful our team retreats are without analyzing some data. (DataNerds) We use Officevibe to track happiness and engagement on a weekly basis. Our team trend includes a big spike in satisfaction right after a team retreat, followed by a slow taper-off: It makes senseface-to-face interaction is good for team morale, but eventually you need a re-up. We've found that the six month cadence is good timing to keep the team at a positive level of engagement. We measure the success of our team retreats themselves via before & after surveys. We always see a big jump in team members feeling aligned right after a retreat. The change was palpable in post-retreat team meetings: people tended to be more positive, and focus on how to make a solution work versus talk about why an idea couldn't work. It isnt enough to simply look at the data, though. We use these feedback loops to constantly improve our process. We always ask, What did you love? What could have been better? What should we do next time? on each of our surveys. Its already given us a huge actionable insight: at our next retreat, were going to try bringing on a facilitator to keep us efficient and focused. Remember to relax All work and no play makes serverless a lustless team. Itd be a bit cruel to have everyone travel halfway around the world just to sit in a conference room all day. That's why we try to find cool retreat locations with easy to access recreation. Plus, its even better bonding material when you can all experience something new together. One of our best moments, actually, was hiking Hendy Woods after a huge rainstormnavigating the flooded redwood trail and hanging with swimming salamanders. Evenings usually include board games, jam sessions or several rounds of Mafia). ...because nothing says go team! quite like calling each other liars and pretending to kill each other one by one. Those moments of communal downtime are just as important as any work sessions we organize. They remind us of how much we like each other as people, and help build a strong foundation of trust and respect. Sometimes it's easy to forget that the person on the other end of the Slack channel is a human being with a family, hobbies and obligations; at their best, retreats lower barriers and foster friendships. Plan your own These are just a few ways that we make sure we are getting the maximum return on our team retreats. If you want the full story, our Retreat Planning Guide is publicly available inside the Serverless Company Handbook. Feel free to peruse. We hope it works as well for you as it has for us! Any questions, drop us a comment below.",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e86",
      "title": "ServerlessConf  Recap - NYC",
      "content": "High level takeaways If the last ServerlessConf was the conference of serverless projects, this one was the conference of serverless tooling. Every presentation has been plugging what they use right now for debugging, monitoring and development (AWS X-ray, IOpipe, Auth extend...)while in the same breath highlighting areas where we still need more tools and more features in them. In our opinion, it's a hugely positive shift. We've reached the point where everyone knows they can make cool stuff with serverless, and now they want an ecosystem to support serverless development. We can already tell: ServerlessConf next year will already be a drastically different landscape. There was also a strong theme of rejecting 'NoOps' in favor of DiffOps (kudos to Ben Kehoe for that sweet hashtag). Point being, the specific role titles might change, but nobody gets to eliminate ops. If anything, you need developers who are capable of learning how to do smart ops for distributed systems. The gritty details We're giving you the full notes from some of our favorite talks! If you were there, we hope they're a nice refresher. If you were home, we hope it's like you were there. Click to jump straight to your fave talk, or scroll down to read them all: Day - The State of Serverless Security - tips for running a serverless business... number will blow your mind! - Shipping Containers As Functions - Harmonizing Serverless and Traditional Applications - Break-up with Your Server, but Dont Commit to a Cloud Platform - Serverless Design Patterns - Event-driven Architectures: are we ready for the paradigm shift? Day - Why the Fuss about Serverless? - Serverless and Software Craftsmanship - Global Resiliency when going Serverless - The CNCF (Cloud Native Computing Foundation) point of view on Serverless - Data Layer in the Serverless World The State of Serverless Security by Mark Nunnikhoven How does security in the serverless world really shape up? Let's first go back to basicsthe part Shared Responsibility Model: - data - application - OS - virtualization - infrastructure - physical When cloud entered the scene, this list got cut in half: - data - application - OS And then containers whittled it away a bit more: - data - application Until under a serverless paradigm, we are left only with: - data So what, then, should be our new model? Mark proposes three components to serverless security: . Functions Code quality is a problem. Okay...it's the problem. If you look at the OWASP top most common vulnerabilities, they've barely changed since . Dependencies are another factor. Their weakness is your weakness; dozens of dependencies mean dozens of possible threat points. And don't think that low-level threats are nothing to worry aboutpeople can get root access by exploiting the right combination of grade threats. Still, we're doing pretty good overall here. Mark gives Serverless Functions security a B+. . Services How does the provider secure their service? Make sure to check their certifications. If they don't have certs (reasonably common in newer companies and smaller start-ups), then grill them. Make sure they are fully transparent with you. Also keep in mind what kind of security controls do they have. Can you encrypt at rest, use your own keys? Vendors have a lot to lose if there's a breach, and they tend to be pretty good about this stuff. Services get a solid A. . Data flows Spoiler alert: this is where we're losing. We don't yet have enough tooling for assurance of protections, data flow visibility or code quality. Though these things seem to be in the works from several people in the space, so we'll see what the state of affairs is by next ServerlessConf. For now though, we're at a C-. Then what's the state of serverless security overall? Mark gives it a B. (Better, he notes, than containers) tips for running a serverless business... number will blow your mind! by Sam Kroonenburg Sam started coding a learning platform years ago. You might know it now as this little company called A Cloud Guru. He knew he'd need to include video lessons, a quiz engine, an online store and sign up / log in, while having something that scaled effortlessly and had low operational overhead. Tl;dr: he cared about the fastest, cheapest way to build a company and get his MVP out there. So he went serverless (check out his non-existent EC for proof): His learning platform started as a serverless monolith, and Sam advocates that choice. It's a great way to launch fast, and as long as you're starting serverless, the transition to microservices later is pretty straightforward. For all those serverless entrepreneurs out there, Sam has some guiding principles and advice: - It's a myth that serverless means you don't need ops. Of course you need ops, but responsibility does shift to the dev team - When you need to fill engineering positions, dont look for serverless developers. They dont exist yet. Instead, filter for a developer who cares about smooth running code in production. - Encourage & reassure job candidates that it's okay if they don't know your stack; you're ready to teach, and it will be exciting for them. Otherwise, they'll be too intimidated to apply at all. - Join the community. That's how Sam learned about Algolia back in the day, and began contributing to the Serverless Framework. - Expect to pioneer. Serverless teams build more tools, all the time. - Its ok to build a serverless monolith. And when you decide to migrate to a microservices architecture, you can do it without thinking about infrastructure. - Automation is not optional. You cant deploy all these pieces manually once you have several. - Test all the things. You can never fully emulate operating environments for development purposes. Shipping Containers As Functions by Amiram Shachar Remember how, with every technological invention, the word on the street was that the 'old' would completely go away? Computers would kill paper, Microsoft would kill IBM, etc. In reality, this process takes way, way longer than we think. So what does the future of serverless look like? Here's what Amiram thinks: As you can see, containers and VMs are still in that chart. He also makes a case for shipping containers as serverless functions. You could, say, use a Docker image as a function. You wouldn't have to zip anything or add dependencies; just put it in a Docker file, package and ship. Harmonizing Serverless and Traditional Applications by Ryan Scott Brown It's still pretty common that when we talk about serverless, we start off talking about a greenfield project. But that's not most people's reality. How do you plug serverless functions into an existing application, bit by bit? The hardest part is changing the model of your app to integrate these new event streams, and breaking up coupling of all the jobs that are currently running. Ryan recommends a slow integration flow, from 'incidental glue' to backend tasks (which will be less likely to make users mad if something goes wrong) before finally moving on to end user features. Protip: beware of scaling, and not in the way you think. Lambda will scale just fine. Too fine. Fine enough to completely ruin every downstream service. Get good at prioritizing end-user events to customer impact is minimal. Use Kinesis to denormalize data into messages. Keep a monorepo. Watch everything: Cloudwatch, IOpipes, Honeycomb, ELK Stack... Read his full notes over at serverlesscode. Break-up with Your Server, but Dont Commit to a Cloud Platform by Linda Nichols How can you go serverless without vendor lock-in? Linda proposes two possibilities: containers multi-provider frameworks Both totally work, and it all depends on your preference. Linda personally prefers multi-provider frameworks and spent most of her talk focused on the Serverless Framework specifically. Her argument was pretty straightforward: look, even if you know AWS really well, that knowledge doesn't transfer. GUIs highly vary and can be hard to navigate. If you don't use a multi-provider framework, you're essentially locking yourself in; there's too much friction to use that cool new Azure feature when you don't know Azure well. The ideal multi-provider framework should abstract just enough to be useful, without completely abstracting away the native deployment frameworks for each cloud vendor. Otherwise, it's too much to keep up with and the ball will eventually drop. As fabulous as her talk was, the twitter conversations around it have been even more fun to follow. Check them out. No one wants to deal w/ containers the serverless path forward is multi-provider frameworks (ie @goserverless)-@lynnaloo serverlessconf pic.twitter.com/gQGkUtUkK&mdash; Joab Jackson (@Joab_Jackson) October , Serverless Design Patterns by Tim Wagner, Yochay Kiriaty & Peter Sbarski Tim spoke about the need for design patterns for serverless and did a walkthrough of how to implement a map-reduce pattern with serverless. Then, he announced some exciting newsthey're releasing a book! It's called Serverless Design Patterns, and it's slated for release in . Yes to more vendor-agnostic learning resources, best practices and standards. Event-driven Architectures: are we ready for the paradigm shift? by Ben Kehoe This talk was so good, we really just want you to hang tight for the video and watch it all. Ben had a sobering but uplifting message: There are some really savvy companies, like Nordstrom, who are doing intricate event-driven design. But it's hard, and it's complicated. Serverless isn't quite there yet. Event-driven isn't quite there yet. So event-driven Serverless? Yeah...not there yet. Serverless has many problems common to technologies that are still in their infancy. There isn't yet a good solution for service discovery or incremental deployments. Ben notes that he'd love to use VPCs for everything, but many services still dont have VPC endpoints. The tooling and ecosystem support just isn't quite there. But! Ben reminds us, \"We're all here because we think it's worth it.\" Improvements are happening incredibly fast. He's already anticipating that, with the current rate of change, his Venn diagram will be out of date by next ServerlessConf. As a community, let's band together and make sure that happens. Why the Fuss About Serverless? by Simon Wardley Simon regaled the crowd with a thoughtful appraisal of how we think about systemsand what this all means for serverless. Here are some of his key points: Maps dont equal Diagrams Maps help us better conceptualize problem spaces; we should all be making them. But what most developers call 'maps' (systems maps, anyone?), aren't actually maps. Maps are more than just a visual; they also need to take into account anchor, position and movement. Imagine if you were to take Australia and plop it down beside Peru. Instantly different globe. So back to our systems 'map'. It doesnt have any of those characteristics. Move CRM over to the right? Nothing about the relationship between components change (as theres no anchor). We've been doing it wrong. These diagrams aren't painting a picture of the market landscape. Simon's prefers to start with a User as his anchor, and flow down from that user on to what they want and need, then what those needs would require, and so on, until he ends up at the technology that supports those needs. We're still following the same patterns With every new technology we say: \"It'll reduce the budget! It'll eliminate such-and-such role!\" But look. Serverless wont reduce your IT budget; youll just make more stuff. Just like Cloud didn't reduce your IT budget. You made more stuff. As technology grows towards the commodity end of the spectrum, more market players build it into their status quo. This is a cyclical process. Each new process begets another, which in turn is supplanted by yet another. Commoditized approaches gather inertia, leaving new technology adoption spotty and often from the bottom-up. But, when the technology merits, its just a case of when, not if. The same is certainly true for Serverless. Adoption is what it is Hold tight. Serverless is a great wave, and it's coming. But adoption of new technology usually takes - years. So here's the adoption curve we can we expect going in to : - Increase in efficiency - Rapid acceleration in speed of development - Explosion of higher order systems of value - No reduction in IT spend - No choice over adoption (if not when) - Non linear, > % of all new IT spend Serverless and Software Craftsmanship by Florian Motlik Serverless can get pretty conceptually complex once you have several functions in production. So why are we doing it? Productivity, plain and simple. But what we need to remember about productivity is that it isn't only about code, it's about pushing things for the end user. Bad code doesn't help anyone, and firefighting isn't productive work. When your infrastructure is an extension of your code code (as it is with Serverless), that means you have to treat your infrastructure as well as your code. This responsibility falls to developers. We need to cultivate a culture of focus on Insight, Resource Management and Operations. If you can't answer questions about your deployed resources in less than seconds, it's a problem. If you don't have resource management automated, it might as well not exist because it's not repeatable. The Cloud has made our infrastructure standardized, but needs aren't uniform; in that case, customize your tools. It's easier now than it's ever been. Global Resiliency when going Serverless by Jared Short Jared came at us with some very practical advice. So you're serverless? Be resilient about it. You need failover, and here are some ways he's explored doing it. He breaks down resiliency into active + passive and active + active. Active + passive resiliency The easiest possible scenario, if your business case allows it, is read-only failover. Using AWS CloudFront, you can swap one APIG to another APIG; takes about minutes to fully roll out and is completely invisible to clients. Active + active resiliency Active + active is way more interesting, but also way more difficult, and a lot of the solutions (gasp) use servers. As for data, how do you manage conflicting edits? There are two bad options: () last write wins, () write your own painful-to-maintain resolver before giving up and crying. Or! (drumroll...) You could try Conflict-free Replicated Data Types (CRDTs). These are pretty mathematically complex, but make it so that it's always possible to resolve changes. If you need advice, Jared is already pumped for a twitter conversation. If you want to go multi-provider, then you will give up some ecosystem benefits of staying within a single provider. But if you are going to do it, then abstract events and context early on in the call. He recommends checking out the Event Gateway for a peek at a tool that makes multi-provider much easier. The CNCF (Cloud Native Computing Foundation) point of view on Serverless by Daniel Krook The CNCF established a serverless working group months ago. Their very first initiatives are to finalize a serverless whitepaper and advocate a common model for event data. They're also collecting and publishing community resourcese.g. a matrix of existing serverless providers and toolsand are moving their next focus into examples, patterns and possibly collaborating on packaging specifications. Even if you're not a CNCF member, you can still attend meetings. So get involved and stay updated! Their GitHub repo is here. Data Layer in the Serverless World by Alex Debrie Our very own Alex DeBrie gave a great overview of the data layer of serverless architectures. He broke them down into two segments: serverful and serverless. Serverfull databases These are the more traditional databases including Postgres, MySQL, and MongoDB, where you have a defined number of instances running and scaling them up and down is more of a challenge. Some of the benefits of serverfull databases: - They have mature ecosystems - Many cloud providers offer them as managed services - there is less vendor lock in Some of the downsides of serverfull databases include: - More maintenance, issues with uptime and scaling difficulties up and down - Networking concerns in the FAAS space, where cold starts can be an issue when running in VPCs - Connection limits and the lack of pooling due to the nature of functions spinning up and down Serverless Databases With serverless databases, you don't know how many instances are running; that is abstracted away from the developers. Some examples would include DynamoDB, Fauna, Google firebase or firestore. They typically will auto scale for you and maintenance is less of a burden. Serverless Database Benefits: - auto scales up and down for you - less maintenance, provider takes care of this for you - faster time to market. Spin up instances very quickly Downsides: - Less developer familiarity - Can be harder to query if data models aren't setup correctly - vendor lock-in and harder migrations to other non proprietary database engines In sum It's not NoOps but DiffOps. We need more tools. Serverless truly is here to stay, and we're all excited about the increases in productivity. It's no longer 'what can we build in a serverless way?' but 'why can't we make this serverless yet, hurry up already'.",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e88",
      "title": "How (and why) we designed the Event Gateway",
      "content": "The Event Gateway is our most recent announcementand honestly, it's a project born out of passion for the serverless movement as much as it is a practical tool motivated by an industry need. Serverless development is still fresh, lacking in best practices and tooling. But people use it because the payoffs are worth the pain. Our job at Serverless.com is to remove that pain: fight for standardization, think from the top-down about what a serverless application should look like, make its development seamless. The biggest sticking point we see here is that serverless development is fundamentally event-driven development. Every function you deploy to your FaaS provider will remain idle until woken up by an event. No one likes to hear that. Event-driven is a new paradigm, and new paradigms mean change. But to move serverless forward, we as a community have to embrace event-driven design. Our job at Serverless.com is to make it painless. As we can, at least. The Event Gateway is a big step for us: a new piece of infrastructure that treats all data flows as events, and lets developers react to those flows with serverless functions. See how we treaded the intersection of 'exciting new ground' and 'real world practicality'. Here, we lay out our design process from planning to execution to getting the tool in your hands. Design Considerations We had two primary considerations: the developer experience when using the Event Gateway, and the developer experience when operating a cluster of Event Gateway instances. To keep us on track, we decided to establish four core guiding principles. It doesn't mean that they won't change in the futurethey surely willbut we wanted to have something that would give us a jump start during the implementation phase and carry throughout the project development. Those guiding principles were: - Simplicity - Keep the number of new concepts low. We didn't want to rename existing concepts just for the sake of it. There would be only events, functions, and subscriptionsas easy as possible. In terms of operational simplicity, our goal was to make it easy to run the event gateway locally during development, while also making it easy to run, manage and scale in production environments. - Cross-cloud - Same as with the Serverless Framework, the event gateway had to provide a seamless user experience, no matter where the user deploys their functions or where they want to deploy the Event Gateway itself. In the latter case, we also needed to support on-premise deployments. - Event-driven - We strongly believed that event-driven was the right approach for building software systems. The Event Gateway should not only enable developers to build them, but should also follow this paradigm internally. - Optimized for fast delivery - The main goal of the project was event delivery. We wanted to make it instant. Architectural Choices With those principles in mind we made a few explicit, architectural choices that drove the implementation. Stateless The event gateway was designed to be a stateless service backed by an external key-value store. This was to make it easy to operate and reason about. Assuming that we want to build a horizontally scalable system, making the event gateway a stateful service meant that we'd need to implement yet another distributed database. As that did not fit with our core values (remember: Simplicity and business value!), we decided to avoid it. One important consequence of that choice is that events lack persistence. There are possible solutions to this, thoughe.g., a plugin system that enables integration with existing storage systems (like AWS S, Kafka). Configuration Store Here, we relied on existing solutions. There are at least few battle-tested key-value stores out there already. Etcd, Consul, and Zookeeper are definitely the most popular for storing configuration in very successful, production-grade systems like Kubernetes or Kafka. But we decided to use the libkv library for supporting all of them. Libkv is an abstraction layer over popular key-value stores that provides a simple interface for common operations. It has some limitations (like lack of atomic operations on multiple keys), but it was a good start, and we might start contributing to it once our needs exceed its provided functionality. Purely for demo/trial purposes, the Event Gateway can be started with a special flag which starts an embedded etcd instance. This allows users to test drive the system without starting up a key-value store cluster first. Eventual Consistency Another choice that highly influenced our overall architecture was making the Event Gateway an eventually consistent system. When the user registers a function or subscribes a function to some event, the configuration is saved in the backing key-value store in a synchronous way. Then, the data is spread across all instances asynchronously, with an event-driven approach. Thanks to libkv, all key-value stores that we support have an ability to watch for changes. Every instance fetches all configuration data during startup, and then watches for changes happening during the instance runtime. We use that to build the internal cache that our routing logic depends on. It means that when the Event Gateway needs to decide which function to call for a specific event, it doesn't need to do any remote calls to the backing store. All configuration data used by routing logic is stored locally. Without watches, we would have to continuously scan ranges of keys in the database until we find new data, which is enormously expensive, slow, and hard to scale. Language Choice We needed a language with mature production environment. We also needed a strong static type system which supported concurrency well and generated binaries that were simple to distribute. Go had it alla rich standard library, a vast ecosystem (libraries, tools, write-ups) and an active community. Building an open source project also meant that we needed to provide a seamless experience for potential contributors. In the infrastructure software space, Go seems to be one of the most popular language choices. As you can probably already tell, we built the Event Gateway in Go. What's next The Event Gateway is still in an early phase. We count on you, our community, to provide feedback and give us frank opinions on project direction. The roadmap is publicly available. Feel free to open an issue in the repo or join contributors Slack and let us know what you think! This is the first blog post of a series on Event Gateway architecture. In the future blog posts I will focus more on internal architecture, cluster architecture and deployments strategies.",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e8a",
      "title": "Creating a Serverless GraphQL Gateway on top of a rd Party REST API",
      "content": "Introduction I've spent a huge chunk of the last year learning how to write GraphQL servers. It took a lot of manual sifting through dozens of blog posts, videos and source code. I wanted to consolidate all this info into a single walk-through. If I've done my job right, it's the only post you'll need to get up and running with your own project. We're going step-by-step through the setup of my most recent project, Flickr-Wormhole: a GraphQL to REST API Gateway built on top of Serverless and AWS Lambda, using Apollo-Server-Hapi (to provide a modern interface to that aging Flickr API). Let's get started! Background As a web developer, I relish the challenge of building my personal website from scratch. It's a great opportunity to spend way too much time on creative solutions to weird problems. My most recent challenge? Adding a gallery to showcase my photography. To ship this feature I had a handful of requirements to work around, which ultimately led me to creating the solution we'll be covering today: - My site is statically generated and hosted on Netlifyno admin console to add/manage photos. - I didn't want to upload my photos along with the rest of the site; it's being built from a GitHub repo, and would require me to write scripts for generating different image sizes for mobile. - The gallery should be able to display titles, descriptions, EXIF metadata, geolocation, tags, comments, etc. - Uploading/managing photos should fit into my existing photo editing workflowI didn't want to create unnecessary steps. - The image hosting solution needed to be dirt cheap, preferably free. - Most importantly: as my site was designed to be a Progressive Web App, data retrieval had to be done in as few network requests as possible. I decided to save myself a lot of coding and piggyback off Flickr for the majority of this work. It already had most of what I needed: free, generates a range of image sizes, public API and Adobe Lightroom integration for bulk uploads all in the press of a button. That just left me with one \"little\" problem: having to use Flickr's horribly outdated REST API. So you fully feel my pain, here's a quick look at what that process was like. The Old Way Remember, I wanted to minimize my number of requests. Here's the bare minimum with Flickr's REST: Get the `userId` of the Flickr user whose albums (referred to as 'photosets') I wanted to grab photos from Use `flickr.photosets.getList` with our `userId` to get a list of `photosetId`s for that user Use `flickr.photosets.getPhotos` using those two ids to get a list of `photoId`s for that album Use `flickr.photos.getSizes` for each of those `photoId`s for a list of URLs linking to automatically generated images for those photos (or use the `id`, `secret` and `server` fields from the previous response to construct the URLs manually) I, however, would need to make even more: a call to `flickr.photosets.getInfo` to get info about the album (title, description, number of views, comments...), a call per photo to `flickr.photos.getInfo` to get its title, caption, views, comments & tags, and another call per photo to `flickr.photos.getExif` to get the EXIF metadata, a call to `flickr.photos.getSizes` to build out a responsive `img` element for each photo in the gallery... For a photo album, I'd need network requests. \\groans\\ How about we not do that? And it got worse. The response data was a mess to handle. The `photos` count was represented as a `string`, `views` was a `number`, the `title` and `description` were nested in an unnecessary object under a `_content` key, and the dates were either formatted as a UNIX timestamp or a MySQL DateTime value wrapped in a `string`. Surely, I thought, there must be a better way. The New Way - GraphQL Enter: GraphQL! Why GraphQL? So glad you asked. Let's go back to those + requests from earlier. I can now grab all of that data in one request using a query that looks a bit like this:  Try it out yourself here: https://flickr.saeris.io/graphiql Not only did I grab all the data I needed to build out the UI in one request, I also got only the specific fields I asked for in exactly the same shape I requested it in. AND I was able to apply some powerful filtering techniques to boot. GraphQL Setup Since the Flickr API doesn't yet have a GraphQL endpoint, I had to create my own GraphQL gateway server that proxies GraphQL queries into requests to Flickr's REST interface. To build the application, we'll need: - A GraphQL endpoint request handler; - An abstraction layer to programmatically build requests to Flickr's REST API; and - A GraphQL Schema of queries mapped to Type definitions that describe our different data structures. Building the Endpoint Request Handler The first step is to choose a GraphQL server implementation and set up the request handler. I'd used Apollo on the front-end and really enjoyed it, but Flickr didn't have a GraphQL endpoint I could connect it to so I had to build one myself. Since I had a goal to reduce my network requests, I really liked Apollo for its automatic request caching, which helps eliminate re-fetching. After choosing Apollo, I needed to adapt it to work within Lambda's function signature. Apollo does have a solution specifically for AWS Lambda. However, I chose to use the Hapi Node.js server framework with Apollo-Server-Hapi plugin. I prefer Hapi as it allows for custom logging, monitoring and caching. Let's take a look at our `serverless.yml` file: serverless.yml ```yml service: flickr-wormhole frameworkVersion: \">=.. { const { path, queryStringParameters: params, httpMethod: method, body: payload, headers } = event server.makeReady(err => { if (err) throw err let url = path if (params) { const qs = Object.keys(params).map(key => `${key}=${params[key]}`) if (qs.length > ) url = `${url}?${qs.join(`&`)}` } server.inject({ method, url, payload, headers, validate: false }, ({ statusCode, headers, result: body }) => { delete headers[`content-encoding`] delete headers[`transfer-encoding`] callback(null, { statusCode, headers, body }) }) }) } ``` The `server.js` file contains our actual Hapi server: server.js  In `server.js` we're defining a custom method, `makeReady`, on our new Hapi server instance to register our plugins. In a server-full world, you'd want to call `server.start()` in the callback for `server.register()`. In the serverless world, we're using `server.inject()` to inject the HTTP request event from Lambda, because we're not using Hapi to listen to HTTP events. Note that we only register our plugins on the initial invocation to a particular Lambda instance. If we called `server.register()` on every invocation of our Serverless event handler, Hapi would throw an error complaining that we've already registered the given plugins. Now let's take a look at our main GraphQL specific files used to create our endpoint: `api.js`, `graphiql.js`, and `schema.js`. First, we have the `api.js` file which will define our main `/graphql` endpoint: `api.js` ```javascript import { graphqlHapi } from \"apollo-server-hapi\" import depthLimit from 'graphql-depth-limit' import queryComplexity from \"graphql-query-complexity\" import as loaders from \"@/loaders\" import { formatError } from \"@/utilities\" import { schema } from \"@/schema\" import { Flickr } from \"@/flickr\" export default { register: graphqlHapi, options: { path: `/graphql`, graphqlOptions: (request) => { // Create a new instance of our Flickr connector for each new GraphQL request const flickr = new Flickr(FLICKR_API_KEY) return { schema: schema, context: { // pass the connector instance to our resolvers and to the loaders which will cache per request flickr, album: loaders.loadAlbum(flickr), albumPhotos: loaders.loadAlbumPhotos(flickr), brands: loaders.loadBrands(), cameras: loaders.loadCamerasByBrand(), photo: loaders.loadPhoto(flickr), images: loaders.loadImages(flickr), licenses: loaders.loadLicenses(), user: loaders.loadUser(flickr), userAlbums: loaders.loadUserAlbums(flickr), userPhotos: loaders.loadUserPhotos(flickr) }, root_value: schema, formatError: formatError, validationRules: [ depthLimit(), // Limits our queries to levels of nesting. queryComplexity({ maximumComplexity: , variables: {}, onComplete: (complexity) => { info(`Determined query complexity: ${complexity}`) }, createError: (max, actual) => new GqlError(`Query is too complex: ${actual}. Maximum allowed complexity: ${max}`) }) ], tracing: true, debug: true } }, route: { cors: true } } } ``` The exported object from `api.js` is a Hapi plugin configuration, which we'll pass along as part of an array to our `server.register()` method on startup. There are a couple interesting things to note: In our context object, we're providing variables that will be available to all of our resolver functions, such as our Flickr connector and our Dataloader instances for caching. Because we're running on Lambda, it's important to perform some Query Complexity Analysis to ensure incoming queries won't max out our execution times. To accomplish this we're going to use two libraries: `graphql-depth-limit` and `graphql-query-complexity`. You'll notice that we have tracing enabled, which will append performance data to our responses. Check out Apollo Tracing and Apollo Engine for more information on how you can use this to enable performance monitoring on your GraphQL endpoint. There's also graphiql.js, which defines the /graphiql endpoint for the GraphiQL IDE: `graphiql.js` ```javascript import { graphiqlHapi } from \"apollo-server-hapi\" export default { register: graphiqlHapi, options: { path: `/graphiql`, graphiqlOptions: { endpointURL: `/graphql` } } } ``` Not a whole lot out of the ordinary here for configuring our graphiql IDE endpoint. Just a few things to note concerning AWS Lambda: - If you're not using a custom domain for your function, you'll need to change your `endpointURL` to add the stage prefix on deployment, otherwise graphiql won't be able to find your API when running on AWS. This can be confusing because it will run just fine locally. Had me scratching my head over this one for a little while! - If you do want to use a custom domain, I used this Serverless article and this part of the AWS documentation to help get mine configured. Finally, let's look at our `schema.js` file which includes our GraphQL Schema for our GraphQL endpoint: `schema.js` ```javascript import Types from '@/types' export const schema = new GqlSchema({ types: Object.values(Types).filter(type => !!type.Definition).map(type => type.Definition ), query: new GqlObject({ name: `Query`, description: `The root query for implementing GraphQL queries.`, fields: () => Object.assign({}, ...Object.values(Types).filter(type => !!type.Queries).map(type => type.Queries)) }) }) ``` Also pretty straightforward. In each of our Type Definition files, we're going to have a default export, which will include our Type Definition and Queries associated with it (we'll see what those look like soon). We'll wrap all of those up in an `index.js` file as a default export, which we'll iterate over to generate pieces of our Schema. If you want to include Mutations in the same manner, simply copy the `query` key and follow its format to import a Mutations object from each of your Type Definitions. I like to do things this way to keep code tidy and co-located. Webpack Notes I do a few different tricks in my Webpack configuration to ease development. Here's my full Webpack configuration for reference: `webpack.config.js` ```javascript const { join } = require(`path`) const slsw = require(`serverless-webpack`) const nodeExternals = require(`webpack-node-externals`) const MinifyPlugin = require(`babel-minify-webpack-plugin`) const { DefinePlugin, ProvidePlugin, optimize } = require(`webpack`) const { ModuleConcatenationPlugin } = optimize const dotenv = require(`dotenv`) dotenv.config() // import environment variables defined in '.env' located in our project root directory const ENV = process.env.NODE_ENV && process.env.NODE_ENV.toLowerCase() || (process.env.NODE_ENV = `development`) const envProd = ENV === `production` const srcDir = join(__dirname, `src`) const outDir = join(__dirname, `dist`) const npmDir = join(__dirname, `node_modules`) module.exports = { entry: slsw.lib.entries, target: `node`, externals: [nodeExternals({ modulesFromFile: true })], output: { libraryTarget: `commonjs`, path: outDir, filename: `[name].js` }, resolve: { extensions: [`.js`, `.gql`, `.graphql`], alias: { '@': srcDir // used to allow root-relative imports, ie: import { invariant } from \"@/utilities\" } }, module: { rules: [ { test: /\\.js$/, loader: `babel-loader`, exclude: npmDir, options: { plugins: [ `transform-optional-chaining`, // enables the usage of Existential Operator, ie: ?. `transform-object-rest-spread`, `transform-es-shorthand-properties` ], presets: [ [`env`, { targets: { node: `.` }, // AWS Lambda uses node v., so transpile our code for that environment useBuiltIns: `usage` }], `stage-` ] } }, { test: /\\.(graphql|gql)$/, exclude: npmDir, loader: `graphql-tag/loader` } // in case you're using .gql files ] }, plugins: [ new DefinePlugin({ // used to provide environment variables as globals in our code ENV: JSON.stringify(ENV), LOGLEVEL: JSON.stringify(process.env.LOGLEVEL), FLICKR_API_KEY: JSON.stringify(process.env.FLICKR_API_KEY) }), new ProvidePlugin({ // used to provide node module exports as globals in our code // GraphQL GqlBool: [`graphql`, `GraphQLBoolean`], // same as import { GraphQLBoolean as GqlBool } from \"graphql\" GqlDate: [`graphql-iso-date`, `GraphQLDate`], GqlDateTime: [`graphql-iso-date`, `GraphQLDateTime`], GqlEmail: [`graphql-custom-types`, `GraphQLEmail`], GqlEnum: [`graphql`, `GraphQLEnumType`], GqlError: [`graphql`, `GraphQLError`], GqlFloat: [`graphql`, `GraphQLFloat`], GqlID: [`graphql`, `GraphQLID`], GqlInput: [`graphql`, `GraphQLInputObjectType`], GqlInt: [`graphql`, `GraphQLInt`], GqlInterface: [`graphql`, `GraphQLInterfaceType`], GqlList: [`graphql`, `GraphQLList`], GqlNonNull: [`graphql`, `GraphQLNonNull`], GqlObject: [`graphql`, `GraphQLObjectType`], GqlScalar: [`graphql`, `GraphQLScalarType`], GqlSchema: [`graphql`, `GraphQLSchema`], GqlString: [`graphql`, `GraphQLString`], GqlTime: [`graphql-iso-date`, `GraphQLTime`], GqlUnion: [`graphql`, `GraphQLUnion`], GqlURL: [`graphql-custom-types`, `GraphQLURL`], globalId: [`graphql-relay`, `globalIdField`], toGlobalId: [`graphql-relay`, `toGlobalId`], fromGlobalId: [`graphql-relay`, `fromGlobalId`], // Daraloader Dataloader: `dataloader`, // Winston info: [`winston`, `info`], error: [`winston`, `error`] }), new ModuleConcatenationPlugin(), new MinifyPlugin({ keepFnName: true, keepClassName: true, booleans: envProd, deadcode: true, evaluate: envProd, flipComparisons: envProd, mangle: false, // some of our debugging functions require variable names to remain intact memberExpressions: envProd, mergeVars: envProd, numericLiterals: envProd, propertyLiterals: envProd, removeConsole: envProd, removeDebugger: envProd, simplify: envProd, simplifyComparisons: envProd, typeConstructors: envProd, undefinedToVoid: envProd }) ] } ``` There are three important things to note: I specifically include the webpack config for this project highlight webpack's provide plugin. It allows you to call exports from node modules without having to explicitly import them in the files in which you use them. So when you see things like `GqlString` instead of `GraphQLString`, this is why. We're using `babel-plugin-transform-optional-chaining`, which adds support for the TC syntax proposal: Optional Chaining, aka the Existential Operator. You'll see this in the code base in the following format: `obj?.property` which is equivalent to `!!object.property ? object.property : undefined`. Using this syntax requires using babel , so keep that in mind before attempting to use the plugin in your own projects. We're using a resolve alias, specifying `@` as the project root directory. This lets us do project root relative imports, such as `import { invariant } from \"@/utilities\"`. I really like the way this webpack helps with code organization and managing relative imports across refactors. Fetching Data from the Flickr API Now that we've built our GraphQL server and endpoint, it's time to fetch data from the Flickr API. Remember: Flickr's data is only accessible via a REST API. We have to write a connector library to interact with Flickr. When I started this project, the first thing I actually put together was the Flickr connector. I probably refactored it about a dozen times before I got things organized in a way that I liked. It's now designed such that you can use it completely independently from GraphQL as a standalone library to interact with the Flickr API. It's also broken up into multiple parts, such that you can import only what you need to keep the bundle size down. The connector is fairly simpleyou can check the code here. There are two methods: `fetchResource`, which is invoked by the GraphQL method handlers to get Flickr data, and `fetch` which is used under the hood to make a request to the Flickr API. The connector includes a Dataloader instance to cache results of each REST call. If a method handler calls `fetchResource` with the same arguments that handler has used before, it will return the cached results. Otherwise, the connector will call `fetch` to hit the Flickr API, cache the results, and return them to the handler. The Flickr connector can be called as follows: `getPhotos.js` ```javascript import Flickr from \"@/flickr\" // A global instance of the connector is export by default as a fallback for each handler export default function getPhotos( { flickr = Flickr, photosetId = ``, userId = `` } = {}, { privacyFilter = , media = `all`, extras = ``, page = , perPage = } = {} ) { return flickr.fetchResource( `flickr.photosets.getPhotos`, { photosetId, userId }, { privacyFilter, media, extras, page, perPage } ) } ``` You'll notice that defaults are set for many of the values, matching the defaults in the Flickr API documentation. This also serves as a type reference, which will later be updated to use Flow typings. I did things this way to minimize the occurrence of typos and referral to the API documentation that would arise from having to invoke `fetchResource` manually. That's it for the Flickr API library! Back to the GraphQL side of things. GraphQL Type Definitions For each different node type in our GraphQL schema, we'll need to create a Type Definition. There are fifteen Type Definitions in total for types such as Albums, Galleries, Images, and Tags. These Type Definitions are quite long, usually over lines each, so I'll omit them for brevity. You can explore them here if you're curious. Below, I'll offer a few tips on how I organize my Type Definition files and how they differ from the reference implementation of a GraphQL Type Definition. Then, I'll cover how I structure resolvers in this project. Finally, I'll show you how I built advanced features like filtering, pagination, and sorting to make it easier to get the exact data I wanted from Flickr. Organization I like to organize my type definition files as follows: - Import any resolver functions and utilities, followed by dependent types. - Create the actual type definition, and name the export the same as the type. This makes it easy to reference when using it in other type definitions. - Create any number of relevant Queries for this type; these will be the graph 'entry points'. Typically, you'll only need about one per type definition. - Create any number of relevant Mutations for this type. This basically follows the same format as Queries. Because the project doesn't have any mutations and the time of this writing, there is no Mutations export shown here. - Create a Definition alias for the type definition and export the Definition, Queries and Mutations as a default export. This default export will be used by `schema.js` to build our Schema definition. This is my method, but feel free to do whatever works best for you. Differences If you look at my Type Definitions, you might be confused by a few of the properties on our fields`complexity`, `sortable`, and `filter` are all custom properties that are not part of the `graphql.js` reference implementation. These fields are used for advanced functionality in my application: - `complexity`: This property is used by the `graphql-query-complexity` library to calculate the complexity score of a field. You provide it with a function that returns an integer value. That function is automatically given your field's query arguments and the computed complexity score of the child type you're fetching. The further we nest our queries, these scores will get exponentially bigger. Requesting too many of these fields will deplete our complexity budget faster. - `sortable`: If set to true, this field will be included in our list of sortable fields in the OrderBy input for this type, which is used in query arguments elsewhere in our schema. - `filter`: When set with an object with at least a `type` property, this field will be included in a list of filterable fields in the Filter input for this type. `type` should be a list of whatever the field's `type` is, or it could be a custom input, such as the `Range` and `DateRange` inputs we've imported. You'll notice we're using a `disabled` argument in our Fields thunk. This is to prevent type errors from popping up when we're generating our `filter` and `orderBy` inputs for this type. Resolvers, Loaders & Data Models While the Flickr connector is concerned with making REST API calls and returning the raw JSON response, our resolvers determine how many requests need to be made from the query arguments with some help from our `pagination` utility. The resolvers pass the results off to our models, which will transform the raw data into a shape that can be consumed by our schema. Loaders are simple factory functions that return a new instance of Dataloader. They create a class with a hash map to memoize the results of the value (or array of values) we pass to it using the `.load()` and `.loadMany()` methods. Loaders will be our first line of defense in ensuring we don't re-fetch data we've already retrieved. If a cached result is not found in the hash map, it will execute one of our resolvers to fetch the value for that input. Applying Filters, Sorting Results & Pagination Finally, let's take a look at some of the utilities we've been using in our resolvers. To keep things as DRY as possible, I created a few abstractions to help with filtering, sorting and pagination in my resolvers. First, I made a filter utility (`createFilter.js`) to iterate over the fields in that type definition and search for fields with a `filter` property set on them. For each one it finds, it will create a hash of the field names and filter values which the returned input object will use as its fields property: ```javascript import { isObject } from \"lodash\" import { invariant, missingArgument } from \"@/utilities\" export function createFilters(type) { invariant(isObject(type), missingArgument({ type }, `object`)) return new GqlInput({ name: `${type._typeConfig.name.toLowerCase()}Filter`, fields: () => Object.entries(type._typeConfig.fields(true)) .filter(([name, values]) => !!values.filter) .reduce((hash, [name, values]) => { hash[name] = values.filter return hash }, {}) }) } GraphQL images(filter: { size: [Small, Medium, Large] }) { size } ``` This will filter a list of Image results to only include images with a `size` value of either `Small`, `Medium`, or `Large`. You can also apply as many filters as you want. Each field you supply a value for will be applied in the order you define them. I also made an orderBy utility (`createOrder.js`). It takes two inputs, `field` and `sort`, where `field` is an enumerable list of all the sortable field names, and `sort` will be the sorting direction, defaulting to ascending: ```javascript import { isObject } from \"lodash\" import { invariant, missingArgument } from \"@/utilities\" const Sort = new GqlEnum({ name: `Sort`, values: { asc: {}, desc: {} } }) export function createOrder(type) { invariant(isObject(type), missingArgument({ type }, `object`)) const FieldsEnum = new GqlEnum({ name: `${type._typeConfig.name.toLowerCase()}OrderByFields`, values: Object.entries(type._typeConfig.fields(true)) .filter(([name, values]) => !!values.sortable) .reduce((hash, [name, values]) => { hash[`${name}`] = {} return hash }, {}) }) return new GqlInput({ name: `${type._typeConfig.name.toLowerCase()}OrderBy`, fields: () => ({ field: { type: new GqlNonNull(FieldsEnum) }, sort: { type: Sort, defaultValue: `asc` } }) }) } GraphQL photos(orderBy: { field: taken sort: desc }) { taken } ``` This will sort the photo results by their taken date in descending order (latest to oldest). Finally, I made a pagination utility (`pagination.js`). This function will take in query arguments plus a total value, and use those to calculate a `start`, `perPage`, and `skip` value to pass along to the resolver: ```javascript import { isNumber } from \"lodash\" import { invariant, missingArgument } from \"@/utilities\" export function pagination({ first = , last = , count = , offset = , total = } = {}) { invariant(isNumber(first) || isNumber(last) || isNumber(count), `Please set either 'first', 'last', or 'count'.`) invariant(isNumber(total), missingArgument({ total }, `number`)) isNumber(offset) const minPerPage = (totalItems, minLimit) => { if (totalItems === minLimit) return minLimit let perPage = minLimit while (totalItems % perPage !== ) { perPage++ } return perPage } if (!!first) { const perPage = first const start = const skip = return { start, perPage, skip } } if (!!last && !!total) { const cursor = total - last const perPage = minPerPage(total, last) const start = Math.ceil(cursor / perPage) || const skip = cursor % perPage return { start, perPage, skip } } if (!!count) { const perPage = offset > count ? minPerPage(offset, count) : count const start = offset > count ? Math.ceil(offset / perPage) : const skip = perPage - count ",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e8c",
      "title": "Embrace the mini CEO",
      "content": "If youve never worked at a venture-backed startup, wellI highly recommend it as an exercise in chaos readiness. There are few rules, fewer resources; no guidelines about who, how or when to get stuff done; and near-constant uncertainty when it comes to your companys future. This, friends, is my rd such startup. Im a masochist, I know. On the bright side! These experiences have led me to come up with a conveniently corny acronym for thriving in such chaos: C.E.O. These goals might not ensure success, but they'll at least give you a good shot at sanity. Communication For small teams, a product might literally change direction between a morning meeting and an afternoon coffee run. Communication is life. I wont advocate for a certain magical set of tools to accomplish this, or get on my soapbox about how Slack is ruining work-life balance (both worthy conversations for another time), but I will stress that people should err on the side of over-communicating. Track decisions in a centralized, easily-accessible place. Say more than you think you need to say, and say it too much. Have all-hands meetings. We all eyeroll at meetings, but its better to embrace them than to have everyone losing their goalposts and focus. When in doubt, talk it out. Expectations Larger companies have established checks and balances. Deadlines run on longer cadences, responsibilities are clearly divvied up and everybody knows whos Management and whos a Plebian. Not so in startups; they prefer self-ownership and relatively flat management structures. It tends to result in divergent pathways and a team thats grasping for guidance. It becomes incredibly important to define the Whys so everyone can set meaningful, achievable expectations. Checks and balances and whatnot. In a tiny company with no safety net, misalignments have huge consequences. Say you expect a co-worker to build a feature and they arent on the same pagemissing that ship date can mean the difference between life and death for a startup. The more chaotic and fluid a companys environment, the more crucial it is that expectations are crystal clear. Ownership All of this (over)communication and expectations-setting doesnt mean a thing if people arent also taking ownership. And when I say that, I intend ownership to mean more than accountability. Plenty of people can be accountable and totally disengaged from the process. Ownership means sometimes stepping outside of the bounds of what youre directly accountable forlending a helping hand, or even stepping in. In a startup, this can mean everything from making sure the snacks are stocked to helping a colleague ship that last feature in time for a deadline. No matter the scenario, a true owner takes pride in the companys output and doesnt balk at making sht happen. Wrap up In short, everyone should be a mini CEO. Communicate more, set expectations well, and take ownership wherever possible. That next round of funding is just around the corner.",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e8e",
      "title": "Building an image recognition endpoint with Serverless and Google Cloud Functions.",
      "content": "One of the great things about the Serverless Framework is that it can be used with multiple providers. We're up to seven (!) different providers with more on the way. Today, I'm going to explore creating a project with Google Cloud Functions. Google has some great surrounding services in their cloud platform, so I'm excited to see the future of how they integrate their Serverless offerings. In this post, we'll deploy a simple HTTP endpoint to GCF to understand the development model. Then, we'll take it up a notch by using the Google Vision API to inspect images that we pass in. Finally, I'll cover what I'm excited about with Google Cloud Functions and what's needed in the future. Let's dive in! Getting started with a Google Cloud account Before we go anywhere, we'll need to get our account and credentials set up. If you've already done this, you can skip to Deploying a Simple Function. First, you'll need to sign up for a Google Cloud Billing Account. Follow the prompts, including entering your billing information. Next, you'll need to create a new project. Make sure you use the Billing Account created in the previous step as your Billing Account for this project. Also, take note of the Project Id which is listed on this screen. Once you have your project, you'll need to enable the required APIs. Navigate to the API Dashboard, and click the link to Enable APIs and Services. Search for \"Google Cloud Functions\", and click on the Google Cloud Functions API. Click Enable to enable it for your project. Then, do the same for the \"Google Cloud Deployment Manager\" API. Last step, we need to grab some credentials. Go to the Credentials page in the API Dashboard. Click the \"Create credentials\" button, and choose the \"Service Account Key\" option. When creating a service account key, choose to create a \"New service account\". Give it an account name. In the _Role_ selector, choose \"Project\", then \"Owner\". This gives you full access in your project. It's more than you should use for production, but it will be easier for the quickstart. Finally, choose a JSON key type, then hit Create to download the credentials to your machine. Once the credentials are on your machine, move them to `~/.gcloud/keyfile.json`. You'll need to refer to this path in your Serverless service. Deploying a Simple Function At this point, you should have a Google Cloud Platform credentials file located at `~/.gcloud/keyfile.json` for a service account that has enabled the Google Cloud Functions API and the Google Cloud Deployment Manager API. You should also have a Project Id. If you don't have either of these, read the previous section. Now, let's deploy a simple function to Google Cloud Functions. First, make sure you have the Serverless Framework installed:  Then, let's create a Serverless service using the Google Cloud Functions template, then change into that directory and install dependencies:  Now, open up the `index.js` file. You'll see two functions that are exported `http` and `event`. These represent the two types of triggers for Google Cloud Functions. An HTTP trigger is one triggered via an HTTP request to a particular URL, while an event trigger is one triggered from the various event source (Google Pub/Sub, Google Storage, or others). You can check the docs for additional information on the trigger types. For now, we're just going to use the HTTP trigger. Remove the `event` function so your `index.js` looks as follows:  The HTTP function signature is very similar to an Express.js handler -- there's a `request` object with information about the request, and a `response` object that you'll use to respond. For now, our function will just return `Hello World!' with a status code. We can cut down some of the configuration in the `serverless.yml` file since we're doing a simple function. Edit your `serverless.yml` so it looks as follows:  This file is pretty simple. We're deploying a NodeJS function to Google under our project. There's a single function, `http`, whose handler is the exported `http` function in `index.js`. Finally, this will be invoked by an HTTP event at `/http`. Let's deploy our function and test it out! Run `serverless deploy` to send it up.  After the deploy is successful, Serverless will show information about your service. This includes the endpoint where you can invoke your function. Let's invoke it using `curl`:  Cool! We got our response back. Let's look at a few more things before moving on. The `request` object will have some useful properties just like a normal web request. This includes the HTTP verb (`request.method`), the request body (`request.body`), and query parameters (`request.query`). To see this, let's play around with the query parameters. Edit your `index.js` to log the `request.query` before returning a response:  Redeploy your function with `serverless deploy`. Once that's done, call it with `curl` and add some query parameters:  We didn't change our response, so we still get \"Hello World!\". We can use Serverless to check our logs to see our query parameters:  Cool! We get some information about the execution, and we're also able to see anything we logged within function. In this case, we see the two parameters -- name and age -- in our HTTP request. With these basics in hand, it's time to move on to a more advanced example. Building an Image Recognition Endpoint Let's build something fun. Google Cloud Platform has the Google Vision API which can examine an image and provide details on what it contains. We're going to build an endpoint that takes in an image URI as a query parameter and returns a list of labels about the image's contents. First, we'll need to enable the Google Vision API for our project. Go back to the API Dashboard and click Enable APIs and Services. Find the Google Vision API and hit Enable. Next, we install the SDK for the Vision API into our project:  Then, let's adjust our HTTP handler. The Vision API accepts a URI to a publicly-accessible image. Our handler will look for a `uri` query parameter with our request, then send that image URI to the Vision API:  The response from the Vision API will include an array of `labelAnnotations`, which are general descriptions of what is in the image. We'll pull out the descriptions from those labels and return them in a JSON response to the client. Deploy your updated handler code with `serverless deploy`. Then, let's try it out with some images! I went to Unsplash to find some cool images. Here's a festive one of some pumpkins: _Credit to Donna on Unsplash_ Let's send it to our endpoint. I'm going to pipe the response through jq, which is a utility for interacting with JSON in the terminal:  Neat! It knows that I've sent it some pumpkins. Let's try one more, with everyone's favorite example:  Sure enough -- it's a hot dog! Concluding Thoughts on Google Cloud Functions I had a lot of fun building this. The Google Cloud Platform has a lot of interesting services, and I'm excited to see where the Cloud Functions product goes from here. In particular, I think Google Cloud Platform has some really great data stores for the Serverless world. Firebase was a groundbreaking product when it released a few years ago, and the new Firestore database looks like a great option -- NoSQL but with a more flexible query pattern, more similar to MongoDB than DynamoDB. For SQL lovers out there (like me!), Cloud Spanner is an awesome, powerful product. Additionally, GCP has some great supporting services as well. The machine learning services like the Vision API and the Natural Language API. There are also some amazing data processing products like BigQuery, Cloud Dataflow, and Cloud Pub/Sub. There are a few areas I'd like to see Cloud Functions mature as well. These are: - A stable version. Cloud Functions are still in beta -- I'd like to see a stable release before relying on it for production services. - More language runtimes. Python, please! . Right now, it's strictly Node.js. - Better integrations with other services. On the AWS side, we're seeing more and more hooks from AWS services into Lambda. I'd like to see the same in Google. - API Gateway. The GCF HTTP function feels like a single random endpoint for small demos. Contrast this with AWS, where endpoints feel more composed as a service. I can easily add custom domains to my Lambda endpoints and do HTTP validation and transformations before it hits my Lambda function. Excited to see where Cloud Functions goes from here!",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e90",
      "title": "How to Make a Serverless GraphQL API using Lambda and DynamoDB",
      "content": "To see how DynamoDB compares to MongoDB, Cassandra, or BigTable, see here. The `graphql` module makes it easy to rapidly create a GraphQL service that validates queries. We use GraphQL at Serverless.com to query our backend services, and we love how well it fits into the serverless paradigm. Interested in building your own GraphQL API? Awesome. Here we go. Building the API In this example, I'll be targeting AWS. Let's build a simplistic version of an API that might be used by the front-end to retrieve a dynamic message to display in the UI; in this case, greeting the user by name. Start by initializing a project and installing the graphql module:  Now we can use it in `handler.js`, where we declare a schema and then use it to serve query requests: ```js / handler.js / const { graphql, GraphQLSchema, GraphQLObjectType, GraphQLString, GraphQLNonNull } = require('graphql') // This method just inserts the user's first name into the greeting message. const getGreeting = firstName => `Hello, ${firstName}.` // Here we declare the schema and resolvers for the query const schema = new GraphQLSchema({ query: new GraphQLObjectType({ name: 'RootQueryType', // an arbitrary name fields: { // the query has a field called 'greeting' greeting: { // we need to know the user's name to greet them args: { firstName: { name: 'firstName', type: new GraphQLNonNull(GraphQLString) } }, // the greeting message is a string type: GraphQLString, // resolve to a greeting message resolve: (parent, args) => getGreeting(args.firstName) } } }), }) // We want to make a GET request with ?query= // The event properties are specific to AWS. Other providers will differ. module.exports.query = (event, context, callback) => graphql(schema, event.queryStringParameters.query) .then( result => callback(null, {statusCode: , body: JSON.stringify(result)}), err => callback(err) ) ``` Pretty simple! To deploy it, define a service in `serverless.yml`, and set the handler to service HTTP requests:  Now we can bring it to life:  Creating the database In the real world, virtually any service that does something valuable has a data store behind it. Suppose users have nicknames that should appear in the greeting message; we need a database to store those nicknames, and we can expand our GraphQL API to update them. Let's start by adding a database to the resource definitions in `serverless.yml`. We need a table keyed on the user's first name, which we define using CloudFormation, as well as some provider configuration to allow our function to access it:  To use it, we'll need the aws-sdk. Here's how you'd use the SDK's vanilla DocumentClient to access DynamoDB records:  Include these in our handler, and then we can get to work:  We started by defining a method that returned a simple string value for the greeting message. However, the GraphQL library can also use Promises as resolvers. Since the DocumentClient uses a callback pattern, we'll wrap these in promises and use the DynamoDB `get` method to check the database for a nickname for the user: ```js // add to handler.js const promisify = foo => new Promise((resolve, reject) => { foo((error, result) => { if(error) { reject(error) } else { resolve(result) } }) }) // replace previous implementation of getGreeting const getGreeting = firstName => promisify(callback => dynamoDb.get({ TableName: process.env.DYNAMODB_TABLE, Key: { firstName }, }, callback)) .then(result => { if(!result.Item) { return firstName } return result.Item.nickname }) .then(name => `Hello, ${name}.`) // add method for updates const changeNickname = (firstName, nickname) => promisify(callback => dynamoDb.update({ TableName: process.env.DYNAMODB_TABLE, Key: { firstName }, UpdateExpression: 'SET nickname = :nickname', ExpressionAttributeValues: { ':nickname': nickname } }, callback)) .then(() => nickname) ``` You can see here that we added a method `changeNickname`, but the GraphQL API is not yet using it. We need to declare a mutation that the front-end can use to perform updates. We previously only added a `query` declaration to the schema; now we need a `mutation` as well:  After these changes, we can make the greeting request again and receive the same result as before:  But if I want the API to call me \"Jer\", I can update the nickname for \"Jeremy\":  The API will now call anyone named \"Jeremy\" by the nickname \"Jer\". Separation of concerns like this let you build front-ends and services that offload logic into backends. Those backends can then encapsulate data access and processing behind a strongly-typed, validating, uniform contract that comes with rich versioning and deprecation strategies. Deploy your own! To deploy this service yourself, download the source code and deploy it with the Serverless Framework. Or, take a look at a larger example project for ideas on project structure and factoring. Architectural Diagram !Architectural Diagram Happy building!",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e92",
      "title": "Rob Gruhl - towards a serverless event-sourced Nordstrom",
      "content": " Rob Gruhl kicked off Emit Conference with a peek behind the scenes at Nordstrom's architecture. In his talk, Rob discussed Nordstrom's use of the event-sourced architecture with Serverless.com: what it is and why it's a good fit for the Serverless paradigm. He also notes some difficulties with the event-sourcing pattern, the ways Nordstrom has worked around it, and ends with a few awswishlist items that would make it easier to develop event-sourced applications with Serverless. Nordstrom were an early adopter of the Serverless Framework for services from personalization to frequently-viewed items. They've since contributed some excellent resources back to the community, including the \"Hello, Retail\" project and a Serverless Artillery project for performance testing. Watch the video or read the transcript below for a complete run-down. More videos: The entire playlist of talks is available on our YouTube channel here: Emit Conf To stay in the loop about Emit Conf, follow us at @emitconf and/or sign up for the Serverless.com newsletter. Transcript Rob: Good morning, my name is Rob Gruhl and I support a small team of engineers at Nordstrom called \"The Technology Acceleration Group.\" For the last three years, we've been exclusively focused on serverless patterns and practices both in production, proof of concepts through workshops. In the last year or so, we've become exceptionally fascinated with event sourced architectures. So I'm gonna talk a little bit today about what we're doing in that area. So this is an image of our team that we used for a recent internal serverless workshop. These are great engineers on the left, and what I like about this is we're waving goodbye to all those old server problems, the coal-powered steamship, but kind of behind us is these new problems coming in. And some of you might recognize the guy driving that boat right there. And we like to just stay a little bit pragmatic about some of the challenges that we face as we take on some of this new architectures, and that's a little bit of what I'm gonna be talking about today. So let's take a quick trip back. So back in the holiday of , I was working on the personalization team and we had a simple feature called \"Recently Viewed\" which basically rendered a shelf on the bottom of every web page, and every mobile page showing customers what they viewed. You view things and then you see those things show up in the shelf. So a simple function, a simple feature, but critical to our business. It drove a huge amount of directly attributable revenue every day. The challenge with the existing system was that it used a combination of cookies and batch processing, with some traditional server architecture behind the scenes, and it could get delayed by to minutes. There were problems with cookie settings. There were problems with multiple sessions or in private browsing. So we decided to do a proof of concept using this distributed ledger. It's also called the stream. It's also called the log. I don't think anyone fully decided how to describe these things yet. But whether it's Kinesis or Kafka, we produced million customer click events onto the stream and then we used a serverless function to process that stream and populate a NoSQL database. So this was an incredibly successful experiment. We reduced the latent feed to less than two seconds. The cost came down tremendously. Maybe less than $ a day for this processing capability. It also allowed us to use that exact same stream to do some more interesting things like frequently viewed. Maybe brand category affinity for personalization. So after that, we did a bunch of request-response services, both in production and as proof of concept. And one thing that we found is anytime you have a lot of functions, you have a lot of serverless architecture, you need a deployment framework to kind of keep yourself organized. So we found the Serverless.com deployment framework and we were able to make a number of contributions that made it work well at Nordstrom. So this ability to update the framework, the responsiveness of the team, and the quality of the community, is one of the reasons why we continue to use this framework and recommend it. So fast forward a few years. At this point we have, I would say on a given day, we have hundreds of millions of serverless invocations. We have hundreds and hundreds of serverless functions live. We have dozens of teams using serverless functions. And across all of these serverless functions, we've found that when you invoke a serverless function, you're looking for an event. So you start looking through your architecture for events, like, how can I pull events. I could steam to find a batch. I could trigger off when something landing in an object store. And we started thinking about how can we have a better way of organizing these events, and feeding them into these serverless functions. And it turned out that this was not a new pattern. This is something that's been kicking around for a while in various forms, and some people call it event sourced architecture. You can also call it a central unified log. You'll see, again, a number of different ways of referring to that. But what it basically means is instead of this connected graph microservices situation where you have to go and talk to an individual microservice to ask them to add an attribute, ask them to scale to your load, you have something more like up at the top where you have individual microservices either producing events some of the events stream or consuming events from the events stream. So it really simplifies how you think about your architecture, and allows you to decouple those producers and consumers. There's a number of other benefits from a distributed system standpoint. As I mentioned, we're not the first to think about this. There's been a lot of people coming before us. This is some of our favorite reading, that we like, around event sourced architecture. If you're kind of newer to things or if you haven't read this one, this is our very favorite. We ask every candidate, that's interviewing with our team, to read through this e-book and take Eric to coffee and talk about it. It covers a bit of the history of streaming and also the future potential of streaming, so it gives a lot of context. If you're more familiar with events sourced architecture, this unfinished e-book by Greg Young, we just ran across, and it is fantastic. It talks about a lot of the deeper and more complex situations that you'll run into in an event sourced architecture. So I highly recommend anything by these folks. They've done a tremendous job of writing that. So let's jump in to \"Hello, Retail.\" So we're hooked on this idea of event sourced architecture. What would the world look like at Nordstrom if we had one stream of product events, everything that's ever happened to a product from the moment it's conceived to the last item being sold on clearance. We have a stream of everything that customers have ever done, and we have a stream of everything our sales people have ever done. Well, if we had that, it would really simplify a lot of what we do, and we'd be able to react very quickly and build new features. So we built a proof of concept called \"Hello, Retail.\" So \"Hello, Retail\" it's an open-source, % serverless functional proof-of-concept showcasing an event-sourced approach as applied to the retail platform space. So I'll quickly kind of click you through what this might look like. If you wanna see it live, I have it on my phone. It's easy to walk you through. So here I'm login in. This is a static web page with ReactJS. I have my top-level navigation. You can tell we're a back-end team and not a front-end team by our beautiful UIs. I can jump in as a customer and view a number of categories and products. So I go back out and I'm gonna register as a photographer. So here I put in my phone number that is produced as an event onto the events stream, and there's a microservice that picks that up. I go back into the menu. I create a new product. This product is CATs that were found in my Airbnb, and that event goes onto the events stream. Now, when that event goes on the events stream, we have a category microservice that picks it up and populates the category list. A product microservice that populates the product details. But we also have this photographer microservice that says, \"Do I have an available photographer? Go contact them using Twilio.\" That photographer takes a picture. Responds and say, \"Thanks so much Rob Gruhl,\" and now that image becomes an event on the stream. At this date and time a new image is available, and when I go back in and I look at my website, there I have my CAT. We also have a workshop that's associated with this, where you will extend the functionality of this, and that's also available through open-source. So here I've placed an order for the CATs. And, again, that order is just, again, an event produced onto the stream. So this is the basic architecture. The tube in the center is the ledger or the stream or the log, and it looks pretty complex. We've got...I'll just walk you through a little bit, of what you're seeing here, with this pointer. Okay. So here's the...these are traditional request-response endpoints as are these. And so for the clients, most of the time they're just thinking about calling an API like you would do before, and really it's the behind the scenes architecture that's changed here. The thing that makes me say, \"Don't panic here,\" is that most of that is just configuration. That's just like a serverless CML and the serverless framework. There's only a small amount of code behind the scenes. So, as usual, our team spent a lot of time writing a small amount of code, which is kind of how we like it. And each one of those languages might be a lines of code or less and maybe a lot of shared code across those. All right, so given that kind of background, I wanna talk a little bit about what we think we learned about event sourcing, and as it's kind of a new endeavor for us. Any of you that have feedback, I'm sure a lot of you have been thinking about this for longer than we have, we really love to hear from you. Also, this goes up on YouTube. Comment below, I'd love to get your open and honest opinions. So we think that it's important to produce simple, high-quality events. So high-quality events are written in the past tense at this date and time this thing occurred. And it can either be an observation of something that happened in the world, like, at this date and time Rob started his talk, at this date and time he finished his talk, hopefully on time. But it can also be the record of a decision. And this caused us to kinda twist our brains a little bit because when you say something like, \"I'm producing an event onto the stream that says, 'At this date and time Rob added this item to his cart' it hasn't actually happened anywhere.\" There's no database that's been updated. So I started to think about this a little bit like a Royal Decree. Let it be known that at this date and time Rob has added his item to his cart, and it's up to all the microservices to scramble and go like, \"Oh my God, I better, like, update a database or do something, right.\" So it's really the decision itself that gets captured onto the stream before any of the actual work occurs. We think it's important to swim upstream. So when you're thinking about your event-sourced system, you say, \"Okay, what is my source of events?\" Maybe it's a batch of data that I have or an enterprise data warehouse. Well, those events came from somewhere, so can you follow those events upstream? Can you find the original person or idea or system that generated those events? Because if you can, and you can generate the events at that point, it becomes more useful for all the systems down, you get fresher events with lower latency, and maybe at a higher quality. Now, when we do design reviews and we talk about this, often people say, \"Well, I can't use that stream because there's something wrong with it or it doesn't have the attributes.\" I really think this is super critical, as you're making a transition into an event sourced environment, is go and fix the stream. Find the team that runs that and help them out because oftentimes let's say you're doing a loyalty-reward system, what you really need is an omnichannel transaction stream, as does the personalization team, as does the purchase history team, as does the fraud detection team. So if you can all work together and find the source of this data and clean it up, everyone is going to benefit from that. As we've looked at streams, I think we've found that there's two fundamental kinds of steams, and it's what I'm calling \"Tech Debt Streams and Published Streams.\" So the \"Tech Debt Stream\" might be something like, you've got an Oracle database and you've got GoldenGate, and you're doing a change data capture off of that. And you're dropping it onto a stream, and it's just blah. All right, you've got all these table updates and it's really hard to understand. There's a lot of technical context. Now, you have some stream processing layers, some stream transform layer that says, \"Well, I'm gonna reverse engineer all those table changes and turn them into an event, like, this shirt now costs $ less.\" That's the business event. That's the thing you wanna publish. Maybe on a developer portal you've got your request response APIs and there's Swagger. You've also got your streams and their schemes. So you want developers to be able to subscribe to or attach to those streams, and so that's the published stream. And I think there's an interesting opportunity where you say, \"I have this tech debt stream and it's being transformed into this published stream. Maybe when I refactor this system in the future, the goal of that system will just be to publish that original event onto the stream, and I can replace this legacy system. So something to think about there. Ordering is a little bit controversial. You'll see a lot of systems that maintain ordering, and systems that don't maintain ordering. We're trying to take the high road and maintain ordering across all of our systems. It's fairly difficult, though, even once you've had those items ordered onto your Kinesis, your Kafka stream or whatever mechanism you're using. There's lots of systems and libraries, and tools and developers that want to process those events asynchronously. So here we have a chess match, you know, with different moves and someone says, \"Great, I've pulled a batch of moves. I'm gonna process them asynchronously. It's gonna be fantastic.\" So we're almost at a point now where you have to do work to back off of that asynchronous reflex and say, \"No, we need to do this single-threaded synchronous processing of these events, and that can be very slow if each one of those events needs to be processed with an external system. So we'll talk a little bit about an optimization that we've come up with for this a little bit later. Partition keys. Some teams say, \"I don't wanna think about partition keys.\" It turns out we believe partition keys are really important because in a distributed system, in your partitions, it's only the partition key that allows you to guarantee ordering. So if you select customer ID, you're going to get that customer's events in order. If you select skew, you're gonna get skew events in order. So you really have to think about it with your architecture, what is the partition key and how are you going to use it. And if you don't think you need ordering guarantees or partition keys, I would ask, \"Will any system in your entire system behave differently if those events were to be reordered? And there are some subtle ways that those issues may come up. So really think about and make a deliberate decision about whether you're going to maintain ordering across your system and what your partition key is going to be. All right, so it is interesting that we've got...I didn't know there's going to be...this is magnificent, Rube Goldberg machine over here. But one question I would have is if this is Austen pushing the first domino here as the first event, could we trigger a serverless function off of that that kind of went all the way over here and did this? You know, can you bypass some of that intermediary system? So just like I mentioned, swimming upstream to find the source of your events, can you also look downstream and look at what the effect is because maybe you can hop over a few steps there. You're not just transforming data and handing it to the next system, you're asking the question, \"What does that system do?\" And if I have a dedicated computer intensive container working for me, can I just do that? So it's not always going to be the case, but something to think about. I mentioned this is a list of things we think we know. I think we're pretty sure about this one. Distributed systems are hard and eventual consistency is weird. So it's a very different intuitive mindset. For example, you wanna do a simple acceptance test. You wanna add an item to your cart and then you wanna read your cart and you say, \"I added, you know, this address and I went and I read the address is not there. Is the system broken or is it just eventual consistency? Or it looks different from the last time I saw it.\" There's all kinds of these really interesting and challenging problems that come up. We absolutely love this book. If you don't have this book, you should get it. It's Martin Clinton's \"Magnum Opus [SP].\" It's like or pages long. It's kind of a explanation of everything with tons and tons of footnotes and pointers. And the thing I love about this book is I read through a chapter and the whole team is reading through this book. I read through a chapter and there's a bunch of stuff and like, \"Oh, I can use that. I really understand that.\" There's a bunch of stuff I'm like, \"I have no idea what he's talking about,\" but I feel dumber and that's an important thing, right, to understand what I don't understand, and there's always the opportunity to follow those footnotes and read those links. And there's some parts where he's like, \"This is a complex topic. If you wanna learn more, read the textbook,\" footnote. So it's like RTF textbook. Okay. So that's kind of what we think we know. Here are some of the areas that we're focused on and we're trying to figure out. So \"Joints and Aggregates across Partition Keys.\" So I've got two different partition keys, like, maybe riders and taxis, and I've got a wonderful set of ordered events across those. But between those two streams, I can't make any causal guarantees. So if I replay into this system as two streams, and then I go and replay into this system as two streams, I'm gonna get a different ordering. So how do I deal with that? Two approaches that we're taking. One, is you aggregate those two streams into a canonical joined stream, and by canonical I mean now everyone who wants that combination has to use that stream, and everyone is gonna agree on what that ordering is. The second opportunity, which I think could be an entire talk in itself and is super exciting, is CRDTs, these convergent data types. I mentioned, at the beginning, that our technology acceleration team has been focused on serverless. We thought that was a big deal. Event sourcing we think that's a big deal. I'm guessing this is going to be the next big deal because it allows you to do things like occasionally connected database, and you can do joints across multiple streams. And I think it just works really well with partitioned event sourcing systems. So we're gonna be looking a lot more at CRDTs. Okay. So I kind of hinted earlier that there might be a better approach, and this came from an idea that Eric had where I was saying, \"I'm really worried about our Fanout Lambda and we either have an asynchronous mode or we have a synchronous mode. And synchronous mode I'm very worried about how slow it is.\" And Eric brought out the point that strictly speaking when you're doing ordering within a partition key, even within a batch of items within the same partition, you only care about the ordering for items with the same partition value. So here A and A represent two items with the same partition value A and a value one and a value two. You can split these events up and process B, C, D, E, and A, A group asynchronously, and only use your blocking single-threaded synchronous processing on A, A. So this is a hybrid approach between synchronous and asynchronous that we're exploring, to see if we can get some performance improvements out of that. Effect after cause. So this one's pretty tricky, and it gets trickier when you have a system with multiple streams and multiple microservices consuming from those streams multiple different databases. What you wanna be able to say is, \"Given that I've added this item to my cart, what are the contents of my cart?\" So three different approaches we're using here. The first approach, if you have the luxury of having a UI that can hold up in a dynamic connection, to your data layer, you can say, \"Well, I'm just gonna show the contents of the carts and as that item drops into the cart it will show up, and maybe there's a little bit of delay. Maybe I can use a UI to represent that. There is a dynamically loading view.\" And really the latency here is usually on the order of seconds anyway, so maybe this is a perfectly acceptable solution if you have the luxury of having a customer facing UI and this works for you. The second solution that we've implemented in production for a few of our systems is to put something like DynamoDB in front of the stream and then you write through that DynamoDB and use DynamoDB streams in order to populate your ledger. And now you have an immediate read after write strong consistency. The thing I don't like about this approach is that it couples your event producer and your event consumer and you lose some of that glorious, like, my job as a producer is I just through something on the stream and I walk away. My job is done. And as a consumer, I'm just watching the stream. So we're doing a bunch of thinking and a little bit of work investigating this third option which is, \"How about when I write to the ledger, I get a ledger receipt which gives me my offset.\" So I say, \"At the cart, you're at offset .\" And then I walk over here to the cart microservice and I say, \"Hey, cart microservice, once you've caught up to the ledger position , can you tell me the contents of the cart?\" This gets again, I mentioned, a little more complex when you have multiple layers, so now we're looking at using some distributed tracing ideas to trace that providence of the event all the way through, and then update the database with a small graph that shows the relationships. So now I can say, \"All right, even though I'm consuming from this, I know that this links to this, links to this, and yet I'm up to the , so you should be good with this.\" So some really interesting things that start happening when you've got this glorious decoupling of your services, but then there's this question of if I through this event out, what happened? When did it happen and how do I synchronize across services? All right, so my serverless event sourcing wish list. As I tell my kids when they go to Nordstrom to see the Santa there, they only get two. So I had to narrow it down to two. The first one is a really big one for us, and this is a...we love the tools that we use, but there's a significant problem with fanout. So you've got this amazing stream that everyone's excited to use, now you have different teams that all want to consume from it. We use both Kafka and Kinesis internally. Kafka does this a lot better. The challenge I have with Kafka is you really need maybe three full-time engineers for the care and feeding of that cluster. So we like to be able to use a managed version of this, but the managed version is strictly limited. You can only do five reads per second. The pulling frequency of the serverless function is once a second and this creates a significant bottleneck. The way we've solved this is we use a Fanout Lambda, and we fan it out to all of our subscribing partners. So you're responsible for standing up your own Kinesis stream. We'll subscribe you to that Fanout Lambda and that will write to your stream. And now each team gets to manage that resource of their stream. The problem with that approach is now you have a single point of failure with that Fanout Lambda. Let's say somebody, one of your consuming or one of your subscribing streams changes the policy, then you can no longer write to that. So now what do you do? Do you stop? Do you create a second Fanout Lambda and subscribe the kind of the sick fish stream? How do you do that? So it's a little bit clunky at this point getting to that enterprise scale of, I've got a whole bunch of developers. You know, the end vision is I've got my developer portal with my APIs and my streams. I want anyone to be able to look at those streams and connect to one of those streams. All right, so I'm waiting for someone to make this magic button because what I really wanna do, in an event sourced system, is recreate application state from the ledger. So I've got my streams. I have some amount of durability on those streams and your Kinesis is up to seven days. And Kafka it's configurable, but really it depends on how much resources you're throwing against it, and at some point, you're gonna have to write it to an archive. So I want the magic button, and my magic button is pretty specific. I have a team that says, \"We have a new feature. We want a request-response API that tells you the average sales velocity by blush color of all cosmetics sold at Nordstrom.\" So I wanna cold start a new database using the last three years of transactional events. So I want to do that using the same serverless function that I'm gonna use on my live stream because I wanna write that function once. So I go in, I write my function. It says, \"Grab a batch of events.\" If it's a cosmetics event look at the shade. Look at the price. Add the price to this table and done. Really simple piece of code. Maybe a couple of lines of code. I want my job at that point to be done. What I want to do is point that function at my archive, push the magic button, and have the entire archive replay through that function at scale. And at , times speed I get three years of transactional data in less than an hour. Maybe I insert $ or $ for all of this massively scaled capability. And then once it's caught up, that database is caught up to the archive, I wanna attach that to my real-time stream. And so now I come back from lunch, and I've got this new capability on my system. So this can also be used for things like disaster recovery and a number of other capabilities. Most of the systems now will do a good job archiving, but we're still in search of a system that will read from the archive. Google dataflow system has an interesting approach where it treats the stream and archive very similarly, so we're investigating that as one possibility. But if you have any ideas or thoughts here, we would love to hear them. All right, so if you need a serverless or haven't tried serverless, I'll do a shameless plug here for our open source project called Serverless Artillery. We found, internally at Nordstrom, this is an excellent way for teams to get started with serverless because it's using serverless functions to test your existing architecture. So Serverless Artillery takes the artillery.io core that was developed by [inaudible ::] in Shoreditch Ops, the good folks in London. And we've hosted that inside of a serverless function which can invoke as many serverless functions as you need to generate arbitrary load instantaneously. So you can hook this into your deployment pipeline. You can run it once a minute as a health check. You can run it once as an acceptance test. It's very versatile, and it's very quick to invoke and very little cost. So the nice thing in a way that this is a good step into serverless is in order to use this as a load test against your service, you have to solve problems like how do I communicate with that service? What subnet is it in? What are the firewalls? What are my authentication authorization concerns? How do I communicate the search, etc., etc. Once you've solved that, for this testing scenario, you've now also solved it for any functional serverless function that you want to communicate with your service. If you're thinking about event-sourcing, we learned so much when we did the recently viewed tray. It's a very simple use case, but it's one that' very visible. It was easy for us to understand when things went wrong. It's very visible on the website. It's very visible testing. You can go and hit an A item and a B item and a C item, and see if they show up. So there's a lot of subtle issues that you'll run up against. So if you can find a use case where you have high visibility and maybe low criticality to it, you can get started and start establishing that stream. And as you establish that stream, think of it in terms of a published stream. See if you can get it to that high quality published stream, and think about how you can load all the attributes that were observed at the moment of that event's existence. So thanks. That's all I have today. We'd love to hear from you and here is a list of the open source products that I mentioned. Thank you.",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e94",
      "title": "Bobby Calderwood - toward a functional programming analogy for microservices",
      "content": " Before schooling us on functional programming, Bobby Calderwood hit us with The Big Question: to microservice, or not to microservice? Well, he sayslet's start from a basic place, something we can all agree on. Death Stars = bad. You know what he's talking about: when your architecture map (er, diagram) looks like a tangled hairy mess that meets in a gigantic black mass in the middle. He talks about a real Death Star architecture out in the wild and poses the question: how did they get here? His opinion? An object-oriented mindset. We won't spoil the talk for you, but let's just say he has a new frame on architectures that can help yours flow more like a river delta. It'll all make sense when you watch the video (or, read the transcript below). More videos: The entire playlist of talks is available on our YouTube channel here: Emit Conf To stay in the loop about Emit Conf, follow us at @emitconf and/or sign up for the Serverless.com newsletter. Transcript Thank you, and thanks to the Serverless team for inviting me to speak. This is a great event, I'm so happy to see us an industry sort of thinking and being intentional about approaching event-driven architecture. I think it's an important topic. Yeah, as was mentioned, like, I'm probably gonna troll a little in this talk, so, you know, but relax, relax, is gonna be all right. So, I'm a functional programmer, by training and by preference, I come from the Clojure community where I worked with, with Rich Hickey, the creator of Clojure to build cool stuff. And so, I'm sort of invested so there's a lot of bias in what I'm gonna say, but I think there are important lessons to be learned. A lot of the problems that Rob brought up have solutions, right? The old masters have already solved a lot of these problems and thought hard about these issues that we're now encountering in a distributed systems world. And so, we should look to the past for some of these solutions. Disclaimers, I'm not gonna be rigorous here. I'm not gonna, like try to rigorously, you know, extend the object-oriented versus the functional programming style out to the distributed case, right? Like that takes a lot of math, that sounds really hard. So, I'm just gonna like hand wave about a lot of that stuff so... And this is more about intuition, it's more about extending principles from these things. Also, you know, relax. I'm trolling, this is funny, come on, everyone smile a little. All right. So, microservices is, like, a big thing, it's a hot topic, has been for a couple years. We talk a lot about this at Capital One, other places where I've been. Do we adopt? A lot of people say, like, \"If we're not doing microservices, all of our competition is gonna pass us by because everyone is doing microservices and they're so fast and agile and they're gonna just pass us by.\" And then a lot of people are, like, \"No, stay away from microservices, it's, like, a dead end.\" Right? So, there's passionate arguments on both sides and most of the time, this is sort of adopt or abandon decision is made given a certain set of assumptions about, like what microservices are and what the architecture is shaped like, right? And most of the time the assumptions look something like this, right? All right. This is...oh, you, you can't see my attribution, that's a shame. This is Werner Vogel's tweet from I think of what AWS looked like, in . I have, you know, friends and fellow travelers from AWS here, this is, kind of, a joke. But, yeah. This is, like, the Death Star diagram of all the microservices that ran AWS at this point in its evolution. And I love AWS. I use it every day, Capital One has a partnership with AWS, love AWS. This is not an accusation towards them, but this is like, this is a disaster, right? I mean, this is a really bad. See the dark part in the middle, see, all these lines here as they converge, form this dark thing in the middle. The technical name for that is a hairball, right? When all of the lines connect to where you can't distinguish them and they just turn into, like, a black mass. This is, this is difficult, right? Reasoning about this system is difficult. Now again, AWS is awesome and they have built this amazing thing that I use every single day. So, I mean, you can build things this way, but maybe there's a better way. This looks really complicated, AWS has engineers that are a lot smarter than me. My tiny brain can't reason about this. So, I need something a little, a little simpler. So, the aforementioned architecture of the sort of Death Star diagram, assumes a lot about the shape of that architecture, right? It's, sort of, built in this object-oriented paradigm. You have these little services that each encapsulates data, right? You can't look at my data unless you ask me nicely, you can't change my data unless you ask me nicely, there's this sort of mutable state change also via asynchronous call, right? So, the only way to get data out is via asynchronous call, the only way to change data is mutable via synchronous call. It creates a sort of dependency web like you have in an object-oriented program and memory. The sequencing and orchestration is imperative and order matters, like, put instruction first it's gonna happen first and then the other thing happens, and it's referentially opaque, right? You don't know the state of the system as a whole and it's hard to reason about the state the system as a whole, you have to interrogate each component of it to get the references and those things can change out from underneath you. So, your inputs, you know, call this system and then call this system and then give me the answer, that'll change over time because it's referentially opaque. Functional Programming, on the other hand, has this, sort of, different approach. Data access is not done by synchronous call, it's done by sharing, you know, a reference to an immutable data structure. So, having these shared data structures, like Clojure's reference types, for example. Everyone, all of the little parts of the system, all your threads, can interrogate those reference types and see what is the immutable value that this reference identifies at this point in time. The underlying value doesn't change, Clojure's, Approach to State, which I highly recommend you look at, clojure.org/about/date I think, there's a reference at the end. It talks about how reasoning about state is most easily and simply done by assigning an identity to different values over time, by building in time as a sort of first-class construct, it's easier to reason about state without having this mutable semantic at the bottom. Functional programs are often organized as, sort of, a data flow graph, rather than a dependency structure. There's declarative orchestration, right,? You just set up a whole bunch of things and you can see how they are related but you don't have to, sort of, specify the order in which things happen, and you have referential transparency, right? I hand two values to a function, it will always give me the same value as the output because of referential transparency. So here's, sort of, the contrast of these principles. So, these are the principles, like, in the programming styles, like, within a single process and memory space. And we can argue....and I encourage, come up and talk to me afterwards. Object-oriented programming might be a good way to do programming within a single process space, let's talk. But, it doesn't scale well to distributed case, when you have these same assumptions that you try to, like, blow out into this microservices architecture, you're gonna have a bad time. Distributed objects face a lot of problems and challenges, right? You end up with this, sort of, deep network of latencies, right? Someone asked me a question, so I call two friends, and they call two friends, and now, all of a sudden, you have, you know, four hops of HTTP latency and you're gonna be max, you know, the slowest path on all of those hops, is gonna be what you're bound by. So, that's bad, you're gonna make your system slow. There's also this, sort of, temporal liveness coupling problem, where, someone talks to me and then I call two friends. If one of those friends is dead at the time that I try to talk to them, I'm dead, right? Because I'm, I'm bound to them in time. And there's you know circuit breakers, and histericks and stuff to, like, kind of, paper over that. But, like, the fundamental problem still remains. There's a, sort of, like, pull orientation thing with distributed objects, where, I don't know the answers to the questions that I'm supposed to answer unless I asked my two friends. So, I have to like pull data out of them. You get cascading failure modes, inconsistency is possible because, you know, I get a question and then I call two friends, in between the time I call my first friend and I call my second friend, something may have happened to my second friend. That now makes me serve an inconsistent response to my to my caller, right? The real bear here, and I think I buried the lead a little bit here with this hidden narrative bullet, this is really a problem, right? There's the famous essay about the Kingdom of Nouns and Object-Oriented programming, right? Only the nouns are first class, this happens and it really, [inaudibe ::] bad way in a microservice architecture that is object-oriented because like only the nouns are reified, right? I've got my product service, and I've got my account service, and I've got my, you know, whatever. So, you can see the nouns and you can walk up and interrogate and ask about the nouns but the verbs are lost, they're ephemeral. I make a call to someone to change something, there's no record of that thing anywhere, it's this ephemeral thing, as soon as the call is over, as soon as that, you know, socket closes, who knows whatever happened. You have to go, like, splunk through your logs or whatever, to find the fact that someone just asked that question, or someone just made that action. So, the narrative is lost, it's literally lost data, and it's also just complex. How do we reason about the state of the system at any given point in time? It's really hard to do so in this distributed objects kind of analogy for designing our systems. So, maybe a better analogy is a river delta. This is a beautiful picture, I found this and I couldn't not put it in. This is, again, I've lost my attribution here, but this is a river delta in Siberia. So, the frozenness, kind of, makes for some of these colors, I guess. This is also complicated, right? Just like the Death Star was complicated, but there's, sort of, a fractal beauty, and symmetry, and order here that makes it easy for a human being to reason about, right? We know how this works, water flows that way to the ocean, right? And so, we can see the priority and the narrative built into this structure and reason about it just by walking up to it. You can't do that with the Death Star, right? In this, we have a notion of priority, right? The big river flows and it's the source and then it ramifies into this beautiful fractal structure and each one of these little fractal bits does its part getting the water to the ocean. Right? So, it's easy for us to reason about it, even though it's still a very complicated thing, it is complicated but not complex. So, Object Oriented Programming is to Death Star as, as functional programming is to river delta in this distributed world, that's kind of the analogy. And, I believe the functional programming and the ideas and principles behind functional programming, does scale really well to a distributed case. Let's talk about how. Now we have a little latency. We'll talk about this a little bit in the future. We have little latency at both read and write time. With this eventual consistency thing in the middle, right? There's some intervening processing time that, that Rob talked about, which can introduce some weirdness but we can talk about that as well. But the actual write and the actual read, can be really fast, right? The services are temporally decoupled, right? If something upstream of me is dead, I don't know or care, if something downstream of me is dead, I don't know or care. All I care about is that I can have ubiquitous access to this data log, as long as it's not dead, right? But I'm not coupled to any given service and you know I trust Kafka or Kinesis to be up, more than I trust the microservice that the knuckleheads in the office down the hall, wrote. Right? So, you have isolated failure modes just like we talked about, you have always consistent reads. They are eventually consistent but you can reason about the point in time at which you are consistent because you have this log, this log forms this sort of clock, this logical clock of where are we in the causal history of things. So, you'll always be consistent, you won't run into this problem of, like, someone calls me and then I call two friends and in between, there was some race and something happened and now I'm serving inconsistent stuff. You don't have that because you can always, sort of, clearly reason about as of time. And you have a reified narrative, again, I think I buried the lead here. This is really important, maybe the most important thing up here. Event-driven architectures are critically important, I think we all in this room agree or else we wouldn't have shown up here. Events sourced architectures may be even more important because, while you're being event-driven, you can still forget that history. If you're event sourced, you're maintaining the narrative of your business, right? The things that happen, the observations you make about reality, outside of, you know, the the membrane of your organization or you're bound to context, the things that your customers ask you to do. Like these are the only things at the end of the day that matter. That's the only reason we're writing these systems is to serve our customers and solve these business problems, and if you're losing the history of what your customers asked you for or what you chose to do about it, then you're you've lost something really valuable. And as Rob talked about, we have clear reasoning about, and possibly replay of, state over time. If we get new business logic, if we have a new set of business rules, if we wanna online a new data store, we can replay our history because we've kept it. So, an example for my domain, I work for a bank, sometimes we calculate people's balances and stuff. So, you know, there was a need to maintain a customer-defined balance. So, you know, if I wanna control Timmy's spending at college, I'm gonna say, you know, only allow on this, you know, card number or whatever, his account number, only allow a certain amount of spending per week, right? Seems like a simple thing, turns out to be kind of thorny and a little bit hairy. So, we have to aggregate these debits over time, potentially emit an event when a certain balance is exceeded, and we wanna be able to show the customer, like where they're at, right? How much money is left and allow them to, you know, tinker with their configuration, their balances and stuff. So, this is a straw man, another disclaimer. Obviously, you wouldn't necessarily write the object-oriented, sort of, microservices this way, but this is kind of how you do object-oriented programming. I've seen people, like, for realsies, do this. In microservices, don't do this, even if you have an Object Oriented Architecture, don't do it this way. At right time, you know, we get a new transaction in, we post that, you know, the boss API here, the frontend that's gonna serve as my API contract to the outside world, calls back and like writes a thing down to the transactions microservice and then does the math and writes something back to the account balances service, because you know, we need microservices for all these different things. So let's have different nouns. At read time, I'm gonna read and then I'm gonna, like, go fetch the transactions from one service and then fetch the balance from another service. Again, this is, like, a real problem because as this, as the transactions are rolling in, you know, there's a strong possibility for a race between these two calls at read time. Don't do it that way. This is sort of the more functional, sort of approach, like a sequence diagram for the more functional approach, right? At write time, my only job is to write something down to this log. A thing happened. At read time, my only job is to read from the aggregate that I'm building in the account API. The domain-specific, problem-specific aggregate that, you know, resides close to my accounts API, I can just read that and it's super duper fast. In the middle, we've got this sort of intervening processing stuff, right? The balance microservice reads the transactions topic and for every transaction it gets, it computes the new balance and emits that new balance on a balances topic. The accounts API is aggregating both transactions and balances so that it can serve consistent results out to its customers, right? So this intervening processing time, you know, could take milliseconds, it could take seconds, there's some intervening time but both the writes and the reads are super fast. So, that's, kind of, our example a lot of the stuff that I'm talking about now, the example included, are gonna be coming out as a blog post in the Confluent blog and the Capital One DevExchange blog here in the next couple days. So, you can, kind of, look at the code listing and stuff for the examples and so forth. Turns out, the actual problem is considerably harder because we have different data sources, there's the settled transactions stream, and a real-time, like authorization stream and authorizations and transactions are different in the making credit card world. So, there's some joining and some windowed aggregates and stuff we have to do but still, the entire, like code listing, fits comfortably within a blog post. I mean, it's really still a small thing. We're using Kafka and Kafka streams, which we'll talk about in the tools section, to do that aggregation, and it's really powerful and cool. With a very small amount of code, we can do this really rich, very interesting processing. So, let's talk about, kind of, some of the techniques, the rules, and tools that we've developed to do this functional view of microservices. One possible architecture that sort of satisfies this functional view, is what I call the command architecture. I presented this at Strange Loop in and at the time, we also open sourced...Capital One open sourced some, like, a reference implementation of the thing in the upper left, the right handling component. So, this is the command architecture and basically, this is made of a few different, sort of, techniques here. The techniques behind this, REST because, at the end of the day, REST is a really nice way, at the edge, to communicate with, you know, customers, callers, consumers. So, I'm not throwing away REST, I like REST, I think it works, it just doesn't need to be as pervasive throughout the whole stack as currently, we have it. CQRS, I think it was, was mentioned, I think other people are gonna talk about as well. I didn't meet Rob until last night at dinner and it turns out we're, like, long lost kindred spirits. I was gonna stand up at the beginning of this talk and just be, like, \"Yes, dido, for banking, what Rob said.\" But CQRS is simply the splitting of the writes from the reads, we've already kind of, seen that the architecture here, it's Command Query Responsibility Segregation. So, splitting the writes and the read paths in your system's. Event sourcing means, I mean, you know, storing state by storing each event that happens, and then you can always synthesize aggregate state from that, right? Just like in your test analogy, storing the moves, instead of the current board state, there's a world of difference between those two things. Pub/Sub as a mechanism for conveying these events around to your different components the need to read them. So, storage and convenience are both important I use Kafka because it does both, I like it but there are other things that do that as well. Sagas, which we'll talk about here in a minute and serverless, which is why we're all here. The rules, capture all observations and changes at the edge. We'll dig into that a little bit, to an immutable event stream. Reactively calculate the drives stream of state, aggregate that state wherever it becomes useful to you, and however, in whatever form for whatever data access pattern you need. And then manage the outgoing reactions, the side effects in functional programming speak really carefully. So, let's dig into each of these rules here. The single writer principle, Ben Stopford has written a bunch of blog posts for Confluent, and they're great, you should go read them. He's a fellow traveler with Martin Clement, who I also very much admire. He talks about a single writer principle where you have to be really careful about writes. Right? You wanna have many readers, you want reads to fan out as much as is necessary but writes, you need to handle really carefully because this has like cascading effects downstream. You wanna be careful about how you capture these things. So, very few authorized teams capture raw observations about reality at the edge of your bounded context. These can include, raw events, you know, things that I observe about reality, or they can include requests for action by customers. Those are called commands or request for action. As was mentioned, events are, sort of, in the past tense, these are things that we are incorporating into our view of truth, commands or speculative, untrusted, they're phrased in the imperative like updated account. You know, sign up customer or whatever. These are things that the customer wants us to do but we've got to think about it kind of hard and make some decisions before we're willing to incorporate it into our view of truth. All right. You wanna do this at the edge of rebounding context with minimal processing, right? You wanna just get the truth of the business event. These are rich composite events, these are not domain or entity level like data tinkering things. That all happens later downstream and I don't care about as much. What you wanna capture at the edge are, rich, composite events that any one of your business stakeholders could walk up and be like, \"Oh, yeah. I see what's happening, he created an account, and then he deposited some money and then he withdraws some money.\" Right? I mean, that's the level of business event you wanna capture, sort of, at the edge. And you wanna store it immutably and durably. You don't wanna throw these things away, ever, I think. Kafka has this really neat ability to just, like set a topic expiration to never. You just keep everything. Causally related events go on the same log, which is a topic partition in Kafka. We talked about some different approaches to how to do that in Rob's talk. You only wanna have one writer per blog, and no change gets into the bounded of context via any other means. In the architecture, you see that here. That's the job of this commander component in the upper left, it's the writer. It's the thing that writes down stuff to these, sort of, canonical streams. These commands and events are like written in pen. These things are, like permanent and durable. Downstream of that, you'll have, you know, a few authorized teams that are gonna process those raw events and do some, like audited calculations. In my domain, there's only one way that you can calculate interest, and it's audited by, you know, lots of government people. So, you have to do that carefully, and then, you know, you wanna compute the state of different entities and maybe those two streams too, great. This can be recursive so you might end up, you know, as you're aggregating and computing the state are different things, emit further events as necessary, and to emit the event, you're probably gonna talk back to that web service that is the commander that handles the writes to that log. And we have that here in the command processing world, in the processing jobs here, I like Kafka Streams for this, we'll talk about that in the technique section or in the tools section, excuse me. And then you wanna aggregate state. And this, lots of teams can do, right? This is where you have huge degrees of freedom in your organization, like when you're trying to beat Conway's law here, lots of teams, without having to coordinate or even know about each other, can consume the streams to build whatever aggregate matters to them. Whatever data access pattern they need, so almost certainly you're gonna have, you know, someone building like, the transactional view, the OLTP, you know, here's the state of things, view. But, in our domain, we have auditors that have to look at the stuff, we have, you know, security and operations people who wanna see what's going on, we have analytics people who wanna build like pretty pictures for the boss to show the state of the business, right? And all of these things can be done without any coordination between those teams, so the teams that are expert in building that particular view for that particular domain audience, can build it without having to, you know, ask nicely for someone to ETL out of your database and build something, right? You don't have to do that, because everyone is, has first-class citizenship, right? The primary application team doesn't have a first-class status and then everyone else has second-class status with regard to data access. Everyone's singing from the same sheet of music here, we're all speaking the same language of business domain events. Manage side effects carefully, I wish I had more time to dig into this, but it looks like I'm about to run out of time. Side effects, so if events and commands are written in pen, the authorized computations and entity state is written in, like, erasable pen. Aggregates are written in pencil, you can throw away the aggregates and regenerate them from the log, right? So, your aggregates, you can experiment with a little bit more. Side effects are written in pen, stuck in an envelope, and mailed somewhere. Like, you can never change that. You can't even really see it, right? So, when you're causing effects outside of maintaining state on these different logs, that's what I call a side effect, right? And in functional programming, you have to avoid side effects, you have to use a monad or something if you're in Haskell. So, you shouldn't do side effects which are you know, causing action outside of the call stack, right? Here, I'm, sort of, defining the call stack as writing to and reading from the log, so that's not side-effecting when you're just, sort of, maintaining state via this log. When you, like, call out to some third-party web service, or when you send an email to your customer, or when you set an SMS message, that's a side effect and you have to manage that carefully. Sagas are a good technique for doing that, writing down the fact that, \"Hey, I tried to call this thing, here was the results, maybe you can have some sort of retry pattern.\" If at some point you need to like reverse that action, you can't reverse the sending of an email, you just have to send a compensate email, like, \"I'm sorry, we sent an email on accident.\" So you have to manage that carefully. Tools, like I said, I really like the Kafka Stack very much for this, but there are other tools that do this, right? We talked about Kinesis, there some new log, like, Apache Pulsar and Twitter's distributed log and they all have the same characteristics of being immutable, append-only stores, that then convey those changes out to listeners. And there's good integrations with serverless techniques and tools Apache, OpenWhisk, Kafka package, is one example. Here's some references, I mentioned throughout the course of the talk. The one that I don't have up there is the commander presentation that I gave at Strange Loop and the open source associated with it, you can Google that find it. That's all I have today, thank you very much. About Emit Conference ",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e96",
      "title": "How Droplr Scales to Millions With The Serverless Framework",
      "content": "I'm Antoni Orfin, a Solution Architect at Droplr. We're used by more than , users, who share thousands of screenshots, screencasts and files every dayso we're constantly looking for technologies that empower growth. When we first heard about AWS Lambda, we were using a Docker-based microservices architecture. It carried some problems: rapid deployments, complexity of Docker-based microservices architecture and underutilized EC. Lambda could help us eliminate some of that unneeded complexity. We immediately knew we wanted to give Lambda a try. Our main goal was to make our development process even as streamlined as possible. Our main goals were to boost productivity and inspire innovationmake it super easy and fun for developers to deploy their own production-grade microservices. So, we started doing some PoCs on the Serverless Framework. Today we're leveraging several Lambda-hosted microservices on top of our existing architecture. It's been quite a journey, but well worth setting off on! Read on for a deeper dive. Our Serverless architecture From the very beginning, we knew we wanted to make a large portion of our infrastructure Serverless. For that reason we approached the process strategically. First of all, we decided to migrate all of our workers that are processing background jobs: - Scheduled tasks - things that should run periodically, CRON-like. - Functions invoked by events - when a new drop (a file that is being shared) is created, multiple Lambda functions are invoked in parallel by AWS SNS notification. Once we had this done, the fun part started. We needed to take care of all the public-facing microservices : - REST APIs - Node.js Express based APIs - Integrations microservices - Droplr is richly integrated with other platforms like Jira, Confluence and Trello. All of these integrations are running serverless. - Server Side Rendered web applications - yep, we do SSR on Lambda :-) basically all we need. We can even set up alerting based on the logs condition. We mostly use it to analyze the CloudFront logs related to our file downloads to spot any anomalies or abusive users. ServerlessForever After having months of experience with the Serverless Framework, we consider it a backbone of modern Lambda deployments. The framework played a crucial role in making our serverless journey successful. More than % of Droplr services are already migrated to Lambda. Every month, new APIs are moving there. For new microservices, it's our new standard to build them on top of Serverless Framework.",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e98",
      "title": "Madhuri Yechuri - unikernels and event-driven serverless platforms",
      "content": " Madhuri Yechuri is the founder of Elotl. She talked with us about shrink wrap needs for applications, and how serverless ranks in cost-savings, agility, security and observability. Madhuri then delved into some surprising benefits of using unikernels in a serverless world: shorter cold starts, better security, and smaller package sizes. Could unikernels replace containers as the engine for FaaS? The full video is below, or scroll on ahead to read the transcript. More videos: The entire playlist of talks is available on our YouTube channel here: Emit Conf To stay in the loop about Emit Conf, follow us at @emitconf and/or sign up for the Serverless.com newsletter. Transcript Madhuri: Good morning everybody, and thank you so much for attending this session about unikernels and their relevance to event-driven serverless platforms. The agenda for this talk, I'm going to give a little bit of introduction about myself, and talk about the application deployment paradigms of the past, present, and looking into the future. And then talk about, what are the pain points that serverless event-driven paradigm is trying to solve, and how it exhibits the potential to solve these pain points. And after that, we are going to look at, what are the shrink wrap needs for applications that are running in a serverless event-driven fashion? What are the shrink wrap needs in the sense that what does a function that's running in an event-driven serverless fashion expect from the underlying infrastructure? How is it going to shrink wrap my function? And we will look at these shrink wrap needs of event-driven serverless applications and evaluate the existing offerings out there for shrink wrap, which is containers, and see how containers solve these pain points of the application shrink wrap needs. And then we'll talk about unikernels and see a demo of unikernels, and compare the metrics of unikernels and containers to see how unikernels fit in or do not fit in, or have potential to fit in, to meet the needs of the shrink wrap for the event-driven serverless applications. And we'll follow that up with acknowledgements and a Q&A. During the talk, if you have any question, please raise your hand so that we can address questions as we move along. A little bit about myself. I've been a systems engineer for years. I started out with database server technologies at Oracle, and then I worked for VMware on virtualization layer in the management infrastructure platform, and then I worked...I spent some time in container ecosystem in a company called ClusterHQ. We were the first providers for dealing with applications that have state associated with them. How do you deal with stateful applications that are running in production deployments? And most recently, I've been playing around with unikernels. So let's look at the application deployment paradigms of the past, present, and future. In the past, we used to happily deploy our large monolithic applications in our secure, private cloud, and we would shrink wrap them using virtual machines. And that's what we knew, and that's what worked fantastically, and that's what we were comfortable with, and that was what was secure. And as of today, we are moving from large monolithic applications to small microservices, and we are not just deploying them in our secure, private cloud, but we are also comfortable deploying them in our public cloud in addition to our private cloud. And we are moving towards containers as the shrink wrap model of choice and away from virtual machines. And moving forward, what does the future entail for our nice little applications? We are not just looking at microservice applications, we are looking at microservices and nanoservices in the format of serverless event functions basically. And we are not just looking at private cloud, we are looking at private cloud and public cloud and also remote IoT edge devices, which are mounted on locations like on an oil rig in the middle of Gulf of Mexico, for example. And this is where the application deployment paradigms get really interesting because we're not just looking at these three deployment options, we are looking at a mix and match of these cloud options. So we're looking at hybrid cloud that stretches between private and public, hybrid cloud that stretches between public cloud and a remote IoT edge device, and a hybrid cloud that stretches between all three potentially. And are containers the only options that we have for shrink wrapping our applications to deploy them on all these permutations and combinations? We are all here at EMIT Conference so we all understand the pain points of deploying always on microservices. You're spending a lot of money as a customer in paying for your compute nodes, whether your microservice is up running or not. And as an infrastructure provider, you have to keep these instances up and running, which is costing you resource utilization with respect to CPU memory and network resources. Whether or not you're exposing that spend of yours to your end customer or not, you as an infrastructure provider, you're still paying for those resources. So there are a lot of issues with keeping these always-on microservices, especially looking forward towards deploying them on these hybrid deployment paradigms of IoT and private and IoT and public cloud, etc. So serverless has the potential to solve these pain points because you only pay for what you're using. So as an end user, your infrastructure cost bills are really low, and as an infrastructure provider, you potentially do not have to keep these microservice instances, the compute nodes to run these microservices up and running, and you do not also need to spend time and money on orchestration frameworks to deploy initial placement, load balancing, and health checks of instances that are sitting idle, not running any microservices. So a serverless event-driven platform, it solves all these pain points of always-on microservices. So now that we understand that, okay, we need to move towards event-driven serverless model in order for cost savings for the end user, cost savings for the infrastructure platform provider as well, what are the needs of this lightweight microservice or nanoservice event-driven function or application for shrink wrapping itself in order to run on the platform? The shrink wrap model needs to be lightweight for the application. By lightweight, what I mean is that when the app is sitting on-disk, and when the app is transiting from on-disk to its eventual compute node, whether it be the first startup or updates of the app, and the third is when the app is up and running on the compute node, when it's occupying CPU and memory resources and network resources potentially on the compute node, it needs to be lightweight because we want to potentially deploy it across these hybrid cloud scenarios. So the first need is the app needs to be super lightweight. And the second need is that the shrink wrap needs to be super agile. By agile, what I mean is it needs to be reusable and recyclable. By reusable, what I mean is that if your function has started and stopped but is going to start back up again pretty soon, we shouldn't be destroying the shrink wrap and recreating the shrink wrap. The amount of resource utilization and time taken to recreate the shrink wrap should be really...the overhead should be minimal. And that is the reusable nature of the shrink wrap. And by recyclable, what we mean is that as soon as your function has stopped running, we should be...and you know that it's no longer going...you don't need to run it again for a long period of time, you should be able to reclaim its CPU memory and network resources as soon as possible so that you can give it to future functions. So this is not of a big concern for private cloud and public cloud where you have access to a lot of resources, but if you think about IoT edge device, you have...you still have limited resources, so you should be able to reclaim the resources really quickly. So that is the second need of the application from its underlying shrink wrap, from the infrastructure platform. So the third need is security. When the app is sitting on-disk, and when it is in transit from on-disk to its target compute node, and when it's up and running, occupying CPU cycles, RAM cycles, and network bandwidth, it needs to be secure. And that's what the application expects from its shrink wrap. And the fourth requirement is observability. Can the event-driven application function that's running in its shrink wrap, how observable it is. Do the existing application, performance, management tools that are available in the market, can I reuse them or do we need to build new tools because the app's behavior is completely different from what these tools were built for? So observability is the fourth need of the application from a shrink wrap. So these are the four main requirements from the application for shrink wrap, from its underlying shrink wrap, in order for it to run smoothly in an event-driven serverless model. So let's take a sample application, which is a Node.js webserver that listens on port and responds with a \"Hello World\" to incoming requests. So let's take this simple Nodejs webserver app and evaluate it for it on all these four axes and collect metrics to see how the app performs and how the shrink wraps perform for this given app. So the first shrink wrap we're going to look at is obviously container. So containers are being used for production for event-driven serverless models nowadays, and there is a reason for that. So let's see how containers perform on all these four axes. So the first metric we look at is the on-disk size of the application as a container image. So for the Node.js webserver, the Docker container base image that was chosen was Alpine because its relatively smaller on-disk size. So this is potentially one of the smallest available ways to get your smallest on-disk size. So the on-disk image size is about MB, which is pretty good. It's better than a couple of hundreds of MBs of the default Ubuntu or CentOS based on its sizes. So our Node.js webserver is small enough, so that's pretty good. And the next metric we're going to look at is the start times, so how quickly can you provision the application, which is an indicator of call start time, So how quickly can you start your event-driven function when the request comes in? So the start time for the Nodejs webserver was around one second, which is not too bad. The experiment was done on an Ubuntu . server with gigs of RAM. And the reason for choosing this server is kind of I wanted to pick a server that was a representative server midway between an average public cloud machine and an average IoT device so that it's a good indicator of how the app is going to perform. So start time of one second is pretty good. And what is the resource overhead associated with running our Node.js webserver as a Docker container? And the memory overhead associated with the Node.js container was around MB, which is low enough, and it's not of a big concern when you're running on your regular compute node, say [inaudible ::] small and for large instance, but it could be of concern when you're deploying to a remote edge device that has gig of RAM, for example. And the last metric is how secure is your app that is running, up and running? So if we look at a regular container that's based on any Linux, so all of the Linux security vulnerabilities are exposed, your app is exposed to all of the underlying base image Linux security vulnerabilities, which is both good and a bad thing. Good in the sense that, I mean, Linux is in production in so many places, so if it works for so many other people in production, it should be good enough for you. And bad in the sense that maybe your app, especially your serverless event-driven app has been custom-built to do one thing and one thing only. So it doesn't need the entire backing of an entire Linux. So you are bringing along a whole amount of extra baggage to run your teeny-tiny single function. So that is the bad part of it. And as far as observability's concerned, you can use the traditional APM tools like Amazon CloudWatch, for example, or you can use the up-and-coming APM tools that are being custom-built for event-driven serverless applications like IOpipes to monitor your apps. So the observability factor is also pretty good. And no wonder containers are the default shrink wrap of choice for all of the major serverless platforms out there because Lambda uses it, Google Cloud Functions uses it, Azure Functions uses it. So it makes sense because looking at these metrics, containers are performing really well. So let's move on to thinking about, are containers the only option for us to shrink wrap our event-driven serverless applications, or is there another option which could be used in addition to containers, right? So let's look at what a unikernel is. Unikernel is a single process, single address base application where you take your app and you statically compile it with only the parts of the OS that your app needs. For example, if your app is a stateless app, like a Node.js webserver that we just looked at, you don't require the entire five-system component of an application, of the operating system to be present in your executable. So it's a single-process, single-address base application that's statically compiled with only the parts of the OS that it actually needs and uses. And it doesn't have any extra things that are baked into it. It doesn't have a shell. There is no way to fork another process because it's a single process. So you could have a multi-threaded application but it will be a single process. So let's look at a demo of a Node.js webserver that is running in a unikernel. So out here, we are looking at the Hellojs webserver, and it prints out attempting to run the webserver in a unikernel to the console, and it listens...responds with a Hello World and listens on port , which is the default port. And also prints out a message saying the server is running on local host at port . So I'm using a simple script to start up the app as a unikernel. So you can see that the app is up and running, and it prints out attempting to start the webserver in a unikernel, and it's listening on port . So that is the IP of the running application as a unikernel. In order to validate that the app is actually running, if we call the IP at port , we should get the Hello World back. So how is this unikernel application running on the Ubuntu server? So it simply starts up as a teeny-tiny VM using QEMU. So you can use emulated virtual machine to run your application on your default Linux server, for example. So the start up of the app is no more different from the starting up of a container. So now, let's compare how this app, that's shrink wrapped as a unikernel performs on the metrics that we are interested in. So if we look at the on-disk image size, comparing the application on-disk size in an Alpine container, which was at -something MB, the on-disk image size for the...after shrink wrap as a unikernel is -something MB. So it's teeny-tiny. And the difference in the image size doesn't make a big deal on a private cloud or a public cloud machine, but if you want to deal with...if you want to think about hybrid cloud opportunities, then yes, the difference in the image size makes a lot of difference, especially with respect to image updates, etc. And the start up time of the application that was shrink wrapped in a unikernel was much less than one second. So this makes a big difference when you're looking at cold starts. So if you do not want to keep a pre-warmed function container or a shrink wrap up and running, but want to have cold start opportunities, then the lower start time is pretty good. And the third metric is the application runtime memory overhead, which is an indicator of how much resource overhead is required to run your app as a unikernel. Out here, the container shrink wrapped application outperformed the unikernel. For the unikernel application, the runtime memory overhead was around MB, which is times larger than the container shrink wrapped image. So there is...out here, container outperformed unikernel. So the fourth metric is security vulnerabilities. When your app is up and running as a unikernel, just by the fact that the application was compiled with only the parts of the OS that it needs, it doesn't have a large attack surface to attack. So the attack surface is much smaller. So it's slightly more secure just by the fact that there isn't much to attack basically. So you don't have a shell to SSH into. And you don't have, like, for example, VENOM attack on the Linux server side. Someone hacked into a Linux server using a CD-ROM device driver. So in our case, it's a Nodejs webserver, so you do not need a CD-ROM driver. So the app is not compiled with a CD-ROM driver. So you avoid a lot of attacks just because the attack surface is really, really small. And the final metric is observability. For application performance measurement metrics, there aren't enough products out in the market yet to observe the apps that are running in unikernels. That doesn't mean that they're not observable, it just means that the products haven't been built yet. So with respect to observability, container is more observable right now than unikernel. And there is room for growth for unikernal in observability. Yeah. Man : Just a quick question on that last one. What would be the difference between just going like [inauible ::] that node app that you had there and any [inaudible ::] or any other APM out there? Like there's other APMs out there for remote servers today and they still work, right? Madhuri: Yes. Yeah. Yes, yeah. Yes, yap, yap, yap. Yeah. You can definitely add in a third-party thing, but there hasn't been...there aren't products out there that have been custom-built for observing event-driven functions running in a unikernel, for example. So you can definitely take the existing ones and use them for Nodejs, for example. So looking at the highlighted red borders, those are the metrics where one outperformed the other. So it kind of makes us realize that, \"Hey, it's not...unikernels have some potential to potentially be useful as a shrink wrap in addition to containers. So there might be scenarios where containers are more suitable, and there might be scenarios where unikernels are more suitable. But there's definitely...they show a lot of promise a potential shrink wrap format for running event-driven serverless functions. So the takeaways from this talk hopefully is that moving from monolithic to microservice apps to nanoservice apps, serverless event-driven model is the right way to go because it demonstrates cost savings to the customers and to the infrastructure providers as well. Containers are a great fit as a shrink wrap model for now, for the application deployment paradigms that exist right now. Moving forward, looking at the hybrid deployment paradigms for private cloud, public cloud and IoT, unikernels show a lot of promise as a potential shrink wrap format in addition to containers. So I don't think, and I don't believe that they are a replacement for containers, but there definitely...there could be use cases where one could be a better fit than the other. Thank you so much for Nick and Casey at serverlesss.com for helping organize the talk. And thank you for listening and I hope to open it up to Q&A. Any other questions? Yeah. Man : Why is the memory overhead so much higher for the [inaudible ::]? Madhuri: That's a really good question. So the memory overhead is coming from QEMU. So there are...so we took the default QEMU to run in emulated VM, so there is a lot of overhead that can be chopped off because you don't need a heavyweight emulator like QEMU to run a unikernel because you just need a monitor that knows that what's running above me is a single process, single address space entity. And there's been some really good work at IBM Almaden on a project called Solo and uKVM. uKVM is a monitor that could be used instead of QEMU. And if you use uKVM, uKVM is conscious of the fact that what's threading above it is a unikernel and not a heavyweight virtual machine. So if you use uKVM, you eliminate that overhead. Does that answer your question? Yeah. Man : Do you think that in general the unikernel would be expected to use less memory or more than a container-based model? Madhuri: So if you use uKVM, it should less memory than a container model, but it's...uKVM work is very much in its infancy right now, so there is a lot of potential for improving that and as is the case in containers as well. So if you look at the image size, right? The image size for the Alpine container with the Nodejs was -something MB, so there are ways to bring it down because there are products in container market that are being developed to actually cherry-pick which parts of the Linux OS you want to compile into your base image. So there is room for improvement on both sides. So the metrics that were collected are with the products that are available today in the market, but there's potential on both sides to, like, converge on the metrics. Does that answer your question? David: Thank you very much.",
      "__v": 0
    },
    {
      "_id": "64e0891cb72e199dda603e9a",
      "title": "Ajay Nair - being a good citizen in an event-driven world",
      "content": " Ajay Nair, Lead Product Manager at AWS Lambda, talked to us about considerations when designing event-driven systems. Okay, he says, we can all admit that serverless architectures tend to be event-driven: all logic is embodied as functions (or, events), that talk to something downstream. And eventually, it'll be more and more common that companies want to emit event streams that let anyone do cool things with them. All kinds of SaaS companies and ISPs will be event sources. So, in Ajay's experience, what are the practicalities and ins-and-outs? This one was definitely a crowd favorite. Watch his full talk below, or read the transcript. More videos: The entire playlist of talks is available on our YouTube channel here: Emit Conf To stay in the loop about Emit Conf, follow us at @emitconf and/or sign up for the Serverless.com newsletter. Transcript Ajay: All right. That's my cue. I was told if I see purple, I should go. Apparently good advice for life. All right, folks. Hi, Ajay. Hi people. I'm Ajay. I lead the PM team for AWS Lambda. I'm really excited to see how many people are excited about how this whole event thing that got sorted out. In the course of the conversations today a lot of us have been talking about event of an architecture, the serverless and serverless event of an architectures. I'm taking an opinion in this talk which is while all serverless architectures tend to be event driven not all events of an architectures are serverless. So, I'm gonna try and stick to the word event driven as much as possible but pardon me if I slip up a little bit. So, to kind of get into it a little bit, we're all here because we're kind of excited about this movement around event driven architecture that this push to serverless has triggered. All of us are excited about the kind of architectures we have seen shared today from Nordstrom and Capital One. There's one that I really like to kick off talks with which is a company called Uber. Many of you may have seen some of their Twitter posts and others. This is the architecture that we kind of talk about as the before of serverless architectures, your conventional load balancers, a few [inaudible ::] servers talking to a standard database and this is the future of serverless as some people start talking about it. Austin showed up a simpler version of this but this is serverless in all its glory. This particular customer has functions and [inaudible ::]. Their deployment time dropped from minutes to seconds. Their cost savings moved about % reduction in costs while shipping about times more features every month than what they were previously doing. So, cost-benefits, agility, time-to-market all of those things are values that we're starting to realize with this. Apparently, my clicker isn't working. I'm sorry. This is what I was trying to talk to. All right. And, we're seeing this part on being spread across a wide variety of scenarios. We've heard customers talk about web applications and frameworks coming out for building, floss base and express based ones using simple synchronous invocations within a [inaudible ::] Lambda all the way to very complex data processing applications. Something like what Nordstrom was talking about with events sourcing and auto manipulation and recommendation engines to something as complex as big data processing even trending towards what you would call HPC. If folks haven't checked it out, check out this framework called Pyron that one of the folks at UC Berkeley has put together for running massively distributed data processing app that allows you to do trillion floating point operations over a billion records. But, the point is all of these are new scenarios and applications enable following what actually boils down to a very simple or grossly simplified architecture pattern. One where we say all logic is embodied as functions that are some things called events that trigger these functions and then the functions then talk to something downstream which in turn in themselves may be event sources or actors that act on that particular thing. Something that's common across events or architectures is the fact that every communication ideally happens through events in APIs. The execution layer is stateless and ephemeral which means there is no concept of anything being retained over there. And that's clear almost a forcing function of separation of logic, data, and state. So, we've been so far kind of standard events or architectures. One thing Austin told me when I was talking about this talk was he says, \"Well, you can get on stage but you can't talk about Lambda,\" which cuts out % of my material that I have. So, what I'll talk about today was how do you go about being an effective event source provider in this particular model? And the reason I bring this up is many of you will be creating SaaS companies or ISPs or products of your own. And at some point, you're gonna be emitting events which you want to participate in applications that other people build. So, say you're the next, next Uber, maybe the next Uber is already built and you wanna emit an event stream that allows anyone to create a function for doing something smart with it. Say, creating a strike payment every time someone sits inside Uber or whatever. What are the decisions that you need to make about creating an event source or participating in this event source infrastructure? A lot of the patterns I'm gonna talk about today are ones that we use internally for our internal services for emitting events across S, Dynamo, and others. Okay. So, how do you effectively become a good event source? So, to quickly start off with making sure that we're all talking about it and framing it the same way, we all talk about events as various things but I'm trying to use a standard definition, which is an event is an indication that something of interest happened and its service is telling other people what that interesting event was. The standard definition of the reactive manifesto is out there. I'm surprised no one mentioned this until now so I'll take first credit for calling that out. But, when you look at it there's kind of two conceptual pieces of what actually delivering events over there. So, one is the event source itself, the component which is responsible for identifying the change that happened and then emitting a payload with some interesting information about what needs to happen over there. And there's a second component, a logically separate component, whether they merge or not remains to be seen that's actually responsible for getting that event to the actual provider. Keeping with kind of the principles of loosely coupling the processor and the source itself you want that to be a potentially separate service. In fact, Austin's talked about kind of event gateway which fits nicely into this router concept that is in place over there. Some examples of these particular pieces within AWS just to kind of bring the point home, the services on the right, sorry, I get mixed up, on the right acts as event sources. So, data stores like Dynamo, S, even EC instances all emit events. So, for example, EC can emit an event that says my instance responds up or down. Dynamo can say my record has been updated. S says a new object has been shown up or deleted and then you have various ways to route them to actors. Most commonly Lambda functions but you can imagine it being routed to a container service to get deployed which could be SNS which is a general POP subconstruct, CloudWatch events which allows you to map arbitrary events to various destinations like SNS, Lambda has this concept of event source mapping, so on and so forth, right. I'm not trying to be comprehensive in explaining these things just to give you a real-life example of what I mean by these two types. Okay. So, with me so far? Okay. So, the first decision you would make is what actually goes into your payload. And here there's these two distinct patterns that we kind of realize even when we are building our ecosystem of event sources. There's one thing which is common, which is sort of a standard baseline which is all your events must contain provenance information. And, by provenance I mean who's the source, what was the relevance of the event that happened and when it happened, right? More often than not times time information is something that you will find useful in terms of determining both. In fact, interestingly both times [inaudible ::] interesting of what was the event that happened that was interesting and when did the event source or the thing that was watching it notified that that particular event happened. More often than not those two are not necessarily separate but maybe potentially worth capturing. What goes in the rest of the event party varies depending on what your actual scenario is and there are two patterns here. And, it kind of goes back to what your philosophy or what you believe an event can be. One pattern is to believe that an event is a notification. In the sense that it tells you that something interesting has happened or if you want one of my teammates called this a passive aggressive notification in the sense that I'll tell you something happened but if you wanna know more you have to come back and talk to me. I'm not gonna tell you what happened. So, the idea behind this is if you're application logic is baseline on the fact that the original event source has to be talked back to this is probably a reasonable pattern to follow. So, in this case, the event payload contains information about the event source resource that was affected, more data about who made the change, what goes into it, the security construct in order it should happen, etc. and the assumption is that the component that's reacting to the event processor reaches back to the event source to find out what happened. So, let's take an example of kind of the next-next Uber that we were talking about. Where we say that in this model if that service wanted to emit it would assume that the function has the ability to talk back to that service and asked for something interesting, say, ice cream information or whatever it is that they wanna kind of find out over there. The real-life example you know our infamous S top mailing example that is now been beaten to death multiple times but I present it to you yet again because it's a great one to talk about is an example of this. So, when an object shows up inside S and a notification is published the notification does not actually contain the pillar of the object that was created. It simply says that an object was. And then, the function has the choice of reaching back into the S bucket and then pulling that content back and deciding what to do with it. So, the tradeoff you have over here is much more lightweight communication across the services however you're ending up with somewhat of a tighter coupling because now each downstream service is now aware of the one that's upstream. The other one to remember is the potential doubling of traffic on your event source. In an extreme case over here imagine if every object that was being uploaded needed to be thumbnail, now has doubled the traffic on your bucket, one for actually putting the object and one for reading the object to go back and process it. So, there is a trade-off that you're setting up for yourself. Imagine if you were in place of S what does that look like for you? Are you okay with that additional traffic coming back to you or if your traffic is going to be all the time there's another pattern for you to consider? The second model to consider is kind of the state transfer pattern or sort of having the event pass state forward in the assumption that the downstream service never comes back and talks to you. And borrowing both these verbages from Martin Fowler over here because it kind of gave a nice clean way to explain about it. Now, what this assumes is the opposite of what was before which is you can't talk back to this event source. Consider your connected device a story where an IoT device is putting something out that's potentially very limited case whether you can go back and reach the device and find out what's going on over there. Or, again if your service doesn't have a public endpoint or something that customers can call back you can't do it. So, in that case, you would lean towards putting a payload on the object itself. Trade-off obviously much more data being schlepped around. You have now considered security constructs where what actually goes in there and now worry about how that gets passed forward but more importantly it brings another component which we typically don't consider over here which is this concept of an events store. When you say, and I'll dig into that just a little bit. So, to these are two examples of AWS events that we actually emit today. One is an S event the other one an IoT data blob being passed in through Kinesis using Kinesis as the event store in that particular case and I'll talk a little bit more about that. The portion on the top is the common element across the board, that's provenance. It's saying this particular event was created at this particular time. The reason it happened was an S object or a Kinesis port and passed on forward. But what you see is a difference in the rest of the body. The S event surfaces things those are characteristics of the event source itself. It says it's the bucket, this was the object etc. gives me all the information that as a function I would need to go back and reach into it. Kinesis, on the other hand, just stuffs the payload in there, encrypted of course, so that the downstream service can provide it and you rarely go back and talk to the thing that was there before Kinesis itself in front of it. And, going back to this, if you are a provider think about which of these things matter to you. Is the information that you're passing to process downstream valuable to stuff in the payload itself or would you require them to talk back to you? And, this doesn't necessarily just have to take this identity model that is put in there. It could be as something as simple as a callback URL that you put in there saying that, \"Hey, this is how you call back and talk to me if you need to when you're ready.\" So, going back to this previous thing I was saying about sort of event store if you have state transfer and you as an event source don't have a concept of state it needs to land somewhere. And, as we already said your quintessential \"fast provider\" is not stateful. So, you're not gonna have any concept or landing over there. So, you need something that is potentially an event store in the middle. And it's a decision to be made not something to be taken lightly. On one hand, with an event store, you get enhanced potential durability and retention. And, this is valuable for many, many scenarios that you see over there. Durability for the reason that events are accessible potentially if the event source itself is done. So, God forbid your next, next Uber doesn't show up and is down for some reason the events are still there for replaying and kind of having a conversation with. Secondly, you have this concept of retention which means they can be revisited, replayed, used for rehydrating production stores, etc. The quintessential scenario here is kind of the event sourcing model. If you have a data store that is having a change log being published you want it to go into durable store that then you can go back and replay. The flip side of it is you have potential additional storage, retention, and complexity of another component being introduced over there. Previously you were dealing with this thing that emitted events and thing that reacted to events and now you have this thing in the middle that is retaining and potentially shifting events around over there. So, now again I'm gonna stress this again, you don't need it in sort of two cases. Don't worry about it in the S model if state is already present over there. Don't worry about it in the case where state is passed back and this is a fancy way of saying if it's a synchronous invocation don't bother. So, let's kind of use a couple of real examples of it because there are a couple of event store models that you can go down. One is the streams model, the advantage being your events can be potentially processed in order. You have this concept of broadcast showing up but you have multiple providers actually being able to act against it. The downside is you start dealing with things like Rob was talking about in his first talk about how do you deal with sharing or distribution off your events across all these different collections that you have within your stream. What do you do, your fan out or kind of the \"finance scale and behavior\" that you get with functions is somewhat restricted because of the ordering fixation that you have over here. We'll talk a little bit about that. So, in this kind of example over here, imagine this is a table that's writing say a product listing into it and it's publishing the update to that product table as a stream which is then processed by Lambda and then backed up. You're creating essentially an eventual consistent copy maybe in another region as well as an entity snapshot for doing archiving in the background. Pretty straightforward. An architecture that many of you would consider but in this model, the right approach that you wanna do is make sure that you have a stream-based event source or an event store as opposed to the next one I'm talking about which is a queue-based one. So, I'm not talking about anything revolutionary here. This queue-based, SoA, you know, maybe there are terms which even I don't know at this point. You can talk about this is your quintessential what if I use an enterprise service bus as a way for kind of decoupling to systems over there. The nice thing about it, it still applies to the concepts we're talking about here. Concurrent process on individual events you can process each one discreetly. The flip side of it being obviously no ordering guarantees and there's the concept of doing multiple consumers on it flips around. But, there are scenarios where it's valuable. So, here's an example of an architecture that one of our customers has set up for running wireless scans on machines images. So, every time someone uploads machine images or armies in the AWS terms into an S bucket, the notification of that gets published in to an SQS queue and then they have multiple workers which actually spin up their antivirus software installed at machine image, run scans on it, and then notify if they find something interesting. So, it's an example that a consumer isn't necessarily a function going back to the whole thing about event driven and serverless but the queue in the middle makes sense because now you have multiple workers, although with limited fan out that's actually being able to act on the event. And it really doesn't match which image gets processed first or later. It's the right thing for you to do. In both these cases, the queue is acting as the durable store over there. And, the reason this is important is that particular...you may have a case where they have their entire fleet down. Someone went home for the night and accidentally switched out the entire fleet. You don't want to lose sight of the fact that images needed to have a virus scan run on them and they just sort of evaporated because no one was there watching for it. That's the case where you need an event store in place. Both viable models but something for you to consider over here. Interestingly enough, like I was kind of alluding to earlier the event story you pick also has implications on how you're scaling and your ability behavior works. In the default case where there is no event stored, if an event source is talking directly to your provider it's the closest thing that you can get to a synchronous operation but the provider has no concept of doing say built-in retries or any kind of retention over there. You bring in something like queues, you now have this concept of potentially doing retry offers because the object is still, the event itself is still present on the queue, you can try to repopulate that image and kind of go from there. And, by the way, this is the model that we use at Lambda Asynchronous API underneath the covers. And this was the reason why we kind of chose to do this is we kind of give you a miniature version of an event store inside our Asynch API so the vendor function runs there's this concept of retry behavior that we can kind of build into it. And finally, when you talk about stream-based ones because of the fact that events are being shouted across multiple streams your concurrency gets limited to the number of shots or partitions that you actually have on your listing which is very different from the other model where your concurrency or parallelism that you're getting is driven by the individual events that are in play over there. So, as an event source provider, the thing you have to think about is if I'm exposing an event store A) decide whether you wanna expose an event store or not and if I do which of these models do I pick? You don't have to. You can always choose to tell your customer saying that, \"Hey give me an SQS queue to publish my stuff onto or tell me your functions directly and I'll push directly too.\" But, more often than not it's valuable for you to consider like do I expose say an event stream or an event queue which then any consumer can go in and publish off of. And, as you can imagine the stream-based model is the one that Dynamo DB has chosen to adopt. Dynamo DB streams is a built-in construct into Dynamo DB that shows up over there. The queue-based approach is what to some degree S has chosen to follow. They have an internal queue where they actually retain some of those messages and do one level of retries before they even get to Lambda over there. All right. So, the third decision is about the router construct. Now again, recapping that we talked about a bunch of services within AWS which are routers but the interesting thing is to think about what are the capabilities that you need for the router to be present. And, as I was actually creating the slide one thing I want to assert over here is a lot of these capabilities are pretty implicitly bound to the downstream actor. So, in many cases, you will find Lambda or [inaudible ::] or others providing services that embody these capabilities in some form or the other. Take SNS is an example that I'm familiar with that allows you to kind of do a pub sub construct on top of this. But you're welcomed to actually write your own event source bridge. So, we have custom providers who use a Lambda function to act as their router for reading off their particular event store and then turning around triggering any functions that they might have on their end. So, in this particular case these three main checkpoints you're to have for your event source or your event router. One, the ability to securely associate multiple providers to multiple sources to multiple destinations. And, I stress the word securely over here. In the world where events are flowing all over the place and you have milling components sitting around over the place is extremely essential to make sure that anyone who's emitting an event is authorized to do so and anyone that's consuming an event is authorized to do so. And, you're sure that the person who's emitting it is the person who says it is. In Bobby's example, you don't want someone else saying here is $, credit to my account and slip that into the event stream without anyone else knowing about it. So, you have to make sure that the mapper itself has this ability to kind of map security between those two. The second key tenet that you will see over there is conditionals. The ability for you to filter down the events that the provider actually has to work on. This is more an efficiency play more than anything else but it's one that we have seen, we've started embodying almost in all our routing services internally. So, you will see this say within S you can filter by prefix and suffix. On SNS you'll start seeing some kind of a filtering construct as well. You can do this with Dynamo. You can say specify which particular objects you actually want to specify then go out over there and then finally because of the fact that they are the ones responsible for communicating with the downstream service, the concept of on failure semantics rest with the router as well. And, this is where the affinity with the downstream consumption service becomes really interesting because you will now have the ability to say, \"Okay. If the function failed to exit the event on the first time what do I do? Do I throw away the event? Do I put it onto a dead letter queue so that it gets replayed? Do I end up going and telling the function to do something differently? Do I rely on the failure and semantics of the function itself?\" So, if you are choosing to build your own event router or if you're using a standard platform put it through kind of this checklist order. Is it able to give you these capabilities and what does it do with it? You can imagine kind of a whole bunch of more ideal scenarios that show up with there. Imagine Dynamic discovery of both event sources and destinations that are there and then dynamically binding the components back and forth. The ability for you to combine events with identical schemas and then doing joins with it. Throw graph QL into the mix and see what happens on that particular front or you bring something like multiplexing where you have fan in, fan out being handled by the centralized service without having each store and destination worry about it. But, I think it's gonna be an evolution that you're gonna see happening more. So, to finish it all off I just kinda wanted to bring this example. So, this is actually one of the services within AWS has this simple architecture of handling automated capacity ordering when someone puts in limit increases. So, the limiting increase is going to Dynamo DB. There's a stream that's published out there. Lambda function processes it, pushes it to SNS which then both notifies an operator and then puts it into a queue for some of our automated ordering processes to go and kick it off on that particular front. And, this kind of embodies a lot of the things we talked about here. The first Dynamo DB table acts as an event source. The Dynamo DB stream is the event store over there. The first Lambda function is acting as your consumer over there but then also acting as a simple event source itself. Because it's not telling a downstream service to do something nor does it use a store. All it's saying is, \"Hey, I got an event going over there and it uses SNS as its router to go to multiple components downstream.\" And over here it uses SQS as its sort of retention window for downstream services to reliably go and process it over there. So, this may be kind of a bigger example but you can replace any one of those components with your particular service and imagine what that particular workflow looks like. Do you want your consumers to be creating Kinesis streams in queues or do you create it on their particular behalf? All right. So, to kind of quickly bring it together be smart about what goes into your payload. Don't overstuff it with information you don't need. Have a good thought process about what your scenario is for it to be happening. If it needs your service to be involved in it make it a notification. If it's something that you can just pass on forward and then be completely disconnected but the payload in there. Second, surface an event store where appropriate. And, I believe things like event streams are probably critical components moving forward although there are scenarios where they're optional. But queues and streams give you a durable and potentially reliable way for events to be replayed, re-created and otherwise to move forward over there. And finally, think about routers as I wouldn't always recommend that you have to go and build routers. My hope is that folks like serverless and all of us together enhance the particular router construct over there. But, if you have to build one there are a few guidelines that you can follow around sort of having multiple support for doing secure access across the two, supporting for filtering, and making sure that you have clear semantics for failures and success that you can actually push forward on. And, my hope is kind of making sure that the more event sources that are out there the more [inaudible ::] serverless architectures of the future get. So, looking forward to all of you contributing to the event source ecosystem. All right. Thanks.",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603e9c",
      "title": "How you can avoid parking tickets with a Serverless reminder",
      "content": "If you live in a city, then you are incredibly familiar with these three, terrifying little words: \"street cleaning day.\" The worst part about street cleaning isn't the morning you wake up at :am, groggy, still in pajamas, and run barefoot into the cold streets. Oh no. The worst part is the morning you forget to. The morning where you innocently slide behind the driver's seat to discover, nested under your windshiled wiper: another parking ticket. Well. Here is your chance to fight back. And learn a little Serverless development at the same time! Today, we're going to make a super simple serverless parking reminder. Set up the environment We're going to create a cron job that sends us an SMS the night before every street cleaning day. This example uses the Serverless Framework, AWS Lambda and Node.js. You can install the Framework with:  Configure your Serverless service Once we're set up, we'll need to configure our Serverless service. In my neighborhood, the street sweeper comes on each second and fourth Wednesday, and each second and fourth Friday. I'll need to trigger a `schedule` event on each of those four days. Create a new directory. Then create a new `serverless.yml` file in your directory with the following configuration:  After giving our service a name and configuring the `provider` section, the key portion is in the `functions` block. We have one function configured, which we've named `parkingReminder`. It will invoke the `reminder` function in the `messenger.js` module, as noted by the `handler` property. Finally, we've configured four events to trigger this function. Each of the events is a schedule event, meaning they'll be invoked on a given schedule. In this example, I use cron syntax to list the four times on which I need my function to be invoked. Hooking into Twilio Now, this service is only useful when it can actually send you a reminder. To do that, we're going to use Twilio inside our Lambda function. (If you don't have a Twilio account, you can set one up for free.) Let's create a `package.json` file, then install the Twilio SDK:  Then, we'll write our `reminder` function in the `messenger.js` module:  Pretty simple! I create a Twilio client, then use the client to send a message inside my `reminder` handler function. Let's deploy our function to AWS:  Now that it's deployed, you can test out your functions by running `sls invoke --function functionName`. And the cost? Basically free. You can run this without paying anything. Lambda has a generous free tier, and Twilio offers a free trial. But even if you were paying full price, it would be dirt cheap. The example here would cost about $./month in Lambda fees and $./month in Twilio fees -- much cheaper than the cost of a parking ticket! See it on GitHub You now have a serverless service for reminding you to move your car for street sweeping! Feel free to check out the complete working example up on GitHub..",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603e9e",
      "title": "Matthew Lancaster - using even-driven architecture to transform core banking",
      "content": " Matthew pulled us out of the tech stack for a second to focus on what's beneath it, the foundational layer of the application pyramid: business drivers. This, he says, is what will turn everyone event-driven. Non-tech companies, even in old-school industries like banking, are increasingly becoming tech-centric. Matt's been working with core banking at Accenture for a while, and it comes with lots of legacy challenges. You can't just wipe everything clean and start over in a greenfield project, you have to respect and learn to integrate with legacy systems, some of which are well over a decade old. He's got some practical advice from his own forays into bringing banks into the future. Watch below or read the transcript for the juicy details. More videos: The entire playlist of talks is available on our YouTube channel here: Emit Conf To stay in the loop about Emit Conf, follow us at @emitconf and/or sign up for the Serverless.com newsletter. Transcript Matt: All righty, so I think we can take a couple of things for granted, just as industry trends. And I wanna take us a little bit outside of the technology for a second and just talk about the sort of business drivers that are behind a lot of this stuff, and why in many cases we need to move to an event-driven future in a lot of traditional industries, otherwise they're going to be disrupted, cannibalized, and something else is gonna come out of them, right? So there's this accelerating trend for every company to become a software company, especially in financial services where the product was already sort of at arm's-length, in many cases. Our friend from Capital One earlier can probably tell us quite a bit about that as well. And then just, you know, creating more interactive systems, creating new financial products and actually getting those out to the marketplace quickly. Responding to regulators in an agile way, right? All of this stuff has become an increasing challenge as they have adopted what I like to call the architecture of the Gordian knot, right? But really, everything are these big monoliths that are all interdependent and intertwined and kind of gross. So we need to make sure that we shift to nicely decoupled event-driven systems that can be released quickly, you know, microservices functions, all the good stuff we've all talked about, right? The challenge in a pretty traditional industry, in that case, is that you have systems that not only are years old or years old in many cases, but that have been continuously developed for or years, right? A lot of the early adopters of computing in the 's, 's and 's built a lot of these big mainframe systems, a lot of complex business rules and business logic, and logic that has been sort of updated slowly for an evolving regulatory environment and an evolving product environment, and an evolving way that customers expect to interact with the bank, right? And a lot of this logic has been there for a long time or has been updated, and you don't necessarily know what touches what, right? It's sort of our standard DevOps problem where, \"What's your unit test coverage on this code?\" We don't know. Somewhere close to %, right? So how do we actually build really interesting, cool products on top of that, remain relevant in the marketplace, but still, you know, still have to deal with the anchor behind the speedboat, so to speak? So a couple of things to keep us grounded. We can't replace Legacy with Greenfield quickly, even though we'd all like to, but we do need the ability to build on top of what is already there to move fast. So that's gonna become one of our core business problems to think through and our core technical problems to think through. In terms of, you know, customers demanding better experience, we all know that stuff. And the regulation's here to stay. We can't get around that. So, you know, when we talk about things like continuous deployment, continuous delivery, that doesn't necessarily exist in highly regulated industries because you have to literally have someone to sign off on certain things, right? So you have to work that into the process as well. So let's actually talk through this. So we have...you know, here's the business situation of nearly every bank on the planet right now. You have mainframes that are still running a large amount of the backend business logic. A large amount of the business is actually run through that, so trillions of dollars of transactions. Mainframe costs are going up every year, right, steadily, generally about % a year. That's a Gartner number. That's not mine. Then we have an explosion of different devices and different channels to actually interact with your financial information through this. And not only through your phone, your tablet, your laptop, but also through different services, like a lot of the stuff that will help you find a credit score or help you plan for certain savings and stuff like that. Those are all accessing your banking information, right? It's actually getting that stuff out of there means getting more information out of that mainframe environment in many cases. And we're kind of stuck in a catch- in the industry, which is write operations cost just as much as read operations. And we all pay the mainframe vendors for the privilege of using our own systems, right? So that's one of those major cost areas that we can actually attack immediately and get a bit of breathing room to start to innovate on top. But how do we unlock the data from that environment? And then we also have very slow innovation in many cases. A lot of that stuff, like I said before, you know, the architecture's all put together, the teams are structured in such a way that it's very difficult to get anything done. And you end up having sort of security theater environments that don't necessarily make our infrastructure and architecture more secure, it just makes it more difficult to do our jobs. So I'm not gonna cover all of that right now. What I do wanna talk about is a particular business case. So I have a team in Austria that...some of the technology's a little dated because it was /, but I think it's a really interesting business case of how to move to microservices, how to move to a really event-driven streaming architecture and still sort of coexist and build the plane in the air with the existing systems. So one of the things we took for granted is that we're not gonna rewrite and extract all of that business logic right away that exists in the mainframe. So the write activity, at least in phase one of the overall program, the write activity needs to stay where it's at, right? So anytime we actually make a transaction, it needs to go back through the mainframe, it needs to go back through all that nasty business logic, and actually post somewhere, right? But for read activity, we don't actually have to go back to DB, right? We don't have to go back to the big database. We can do something more interesting. So what we ended up doing in that situation was put a little reader sitting on top of the commit log for DB, right? So all databases are really only three things, right? They're the data and big binary blobs that are sitting in various storage partitions, the actual application logic of the database, and then there's a big, essentially glorified text file that is actually the single source of truth for the database, whether it's Oracle or DB or any of the old SQL databases, right? It's just a big insert, update, read, etc. All those operations are stored in that commit log. You play back from it when you roll back, etc., etc. So if we read the changes directly to the commit log and we sit it really close, and we re-replicate those changes out to, in this case, it was Hadoop. I probably would do it with something more interesting today, but re-replicate out to Hadoop. By the time the record was unlocked by DB, most of the time it was already replicated out, a sub-second replication for inserts or for changes to the Legacy database. So now we have a full replica of that database that we can start to do interesting stuff to because we have it in a really fast data environment, right? So all of this stuff is sitting on top of HBase. We can build some microservices running off the same JBMs because HBase is pretty...it sips CPU from the nodes, so we can sort of have co-tenant architecture there. These folks were in their own data centers and in rented data centers because the regulations in the EU prevented them from being in the public cloud. That may no longer be true soon enough, and we'll be able to use a lot of cool AWS stuff. But for the time being, it was locked in their existing environment. So one of the other interesting things here is that since we can stream this data out, suddenly all of those transactions we can feed to an event log, and then we can start to attach and do more and more interesting things with it. We'll get into one of those particular business cases in just a second. So when we did this, it actually took about seven months and eight people, right, to pull all of this out. I'll share some of the business results of it in a second, which were actually really, really interesting. One of the things that happened in the middle of this program was that a new EU regulation came down that had much more stringent fraud detection requirements for basically your commercial banking transactions, right? So most of the competition set for a client was running around with their...you know, running around like the world was on fire because they were only given seven months to implement this new fairly stringent fraud detection regulation. And you can imagine in that world, that's actually at least to their traditional waterfall mindset with really big release cycles and five-month in many cases integration testing cycles for core system changes, that's a huge, huge deal. So we were actually able to do that in three-and-a-half weeks because we just read the changes on the new data environment, read in UI changes from the mobile apps and from the web, and then looked for fraudulent activity and then we were able to kick people out from there, right? So we used our read copy of the data to do a new business function that wouldn't have been possible without essentially sort of grappling a nice event-driven architecture, nice set of microservices on top of the Legacy architecture and sort of slowly starting to build value on top of that, right? And it was actually kind of interesting because when you look at it, if somebody is accessing their account in Vienna and they're following sort of their normal patterns, they're probably just fine, right? If they're accessing it from Thailand and they're behaving really weirdly about filling out a mortgage application, you may actually want to engage...you know, pull the emergency brakes there, right? So actually being able to look at that data and look at where they're at, all of that is essentially data exhaust of write activity and then the read copy that we have, right? So we can suddenly start to do much more interesting things. So on top of that, some of these microservices started to extract business logic for new products, business logic for making modifications to existing products to make them a bit more customer-friendly and more user experience-friendly. We can slowly extract that stuff out of the mainframe because we've isolated a lot of the big mainframe components such that it becomes a routing problem to move around them as opposed to actually changing anything in Cobalt that somebody who's now and retired in Florida wrote, right? So some interesting stuff can go on there. And the happy accident of this, or actually the original business case, the speed to market and all that, was the happy accident. The original business case was just reducing mainframe cost. But within that first -month project, the mainframe cost was reduced by % because we reduced the CPU load on the mainframe by % just because we rerouted all of the read transactions that no longer needed to be there, right? So the first project paid for itself and it paid for the second project in just reduction in OPEX, right? So the message that I wanna leave you with, with that, is that in any industry there's really clever things that we can do with the tools that we have at hand, with a lot of the patterns that we've been talking about all day, a lot of the technologies to not only create new, innovative things, not only to essentially, you know, sort of flip how we're doing the user experience and the digital engagement of a lot of these customer-facing systems, it can also be a cost play, and it can also be a time to market and sort of, you know, mature play, right? That becomes a very, very powerful thing when we're trying to negotiate for money for a few shekels from the business in many cases. Well, you know, if we do this, we can also reduce costs and have a -month payback period. That starts to become music to a CFO's ears, right? So that got us to thinking, \"What if we could replicate this kind of success on top of multiple mainframe-ish or big JEE, or big sort of traditional monolithic environments,\" right? So how would we actually design a reference architecture where we could have a repeatable process to build on top of that? And right now, it's mostly focused in the banking world. I have a few folks who we're working with to actually implement this, but it should look strikingly like a lot of the things that we've already discussed today. You can see a set of microservices that are handling REST transactions, but they're only communicating with the back-end through an event stream. It kind of sounds like an event-sourcing pattern, right? You have a set of utility services that are listening to the event stream and then acting upon the rest of the system, and then we're able to actually, you know, have real-time analytics, real-time, say, next best offer. So if you can imagine the real-life scenarios of this, if you're filling out a mortgage form but we can tell halfway through that you don't actually qualify for what you're filling out, we can give you something that you do qualify for and potentially keep you as a customer because we're listening to the events that are coming off of you filling out that form. Or we can have a customer service agent literally share the same screen with you because we're just capturing the down events, right? Because all of this is...you know, they're synchronous transactions as far as submitting forms, what have you, but most of it is streamed over either WebSockets or MQTT or what have you. So a couple of other interesting things have come out of that. So it's specifically in the financial services world, but I suspect in many other places as well there's a really strong, almost religious attachment to the concept of a session, right? You need that sort of transaction integrity, right? I see laughing from the other banking guy here. It's like, \"Yeah.\" I suspect that some of the older VPs in many banks have an altar somewhere in their house where they sacrifice to the session gods, right? But they actually have a point, right? Because you need to be able to guarantee transaction integrity, you need to be able to play it back, you need to be able to send transactions in order to regulators so that they know everything's on the up and up. There's a lot of heavyweight behind that. And frankly, that's sort of, you know, it's how a lot of the back-end business processes work. So we need to have, you know, nice double handshake asset-compliant transactions in many cases. But what does the concept of sticky sessions leave us with, with our services, with the rest of our architecture? It has a massive tradeoff, right? We find it very, very difficult to be scalable, to be distributed, to have developers work on isolated pieces, right? So what we're doing here is a little bit different as we have this thing up at the top that we call the reactive API gateway. I've already been talking to the server-less folks about doing a little bit of integration with what they're doing. But what we care about here is, number one, that we can apply standard sort of API gateway as policies, RegEx security policies etc., to streaming connections as well as REST in a really, really lightweight way, and keep track of what customer or what node the particular set of transactions is connected to and be able to order those, play them back in order, send them out across the stream, etc., without having any concept of sticky session in the rest of the system. So we used probably one of my favorite sets of technologies that in the enterprise world they haven't got to use a whole lot. You know, I tend to be a PowerPoint engineer these days a lot and I got to actually write some error line code for the first time in like three years, and I was super happy about it. And then my whole organization was like, \"Hey, can you come up for error and actually answer our questions and do stuff?\" It's like, \"No, I'm coding. It's fun,\" right? But we wrote that in Elixir, built a lot of interesting stuff there. We're keeping track of the transactions via CRDTs so, you know, keeping them ordered, getting them tagged to a particular customer. I always like to call it a customer-centric architecture because everything's based around the customer transactions, whether it's commercial or whether it's, you know, personal commercial banking or whether it's, you know, business banking, etc. The other interesting thing here is this re-replication system. We've sort of industrialized that a little bit so that we can get data out of the Legacy system. And oftentimes the communication pattern back with the mainframe is we'll drop...if it's queue-based we'll drop a single message on the queue and do sort of a micro-batch, or communicate with it over actually hooking into the transaction manager, which is actually fairly parallelized itself. It's just an internal communication mechanism, so we can do a couple of different things. But what that ultimately does for us is it separates out the different sort of core pieces of the back-end architecture and allows us to, you know, treat them separately, right? And when we start to pull out functionality and deliver incremental value to the business, right, when they ask for X to be done in X amount of time with different systems of record, etc., we can pull that stuff out, eventually sort of reroute away from the mainframe, and maybe three or four years down the road have the Holy Grail conversation in most traditional enterprises, which is maybe we can shut this thing off, right? So one of the big things to kind of talk through there, I have another client where we're doing this in hospitality space and a couple of my colleagues back there are familiar with, where we're already two years down this journey and we've turned off half of the mainframe environment. And two years from now we'll be able to turn off the rest of it, right? But it was largely through the initial set of cost savings that we were able to bring in the initial sort of speed to market around some of their products, integrating new brands, doing new experiences for some of their hotel brands that were focused toward younger customers, where they...you know, we all expect a little bit more high-tech approach. So moving reservations and loyalty to a set of services, sitting in front of a big Kafka Stream, and then eventually hooking that into the rest of the customer-facing systems so we can make really interesting intelligent decisions, like maybe if you were coming close to the hotel and you were ready to check in that day, we can check you in and have somebody greet you by name rather than you walking up to the desk. And it's always funny, in sort of the coded business speech that a lot of these folks have. When you walk up to the front desk and somebody says, \"Hi, how are you doing? Can I see your ID?\" It's really code for, \"Who the hell are you and why are you here,\" right? Which is not a very hospitable interaction, if you think about it, right? It breaks the immersiveness and kind of the customer experience in that space. So if we can use all of the intelligence that we have to actually, you know, talk to you and know who you are, and already have you checked in, already have your rewards amenity ready, etc., etc., we're using the same event-driven paradigm that we were talking about in terms of the technology side of it to sort of hack the business side of it as well, right? A colleague who's here, I think one of his favorite things to say around this is maybe we can get rid of the front desk entirely, right? Why do we need queues in real life when we can have parallel streams and just serve people quickly, get them what they need and get them on their way? So, you know, how would we think about hooking onto those back-end systems in other industries, in other places, using some of these concepts of the standard reference architecture, pulling out some of the business functionality piece by piece, right, putting it into whether it's functions or microservices. I think in that case, most of the time it really doesn't matter, right? As long as we have a good set of patterns, and we've usually chosen a few technologies that we're gonna coalesce around. I see a lot of these efforts fail both in the startup space and in enterprise where when we move to microservices it becomes the Wild West, and there's different technologies that everybody's picking up. You know, there's a bunch of stuff sitting on Lambda, there's a bunch of stuff that some guy that is in the basement of one of the offices wrote in Go lang for some reason, and then there's a bunch of node stuff and a bunch of legacy.net stuff, and then suddenly there's different pipelines for all of this, and you have Docker or you have, you know, your event gateway. You have all kinds of stuff to manage the spaghetti that you've just built for yourself. You know, there's good, solid patterns that we can attach a few technologies to, and then sort of industrialize and repeat, right? And I think the hardest thing here is that moving in this direction, once we get the right technology sitting on top of the Legacy applications, and in many ways in the Legacy business, right, is to...you know, the technology is one leg of the three-legged stool. We have to figure out how to make sure that the delivery process and the engineering systems are in line, but I think more importantly, we need to work with the business folks and actually get them working in the same way where they're focused on the individual product areas rather than, you know, these big sort of integrations. It's not an easy thing, right? But when we can initially deliver, \"Okay, here's your cost savings. Here's your -month payback period and the initial project.\" Now, we can deliver in two weeks to a month for fairly large pieces of functionality that it used to take you years to get. You suddenly get folks that start to question their old religious beliefs, right? And then you slowly bring them into new projects, and then you get evangelists that can come down from the shining city on the hill that you just built and convert the rest of the masses. And suddenly, we're all living in a world that we want to live in where this technology isn't just the fringe stuff that we can all get excited about in the bleeding age, it's actually suddenly becomes the new core business systems that...you know, Sunday can become Sunday again because we don't have all the nasty outages and whatnot with the traditional error-prone systems are, you know, susceptible to. I mean you guys at Nordstrom, you probably never had a big outage of core retail systems anywhere around like Black Friday or anything like that, never happens. One of the great things about moving in this direction is we take so much load off of the traditional systems that is cost savings, that's great, but we're also acting as a back pressure valve, right? When you have asynchronous load environments or you have services that you have to stand up in a short period of time, if you're building that on technology that was fundamentally designed to lock threads until it tips over, you're gonna be in for a bad experience if you have more customer influx than you want. If you build it on something that's naturally designed to scale out, then you're building for resiliency and sort of rather than protecting from failure, which is the old mindset, you're essentially embracing failure and then letting it only affect one or two folks, right? So it looks like I'm running out of time here, so any questions or comments or horror stories from anybody else? Man : So you talked about how to read software from mainframe, what's your approach [inaudible ::]? Matt: That's a longer conversation. But the first part of the answer, which is the most cop-out piece, I promise, then I can get into more of it, but is carefully. But I think one of the soapboxes I often get up on is that we've forgotten some of the brass tacks stuff that we used to do in technology really well, like domain-driven design and some of the other fundamentals. If we separate out those big, nasty pieces of the mainframe into core business areas, then we do have to do some code analysis and see where things are ticking. But we can usually rope it off to a pretty good degree of certainty, and then extract it piece by piece into new services. And as we do that, rather than route those particular transactions back to the mainframe, we just intercept it and route it to the new services. And then it slowly gets replaced and strangled off, and it's replaced and strangled off with things that have all their unit test coverage, that have good definitions of done and can actually be completed, right? And so you get this sort of comminatory effect where you can start to tackle more of these a little bit more confidently, and then you can attack the really big ones that are spidered everywhere, right? And then eventually that knot, you cut it to death by strands, and then you can unhook it. Man : Do you feel like there's a maybe sort of like a doomsday clock on this kind of moving off the mainframes, or are you finding it hard to find people who can [inaudible ::]? Matt: Oh, yeah, absolutely, absolutely. I have a client in the U.K. who they had a major system issue. They had to bring back an -year-old woman from retirement to look at the system and actually fix the problem because she was one of the original authors. She was the last one left from about the people that had actually built it. And had they not been able to do that, they would have had to rebuild a good portion of it, and this is a big retailer. So you can imagine how screwed they would have been had they been down for more than two days, right, especially around the November timeframe. So absolutely, there is a big doomsday clock. And I think the hesitancy is that a lot of folks have tried to move to this a couple times and they failed because they try these big bang, big replacement projects where they can turn a key. What we're talking about is implementing the strangler pattern. My marketing folks have a nicer term for it, which we call it hallowing out the core, right, because strangler patterns sound slightly serial killery. But, you know, so hopefully we'll all be able to move in this direction as we move forward. And keep in mind some of the business case stuff because it really can convince the folks who don't necessarily understand the nitty-gritty of event sourcing and what have you. David: Any other questions? All right, thank you very much, Matt. Matt: Thanks.",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603ea0",
      "title": "Dave Copeland - contract-based testing for event-driven architectures",
      "content": " Here we have Dave Copeland, Director of Engineering at StitchFix and purveyor of consumer-driven contracts. StitchFix is an e-commerce company: they send you clothes through the mail, and if you like them you buy them. If you buy the whole box, you get a discount. So the engineering team is maintaining some logic about how to charge customers and how to apply the approapriate discount for the number of items they buy. The engineering team manages most of the operational side of the businesswarehousing, purchasing, printing packing slipsand each team owns and maintains their own software, and all of this software talks to each other. So it gets complicated to update different parts of the system without harming the core business. Their solution? Consumer-based contracting. Watch Dave's talk below or peruse the transcript for some real-world examples of how this works. More videos: The entire playlist of talks is available on our YouTube channel here: Emit Conf To stay in the loop about Emit Conf, follow us at @emitconf and/or sign up for the Serverless.com newsletter. Transcript Dave: Hi, thanks for having me here. We're gonna talk about Imagining Contract-Based Testing for Event-driven Architectures. Anyone know what consumer-driven contracts are? Contract-based, have you heard of that? Pact maybe? Cool. So, we'll talk about that. But technology, for technology's sake, is pointless and annoying. We have to have an actual problem that we're solving. So what problem does all this pertain to, right? So we build these software systems, right? And unless we're building a tightly integrated monolithic system, we have lots of different bits of software, particularly, the subject matter of this conference. And those pieces of software don't do anyone business thing by themselves, they collaborate together to do a business thing, a business process. And we wanna know if that works, and we wanna know that as we make changes to these little bits of software like, \"Does the business process they're supporting still work? Will the change I'm introducing break things?\" And we want to know that without clicking around web browsers and doing manual things. We want to know that, you know, in a more automated fashion. So, as mentioned, I'm a Director of Engineering at Stitch Fix. I'll give you a little back story of us because you'll need that to follow along with my example. So we're a personal styling service for clothes. And at the core of our company is an e-Commerce business, right? We're shipping things that we purchased at wholesale and we're selling them for retail and all that. The difference is that our customers don't get to choose what they get, we choose for them. So we have an algorithm that chooses what clothes we think they're going to like based on information they've given us. The human stylist will look at the output of that algorithm and decide what they're going to get. And then in our warehouse, we find those five items, ship them to the customer, the customer opens the box and sees them for the first time, hopefully, loves them, tries it on, pays for whatever they like, returns whatever they don't like. And one thing that's worth pointing out is that we, the engineering team at Stitch Fix, build all of the systems that do all of the internal operations. So, I was just talking about a warehouse, so we have software that runs our warehouse, we build that. Software that our buyers use, we have buyers just like Nordstrom's and Macy's would, they have software that we use to manage this whole process. The styling thing that I talked about, we have software for that. So everything is something that we write. And a lot of these systems interact with Synchronous HTTP services. But messaging and events are even more a thing that we do. We have a lot more of that going on and that is a big part of how all of our software works together to implement business processes. So we're gonna talk about one in particular called the pack slip. This is a picture of it. So this is what you get in the box when you open it up and it's got some information. So there's an order ID, right? So the warehouse knows what it is. There's some items, right? That's what you're getting. And we can see there's some metadata like the descriptions of them and their price. And then we've got this discount... charging a discount logic, right? So if you wanted to buy all five, you get a discount. We want to show that to the customer so they get excited about buying everything in there. So the process by which this gets created is kind of...there's this two parts. So we'll talk about the first part which is all about Synchronous services. And I'll talk about how we test those using contract-based testing. So we all know what that is. And then we'll see how that applies to messaging because messaging is more involved in this. Cool. All right. So, we've got the basics here, right? So an order ID comes into our warehouse management system and to generate a pack slip. It will contact the inventory metadata system that has stuff like the item description and the price. It will contact the financial transaction service to get that charging and discount logic, right? Because we don't want the warehouse management system having to know how to do discounts. It'll put all that stuff together and it'll put that into a cache. And then when we wanna print that thing out, the associate is able to grab it out of the cache and print it quickly. So the associate on the floor printing things doesn't have to wait for all these, like, really slow Synchronous Calls. So we wanna know if this works. So we've already got, like, three pieces of software involved in making this work. And, of course, each piece of software is maintained by a different engineering team. So at Stitch Fix our teams are aligned with different parts of the business. And so you could imagine that this could create a problem, right? Why would the finance engineering team need to know how pack slip printing works, just to make changes to the service that they own and maintain? Why would the merchandise engineering team have to know this whole thing just so they can make changes to the inventory metadata? And maybe if the team is small enough, everybody can understand how things work but as you get bigger, you just really can't. I mean, our team has engineers, which is kind of big but not really that big, and there's no way anyone can understand everything every time they make a change. So how can we make sure that when we change these ancillary systems, we're not breaking the core business process? So, this just demonstrates more of how our engineering team is organized. We all have our own roadmaps aligned with these parts of the business, and that allows each team to sort of deeply understand that part of the business. And the cost is, again, that they cannot understand the entire business in detail, in whole. So there really is a need for these teams to be able to change the systems that they own, without breaking everything that everybody depends on them for. So let's focus on the interaction between the warehouse management system and the financial transaction service. So as good engineers, right? We're gonna write a test. I work on the warehouse engineering team. I've written a test of my pack slip printing code, and part of that test is gonna be to assume I'm sending some requests to the financial transaction service. And then it comes back with some sort of payload. I'm gonna assume that payload looks a certain way and I'm gonna feed that into the rest of my test, make sure everything works, right? So we could run that against the actual service, but that, sort of, is very difficult and potentially breaks down at even moderate complexity. So, instead, we run it against a mock version of the service but we capture what happens. So in our tests of the warehouse management system, we're capturing the URL we hit and the payload that we got back, and all these expectations as a contract. So the financial transaction service, it can have a test and the tests sole purpose is to grab this contract and execute it against itself. Ideally, we could test against the real thing but that's not really feasible, this achieves that. So the contract is what the WMS is expecting to happen in production. And then the financial transaction service can actually execute that contract and see if it actually does what the warehouse management system is expecting it to do. And so if all of that passes, then we can have relative confidence that everything's working in production. And what it does mean is if the financial transaction service makes a change, it can evaluate the potential change against this contract and if it violates the contract, it knows that that change will break the warehouse management system, so we don't go forward with that change. And so this kind of is nice, right? This is a nice property of having Synchronous services because they're very static, right? This picture here, that you are not expected to be able to read, was derived from our infrastructure description. And it's all the applications and all the Synchronous services that they consume. And I'm able to do this because the application... an application that consumes a Synchronous service, like, it knows that in its code or configuration. Like, it might not know where the service is, but it absolutely knows, \"I need to contact something called the financial transaction service.\" So we can take that and do stuff like what I've described. So we can put a contract with every one of those lines and we can evaluate those contracts every time we make a change, and we can refuse to promote any change that violates these contracts. And we do this and this works as advertised. It's pretty nice and we don't have to stand up like every known service to run a test. So, what about messaging? Did that makes sense, kind of? Okay. So there's more to this pack slip printing thing then I let on, right? So the merchandise app, where our buyers, you know, do their business, they might decide to mark down the price of an item, and if that item is in a pack slip, we wanna regenerate that so the customer sees the new lower price. We might change the item metadata, like, there's a typo or maybe we've got a better description of an item, so we want the customer, again, to see that update. And then we could do something more drastic, change what items are actually in the order. And so then we want the pack slip to be updated. And so rather than have the pack slip or the warehouse management system, like, know all these details, it just consumes these messages. And whenever it gets one of these it's like, \"Oh, something changed that I care about. I'm gonna go see if there's a pack slip that has its item in it. I'm gonna regenerate it and put it in the cache. And now, I'm up to date, everything's good.\" So what we've introduced here is three new ways to break this entire process. The problem, though, is with these Synchronous Calls, right over here with the inventory metadata and all that, if that breaks it's pretty loud and obvious. If these messages cause something to not work, it's not obvious, right? You could imagine the price updated event. What if that gets sent to a different topic or a different routing key and it just never gets to the warehouse management system. We'll never know that, we'll never ever know. We'll potentially never know that these things didn't work. What if they're missing data? And so the warehouse management system ignores the message, right? We could potentially never know that this thing is broken. We don't want that. We wanna know that these things work. Okay. So, how could we apply the contract-based testing that we just talked about to this? All right. So let's talk about the interaction between the styling app and the warehouse management system. Now, they don't really know that they interact but we know that they do. So the developers of the styling application, right? They'll write a test of some feature like of adding an item to an order or taking an item out. And part of that test, regardless of anything else, that test is gonna also say, \"Hey, I'm sending a message.\" Like, \"When this happens, a message needs to get sent.\" So the test will assert that that happens. So we could capture that as an artifact to say, \"I'm guaranteeing, under certain circumstances, that a message that looks like this is gonna get sent.\" Now, the warehouse management system... it has a similar test for when it gets this message, right? And it will hard code some sort of payload, feed that into the test and say, \"When I get this payload on this topic or whatever, make sure that the pack slip gets regenerated.\" Instead of hard-coding the test data, it could just grab this guarantee and use that as input to its test, which... I'm gonna call an expectation. I'm making up all the works here. So, you know, they're not, like, official but... And so the cool thing about this is the styling applications actual output of what it actually does is feeding the test of the warehouse management system. But they don't actually necessarily have to know about that. So, what are in these guarantees? So there's some sort of schema. You're probably gonna want a schema that says, \"The payload is gonna conform to the schema.\" You can rely on that. You might have guarantees about the metadata. In one of the slides... we use RabbitMQ and that has this concept of routing keys. So that's, like, a part of our messages or whatever it might be. You also need some identifier, right? Because you want these things decoupled. So the warehouse management system wants to say, \"I depend on that message,\" without knowing that it's coming from the styling application, it needs to not have to care about that. Now, the expectation, on the other hand, it has to have the ID of the thing that it's hooked into. \"I'm expecting the message that someone has guaranteed me.\" It has its own schema and it may or may not be the same scheme as the one it sends. Like, it could be, we'll talk about that later, but it doesn't necessarily have to be. It just says, \"Whatever messages I get, they better conform to the schema and if they do, I'm good.\" They could have expectations about metadata as well. And you could also imagine that you might wanna have many different types of messages to feed different test cases or however that goes, right? Like, you might treat a markup and a markdown differently, so you might wanna be able to simulate that. Okay. So, how, if we had all this in place that I'm hand waving for the moment, how could the consumer of a message safely make changes, right? Which means how can it make a change and know that it's not gonna be broken in production? Well, first, everyone has to agree on how we're defining guarantees. The producer of the message has to have some sort of test framework that will produce this artifact that I was talking about. And we need to have some sort of central authority that has all of the artifacts in it, right? So that when I'm writing the test, as the consumer, I can register my interest in this particular guarantee. And when I run my test, it will bring it down and it will feed it into my test case and make sure that I'm good. And if it passes, I can be pretty confident that I haven't broken myself by the messages that I know have been guaranteed are being sent. So, the producer can also benefit from this. So the producer will never break because the producer is the authority of the messages. But the producer wants to know that they're not breaking anyone else. So, as we've mentioned, the consumer is grabbing this artifact and running a test. Well, the consumer could produce its own artifact of what happened, what it did, what it was expecting and what the results were. And it could publish its expectation up to the central authority. Now, when the producer makes the change and it's running its tests, well, it could go ask the central authority, \"Who is expecting to get this message that I'm testing, that I'm sending?\" And it could pull all of those down, evaluate those against itself to see if the change it's making is gonna break anybody that's expecting that message. Kinda makes sense? Yeah. Right. Cool. So, the failure modes. There's one interesting one. There's two obvious ones but there's one interesting one. The interesting one is first, right? If I'm a consumer, I'm running a test and I'm expecting a message to be sent, and there is no guarantee in the central authority, that means I'll ship my code and it'll never be executed. I'll probably wanna know that before writing it. If the guarantee exists but my tests fail, that means when I go to production, I'm broken. Producer checks expectations. If they aren't compatible with what the producer's going to produce, then we know that at least one consumer is gonna be broken in production. So we could know these things in advance, in our CI system or we could prevent changes that cause these failures from happening. So you could write...you could, maybe just decide that everybody's using the same schema everywhere and kind of enforce that. And it sort of feels a little simpler, right? Like, all consumers of a message must accept the schema that the producer is sending and kinda do it that way. But if you had a system, like I've described, there's some additional benefits which are, I think, kind of interesting. The central authority, it could listen to the actual production traffic. It could receive every message sent in production and do stuff with it like, perhaps a message comes in and no one is guaranteeing that that message will be sent. Could mean that we were lacking test coverage, could mean unauthorized access to the messaging system, like, that would be good to know. I mean, how could you ever know that now? That would be very hard to know now. What if a consumer is expecting a message that is, in fact, guaranteed to be sent but, in reality, the message hasn't been sent for weeks, right? That would be really, really hard to detect because it basically means that your code just stopped running. And that can often be harder to know about than your code ran and broke. So this could detect that, right? \"Hey, you want this message and it hasn't been sent for a long time. Maybe you wanna look into what happened.\" It could also document the inter-dependencies between all these systems, right? Of course, we want them decoupled, of course, we want them to be able to change. But, at any given moment in time, there does exist a mapping of who's sending what and who's consuming what. And this system could document that, and that would make it easier to understand how these business processes are actually implemented. If I need to make a change to the pack slip printing process, it's pretty hard to piece together that those pieces are involved in this unless you happen to build it. So this could help figure that out. Okay. So, I'm gonna go one level deeper from what I did. I'm still hand waving, right? Because we're still imagining things. But how would the verification, that I hand-waved over, how would that actually work, right? So the guarantee I mentioned is a schema. So we could make sure that the messages that are getting sent are, like, you know, conform to that schema. Expectation also is schema and, as I mentioned, they could be the same. You could just decide that they're all the same but that has this, like, tight coupling, right? If you just assume everyone has to have the same schema, yeah, you could do that but you might not want that tight coupling, right? Perhaps the consumers only care about a couple of fields and some payloads, so why should they be expected to care about the ones that are extraneous to them? So, I was trying to find this as a concept. So if the guaranteed schema subsumes the expectation schema, right? The schema that's going out from the producer subsumes the schema that everyone's expecting to receive, then everything should be fine. I couldn't really find a formal definition of this and it might not be a real thing. But let's have an example. So, here's a schema of an item price change event, right? So, we've got three fields, they're all required. I left that out, but they're all required by item ID, old price, new price. So our consumer doesn't care about the old price, the consumer just cares about the item ID and the new price. So here's the schema the consumer's expecting, right? So we can see by inspection the producer's schema subsumes this one, so therefore, any message that conforms to the producer's schema will conform to this. Meaning, the producer can add fields. Like here, the producer's adding the user ID of who initiated the price change. Well, the consumer doesn't care about the ID, they're just gonna ignore it anyway so this is totally safe to add, this still subsumes. But the producer could change the name of a field, right? It used to be called, \"New price,\" and now it's called, \"Updated price.\" Well, this no longer subsumes it and it will actually break the consumer because the consumer's relying on that field to be there. The consumer can break itself. The consumer can say, \"Hey, I'm expecting this field called reason to show up in the message,\" but the producer never promised to send that. So the producer's schema does not subsume this one, so the consumer will break. So, again, this is kinda hand wavy but we're getting closer to something real. We saw some brackets and colons so that's one step forward. Yeah. So, schemas are complex. Like, those are really simple ones and, like, the stuff you can do like in a JSON schema is very complicated. And I don't know that you could programmatically check the subsumption concepts. Like, I couldn't really find... I found a few people asking for it but no one had said that it could be done, so probably might not be a thing. Uniquely identifying these guarantees, right? Think about the systems you work on that have events or messages. Like, how many different ones are there? Can you think of a name for all of them that is comprehensible but doesn't introduce the weird coupling, right? So we could call it, \"Styling app changes order items,\" but that's saying the styling app, like, we're not supposed to know that and we're not supposed to care about that. And the owners of the styling app might wanna send this message from somewhere else later. And so now they have this weird guarantee ID that has an app name that's not even sending it. That doesn't make any sense. You could too vague, right? Changes order items, like, \"Is this the only place that's gonna change order items?\" Like, maybe not. Unclear how to specify this. And then you need to motivate the developers to actually write these tests. So, I talked about consumer-driven contracts for Synchronous services. We use a thing called, \"Pact,\" and it has a way to write the test and it's pretty nice, but it's something. It's friction that the developers have to do and it's hard to enforce. You have to do it by code review. So whatever this system is, the developers have to be relatively incented to do it and it has to be not that hard or they're just not gonna do it. And, you know, this stuff has to be built. I've been imagining things this whole time. The system I'm imagining, like, it doesn't exist. I think it could exist. So, I took some time on the plane over here. So, I live in Washington D.C. It's about a five-hour plane ride. So, I figured, \"Can I hack this together in a way to see if this concept even works?\" So, I set up some producers and consumers, right? So we've got this item price updater. It sends a message like that which says the price of this item is updated. So that connects to Rabbit and just sends a bunch of messages. The price cache is a consumer for whatever reason keeps a cache of the price of items. Maybe our finance team wants that. And then we have the pack slip updater which, you know, kinda works with the example that we've been talking about. So, I had this set up locally. Producing, consuming, all that's good. So, I wanna write a test to do what I'm thinking and I'm really sorry this is Ruby and RSpec. I will explain the things that are relevant. This is what I do so it was the quickest thing. So, generally, we're saying, this is the thing that we're gonna initiate that we're gonna test. Our updater is updating the price of an item and then what should happen when that happens. Well, two things, one is very obvious. It should update the price of the item, who cares. But this is the interesting part here. So let's look at that. And, again, really sorry that it's Ruby. So we're gonna expect that we got a message sent on the bus. This thing, Puka transmitter, that's like a API we use to talk to Rabbit. So this is saying like, \"A message should have been sent when the code was executed.\" So this thing, the thing that implements this is gonna check to see if the payload that the code sent matched the schema. So the producer is providing the schema, we're gonna check against that. And they're also gonna check that the payload that was sent matches this, right? Because you wanna assert that some values got into the payload. And this is gonna produce that guarantee that we talked about. So, here's the schema that we've written that says, \"We will conform to the schema.\" And then we run that test that I showed you and it outputs this. Don't worry about the tiny font. So this is, again, I made this up but this actually does work. So here's the ID of our guarantee. So this is how others can register their interest in this message. Here is the schema just copied from what we saw. And then here's an example payload that other people can use to feed their test if they want to. This is the payload that was actually produced from the test, so this is kind of as real as it gets without actually integrating. So that exists, so what about the consumer? So, here's the consumer and I've bolded the thing that is important, but the consumer is doing things opposite, right? The consumer is saying, \"I want something fed to me as input to my test.\" So that's what this first bolded thing is doing. It's saying, \"Send me a message and I want... this is the one I'm guaranteeing.\" So it's gonna find from the central authority, which was my laptop, a guarantee with this ID. And then make sure that the payload that you're gonna send matches my schema, right? So the consumer is saying, \"This is the schema I expect.\" So it'll check that and then write out an expectation with some metadata so we know kinda who's expecting this. So this kinda doesn't matter but turned out to be interesting and needed. And then we execute, you know, we execute our code and see that everything worked. So this is the expectation, sorry the slide's wrong, expectation that is produced by that test, right? So we've got the ID of the guarantee that we are coupled to, so that's the same thing we saw in the other one. We've got our schema, and you'll notice we only have two fields because we only care about those two fields. We don't care about the third one. And then here's the payload that was generated and fed into our test. So we know that that payload got into our test and that we passed. Okay. So the PackSlip Spec, right? Same consumer, slightly different. So it's overriding the sample payload that's gonna be sent into it, but the overriding payload is also gonna be checked against a schema. So if we were to write something in here that is invalid, it would fail the test. Good. So, this does work. Here's an animated GIF. So I'm running that. And so we can see that the producer correctly figured out that it sent the right thing and it knows these three consumers want its messages, and it checked and everything's good. The consumers similarly less exciting output but the consumers execute all that code that we just saw and verify everything that I just said. That's cool. So the proof is in the pudding though, let's break something. So let's say we're working on a new feature and our consumer, and we wanna reason that item price was changed. This will break because we're not gonna be sent this. So when the producer goes to verify, it's gonna pull that expectation down, and low and behold, it detects properly that this consumer's gonna break. This consumer's expecting something from it that it is not giving. And so therefore, there's going to be a problem. And we get a decent explanation, courtesy of the Ruby stuff that generates this. And our test failed. So, how real is this? I've shown you animated GIFs, I've hand-waved. There is a GitHub repo, you could go look at it. All that stuff is here, it actually does work. It just runs locally on your laptop. But it was enough for me to feel like this concept has potential and I think I want to use it, right? This seems really handy because it kinda does what, you know, what I was hand-waving that it would. So that's kinda cool. Fortunately, this is the last slide. So that's all I have. Follow me on Twitter, come talk to me for a job or read my blog. And, thank you.",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603ea2",
      "title": "How to Test Serverless Applications",
      "content": "Serverless applications are quickly gaining in complexitytesting is key. Ive been building the Serverless Framework for years now, and during that time it's been my focus to create as smooth a testing and debugging experience as possible. In this article, Ill share some techniques that you can use with the Serverless Framework to test and debug your serverless application during development. I'll focus on testing serverless functions, since this is where most of the development happens. Unit testing This one is a gimme. You should always start with unit tests, whether or not your codebase is serverless. Forget about Lambda, the handlers, the eventsjust organize your codebase in an easy-to-test structure that follows your languages best practice. Your handler should always be a thin layer that uses modules out of your code library. If those modules are well-covered with unit tests, then testing the serverless part of your application (i.e., the handlers) will be easy during the integration tests discussed below. To demonstrate this, here's what your handler should look like:  As you can see, the handler itself doesn't contain any core logic; it just uses modules that should be unit tested independently. Read here for our much more in-depth guide to unit testing. Integration testing Now that youve covered your codebase, its time to move on to your handlers with overall integration tests. Let's see how all of those units youve been testing individually work together. Work with stages Since youll be interacting with the actual infrastructure pieces in your application, you'll need to make sure you stage your application during development. Set up a dev stagethe default when using the Serverless Frameworkfor all application infrastructure (databases, buckets, etc.) that your codebase will use during the integration tests. Set up event mocks You'll also need to have some event mocks prepared for all your handlers, depending on the type of event the handler is expecting. For example, if your serverless function is subscribing to an S event source, make sure you have a JSON file that mocks the S event that AWS sends out. You can get that by trying it out yourself only once on AWS, and store it somewhere for future reference. A super simple handler would look something like this:  Deploy that, and upload a file to the bucket that this function is subscribing to. This will invoke the function with the S event. You can then take a peek at event shape by looking in the logs with `serverless logs -f resizeImage`. Copy the logged event object into a `mock.json` file. During development, you don't need to go back and forth to S for debugging anymore, you can just invoke the function directly with that mock event using `serverless invoke -f resizeImage -p mock.json`. Your development cycle will be much faster this way. Local Lambda invocation Let's test those thin handler layers and how they fit in within your codebase. You can do so by invoking your function locally, using the `serverless invoke local` command. Provide it with the function youd like to invoke, and an accurate event mock. (...Which youve totally set up already, right?!) For example, let's test a function called `createThumbnail` that is subscribing to an S event source. We'll do this by putting the S mocked event in a `createThumbnail.json` file, and then we'll run: `serverless invoke local -f createThumbnail -p createThumbnail.json` While `invoke local` doesnt emulate Lambda %, youll still be able to find issues in your codebase quickly without having to wait for a deployment. Using the Event Gateway Weve recently announced a new project called the Event Gateway that helps manage all events happening in your serverless application. You can use the Event Gateway to locally and rapidly test your functions. We'll need two terminal sessions for this. First, spin up the Event Gateway in your current terminal session with `serverless run`. Then, open another session and emit events with the `serverless emit` command. This will invoke any function that is subscribing to that event, and youll be able to see the result of all function invocations in the `serverless run` session. Remote Lambda invocation After testing and debugging your serverless application locally, you probably feel confident enough to deploy your applicationat least to the dev stage. Keep in mind that the local environment is a bit different than the actual deployment environment: e.g., AWS Lambda limits dont apply locally, so you'll need to be careful to make sure you won't hitting any of those limits on deployment. Using multiple stages is a great way to have more confidence about your tests. You can have a QA environment that is an exact replica of your production environment, since they're running on the same infrastructure. This can ferret out hidden bugs you might miss when developing locally, such as issues with your function's IAM permissions or limitations around Lambda. Just like local testing, you can pass a mocked event to the `serverless invoke` command to test your deployed functions. But even better, now that your functions are deployed, you have the additional option of triggering the real event. In our `createThumbnail` example above, you can actually upload a photo to the S bucket in the dev stage and see how the `createThumbnail` Lambda reacts to that event. Investigating Internal Server Errors During deployment and development, youll almost always be hit by the unhelpful `internal server error` from Lambda. To figure out whats actually going on inside your code, youll need to check the Lambda logs. Open your terminal and run: `serverless logs -f createThumbnails --tail` Notice the `--tail` option. That will keep an open terminal session and listen for log events as you invoke and test your functions. Just keep going back to this terminal session whenever you get an `internal server error` as you test your function. One small trick I like to do to avoid this `internal server error`, and know exactly whats happening on invocation response, is to wrap my entire handler code into a `try/catch block`, or a `.catch` block if its async). Then, instead of throwing an error directly, you can pass it in the handler callback:  Recap Test your application in the dev stage and make sure that everything is working as expected. Then, you should feel safe to deploy your application to QA or production. If youve resolved all the code-specific errors you find in the dev stage, it's unlikely they'll crop up again later. But you might still face some rare infrastructure errors (e.g., 'maximum stack count exceeded'). Those are AWS-specific errors that sometimes you cant avoid, and unfortunately theyre outside the Serverless Framework scope. Happy bug hunting!",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603ea4",
      "title": "Shawn Burke - building the Catalyst serverless platform at Uber",
      "content": " Shawn Burke knows something about scalehe works on scalability infrastructure at Uber. The number of total rides they've served has roughly quintupled in a mere ~ years. He arrived at Uber to discover a flurry of snowflakes: aka, the visual multiplication that results from having + microservices and languages in regular use. Serverless was a great way for them to deal with this; they could decouple consumption from production across a variety of domains. That simplicity was a big deal. They ended up building a very dev-centric serverless platform called Catalyst, which you can learn all about in Shawn's talk below (or by reading the transcript!). More videos: The entire playlist of talks is available on our YouTube channel here: Emit Conf To stay in the loop about Emit Conf, follow us at @emitconf and/or sign up for the Serverless.com newsletter. Transcript Shawn: Hi, folks. So, everybody here today has been thinking about great ways to use serverless architectures. You're probably been thinking to yourself, \"Gosh, I would sure like to build one of those systems.\" And so, that's what we decided to do at Uber, and today I'm gonna take you through that process of how that happened, why we're doing it, and we're gonna kind of get into the muck about how we built a little bit, some of the concerns that you have to think about if you build a system like this, for scale. So, why serverless at a company like Uber? It's really about simplicity. So, when I arrived at Uber...so, my background is, I worked in developer tools at Microsoft for like years, I had a startup adventure and then ended up at Uber about two and a half years ago. And I arrived and what I was most surprised by, was the amount of complexity that was found in the ecosystem there. So, last few years, the world has been, like, \"Microservices man, we just got to do microservices, this is gonna be amazing.\" When you have a company that's growing as quickly as Uber has grown...so Uber, when I arrived was growing very quickly on three axes. One axis was the number of developers that we had at the company. I was engineer like, , a week before I showed up they had like engineers. So, number of people on the platform was scaling and then the amount of load on the platform was scaling, like Uber trips. It's insane, we did our billionth trip in like, late after I'd been there a couple of months. Probably you saw on the news, we've done billion as of, I don't know, a couple months ago. And, you know, if you calculate the slope of that, it's insane. So, for all of the good times in \"The New York Times\" and \"Hacker News for Uber\" the business is still growing really quickly. And then the number of businesses that we're getting into has also been scaling quite rapidly. So there was trips, and then there was pool, and then there was UberEATS, and then there's freight, and then there's just a lot of other businesses. So this puts a lot of pressure on infrastructure, and just to be honest, the infrastructure there wasn't quite ready for that because they had made some choices about growing really quickly. And those choices that allowed you to grow really quickly early in your career as a software company become very painful later in your career. Specifically, when I arrived there was well over micro-services, there was languages in regular use, and then you take that matrix and you multiply it out, what happens is you have a bunch of snowflakes. You have a bunch of services that are built a little bit differently because you can't write good frameworks for four languages, you can't write good client libraries for four languages. So one team's client library, that's a Go team, so they have an amazing library in Go and then the Python library, sort of, sucks. And then another team they're a Python team, so the inverse happens. When you want to add a new feature to your RPC framework, which one do you add in first, how do you roll it out? There's just complexity up and down the stack. So, serverless was a great way to deal with this because it allowed us to decouple consumption from production across a variety of domains, whether it's HTTP or RPC, we use mostly Thrift at Uber. Whether it was consuming Kafka, we do, like, a trillion Kafka messages a day at Uber, very big Kafka shop. So, that simplicity was a big deal, that ability to put an abstract platform between, between our customers, or our users, our developers, and our infrastructure. And then, if you put that abstraction in place, you get some really great benefits out of it. Not only do you make it better for your new developers coming in because your onboarding got simpler and your docs got better, but it allows you to do some really cool things on the backend as well. Because, now instead of developers linking to all the libraries directly, they're linking through this sort of abstract, \"Hey I just want this to happen,\" sort of, an interface. Then you can do some cool stuff that I'll talk about here in Catalyst as a way to go. And so the first question that people ask, it's a good question is, \"Why aren't you guys just using AWS Lambda?\" And so there's a bunch of reasons for that, the major reason is that, we are not in an AWS shop in production. We run our own data centers and are moving towards a multi-DC strategy. We have to because of the number of places that our business operates in. In terms of around the world different, localities things like this. But also, at our scale, we need better introspection to the system than we get from sitting on top of another system. In terms of, what are the types of messages transports that we support, how fast is it, what are the SLAs guarantees for how fast messages are routed, can we get at the messages, can we inspect the packets? Lots of stuff like that. So, naturally, as an engineer, I was, like, \"Sweet, let's build one, that sounds like a good thing to do.\" This was when the Seattle office was new, I had just joined the Seattle office, I was the fourth hire there. So, I got the charter and we built a team and we built it. Yeah, and then one of the goals that we had here was to be able to really add different sorts of event sources into the system. So, what were the goals for Catalyst? I wanted this to be a very, very developer focused product. So, the tagline is essentially, \"Write your code, tell the system when to run it, and you're done.\" And so we tried to keep that promise throughout the entire system. For example, Catalyst runs on your local desktop just fine, just like it runs in production. One of the goals I have for things like this, is I want to be able to write code on an airplane. That's my goal. If I can't write code on an airplane, something is wrong. So we built a system that runs happily on a local machine, and we came up with an architecture that is actually exactly the same locally, as it is in production. So it's really nice and I wish I had time to give a demo here today, maybe we can do it at Auckland later. Overall, we wanted to dramatically simplify the process of developing business logic, our developers had a tremendous amount of other play code in their services. As these thousands of services were proliferating, each of them had this giant stack of like setting up its RPC server and getting configuration running and getting logging set up. Get the stuff out of there nobody cares. And then pluggability, we had four languages we pushed hard to get down to two, we've, sort of, focused on Go, or...Google tells us we're the biggest Go shop in the world, I'm sure they tell everybody that, but we think we might be the biggest Go shop in the world. So, we really focused on Go and Java for some of our,sort of finance and streaming cases. And that multi-language support was important to limit it to the fact that we didn't have to write every single library for every single language, and when we get to the architecture in a second, I'll show you how we solve that problem. And then it has to be minimal compute, minimal infrastructure tie-in, so we have a specific Uber compute, we run on Mase us Aurora, but we want to be able to run on AWS or GCP or these other things. It has to be really fast. So, the goal we had is a five-millisecond P tax for running a catalyst versus handling your own thing. And so, those are the goals that we had setting out, in about a year ago, is when we started building it. And so, we wanted to end an experience, we want to deliver a product so that when developers came to Uber and did their job, they felt like they were interacting with a product and not a sort of a bunch of different technologies. There's a easy fallacy that a lot of organizations fall into, which is, well, the metric system is not that hard, you go and you look at the docs and figure out the metric systems work.Okay, now the auto-scaling system's not that hard, you go and look at the auto-scaling system and see how that works. Okay. So, logging system's not that hard, on and on and on and on and on, you get this proliferation of not that hard technologies. But you end up falling back to tribal knowledge, because until you've learned how they work, it's too hard to discover them, no we're not consistent. So, we really wanted to fight that battle, so with Catalyst, batteries are included, we make sure that developers logging, metrics, config., local run, running into bugger, all these things just work out of the box. No code needed. We wanna have, you know, important breaking glass so that you can get to the underlying systems if you need to. But for the standard developer, it just works. So, that's the product level experience that we wanted to deliver. And then the other thing that we wanted to deliver, and we'll talk about some of the concerns here, is essentially, we wanted to insulate our developers from a large set of the distributed systems concerns that they face building stuff at scale. So, a lot of cases here are difficult. I mentioned we're a super heavy Kafka shop. What happens when one data center fails over? How do you deal with the fact that now you're running instances in another data center, and they're on different offsets because the Kafka's aren't actually connected, they're asynchronous replication? How do you deal with that? It's not something we want product level, full stack engineers having to reroll every time. Just generally, capacity, so you have three data centers, how do you make sure that you have the right capacity in each data center so that if usage is growing in the main data center and it's New Years Eve, and something bad happens and all that traffic flips over to the other one, that you don't end up having a cascading failure? These are all really difficult concerns, both from a, \"How do you manage your capacity?\" Point of view, but also from a correctness point of view. How do you replicate this over, how do you make sure that if you start writing your second data center, and the other one comes back up, the systems come back to their direction? So, we're really trying to insulate developers from that with catalyst and make sure that it just works. And then the final thing that I really wanted to deliver was this just really enjoyable loop that we all get into. This, like, write code, test the code, debug what's wrong, deploy it, you know, rinse and repeat loops. That's what you're really focused on, you're not, like, trying to figure out how things work or frustrated that you just want to... You know, all of us, when we build something, there's this kind of motion we go through of getting everything set up. And you just really want to write your first line of code and hit that breakpoint and then start really doing your thing. And how fast can we get people to that? And with Catalyst, we've gotten people there in about seconds from nothing, so works pretty well. So let's see how Catalyst works, so I'll show you some Go code first since we're mostly Go and the interesting thing about doing Go first, as our first language, is that Go is really hard for this sort of stuff. Go does not have any, sort of a pluggability model, there's no library model, at runtime, everything has to be compiled together. The dependency management is still young, very difficult, and there's no metadata, there's no attributes or things that you would use to, sort of, add functionality of code. So, we went about a bunch of different models for this and I'll show you the one we settle on here. So with Catalyst, you write a handler, hello world handler, just a plain old function, not a big idea. This is the code that you wanna run your business logic and then you gotta tell Catalyst when to run it. So what we have is, essentially, we'll call this registration handler for you, we parse your code at, we have it built to a parse of the code and generates code for you. So what it's gonna do is your handler run, you declare your age to be handler in this case, and then down here, you tell Catalyst, Catalyst register, tell it the handler name and the route and the method, get post, put whatever. That's all you need to tell Catalyst, we'll figure out the rest. Likewise, Kafka, exactly the same experience. Write my handler, tell Catalyst when to invoke it, all Catalyst really needs to know is the topic name, it can figure everything else out and then, that's it. Because we parse code, we can make it do other smart things. So, if you notice, that message there is a byte array, it's just a raw message, that's not very fun. Maybe your topic has got strings in it, so you just change that to a string Catalyst will figure out what, sort of, marshaling you need to do, to do that based on the code that you wrote. But usually things aren't strings or raw byte arrays, as we saw in the last talk, usually, they're Avro or some other encoded binary format so maybe it's JSON. So you just tell Catalyst that it's JSON and we figure it out. So, we're able to do that deserialization for you automatically, we're able to send error, smart errors if it doesn't work, and just free developers from a lot of that sort of stuff. Batteries included, so there's this thing called a context up there in the message. Hanging off that context is some accessor functions to logging, to configuration, to metrics, so that when you decide you need to log something or look at configuration, we've already worked out for you. So, the metrics interface you get is already tagged with your project name, and your service name, and the hostname and all that stuff that you would have, otherwise have to manually code up. The logger already has the same stuff on it and we have configurations pre-baked. So, you can handler level configuration or project level configuration but your code only sees the configuration that's valid for your scope. So, we also support Java, this is how that sort of same functionality looks in Java. And I'll let you stare at this for a second, it's way nicer in Java, isn't it? Because there's attributes, and just metadata, you just say, \"Hey, it's a handler, it's Kafka, there's the topic name, off you go, so. Architecturally, we can do this really simply, building a new language support in the Catalyst is a person weeks effort, it's not very difficult. And how do we accomplish that? So, first, before I get into the nitty-gritty details, some glossaries. So, we're gonna talk about a few different things, the first thing is a source. A source is Catalyst abstraction around how to get an event from the outside world into Catalyst. So, there's a Kafka source, runs a Kafka ingester. There's an HTTP source that runs an HTTP server, there's a thrift source that runs a thrift server, it goes on. We have an STK, you can build these, so these are extensible. Anything that you can sort of model as an external event, very easy to build a source on for Catalyst and we've got, sort of, a growing ecosystem of these at Uber right now. A worker, when I talk about a worker, it's the binary that hosts user code, it's the handlers we wrote, end up in a worker. And then in the worker is handlers and then a group is a, just a grouping of handlers. Developers can group them however they want, but that's the inter-deployment, so when you roll out a group those are the things that version together. So, if you build a big project, you probably have a group of your Kafka handlers and a group of your RPC front ends and those are the things that you would version. So, how does it work? So we, sort of, double-click into the infrastructure here and we'll get into the details of how this works. So, at runtime we have three main components, I already told you about sources and workers. We have a component called the nanny, which is a very thin layer that manages these other processes, that manages process lifecycle and monitors metrics, and it receives what we call goal states and we'll talk about goal states in the second. But this is essentially the same as our local...this is a container in production and this is local, what we run locally as well. So, this is our, sort of, unit of deployment that we can run locally. You can attach a debugger to these, and the sources themselves and the worker have STKs that we've developed that you build on top of. So, you get things for free, like metric, so no matter what source you have, we emit like traffic metrics, how many requests came in, how many were successful, how many had errors? You get all that stuff for free. More importantly, I mentioned this leverage point, we can do capture, replay, generically. So, with any source, you can run a command on RCLI and say, \"capture traffic,\" and it'll capture traffic, bring it back and you can run it locally, we're working out the PI concerns for that, but from a technology point of view we can do that generically across Kafka and HTTP and everything else. And so, we've got a bunch of new stuff coming that we think is gonna be really exciting there, but that's that's basically how that works. And then the important thing here is that Catalyst is a shared nothing architecture, meaning at runtime, out in production, the Catalyst Control plane can go down, your handlers just keep running. There's no intermediate layer in between, so all that stuff is local, super durable system design for availability. So, let's walk through what it takes to run Catalyst on your user desktop. So, you run this command called Catalyst, we've got a CLI that has all of our functionality in it, you scaffold out your project, you add some code, your run Catalyst Run. When we run Catalyst Run, we do two things, we build what's called a manifest, which is metadata file that has, like, oh, it's Shawn's project, it's got a Kafka handler and it's got an HTTP handler, and it's got some other stuff and then we build the actual binary, we don't care what language is after this point. Once it's built, we don't care, it's just it's just a package. And then, so this gives you an idea of what's in the manifest, it's just a Jason file, it's got, like, oh, I'm an HTTP source, this is the method, here's the path, same thing with Kafka. So, if you think about it, architecturally, what happens is this manifest goes up somewhere and then we can use this manifest to figure out which of those sources to start up and how to connect them all together, so this is how we glue the whole thing back together at runtime. So then we send that manifest to the nanny, it downloads the HTTP source and the Kafka source, it downloads the worker binary, it makes sure they're all healthy, it connects them together, and then in comes traffic. That's pretty much it. So at the, at a user desktop, what's cool about this is that we actually have a process we can attach to with a debugger now. So that worker process, we have a mode where we'll launch that thing for you and wait, and so we can actually, sorry. We have configuration that allows you to bugger to launch that, the nanny knows to wait. So this means, now you have debugability and you can actually run them, you don't have to rely % on unit tests and other sorts of mocking, you can actually run it. We have a capture, replay for Kafka, so you can run a local Kafka, capture production traffic replay it locally as many times you want and do it in this scenario, so you have a debugger and you can run tests, it just really tightens that loop that I mentioned. So then at runtime, at production time, that same shape is what we run a container in, so it's exactly the same as I mentioned the same infrastructure that runs out in production. So it works great. Super reliable, it's easy to unit test, it's easy to run locally, it's easy to run in CI, it works really, really well. So let's talk about what happens when we get a request. So, the source, if you're gonna sit down and write your own source, say you're writing a Kafka source or, say you're wanting to write an AWS SQS source, which you could totally do. You would take our STK, you would write a piece that would go and pull SQS based on configuration which we've been in that manifest because then it would know which thing to pull. And so it gets that manifest, it knows which things to pull or which...let's see, SCS ques to pull. And then it's super, most of the functionality is in a backend STK that we provide. So you just call into that, so SQS message, we get one from your poller, you send it to our backend, we figure out which handler idea it is, which we can just look up in that manifest. We know which things are in there, we know that we got a message off of this queue, we send it down into the backend, along with the raw payload. So, we do not deserialize in here. This is for performance, it's just raw bytes, we figure out the headers and the metadata but we don't actually touch the payload. Then send that thing, that message that we created, over to the worker, it's over a UDS socket, Unix domain socket over GRPC. It's like microseconds of overhead, it's superfast, hits the worker, this is the part that you have to write for every language in the worker, it's a dispatcher to take the thing and call the user's function. DC realizes that payload, calls the users function. So, whether that happens over reflection or some other code generation doesn't really matter, you just code fires and then this whole process works backwards if you want to return the result. That's basically how it works. So, the key thing here is that these are separate processes. So I mentioned that level of abstraction, it means that this worker process is not bound to a Kafka seven library, or a Kafka eight library or Kafka nine library, it's just bound to a thing that says I care about Kafka with some topic name. And maybe a thing has a hint about what version of Kafka it is, but it's different in a library, it's really abstracted. Same thing with lots of other sources, so for example, right now we're on thrift, we're moving towards GRPC and proto at Uber. Same thing, we can decouple that process and not have to replicate that in every single handler or even necessarily have handlers have to rebuild to upgrade infrastructure. And then with, these things generate a ton of metrics and logging for free. So, this is basically how it works in production now, so that was the basic flow for a worker and, sort of local to your sources in your workers and stuff like that. But what happens when you want to play this to production? So, that same CLI allows you to upload and deploy that code that you built to production when you do that, we have a component called the registry that stores those manifest I showed you, then that registry sends the actual binaries up to S, is what we use for storage. There's a component called controller that's listening to registry, did anything change? Is there something interesting that's happened here? Once Registry says, \"Yes, Shawn has a new project, he would like two instances.\" Controller goes to a component called Mission Control and says, \"Please give me two instances.\" So, we maintain a warming pool of instances that we've already managed if we need more, we just grow it, if we need less, we just shrink it. So, we're ready to go, mission control gives us back two instances. That's all strongly consistent, so there's no weird stuff that happens there. And then mission control picks one of those instances, walks up to its nanny and says, \"Please do this.\" The nanny then downloads us bits from S, downloads the worker, downloads the sources, lights up, off it goes. So, that's how it works in production, really similar to how it works on your local desktop and it scales out beautifully because there's no shared components here. So, people ask us, \"What's the max QPS that Catalyst can handle?\" And we actually don't really care, doesn't matter. So Catalyst Today, it's written in Go. We're onboarding customers, so it's a real thing, this isn't isn't dream-ware. We're using Persistent Containers, so we're not quite the same as the other serverless platforms today that we don't go away after each method invocation. That's on our roadmap, but right now, Uber has got servers sitting in a giant room somewhere doing something, so having a container sitting on then idle is not a big deal. And our tax right now is P in about two milliseconds. P is like . milliseconds, so it's really small, most of that is, like, Go GC and other stuff happening on the box. We're hardening in production, we're load testing, we're testing negative scenarios, I'm obsessive about testing negative scenarios, just, like when things fail do they fail correctly? When your worker panics, or times out, or who knows what it does that you get, like, really good information and tooling back. And then where we're headed, we don't have auto-scaling today, we're working with the compute team to get auto-scaling. And we're working on some really cool stuff, long tail handlers, which means, how do we share a single source across a lot of workers for things that only get called once a day or twice a day, so we can sort of share that capacity. SLA-base priority, that says basically, when we're routing, can we hold certain messages to save compute infrastructure if they're low SLA? So, because those messages come into the source, they get converted into a Catalyst format, we could dump them somewhere else and then reingest them later and it would work fine, as long as they're not request response. And traffic-based placement, this is something that's really cool. So, there's a couple of core flows at Uber that are like all the time. It's like all the trip flows, essentially. Every time you, you know, request a trip or take a ride, those are core trip flows and we have tracing at Uber that tells us all the micro-services that are involved in that. And so we can compute the graph of things that run in the hot path, and then we wanna be able to tell the compute system to co-locate everything within the same switches so that there's no cross traffic. So, that's something that we're headed towards doing, and we do intend to open source this platform. We made an intentional decision to take the hit of not open sourcing it up front, so we could battle hardened it, get it super flexible and stable before we unleash it on the world. But that's where we're headed, and so that's pretty much it. Got two minutes for questions. Yeah? Man: What are your message guarantee semantics from this source to a worker? Shawn: So, the question is, what are the message guarantee semantics from this source to a worker? So, we fully expect the worker crash. So, what happens is that that GRPC connection, that source STK, has got a bunch of telemetry around that and so, what happens basically, is if the worker crashes, so if the worker returns an error, that's actually, from our point of that's fine, that it returned an error because an error is a successful indication. We mark that it was an error, but we don't do anything weird. If the worker was to crash, nanny is gonna restart it and reconnect it to the source. If the worker, we use governors, so if the worker or the source crashed a certain amount of time within a certain period of time, we'll actually mark the instance as bad, unroll it, roll out a new one. If enough instances do the same thing, at the next level we'll mark the whole deployment as bad try to roll back to the last version. So, does that's your question? Man: I got the response that, the worker might [inaudible ::] at least once. Shawn: Yes, it's definitely at least once. That's right, that's right. Yeah? Woman: What customers are already using it? Shawn: Question was, what customers are already using it? Some customers at uber that have, let's see, what are some of the use cases? One of the use cases that we're using right now is that there was a tax issue in New York that if we didn't have the correct setting on one of the vehicles, that we got charged, like, $ million a day, or that's not that much. Like, it was a fairly reasonable fine, and we relied on ops people when something changed in a drivers profile, to change a thing manually and so, that was work that was too small for a team to actually pick up. It was this weird piece of work, but two guys did it over beers one night and so now, when that change happens they automatically update it and that doesn't happen. Saves us, like, half a million dollars a year. And there's a bunch of scenarios like that today. Anything else? All right, thanks for paying attention.",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603ea6",
      "title": "Cornelia Davis - models for event-driven programming",
      "content": " Cornelia, ever the modest one, began her talk by saying that she \"probably wouldn't blow any minds here.\" Au contraireher talk was one of the most talked-about at the afterparty. She focused on the way we educate upcoming programmers, and the implications it has down the line for their ease in reasoning about event-driven systems. She happened to learn Pascal first, which gave her primitives of variables and assignments, pushed her straight into iterations and control loops. It forced her very early on to get comfortable with leaps of faith. What about students who start off with object-oriented languages? They tend have a hard time reasoning about recursion at first; it just breaks too far outside the paradigm they were given. When our baseline is object-oriented programming, it structures our mindset such that it can be harder to naturally reason about event-driven systems. But she has great suggestions for turning this around, and making event-driven systems more intuitive. Her talk is well worth watching, or read the transcript below. More videos: The entire playlist of talks is available on our YouTube channel here: Emit Conf To stay in the loop about Emit Conf, follow us at @emitconf and/or sign up for the Serverless.com newsletter. Transcript Cornelia: Boy, I don't think I'm gonna blow any minds here. But it is kind of a provocative title, intentionally so. So the interesting thing is that I'm coming at the end of the day, as you said, it's getting late in the day. And so I've had the benefit of reflecting on the talks that have gone on throughout the day. And I wanted to share that reflection even before I jumped into the talk because I think it's relevant. So over the course of the day I've heard us talk about things like CRDTs and functional programming, and computer sciences stuff. And I even texted my husband and said, \"Holy crap, this is so cool. We're doing real computer science here in this environment.\" And I think that's a reflection of us being still very early on in this new paradigm, in that the primitives that we're dealing with are still kind of these low-level primitive. And, I think, all of us computer scientists in the room know that when you're doing distributed systems, things like immutability are key, but we have to. I think, one of the challenges though, is that we have to translate that. Because we can't expect people with PhDs to be developing our web apps in the future. And that's really what I wanna talk about today. Is kind of, sort of, start to postulate on what those things are. Now, a little bit more about me, I do work for Pivotal. I've been working as a part of the Cloud Foundry team for four and a half, five years now. And so, I've had a little bit of a benefit of going through something similar here. So I'll tell you that four or five years ago, that what we know now and are even kind of poopooing about microservices, \"Oh that is so early s, right?\" Was something that was newer few years ago. And there were a lot of hard problems that we as an industry made consumable and made usable. And I think that's our challenge here. And so that's, I guess, the benefit of going very late in the day as I've had a chance to reflect on that. So let me start out with a little bit of a personal story. So the first thing that I'll ask is, \"What language is this?\" Yes. You guys are good. This was the first language that I programmed in college. I actually was very lucky that I programmed in high school a little bit in BASIC on a TRS-, but when I got to college as an undergraduate at Cal State University, Northridge in the greater Los Angeles area, a university whose computer science department was really focused on teaching people what they needed to know to get out into industry and start being effective right away. I learned Pascal. That was the first language. And when I put together this example, a couple of days ago, I have to tell you that I didn't remember the first thing about Pascal Syntax. So that's what I meant by, \"You're good.\" I wouldn't have recognized that as Pascal. But what we learned there was it was a semester system, and what we did was we learned this. And I'll go through this in a little bit more detail in just a moment. Now, after going to school and then working for a number of years my husband and I said, \" Ah, screw this work thing.\" And we went back to school full-time. And I don't know if majority's still here, but I went to IU for graduate school. A school in southern Indiana out in the middle of nowhere, and I like to say that they really couldn't have given a damn about getting people ready for industry. They were really kind of just a theoretical computer scientist school. And I studied programming languages, and I did my course in essentially theoretical computer science, theoretical. And so, I was talking with somebody earlier today that one of the classes I was minoring in philosophy, and one of the classes, single hardest class I ever took was a set theory class. And so it's really hard core, you know, mathematical computer science. Well, at IU they teach in their very first programming language, in their very first freshman course in computer science they teach it in scheme. And so this is the same program in Pascal and scheme. So let's look at a couple of differences here. So if we go back to the Pascal one, what are the primitives that they were teaching me? Well, the primitives that they were teaching me were variables. They were also teaching me assignment. So right from the get-go they taught me side effect. And from my preamble, do you know where I'm going with this? Right? So the primitives that they taught me here were side effects and that worked okay in that setting. We learned about iteration and control loops, and look at that. I was delighted when I went back and looked at Pascal because even the syntax drove home the point that I wanted to make here, which is when we're programming iteratively, we start at the beginning and we go to the end. Now, if we contrast that to a functional programming model...and so when Bobby and I were chatting earlier, when I saw the title of this talk, I'm like, \"Oh, he's doing the functional thing too.\" I'm a functional programmer at heart. If we take a look at the primitives here, what we have are conditionals, we have a base case, so we really need the conditional to decide between things, and then we have the general case. And what's so interesting about the general case is what's in the parentheses there, is that in order to program in a functional style, there is a point where you make a leap of faith. You say, \"I'm going to solve the non-base case, the general case, by assuming that the previous case was solved for me, and I'm just going to figure out what the difference is.\" And as we go through this, I'm starting to see those moments where we need to have the leaps of faith. As a programmer in this environment, I sometimes have to have a leap of faith. It's up to us as a community to fulfill that leap of faith. Now, there's another interesting thing about this. And this is really where when I was thinking about this talk, you know, several weeks ago, I'm where I wanted to get...that's why I came up with this rethinking thinking. I was at Cal State, Northridge, like I said, we were on semesters, and we learned those control structures. And when we got to recursion in week of a -week semester, holy crap, was it hard? People just didn't get it. The people who were still left in the room, it was something that they had to wrap their head around. It was a completely different way of thinking, because it required that leap of faith, and that wasn't a programming model that they had. At IU, and I taught while I was there, we taught recursion in week three. And you know what? Everybody got it. It wasn't that hard. And it's because we started there. And it wasn't that the students were just that much smarter. All right. So recursion was completely natural and it had to do with the primitives that we taught in the way that we taught it. All right. So then, what are the questions? And I'm going to assert today that what we're gonna be doing here in event-driven architectures is we are asking people to change the way that they're thinking. We're asking them to go from iteration to recursion, that analogy. And so, now it's our job to say what are the primitives? So is it a variable and a side effect? Or is it a base case in a general case? What are the primitives? What are the patterns around those and I'll say more about those in just a moment. And what are the platforms? And it was really awesome to see Austin talking about this, and many of the other speakers talking about these platforms that they're building. We just had that talk from Uber. We had a platform that we're building to make this course accessible. And so all three of those things are very, very important. So are those primitives functions and events? Maybe. I mean, certainly we're here at Emit, and I think events are central. Is our function central? I don't know. I'm being a little provocative here, I know we've talked about server listen functions all along. I'm pretty set on events, but I don't know. But honestly, I have no idea. I really have no idea. So I will tell you that this is the world I've been living in. I am going to be going through that transformation. I'm on the beginning of that journey transforming from the imperative to the functional model to the recursive model. And so this is the world that I've been living in, and so I will tell you right now this is the area that I'm the most comfortable in, and when I get later on to some of the event stuff, I'm postulating. I'm imagining, if you will, we've had some of those other talks as well. Now, this model here, is a pretty basic model. It's a simple mental model for kind of a micro-service architecture. I like to talk about things like apps, services which kind of take apps and make add some additional things to them, and I'll talk about some of those things. There's other services like data services, or Rabbit MQ, we've heard a lot about that as well. And then there's this notion of cloud native data. How do we deal with this distributed data fabric? And it's interesting that it's in the data space, that events are starting to percolate into this microservices world that I live in on a day-in and day-out basis. So those are kind of some of the primitives that I wanna talk about. Now, the interesting thing that happened, like I said, I've been doing this for about the last four or five years, is that when we started coming up with these primitives, and we started, and we actually brought a platform to market, we brought Cloud Foundry to market, open source and commercial versions of it, I was spending a lot of time, I'm in a position where I spend a lot of time with our customers helping them get wrapped around this new thing, this thing that was very new for them. And we're asking them to go from their monoliths to their microservices which, again, we've been poopooing out a little bit today, but that has been pretty transformative for a lot of people in the industry. It's this realization that there's no free lunch with that. And somebody had a slide earlier. I don't remember anymore who it was, they had a really great slide, I think it was Nordst...was it you Rob, who had the cartoon slide, where you had, okay, where the old problems are sailing away, but the truck was coming in with the new problems. So when we went from monoliths to microservices we had the new truck, we're gonna have the new truck again here. And so the whole point here and the reason I brought this slide up is that we're in the same position. We're gonna see this article written about events if it hasn't already been. There's no free lunch. And we have to figure out what those things are. So I'll talk about a few of those. And I'm gonna go through this relatively quickly for two...the reason I'm spending time on the microservices approach is first of all, to share with you what I've learned over this journey over the last four or five years. Maybe for us many of us have learned this in our journeys over the last four or five years to remind us of what we went through and how we came to understand what those patterns are, and what those practices are. Also, I wanna drop parallels because it feels in some ways like the event driven model is completely new, but there are a lot of things that carry over from this world as well. And so I'll be pointing those things out as we go. All right. So, one of the other things that I wanna point out here is on the slides, the things that I have bolded in white there, is I keep coming back to, why are we architecting things the way that we are? Whether we're doing it in an event-driven space or in a micro-services space space, why are we doing things? What are the things that we're trying to achieve? So when it comes to things like HA and scaling, well, we created multiple instances. We didn't used to have that, we know that the kind of the model in the cloud is to do scale out. Well, as soon as I have multiple instances that means I have to take on the burden of load-balancing across those multiple instances. So I had to figure out all sorts of things about dynamic routing, load balancing. And one of the things about the cloud is things are always changing. So how do I keep my load balancer up to date? All of those things. I also have to deal with externalizing my configuration even more so than before, because I now have multiple instances, and I need to configure them all the same way. So I can't have everything totally embedded. When it comes to resilience, I need to have statelessness. And, oh, and one of the other interesting things there is, whether we know that one of the fallacies of distributed computing is that the network is stable, so my services are always gonna be available when I need them. So to compensate for that, we came up with this concept of retries. Oh, but as soon as we do retries, we actually invite the next problem which is DDoS attacks. So retry storms. And there have been some great stories out in the media about, you know, even AWS has gone down because of retries. And they're kind of good at this stuff. So the next thing, if we talk about...that was kind of starting with apps. But now if we talk about services, it's everything that we did with apps, but now, we also want to address autonomy and agility. So we want these services to be independent. We want teams to be able to develop them independently. We've heard those same themes all throughout today. And so we need to do things like version our services, so that we know that if we have to make backward breaking changes, that our consumers know which versions that they can have and we have that great talk from Stitch Fix on even how you do some testing around that and breaking changes. The environments are always changing. That's one of the fundamentals that it used to be years ago that we measured up time in a matter of months, or years, and now the average container sticks around for four minutes. So things are always changing. So I have to do things like, I have to deal with service discovery. So if my service went poof over here, I need to know where I can find it next. Some of these things, the same problems exist but they shift into different places when we move to events. And talking about data, speaking of data, it's an interesting one here. So again, agility and autonomy. I'll tell you that one of the things that we've done in the microservices world, and I live this day-in and day-out with our enterprise customers, is that they've created these independent microservices and they think that they're loosely coupled, but if you peel back the curtain, you pull back the curtain just a little bit, all of those independent microservices are tied to the same shared database and same shared schemer. So what looks to be an independently loosely coupled isn't at all. And so what we do there is we...what the kind of the narrative that's out in the industry now is, well, every microservice gets its own database. And then people start talking about, \"Okay, well then, I've got these distributed data fabric, how do I treat it as a single hole?\" And people start talking about things like master data management. That doesn't make you like kind of shiver in your boots. And then, of course, data also is about resilience. So I spent the last year really working on what we call Pivotal Cloud Cache which is a...and I wanna emphasize the cloud, it's a cache that adapts to all of these like dynamic environments and things like that. And being able to use caches to be part of the bulkhead is an important pattern that's in microservice architectures. So and then, of course, there's the whole collective. So I've been talking about some of these independent pieces, but how do we treat that as a whole? Now, none of that has really been talking specifically about events. And so here's kind of a summary just to remind us all of these are the things that we're trying to achieve, and by the way, these are the same things that we wanna achieve with our new architecture as well, right? It's not new outcomes we're trying to achieve, it's the same outcomes we're trying different architectures to see if it works a little bit better. So what I wanna do now is, again, I am not the expert in the space. There are many of you in this room who are far more experienced with event-driven architectures than I am. But I wanna start kind of postulating, imagining, if you will. On the left-hand side, remember just a moment ago I talked about retries. And as soon as we did retries, which was an important thing to do, I mean, even our browsers can sometimes do retries on our behalf. Retries are an important pattern in the distributed system. But then we have to deal with circuit breakers. Instead, over on the event-driven side is the primitive that we need to start teaching, is the primitive that we need to start thinking about instead of going from top to bottom, from begin to end, and say, \"I need to go get something, and if I don't get anything back I'll do a retry.\" Am I taking kind of the leap of faith here? Is this the general case? Where I say, \"Ah, I'm just going to have faith that this promise is going to be fulfilled at some point.\" Now, I want us pause here for a moment because I think this is one of the most important things for us in this event-driven architecture, is that the platform is responsible for fulfilling this promise. The platform has to ensure that that event doesn't get lost. And so somebody talked about this a little bit earlier, and it's kind of permeated through the entire day is that, \"Okay, we have to make sure that this substrate that's managing these events are managing them in such a way that the programming model can just work against that.\" If we burden the developer with not just worrying about writing the promise, if we don't give them the opportunity to have that leap of faith, then that changes the programming burden that we put on them completely. Okay. All right. Oh, and from circuit breakers, again, this is the point that I was just making, ensuring that they don't get lost. Now, another one, like I said, I spent the last year or so really focusing on our cloud caching product, and it's an interesting thing. I work with some colleagues who have been working in this space for a long time, and they spend a lot of time talking about things like expiration. So that's one of the hardest things in caching, is to figure out when to vacate things out of the cache. That's a really challenging problem. Is it time-based? But how do we decide that? And the interesting thing is I think this is hugely powerful as over on the event space. Instead of worrying about when we stop believing what's there, we depend on, and again, we've got a leap of faith here. We depend on the fact that the substrate is actually refreshing that cache for me. So I actually like to talk about those things as materialized views, and we've been talking about that from an event sourcing perspective. So you can kind of think of it as a cache. In the event-sourced model, we know that the unified log is the source of record, that is the source of truth, everything else is just a cache, it's a materialized view whatever word you wanna use for it. Again, we have to ensure that the events are not lost. Now, from kind of this service-oriented thing, and that's where I was saying earlier, I don't know. I'm still kind of struggling between this. Are services different from functions? Or functions just one type of service, I think that's something that we need to kind of figure out as an industry. But I should actually have multiple instances of the service there because that's why we need load balancing. But what's interesting here, and I couldn't help when I was putting these slides together, thinking again about the difference between kind of iteration and recursion, is that over here we've got a service that stays up and running. And so we are always thinking about that services staying up and running and we tell people, we heard Cloud Foundry have been saying for a long time, make your stuff stateless. Because you can't guarantee that that's gonna be around. And, you know what? We also support sticky sessions. Drives me insane, I'm like, \"No. I don't wanna support sticky sessions.\" But we allow, we still give people a little bit of rope to hang themselves with. Instead, over here, we've got functions that come into being with every single request. That right there is a significant change. What I think is really interesting about this, and so from the load balancing, to functions I have another call out, is I kind of feel like this is a little bit of a stack frame. Each one of those, every time you do a recursive call, and if you're doing proper tail recursion, you're replacing the stack frame from before. And even if you're not properly tail recursive, the only scope that you're allowed to be in is that stack frame. You get a fresh stack frame with every single call. You don't get to reuse what was in that context before. In an iterative mode, you've always got the possibility that you're pulling something from some other context that wasn't inside of that loop. All right. And then a couple of other things when we start moving into the data fabric is going from this world where we have a data bottleneck, and I mentioned this already, that this is to a large extent what we see is we've got. We've started to scale out and we know that one of the fundamental models in the cloud is to scale out via more instances, but what we haven't done is figured out how to scale out the data TRS at the back-end. This is someplace where we definitely, even in the world that I live in, where we're very heavily microservices based, where we are starting to see some movement there. What I do find interesting though, is that it's an area where it shifts from this modeling the functionality, the functions themselves, and I don't mean functions in the serverless sense. But modeling the compute over to making the first-class entity, the actual data topology. So the schemers and those types of things, so making something like data partitioning, a first-class concern and something that you're thinking about right from the get-go, that's not something I think most developers are thinking about right at the get-go. So that's something we need to do. And, oh, and partition resolvers, really cool stuff. I'm running out of time, so I won't talk about that. And then this is the image that we've seen from several talks today which is to say instead of having a shared database, we want each one of our services to have their own views, their own materialized views, and this is where we're going in with. And, of course, eventing is at the core of that. So there's a couple of things...oh, and then of course, event producing, but we've seen lots of that. And I am down to my last minute or so. There are many, many, many, many questions. And I can't even begin to write them all down. But what I wanna leave you with is, these are the kind of set of things that we think about from a service-oriented perspective. And these are just my early ideas of some of the primitives that I think about when I'm making this transition over into the event world. But the last thing that I wanna leave you with, is I wanna go back to scheme. And who knows what call/cc is? The other functional programmer in the room. Call/cc is call-with current-continuation. And so I was thinking, \"How can I express this?\" And I did a little bit of Googling, and I found a question on Quora that said, \"What is call/cc in layman's term?\" So, Bobby, you must think that's kind of funny, right? Because there's nothing layman about call/cc. So I wanna read this to you. The answer is, suppose you have a function F which takes one argument which is supposed to be a continuation. Wait a minute, what's a continuation? Anyway, then call/cc is a special function that takes F as an argument, and calls F passing it as its argument the current continuation, which is the continuation in which call/cc itself was called. Yeah. And look at the first line, hard to explain in layman's term in just a sentence. It's hard to explain this in layman's terms if you have a volume. So I just wanted to close on this. Again, we can't expect people if we want this event-driven world to go huge and have a huge market, we can't expect people to understand call/cc. So that's our challenge and that's something that I'm excited to see us in this industry doing in the next several years. And that's all I've got. Thanks so much.",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603ea8",
      "title": "The State of Serverless Multi-cloud",
      "content": "To multi-cloud, or not to multi-cloud Vendor lock-in runs deep in serverless applications. Cloud provider used to mean whoever hosts your servers. In a serverless paradigm, it means whoever runs your functions. And when the space doesnt (yet) have standardization, developers must twirl those functions round and round in a whole vendor ecosystem of events and data storage. Theres no way to use Azure Functions and EC together. But, you say, vendor-agnostic frameworks let you easily deploy functions across providers, at least. That they do! But then, theres the small technicality of language choice. Write your application in Python and youll have a hard time moving that over to Google Cloud Functions. Given all this, what do we make of the multi-cloud? Is it a pipe dream, or an attainable goaland do we actually need it? Multi-cloud gives you wings The biggest advantage to serverless multi-cloud we hear in the field is feature arbitrage. Imagine plucking all your favorite aspects of each cloud provider and placing them nicely together in your very own, custom-made bouquet. Its hard to commit to a single ecosystem, especially when serverless compute vendors are constantly adding new features that change the value equation. AWS Lambda is adding traffic shifting in Lambda aliases any day now; Microsoft Azure has their (still unique) Logic Apps, which lets you manage event-driven services much like youre composing an IFTTT. Pricing works out differently across vendors for different services. The same project could work better elsewhere in less than months because of all this rapid feature launching. We fear lock-in because it removes our flexibility of choice. And then, add failover into the mix. With serverless compute, you dont have to worry about redundancy quite as muchLambda, for instance, automatically scales across multiple availability zones for you. But entire regions can (and do) go down. While its a rare corner case, cloud outages can be devastating; we see larger companies caring more about this and moving to incorporate strategies for full cloud redundancy. Multi-cloud gives you pause As fun as dreamspace is, we do still have to wake up in the morning and ask ourselves: is a multi-cloud attainable and worth it? Lets say you want to actually try and instrument full cloud failover. The first thing youll have to do is write everything in the only language all four major cloud providers support. Aka: JavaScript. Then, youll need to abandon your cloud databases for something like MySQL. Youll need to constantly replicate that data from one cloud provider to another so that everything is up to date when failover occurs. You have to think, hard, about how each cloud handles logging? Secrets? Metrics? The rule of the game is to make everything as generic as possiblewhich seems to go against the serverless ethos, in a way, and prevents you from utilizing those powerful features you were trying to get with multi-cloud in the first place. Its also worth mentioning that, for those who do choose to run an ecosystem across multiple providers, youre paying for transfer. Not cheap. Maybe the answer ends up being: yes, it would be cool to leverage any service I want, whenever I want, and still maintain that serverless flexibility, but things just arent there yet. We dont have a Schroedingers cake. The multi-way forward There are a series of things that could happen to make multi-cloud easier. Cross-cloud service compatibility Data management and storage, for instance, are ecosystem-dependent. Google has best machine learning right now; and while its feasible to use GC services on different cloud providers, it isnt necessarily simple. To make multi-cloud less work and less compromise, we need to be better ways to share data across cloud providers, and better ways react to any event source regardless of cloud provider. Add shims for polyglot language support That way, it wouldnt matter whether or not you wrote your functions in Go. Doing this yourself could be cumbersome, but soon there will probably be tools that facilitate this for you. Smartly route your data This ones on you, the spritely developer. Divide your application into two conceptual parts: critical path and specialized features that dont need need to work % of the time. Anything in the critical path (things that serve your site, for instance) should be written in a cloud-agnostic way. That makes it easier to implement failover or port things over to another provider, should you need. Specialized services (e.g., image tagging) can be maintained separately for a time, or be made to process important data later in case of an outage. The...multi-verse? Multi-cloud probably wont ever be completely work-free, but we expect itll be easy enoughsooner more than later. And then well start to see the landscape shift. Cloud providers wont be fighting for bigger chunks of your server space; theyll be fighting for bigger chunks of your application, in the form of features and services. This is frankly already happening. We stereotype giants like Microsoft and Amazon as being slow to innovate, yet theyve been rushing to push feature after serverless feature for the past two years. Theyre moving faster than most startups. As an industry, were headed for a user-centric software reality. Businesses will increasingly differentiate themselves with highly customized software, and multi-cloud is how theyll do it.",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603eaa",
      "title": "Announcing: Serverless Workshops!",
      "content": "Why we started them Let's be honestserverless is still new. Maybe you only recently started looking into it. Maybe none of your coworkers even know what serverless is. Maybe you've just deployed your first serverless API and you're dying to try it out on a larger scale. We designed these workshops to help you (and your whole team) get the best practices and how-tos straight from us. What you'll learn Learn how to deploy serverless APIs and applications from start to finish. We'll cover things like serverless development, ops, monitoring, secrets, debugging and testing. Our goal is to get you ready to confidently create serverless applications, and give you the skillset you need to operationalize serverless. How to register The workshops last one day. We're hosting them in cities across the US, plus a bonus stop in London. Head over to the Serverless.com workshops page to get your name on the waitlist. See you there!",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603eac",
      "title": "Serverless panel - the future of event-driven compute",
      "content": " Emit Conference closed out with a panel on the future of event-driven computing. On the panel, we had Chris Anderson from Azure Functions, Jason Polites from Google Cloud Functions, and Anne Thomas from Gartner. If anyone would have solid insight and future predictions, it'd be this crew. Austen Collins facilitated. He asked them for all kinds of tidbits: whether pricing was really the main driver for serverless adoption, what were the most common use cases they saw (as well as the most interestingserverless in academia, anyone?), and what concerns and problems they were dealing with to keep this stuff up and running. Watch the panel discussion in the video below, or read on to see the transcript. More videos: The entire playlist of talks is available on our YouTube channel here: Emit Conf To stay in the loop about Emit Conf, follow us at @emitconf and/or sign up for the Serverless.com newsletter. Transcript Austen: We have a very interesting panel here. We've heard a lot from the Serverless community, the Serverless user base, a lot of the smaller Serverless vendors. But I don't think there's ever been a conversation between the actual service providers, the major ones. And we were fortunate enough to have a couple of them attend and agree to participate in a casual conversation about Serverless, serverless architectures, adventure of an architectures, talking about use cases, problems, best practices, and potentially what the future looks like. So, we have one person who wasn't able to make it today and that was Michael from IBM, but to fill in his shoes at the very last minute, we have Anne Thomas over here, a distinguished analyst from Gartner. And this is kind of designed as a casual conversation and the questions were designed for vendors, but Anne's gonna do her best to fill in here, and provide the industry perspective from outside the vendor. Joining us also, we have Jason Polites, the PM of Google Cloud Functions. Here, everyone, this is Jason. Jason: Hi, guys. Austin: Also, we have Chris Anderson, the PM of Azure Functions as well. Chris: Hey, hey. Austen: All right, let's jump right into it. I'm personally super excited about this, and again, thank you for joining. I think everybody is so interested in hearing from you guys in particular. So, let's start off with something kind of lighthearted, maybe a little controversial, actually, and it's that Serverless name. So, what does serverless mean to you? Because we've heard from everybody, everybody on Twitter, of course, they've weighed in on this and pretty much everything else that's ever happened. What does serverless mean to you and what is your preferred name for this new cloud computing service in general? Is it event-driven computing, serverless computing, functions as a service or something else? Jason: Yeah, so, you know, this is one of those terms that you might say is poorly defined but actually, it's overly defined, seems to have many definitions. I think what we see happening is...so three categories of definitions that people tend to assign to this term. One is in the spirit of the word, \"I don't have a server,\" or, \"I don't manage a server.\" And so that component of administration is relieved of me. Then there's these other two things that sort of get interwoven. One is the event-driven side of it and then the third is the economics associated with it. Am I paying for the machine when I'm not executing code on it? And those sort of emerge into the market as a bundle and all called serverless. And then you also have the fourth, I suppose, which is functions as a service as distinct from a platforms and service or a container or any other kind of unit of compute. I'm not sure that the bundling of that is necessary. I think that, you know, we see a lot of customers who gravitate towards one or the other depending on the nature of the nature of the customer. The abstractional way of infrastructure is probably the most dominant characteristic that I see. I think it's the most appropriate. And I think that for two reasons. One, because the eventing side of it sort of goes well like peanut butter and jelly with serverless, but it's not necessarily...you should be able to emit events to something that's not serverless and that should work. And so, for me, the best definition I think is just really as the word describes, no servers. Austen: That's great. Chris, any thoughts? Chris: Yeah, I mean, I think the word is pretty good. I think it's like two-thirds there, right? I'm going to think less about my servers. That's kind of the promise of serverless. I think it's just everyone likes the short form of it. I think the full version of what we should describe would be like eventful serverless. And then you've got the full story there. That's the last third that's missing where having to think about, you know, what your events are, how you're gonna source your events, how you're gonna store your events. That whole piece is missing from just the names. So, whenever we're talking about serverless, everyone always indexes on the lack of infrastructure. But then I almost feel like it's kind of a violation on what we're trying to do. I want to think less about the servers, I want to not have that conversation, and I actually wanna talk about the events more. That's the whole point. We're unblocking the way to using the events in a clingier fashion. Then, of course, you get into the confusion of everyone things of serverless as functions as a service and is clearly way more in the area both in terms of, you know, data services, workflow as a service. You know, now, we have these new kind of event, you know, gateways to think about. I think that the space is gonna widen up, and maybe by widening the space will help to reduce the over index thing at serverless on functions as a service, but I like that it's kind of a community-driven word. It wasn't like one of the vendors came up with it. So, from that point of view, it's really up the crowd how that word will evolve. Austen: Right, that's true. Anne, would you like to weigh in on this? Anne: Yeah. Well, Gartner loves to define things, so obviously, we have... Austen: Yes. You're very good at it. Anne: ...we have plenty of definitions on this particular topic. So, as Jason was saying, I think that the economics is a really big part of what serverless is all about because I don't want to have to pay for a bunch of infrastructure that I'm not currently using, right? And so, the real appeal to serverless is the fact that I am only paying for what I'm actually using for the periods that I'm actually using it in -millisecond increments and things like that. I think that is a core definitional aspect of serverless. But I don't think that what we call function pass, Fpass [SP] is, in fact, the only type of serverless environment we can have. So, my colleague Lidia Young [SP] will certainly identify the fact that we think that something like Apenchen [SP] is a serverless environment because you don't really need to be concerned with how many machines am I actually allocating underneath? I don't need to you know, you know, I don't even have to pay for all that stuff, it's not like I have to pre-allocate a bunch of stuff, right? And in fact, if you look at the AWS serverless platform, Lambda is only one tiny piece of the full serverless platform. There's a whole bunch of other technologies behind or that go along with the whole platform like the API Gateway and DynamoDB and you know, a bunch of the other components out there which are necessarily to build applications because you can't build an entire application and run it just in your little function environment, right? And I call it Fpass as opposed to just FAS because it's a platform on which I'm running functions. I'm not getting functions provided for me. I have to actually put the functions onto the platform. And besides, we don't like to create new excess of service primary levels and things like that. So, is event-driven or eventful? I like that, I like that a lot. Are those essential components to it? I think that in the current version of what people are doing in serverless computing, we are doing it entirely through this event-driven model because how do you trigger functions? They're triggered by events. But what I found really interesting right after the AWS Lambda, they came out with the API Gateway because people were saying, \"Well, how do I actually invoke my functions? I want a request response invocation model to invoke my functions.\" So, you know, I think that there are a lot of people out there who are currently using these function platforms and not necessarily really building an event-driven model. And I think event-driven models are really hard for the vast majority, certainly for mainstream organizations. Jason: There's one characteristic that I've noticed that both agrees and disagrees with that point. So, the first thing, I think a lot of people...my intuition is a lot of people gravitated towards the functions as a service in the context of HTTP and the context of the synchronous request from the client. Not necessarily because, \"Well, this is a better unit of compute for that model.\" But there was just a simplicity to it, you know, I could just get up and running with no frameworks, with no extra additional work. And my actual need was to do an HTTP request, and this just seemed the simplest sort of within arm's reach kind of thing I could grab hold of. But the other point I was gonna make is that events from what I would refer to from the Google perspective as a third-party service, so this would be like a Stripe or a GitHub or some existing service out there. Events emitted from those things have been around for a long time and they tend to be delivered over HTTP via webhooks. So, I think there is also an argument to be made for the API gateway style deployment still being within the realm of eventing, although it arguably lacks some of the orchestration or frameworks around that to give guarantees and so on, but we do see a lot of those sorts of use cases as well. Anne: Well, I like from the examples that we saw today that almost all requests coming from outside tend to be request-driven, but then once you get in through the outer layer, everything else behind it winds up becoming [inaudible ::]. And I actually anticipate that that's going to be the more normal way of doing it, but obviously, you know, supply change generate events all the time, and as you start building out an ecosystem, your environment has to be able to handle events that are coming in because they're not necessarily going to be request-driven. And if your systems aren't event-driven, you're not gonna be able to play very effectively in the digital business market. Austen: Yes, absolutely. Good answers. There's definitely a lot of definitions here being put under this big umbrella term. And it's been hard to nail down but there's no mystery from my perspective as to why it's caught on, because when you say that word to an engineer or a developer you can see them light up, right? It's this purely emotional response and Jason kind of touched on it too. It's just like the simplest solution to get something done. And so when I see that reaction in people, just saying that word it's clear that you know, that this is the right term even though it's not technically accurate. Still, it's been hard to pin down. So, from all your perspectives, you know, why should people be excited about serverless computing, and is there a specific value prop that you say gets people the most excited? Chris: I think for me, the value prop of serverless is really delivering on like the most concentrated value of the cloud, right, it's all about trying to get the most value out of someone who can solve the problems for me that are unrelated to solving business problems. Serverless gets me the closed to the point where everything that I do is unique to my business problem and I'm not doing nearly as much technical chores. There's still cases where people are having to use serverless technologies to go and build technologies in which they then solve their business problems, but for a lot of cases, people can walk up to it and get an HTPN point for their [inaudible ::] just like that. And before, you know, it took a couple of extra steps of deciding the app that I was gonna run on, the infrastructure I was gonna run on. You know, that to me is the promise is the agility that comes from shedding the chores that are unnecessary to my business. Jason: Yeah, I definitely think I would agree absolutely with that. The other thing I'll say is that this may not be obvious and I'm not even convinced it's real yet, but there's somewhat of a forcing function to encourage people into a given architecture. So, by saying that while you have this small unit of code, it really has to be stateless because it might disappear at any moment. It's going to scale up and scale down. You are sort of drawn into a service orientated architecture or microservices architecture, and anyone that's trying to operate at scale needs to have certain primitives in place and you know, some mentions have been made around databases, and sharding, and scaling those things, and so on. And so, the entry into that, you know, give a person a machine in a programming language and they might come up with a monolith because that's the way that they think. But I wonder if there's an evolution happening here that people are just sort of being drawn into because of, you know, getting started with being very, very simple, getting started with not having to worry about infrastructure, but just ending up in a services-based model as, you know, assuming that's a better place to be. Austen: Do you feel that you guys kind of touched on similar topics there? Do you feel that those value propositions are more compelling than the pricing model which is also fairly disruptive? And from our perspective, I'd say we see a lot of people come in through the front door of serverless because they're attracted to that pricing model and they stay because they realize that this is a lowest total cost of ownership situation that helps people get onboarded into microservice architectures and all of that. So, I'm just curious, you know, from your perspective, how compelling has that pricing model been for people? Chris: I mean, to me, agility is like, \"How many decisions do I have to make before I've solved the problem?\" Right? And that point of view, if I don't have to think as hard about pricing because I know that I don't have to worry about that zero to one kind of debt that I've got to get through. That's, you know, agility, that's some burden I don't have to pay. We do know that there's cases where I can essentially save money after I reach a certain amount of scale or it's not running on Serverless. You know, for us, we actually have the ability to run on dedicated instances or the serverless instances, but we always have the default be serverless because we know that for getting started, you're just trying to get in there and not think, you're trying to solve the problem that you have, not solve problems that you don't know you had yet. You know, we can have tools beyond that, but we want that intro experience to be just buttery smooth. Anne: So... Jason: Yeah, I think that... Oh, I'm sorry. Go ahead. Anne: I was just gonna say that most of the increase that I have with my clients regarding serverless are % about the cost. And I'll push back on that. I mean, it's like almost any cloud service you can find, there's a free tier. And so, I mean, I can go start and build applications for free and do, you know, a whole bunch of testing on it. So, that's not the cost concern. The cost concern is once I put this into production and my load suddenly starts to jettison up, how much is that gonna cost me? And is there a way to me to actually reduce that? So, almost all the questions I get related to serverless invariably are, \"How much money can we save if we go this route versus going the more traditional approach?\" Jason: Yeah, I mean, there's certainly a point at a particular scale where if you have a persistent QPS, you know, , QPS sort of relatively, persistently, and maybe there's some sort of diagonal pattern to it, but you have some auto-scaling capability even on virtual machines or containers, then, you know, an argument might be made that, \"I'm already optimized because my infrastructure is matching the shape of my curve.\" But what actually happens is when, to the points being made, when somebody gets started, they are price-sensitive. Even if it's a small group in a large organization, they might start with a small virtual machine because it's the most cost effective. And that might be cheap, but then what do they do? Do they scale vertically? Do they now have to think about horizontal scaling? And the other important point is that if we do take this leap towards sort of microservices or nanoservices or even smaller, then we're taking sort of an application that was shaped like this and we're breaking it up and spreading it like this. Now, some of those services are not gonna be called very frequently, you'll have sort of a distribution. So, the ones at the tail that are not called very frequently are at zero for most of the time, so it's almost...so you can't really go to that model and pay for all of these [inaudible ::] infrastructure. So, that's where the two sort of relate very well together, I think. Anne: But I've talked to a lot of organizations that are now going and rearchitecting their applications because they're trying to reduce that operational cost. You know, so for example, they built out web applications and they're running in ECS or something else like that. And they're saying, \"Okay, we've got all of these VMs that are running all these containers that are running all of the time, and they're not being used most of the time. You know, and then I talked to them and it's like, \"Well, okay, you can basically define your web code as a file which can be downloaded, and when they execute different individual methods, that'll trigger a function which is gonna go away as soon as it's done. And, okay, maybe your data store, that's still gonna be running in a more persistent type of environment. But they're saying, \"Yeah, that's like maybe % of our system and all the rest of it is where, you know, we're getting all the costs.\" So, yes, they're now talking about doing a complete rearchitecture of the application just so that they can reduce that cost. Austen: Right. So, we chatted on about what the definition of this is, we've talked about why it's exciting, what the value props are. So, from each of your perspectives, what are the major use cases for this on your respective platforms? And maybe, Anne, if you could chime in on just from a broader industry perspective, what are people using this stuff for? And also, on a personal level, what are the use cases that you've seen that are most exciting to each of you? Jason: Yeah, so the first thing I would say is the use cases we see today, it's not clear whether they're representative of the longer term when some of these customers you're referring to start adopting in force. But what we do see today is largely, I would say, three main use cases. One is on the synchronous HTTP side which is really just a replacement for, you know, platforms and service, and app engine style solution where they have a web client or a mobile client and they're using it as a back-end for that and then they're [crosstalk ::] some data source somewhere. On the event-based side, that tends to be largely around what we would call lightweight ETL, so data processing in some way. A mutation occurred in some data source, so I'm gonna pull the data from that source, transform it in some way, and then send it somewhere else. That pops up a lot in kind of IOT style use cases where you have, you know, [inaudible ::] coming from the field, they need to be processed in some way. And then the third use case which also answers your second question which is the one that I personally find the most interesting. We see emerging from academia where they might need to do a huge amount of processing on data in a very short space of time and then not need anything for a week. And so, you know, spinning up a large number of virtual machines takes a long period of time, it requires a certain amount of sophistication in understanding infrastructure as opposed to just pushing a button and letting the provider deal with the pain of starting that thing up, you know, instantly. Austen: It's powerful. Jason: And then disappearing an hour later or whatever it is. So, getting massively parallel processing by virtual of the fact that they don't need to concern themselves with how to scale it. Austen: Can you give us any type of breakdown on those use cases, anything that's more popular than the other? Jason: It depends on what metric you're talking about. On numbers of customers more on the HTTP side. On consumption of compute time, way more on the data processing side. Austin: Right. Jason: And the third is sort of more of a fringe use case at this point. You know, smaller numbers of customers, large amounts of compute, but not for extended periods of time. Austen: Interesting. Chris? Chris: Yeah, I mean I actually liked, and I made sure I got a picture of it. One of the slides which you put up there in terms of just... It's been amazing how many different types of scenarios people have come in there and tried to use it. I haven't found it getting pigeonholed in any one particular scenario. There's clearly, you know, we can see that HTTP is the most popular in terms of number of functions, we see that as well. And in terms of, you know, compute time, it's you know, stream processing, for the most part, stream processing very, very high-scale stream processing consumes the most, you know, amount of compute. But how you can approach that can solve lots of different problems from just, you know, various crud style applications, service-to-service webhook type things, to really, really high-scale very important IOT scenarios. I haven't really seen anyone shy away from the space so far. You know, we got hit by a PCI compliant certified, so all the various, you know, hostiles that we're trying to use this could finally, you know, stop knocking on our door. It really hasn't been one of those things that has excluded anyone thus far. The coolest scenario that I've seen has generally been the cases where customers have used us to build like an extensibility platform for their business. So, we had [inaudible ::] I guess is one of the case studies we have for this where they were using functions to build these various extensions to their service and they got tired of that and just built functions into their own portals so that way their customers could then write the code for them. And then that saves them time of having to have developers to write these custom extensions, they can empower their developers to solve their problems without having to go through some kind of salesperson to do that kind of stuff. And so, that to me, is a unique way where someone's able to use our platform to build a platform which is always fun to see. Austen: Yeah, that's it. Interesting trend. We see a lot of these SAS companies now offering their own version of serverless compute for that same reason. And that's been fun to watch but that's a whole other subject, so. Anne, when you hear clients talk about this, what are they using this for, what are the most popular use cases, and is there anything particularly interesting out there that you've seen? Anne: Yeah, so as both Jason and Chris have said, you know, I think that the web back-end and the mobile back-end are pretty popular. Number one, that's probably the most common application people are building, right? But I also think that the analytics. I think potentially, you know, some AI and machine learning type things would actually be great to put into that kind of scenario. But I would say that the vast majority of applications that are being developed for the serverless environment are relatively simple applications that don't have a really complex set of events triggering other events, triggering other events because we don't have tools yet, although we will very soon. Austen: That's why we're here. Anne: Which enable people to grok the massive event, the tendencies that, you know, so how do I go build an account management system in serverless, you know, how do I build an ERP system in serverless? That's kind of a scary thought right now, but you know, with proper tooling and as we start rethinking the way we think, and we start thinking in terms of events triggering things, working in a functional model [inaudible ::] imperative model it might become simpler and easier, and we can, in fact, build really complex systems this way. Austen: So, the cloud providers are providing this fantastic service, serverless compute, it's great. We offload all the maintenance, all the management, all that hard effort over to you and occasionally get upset, you know when it goes down. But it's fantastic for us because the rest of us can focus on solving business problems, getting out to market, you know, reducing overhead and all that stuff. Given that we have you here and for the sake of empathy, just so we can get this out there, I'm curious, what concerns and problems are you guys worried about on your end to keep this stuff up and running so that we can you know, drive benefit from it. Jason: Yes, so I think that's a good question. I think the way I would describe it is pace, and so, you know, the main is that large companies move slowly, and that's true, but it's less about a lot of bureaucracy and red tape and more about the fact that certainly Google we're concurrently building many things as a platform, and so it's not just one sort of product that is being built as fast as it can, it's a platform approach and that necessarily means that our expectation, I'd say I hope our expectation is that customers will use more than one other products on the platform and that sort of implies that there's an amount of consistency between them if I go from one product to the next. I sort of have some expectation that there's some consistent paradigms in play, and that means that although product X might say, \"Well, really we wanna do things this way.\" And product Y says, \"Well, we wanna do things the other way.\" There's a level of sort of consistency that needs to be brought to bear. And the challenge is that the cost of getting it wrong is high because it's the customer that pays for it. If we, you know, misjudge a trajectory of where we think, you know, something's going, then later we have to unwind that or we have to reset. Have we created a deprecation problem for the customer? And so, that's on the one side as a challenge generally, and then on the other side you have just the hyper iterative market, so things moving very, very quickly, peoples ideas materializing very, very quickly. And that tension is good but that I would say if I put the better term of pace, the pace at which we can operate and the pace at which the market operates, and the impedance mismatch between those. Austen: Chris, what keeps you up at night in order to provide this service? Chris: Yeah, if you ask my engineering team they'll probably have a different answer than me, but for me, like you know, my job is to kind of help figure out what do we need to do next? And in a lot of ways I have those same kind of problems. I think maybe more specifically, you know, my background before this was working on app service, Azure Mobile Services, and as we move up the stack from like IAZ and these other services, the problem becomes...we have to become opinionated in order to solve the problem, and then so the question is, \"What are the right opinions? Are we sure that the opinions we have are correct? How can we get other peoples opinions on whether or not our opinions are proper?\" Those kinds of things. And I think that with serverless we're at the point now where the primitives of functions are getting pretty close to well-defined, and I wonder is there direction to even move further up the stack with functions? Do we need, you know, opinionated frameworks to help us solve these more complex problems? You know, I can't build an ERP system with just throwing functions at the cloud and hoping it sticks. I might need a framework, like serverless framework or something else to solve those problems more opinionated. That kind of keeps me up in terms of like, \"What is the right direction for us to go? Should we just keep on offering functions the best way possible?\" That isn't necessarily going to guarantee us more successful customers. We might have to think about going out there and trying to be more opinionated to make sure that we have more customers who are more successful with more complex scenarios. And how to do that in a way where I don't feel like I'm stomping other peoples ideas of serverless either. Like, I want to do it in a way that is inclusive of what everyone wants to go and do. So, it's a lot of things to balance as you're trying to approach the next set of problems. If you ask my dads, they'll probably be like throwing more servers at the problem. How do we keep on doing that more and more efficiently? That's what keeps them up at night, but as far as the product moving forward, it's definitely how do we start to approach more complex scenarios for serverless? Austen: And I have to give all the providers credit, I think you've all done a great job interacting with the community embracing open source especially to get that feedback and make sure you're coming out with the right opinions there. It seems like you are evolving in all the right ways as these large organizations, and it's really great to watch, so lots of credit to you. Anne, I think you wanted to get in a question or two, and we've got about minutes left. Anne: Well, so just in response to this. I just wanna point out that event-driven architecture has been around for a very long time. It has always been an edge case, you know. But, I mean, there are a lot of systems that were built on Tuxedo Rendezvous or Tuxedo and TIBCO Rendezvous which were both event-driven middleware systems. And, you know, but that's still just this tiny little slice of the vast majority of applications out there. And as I've pointed out before, I think the biggest challenge that we have from this perspective is getting people to learn their way around event-driven architecture because it is a really [inaudible ::]. As Cornelius said, you know, \"It's like you have to rethink thinking.\" Chris: And that's exactly what I face, like, \"How do I help people take these things to the next level?\" Anne: Frameworks, I think, are fundamental to making this. Well, and also, there's a whole bunch of infrastructure which I think is still missing, like especially the observability infrastructure is really primitive right now. Austen: Right, right. I guess kind of this leads perfectly into the next question, and that is, what's the next step in the serverless movement? You know, what should we expect from the cloud providers? Do you predict any common patterns emerging here, standardization, what are your thoughts about standardization across the different cloud providers? Where are we going from here? Jason: Yeah, so I think that there's two macro trends that I would suggest are plausible. One is that, you know, the simplicity of the serverless model on the compute side has meant...sort of a center of gravity has formed around this model of building systems and applications. And almost by definition, we don't have to think about some of the more complicated things, but those complicated things still exist at scale. And I think Cornelia, in the Pivotal presentation, really, really nailed it. And if you have a chance to watch the replay of that, I really encourage you to because there are a whole class of challenges surrounding what an event means, what is at least wants, what is at most wants guarantees, what is item potency? How does that all relate to how I build my application when all I wanna do is write a little bit of just a function? And so what I think...a plausible trend is that these problems will emerge and solutions will emerge as they typically do in a fragmented way and then there'll be some level of consolidation. And the real challenge here I think for the providers is how do we contribute to that in a meaningful way? You know, if you get a bunch of smart people in a room and imagine what the future world looks like, they might pop out with a great solution five years from now. Meanwhile, the market has taken off in some other direction. So, that's one thing. I think these problems are [inaudible ::] right now. I think they will become...they'll come to the floor as people start to do more complicated systems. And the other smaller trend I would say is that, you know, people, younger people now, programming is becoming more ambient to their education, to their experience. And so, you know, the comment of, you know, customers using the platform to provide an executioner, code executioner environment to their customers. I think it's reasonable to say that most many services will have a programmability aspect to them, and I think serverless as a general sort of umbrella is a great fit for that sort of model, so whether it's...you know, whether you're in a word processor and you need to execute some script or, you know, whatever the tool is, having an augmentation of that that makes it programmable and having that as a serverless thing, I think we'll see more of that emerge as well. Austen: Chris, any thoughts? Chris: Yeah, I mean, kind of related to like my answer for the last question. As we've tried to go ahead and take approaches to solving these more complex problems in consistent ways so that the customers don't have to reinvent the wheel every time they walk up to the thing and figure out how to rate my ERP system, I think the problem we're gonna see, and we already see it today, we have, you know, Apex/up, we've got Arc, we've got Serverless framework, all these things produce different outputs that all end up, you know, touching some cloud service at the very end. I do wonder whether or not we need to think about how can these various things work together, like an opinionated framework which helps get you a web page standing up very quickly like Up. It's much different from, you know, being able to handle the more complex scenarios that serverless tries to approach. Can they work together? Can they have [inaudible ::]. They're trying to address different parts of the application problem, but they do it entirely independently with their own sets of assumptions with how things are gonna work. I do wonder whether or not we need to spend time over the next, you know, however long out time frames actually are nowadays. They get quite short. We move very quickly nowadays. Thinking about how do things like the tools and the frameworks that we have coordinate with the cloud vendors to make sure that we're not having to reinvent the wheel every time we go and approach one of these new novel sets of problems. We don't need a fourth way of, you know, deploying the code out there potentially, but there's totally room for, you know, , , , , , , different ways of building my application. There's lots of different ways to think about that. I wonder if that's where we kind of need to think about of how can we coordinate better to, you know, get more productive value. We don't wanna solve the same problems twice. Austen: Yeah, I think we see some effort around this right now especially in the CNCF in their serverless working group I think it's needed but it'll be interesting to see how it checks out. Outside of the vendors, where do you see this going, Anne? Where's the future of serverless event-driven architectures? Anne: Well, those are two separate distinct things. Serverless I think is very much being driven by the economics as I've said a couple of times. But I think that right now we have a girth [SP] of tooling to help people build real systems that way. Right now it's all very primitive, so at what point does serverless kind of replace the traditional PAS? [SP] I don't know. I also think that there's certain applications that probably are not well-suited to be deployed in serverless, so that means that regular PAS may stick around in perpetuity. But the next question is, you know, when I get Pivotal or OpenShift or IBM Cloud or something like that, should I expect that there's a serverless space inside that standard PAS or do I continue to look at this decomposition of platform into, you know, , different types of services that I then have to pick and choose and figure out. It becomes a really complex world. For example, if I'm using app service, I've got like basically four different types of services that I work with. Austen: Just one. Anne: But you know, it's a relatively simple thing for me to create an application in that environment. On the other hand, when I start building things based on Lambda and the other serverless services that AWS supplies, and I have to kind of navigate my way through all these different... That's too difficult for the average mainstream application development organization in you know, a big insurance company to really grok. [SP] You know? And meanwhile, you've got a bunch of braggarts out there who still haven't quite figured out how to build services in the first place, right? When are they gonna get into that space? So, you know, I think that the biggest challenge right now is that significant learning curve to move into the event-driven architecture. And I still have big debates with my colleague. [inaudible ::] about whether or not event-driven architecture is going to become a mainstream approach or whether it's gonna remain a fringe. I think it's gonna become bigger than it has been, but I don't know. Jason: I would also say that the vendors are on the hook for some of this as well. You know, we've spent time trying to create a primitive that we think, you know, we generally I feel like we've spent time creating primitives that are the building blocks, but what we lack now are the design patterns that make the most sense. And the reason I say it's incumbent upon the providers in some part is because as soon as you make something auto awesome, it's hard to be completely auto awesome. There are going to be characteristics that sort of bleed through that if you do something a certain way it's gonna behave differently in the infrastructure, but there's no way for the customer to know what because deliberately it's a black box. And so, I'll give you a concrete example. In a serverless space when these functions spin up out of thin air, there's this period of time referred to as cold start. Well, what affects the duration of a cold start? It's partially the infrastructure, it's partially the code that you write, it's partially what happens during that cold start. All the customer knows is, \"Oh, this thing is slow,\" or, \"It's taking a long time.\" And have they architectured it wrong? Have they written their code wrong? It's really not clear. And so, even before you even get to, \"How do I build an ER peace system, how do I do service discovery? There's basics that I think certainly there's some assistance that providers can make to help people understand, help people grok not necessarily what it is, but where do I put it where it makes sense? Chris: I think I'm glass half full there already, you know, we can already do a lot...at least talking about my platform like we have lots of data about what happened when your function ran. The nice thing about functions versus like app service is the service area which you're programming against a platform. I know a lot about your function, I know what's gonna trigger it, I know how long it ran for, I can even see the kind of the state of the memory at the time it was going. We can do a lot of things to plug, you know, EPM's deeply into the platform to get insights in a way that on app service I have to beg you to please include the EPM on your service, and if you don't, i try my best to look at your logs and see if I can help you if there's a debugging event. I've actually enjoyed how much easier it is to debug functions problems when it comes to servicing loops that I have to do compared to app service problems in a lot of cases because I have a lot more insight into what the customer may or may not have done correctly. So, I actually see a pretty bright future here where not only it'll be easier as we make these tools and frameworks better to tell people that you're doing something wrong. It'll be easier to guide in through a direction. so it's not just saying, \"Oh, you had an oom issue.\" I can tell you why you had an oom issue to an extent. Anne: Yeah. You know, personally, I get really excited when I look at systems being built with event sourcing, CQRS because I think I have so much more control over how updates are being executed and where that information has to get directed to. And you know, I look at this and I think, \"This is like the best way to do master data management. Because I have the ability to capture every single event and I have the ability to process every single event, and I have the ability to push that even to wherever it needs to go.But I also recognize that because it's a very complex environment and there's so many interdependencies within the system, that lots and lots of people are going to build systems that are just fundamentally awful. Austen: Yeah. I'm sure that these gentlemen have seen that. We've certainly seen it from the vantage point of the serverless framework. Yeah, guidance is needed here, certainly. And actually, on that topic, if there's a team or organization that wants to get started doing serverless stuff, do you have any best practices, logical starting points for them that you recommend? How is this application, how is this architecture similar to past architectures and, also what do developers have to think about what's different with the serverless architecture? Jason: I think there's a couple of things that come to mind. One is the stateless nature of the compute that you're running. And that's an easy word to say but people still have a tendency to rely on state in some form, and things like...even things as trivial as maybe a database connection pool. You know, in a traditional model you would say, \"Well, I have a connection pool because it's a performance optimization that I'm going to do.\" But that pool is stateful in the container and when that container disappears that pool disappears, and so what are you really pooling at that point? So, thinking about things in those terms and then on the topic of data, do you have just a regular database and then how is that going to scale? I think there's a sort of a...not a hurdle so much but a step that needs to be taken to understand again, coming back to this being instructed in some way, how does this actually work? What is the execution environment for this? What are the things that I need to understand? Under what conditions does it scale? Where does it scale? So, there's those sorts of questions that we get asked quite a lot. And then the other side of it which is a much more gnarly set of problems is about security and authorization. You know, comments have been made today about different types of events and what data is inside the event. You can if you're not very careful run into situations where the person viewing the payload of the event was not authorized to view the content that emitted the event. And who's making sure that that's all as it should be? Again, one of these sort of nascent challenges that will hit when things start to grow in complexity. Austen: Chris, any thoughts for developers, teams, or organizations getting started with this architecture? What's similar and what's different about it? Chris: Yeah, I mean again, it really depends on your provider. We've done a lot of stuff with functions that can be a bit different from other providers, so we've even had to spend as much time in like the deployment madness world because we do the multi-functional deployment thing. So, that's generally the first advice in the rest in the world, but for us, we have to skip that particular nightmare. The thing to think about, you have to kind of think about the sense of problems that you'd have with microservices, right? What functions do I associate with each other? How do I actually approach these things as a team? If I've got you know, one section of things that have to be modified, I have to manage the service versioning, you know, stuff that was talked about in the last talk. You know, a lot of problems that you have with functions after you get over the fact that you don't have to worry about managing them, your architecture problems are pretty similar to, you know, traditional microservices problems. So, you can generally start there, go read some blogs, Martin Fowler's blog and you know, you'll be in a kind of a good state in terms of where your headspace needs to be. And a lot of times it's just, you know, it's really simple. Like for the most part just to kind of take each problem step by step by step. There's a lot of good documentation out there. You know, a year ago if you asked me this problem, I'd be like, \"Oh, there's a ton of docs and samples and stuff like that that needs to happen.\" But it's been a fast year, right? I mean, I'm starting to see a lot more good content out there for people to get started with this stuff. You know, there's a lot more answers out there than there used to be, and I think that we'll continue to kinda see that progress. You know, if someone is watching this video, you know, three months in the future, six months in the future, even by then, they'll be a new set of, you know, tools, frameworks, docs out there to go and address these problems. You know, I think that we're gonna keep on seeing great progress there. And, you know, just like any other sort of technology stuff that we've been doing, progress happens pretty quickly and it's always targeted towards solving these problems, so, you know, kind of have that faith that it's gonna move in that direction. Austen: Sure. And your providers have been moving incredibly fast. It's been amazing to watch the rate of innovation. There's a time where I thought that startups had the advantage of moving faster, but these big organizations are moving at incredible speed, and I think they've adopted a lot of the startup methodologies to be able to do that, smaller teams, microservices, so kudos to you in managing these teams and guiding them to success. So, it's been exciting to watch this whole space evolve. And again, three months later, I mean, three months from now, we might be having a totally different conversation, but it'll be a very interesting one, I'm sure. Anne, from your perspective, any advice for people just getting started, organizations and teams just getting started with serverless architectures? Anne: Yeah, focus on data. You know, this is not just about your code, but it's also about data that's owned by the code and it's how you partition that data. You know, one of the presenters, I forget which one, was talking about doing aggregations and letting anybody do aggregations of your data. But are you sure that's actually gonna wind up being valid data? Is it quality data if everybody can just go create their own aggregations? So, yeah, I think that data partitioning issue is gonna bite a lot of people and, you know, if you don't actually have a decent large-scale perspective of how this data is coming together. And then, of course, there's also the event schemas which now become the tight binding between your components. So, putting in some type of mediation in there that enables a little bit flexibility between consumers and providers of those events, that's also another really critical piece. Jason: One other thing I'll say is that what we said before about it being simple, I think the best way is just to actually roll your sleeves up and deploy a function. And a lot of these things will become apparent really quickly. It's one of the reasons why it's such a wave of adoption of this is because it's so easy to consume. And I think if there's uncertainty around, \"I'm not sure what serverless is,\" from customers, then, you know, the simplest thing is just to put a person in front of the console and have them deploy \"Hello, World!\" in you know, a matter of minutes, so a light bulb will go off. Austen: That magic moment, yeah. Anne, did you have any questions you wanna ask? We're just coming up on the end of the session here, so... Anne: So, let's see. I would say that my biggest question is how you're going to bring together the serverless capabilities with the rest of your cloud services? Chris: Yeah, I mean, it's really a good question. Now that we added the event grid service, like, a lot of, you know, our conversations have been about how do we actually bring these things together in the same way? We've had to do the same thing with logic apps where we kinda have this nice little visual UI for combining logic apps plus the functions, but then you start to jump down into like Visual Studio or something and the tooling is a little bit less connected. I think the way that we approach it is the same way that we approach a lot of problems. We need tools and frameworks. We have the primitives now and the services. I think, in order to build more complex systems, you want the tool and the frameworks to help address that complexity. The primitives themselves need to get more complex. The tooling needs to become better to solve complexities of combining those various primitives together. Jason: Yeah, I think it's all about events in that space. You know, I think I agree with you in some sense that event-driven model, for a lot of organizations, is a big scary monster. Within Google, it's all events, and the way that we tie, you know, the components of the platform, other resources is all through an event model. So, you know, and I mentioned that I think, Austin, you mentioned that the rate at which we're doing things. And I said before that we're focused on building a platform. Well, one of the advantages of that is that there is internal consistency across services, and so it's not a huge challenge, implementation-wise, but really, it's just all about events. Austen: I agree. Again, it's why we're here. We just hit the end of the session, so I'd like to thank Jason and Chris. Thank you so much for volunteering and agreeing to join this panel. Anne, you are a saint for jumping in at the last minute and giving a wider industry perspective. I really appreciate that. And I think that's the end of our conference. I'm looking forward to seeing all of you at the after party. And thank you so much for attending, it's been a lot of fun.",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603eae",
      "title": "The Serverless guide to AWS re:Invent",
      "content": "What to expect and which sessions to sign up for. General re:Invent protips Never been to re:Invent before? Here's what you're in for. The venues are pretty spread out, so getting from one to the other can be time-consuming. Try to plan your day such that you stay in the same building if you can. It's tempting to be ambitious and keep your days jam packed, but really, don't wear yourself out. Aim to hit - sessions a day. If you're familiar with serverless already, stick with the + level classes. The s tend to be more basic. Remember that the biggest value you'll get out of re:Invent are the conversations you'll have there. So during the breaks, meet people! We really can't emphasize this one enough. Go to the keynotes. This one's in bold for emphasis. All the cool announcements and launches at re:Invent are going to happen during the keynotes. You probably don't want to miss these. We're predicting AWS will have some surprise Lambda updates to talk about. Perhaps more event sources? New language runtimes? Longer time-outs?! Serverless.com's re:Invent Happy Hour Did we mention we're having our own happy hour? November th at the Rhumbar. Drinks! Swag! Serverless! RSVP to snag a spot. Your recommended serverless track Feel free to peruse a whole list of serverless-forward re:Invent sessions=&i()=) in those handy search results. Or! Our curated list is below. Advanced (- level) ARC - Serverless Architectural Patterns and Best Practices | Venetian Security best practices, operations, and nuances of serverless architectures. The session will also cover how to migrate server-full workloads over to serverless. Includes: X-Ray, step functions, security automation using AWS Config, and CI/CD development pipelines. ARC - Getting from Here to There: A Journey from On-premises to Serverless Architecture | Venetian How do you get from on-premise to cloud-native environments? They'll present migration strategies for different types of workloads. Includes: API Gateway, Lambda, Cognito, SQS, SNS SRV - Building CI/CD Pipelines for Serverless Applications | Aria Automate serverless application deployment! We get asked about CI/CD pipelines a lot, so here's your chance to learn 'em. This session is run by Chris Munns and Ben Kehoeboth big names in serverless. Includes: CodePipeline, CodeBuild, CodeStar, best practices straight from iRobot SRV - Workshop: Serverless DevOps to the Rescue | Aria As was said over and over at ServerlessConf, it's not NoOps, but rather DiffOps. Learn what that means in this workshop. You'll join the DevOps team for a popular ride-sharing application. Build, test and deploy changes to their serverless app. Includes: X-Ray, CodePipeline, CodeBuild, CI/CD pipelines SRV - Designing Microservices with Serverless | Aria How do you deploy and manage microservices in a serverless paradigm? Here are some things to consider around code structure, how they communicate with their dependencies, and more. Includes: National Geographic presenting on their own serverless microservices architecture SRV - Building Resilient, Multi-Region Serverless Applications | Aria While serverless is already high availability, you can go one step further with multi-region. Here are some different options for active/active and active/passive setups. Includes: API Gateway, Lambda SRV - Workshop: Build a Multi-Region Serverless Application for Resilience and High Availability | Aria Or try the workshop version! You'll become a developer at a ridesharing company, and work to design a highly-available user reporting feature. Includes: API Gateway, DynamoDB, Route , CloudFront, S SRV - State Machines in the Wild! How Customers use AWS Step Functions | Aria Shameless plug: Coca-cola uses the Serverless Framework. Patrick Brandt will go over how they created a customer loyalty program with Step Functions. The session explores all kinds of neat things that you can do with state machines. Includes: Step Functions, operations automation, state management SRV - Serverless OAuth: Authorizing rd-party Applications to your Serverless API | Aria Build a serverless web app with a heavy focus on security controls and best practices. The session will further help you integrate social media sign-in to create a universal user directory. Learn more about identity management, role-based access, and more. Includes: API Gateway, Lambda, DynamoDB Introductory ( level) SRV - Build a Scalable Serverless Web Application on AWS That Can Span Millions of Concurrent Users | Aria This session is clearly based off the Australian census problem. You'll learn how to build a highly-available, low latency website that can handle a country-wide public poll. Includes: Lambda, API Gateway, S, DynamoDB SRV - Thirty Serverless Architectures in Minutes | Aria Use Lambda to build slack bots, automate testing, and more. They'll cover basics on deploying, monitoring and profiling serverless applications. Includes: Lambda, API Gateway, fun serverless projects SRV - Improving Microservice and Serverless Observability with Monitoring Data | Aria Synthesize best practices around metrics, logs, and observability. Learn how to troubleshoot applications and serverless infrastructure. CMP - Getting Started with Serverless Architectures | Venetian Curious about what serverless can do for you? Learn about their benefits in handling APIs, data processing, and website backends. Includes: Lambda, API Gateway, Step Functions CTD - Introduction to Amazon CloudFront and AWS Lambda@Edge | Venetian Learn how to accelerate delivery of websites, APIs, and streamed media content. Lambda@Edge is a serverless service that lets you customize content delivery through CloudFront. Includes: Lambda@Edge, CloudFront, CDN strategy And finally... Don't forget to keep your eye on those afterparties and happy hours. Here's a pretty good unofficial list of re:Invent happenings. Have fun out there. ",
      "__v": 0
    },
    {
      "_id": "64e0891db72e199dda603eb0",
      "title": "CRDTs explained - supercharge your serverless with CRDTs at the Edge",
      "content": "You all read this blog, and are probably pretty familiar with serverless concepts. But few of you are likely to be fluent in CRDTs. Its okay, theyre super new. Like, new new. But CRDTs are quickly gaining traction. Im Kuhir founder Russell Sullivan; Im here to yank these concepts out of the lofty academia-sphere, and place them squarely in the what can I use these for engineering-sphere. By the end of this post, you will be the foremost CRDT expert you know (unless you know Jared Short). In this post, were going to do two things: set the stage by defining a few high-level CRDT concepts, and then take a deep dive by walking step-by-step through a CRDT video explainer. Part : CRDT concepts Here, well set the stage for our deep dive a bit further down. If you already feel pretty primed for CRDTs, then feel free to skip down to Part . What even is a CRDT? CRDT stands for Conflict-free Replicated Data Type. In sum, CRDTs provide a way for you to merge concurrent modifications, always, in any order. Lets talk more about what CRDTs are, how they work, plus what they mean for serverless multi-region and failover. Serverless & CRDTs At its core, serverless is based on event-driven functions. The further you commit to the serverless architecture, the further you embrace asynchrony. CRDTs are designed from the bottom up to thrive in asynchronous environments. The marriage of serverless and CRDTs has implications ranging from IoT to the cloud. Think: CDN-Edge data replication, multi-master data center replication, shared data between IoT/mobile devices, offline first data, asynchronous materialized views...it goes on. All of these use cases are event-driven and asynchronous. Right in CRDTs wheelhouse. The serverless data layer At Kuhir, we use CRDTs as the base data layer of our serverless stack. Kuhir runs a globally distributed Stateful Serverless At the CDN-Edge system (SSAE). SSAE pushes both serverless and real-time state to the CDN edge. This means your edge-based functions call into shared global data at the CDN edge. Your global user base calls into a nearby SSAE edge to process dynamic web requests with predictably low latency. (More details on this up at High Scalability.) Part : CRDTs explained Below is a video walking through some slides I made called CRDTs for Non-Academics. My goal with those slides was to keep it simple: explain what CRDTs are, how they work, the gotchas, and their overall flowall with a bare minimum of academic terms. To make sure its fully grok-able, there will also be GI-Joes and Gandalfs. Without further ado, heres the video! For those not into watching a minute video rant on the ins and out of CRDTs, I wrote a summary. Keep scrolling to read that instead. The summary is short on text and can be skimmed quickly: you can pick and choose which snippets you watch. Full video: CRDTs for non-academics: Scene by scene video break-down WTF are CRDTs? When you ask most engineers about CRDTs, you usually get a reaction like this: The standard response to this WTF is to explain CRDTs via an acronym-filled, high-level mathematical/comp-sci dissertation. One that involves words like: semi-lattices, state-based, causality. Too complicated. IMHO, CRDT explanations should go like this: They act autonomously and still provide consistencylike the magical offspring of a Pegasus and Gandalf. Strong Eventual Consistency (SEC) How do CRDTs provide Autonomy and Consistency? By relaxing ACID consistency into what is called Strong Eventual Consistency (SEC). SECs guarantee all actors will eventually converge to the same state without data loss. These guarantees are tailor-made for a distributed asynchronous world. In this distributed world, there is no guarantee that all actors have the same value at a given point in time, but they will get there eventually. And no data will be discarded in the process (which happens in EC). CRDTs perform replication as commutative operations. This has the desirable quality (for distributed systems) that order of replication does not matter. Replication in an arbitrary order fundamentally reframes many distributed race conditions, and its usefulness increases as asynchrony (e.g. distribution) increases. CRDT counter increment example For a real world example, lets take a look at what happens when different actors concurrently increment the same counter. It does a good job of showing how CRDTs replicate the commutative addition operation. Video : CRDT counter increment example Now that we have shown how commutative replication works for incrementing counters, we will make a bold assertion: Commutative Replication works for all of JSON (nested JSON included). JSON consists of data types: `[string, number, object, array]`, for example:  These data types only require base operations to build all higher level operations. JSONs data types and respective operations are shown in the table below: The operations `increment`, `insert`, and `delete` are commutative and function much as the increment counter example we just walked thru. The `set` operation is convergent and behaves differently. A convergent operation is one where all actors eventually converge to the same state (set uses Last-Writer-Wins) but values during convergence may differ between actors. Last Writer Wins (LWW) To demonstrate how Last-Writer-Wins (LWW) works in a distributed environment, we show an example of gurus sitting on different mountains answering the question: What is the meaning of life. Since we are using LWW, the last one to give an answer wins. Simple, right? But theres more to it. As the end of the video shows, while all of the gurus followers eventually converge on the same final answer, the answers they have during this time of convergence differ. This oddity to distributed LWW is something CRDT users must take into account while architecting. Video : three gurus Decrementing counters Enough philosophy, lets get back to straightforward CRDT examples. We started with counter incrementing, next up we decrement counters. (Note: these examples build on one another, so youll really want to watch them all in order from the beginning.) This shows a simple mechanism CRDTs use for increment/decrement counters. We use two countersone for increments, one for decrements. The final value is adding them together (yet another commutative action). Video : decrement counter example: SET example Next, lets see an example of setting values. In this case, well do concurrent numerical `SET`s. This example shows how the `SET` operation converges to the value + via LWW. Video : SET example: Resetting a field Now, to demonstrate the full life cycle of a field, lets look at an example of resetting a field. This will illustrate how a late-coming operation is ignored via versioning (implemented via per-field UUIDs). Sounds complicated, but the video makes it simple and intuitive. Video : SET then RESET CRDTs and versioning One of CRDTs core functionalities is versioning, and they use a lot of additional metadata to resolve conflicts. Examples of this metadata are: Document: UUID & Garbage Collection version Field: UUID & Timestamp Delta: Dependency vector clocks Versioning holds the state commutative algorithms require to be able to autonomously resolve conflicts and converge to the same state without using consensus. In the next example we `DELETE` a field and then we (RE)`SET` the field. Then, similar to the `RESET` example, we show a late-coming operation being (correctly) ignored. Video : DELETE example Modifying nested JSON Time to move up to modifying nested JSON. We start with JSON Objects, often referred to as dictionaries. CRDTs create UUIDs for each nested field. They reference those nested fields by specifying all field-UUIDs in the nested fields path. In order to be applied, operations for a nested field must match all UUIDs in the fields path. Otherwise, they are ignored. Video : objects & dictionaries JSON arrays Next up is JSONs other nested field: the Array. Arrays are initialized via set operations (e.g. `SET H=[]`). Array elements can be overwritten (`SET H[]=X`), or inserted between existing elements (`INSERT` value Y between the nd and rd array elements). Video : arrays & linked lists Arrays in JSON are also used to represent causality (such as linked lists). They can represent that something (A) happened and then something else (B) happened afterwardsrepresented as A This next video shows an example of how concurrent inserts work in practice. Video : concurrent array inserts CRDT consistency models Finally, lets take a look at causal+ consistency, the academic term for CRDTs consistency model. CRDTs accomplish causality via vector clocks, which are created on each modification and sent with each delta. The vector clocks represent a deltas distributed dependencies. Upon replication, a delta will only be applied once all of its dependencies have been satisfied. This video is silly, but it does a good job at explaining a difficult distributed concept. When a team of GI Joes fail to respect causality of inserts into a chat array, things go pretty badly for them. :) Video : causal+ consistency (GO JOE!) Conclusion We have examined how CRDT operations work on the operations required to cover the JSON data types. We have explained the convergent property of `set` to be sub-optimal but still useful when utilized correctly. We walked through examples demonstrating how replication race-conditions are handled by commutative algorithms and lots of versioning. And finally, we (us and the Joes) learned that CRDTs provide causal+ consistency via delta vector clocks representing the deltas dependencies. Even more to learn This is just an introduction to CRDTs; the rabbit hole goes a lot deeper. But its a good start, and lets be honestit was a ton of content in a short amount of video/text. One of these days, Ill write about the rest of CRDT concepts: tombstones & garbage collection, peer-to-peer mesh of clusters, extreme robustness, and architectures spanning IoT to CDN-Edge to DataCenter. For now though, lets just let this post marinate a bit. Weve learned a lot today, yeah?",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603eb2",
      "title": "The ABCs of IAM: Managing permissions with Serverless",
      "content": "When getting started with Serverless, one of the hardest things to grok is IAMAWS Identity and Access Management. IAM is how you manage access to resources in your AWS account. Who is allowed to create a Lambda function? To _delete_ a function? This isn't the only IAM guide you'll ever need, but you should understand how IAM works with Lambda and the Serverless Framework. We'll cover the basics of IAM to get you on your way. In this guide, we'll go over: - Basic IAM concepts - The two kinds of IAM entities with the Serverless Framework - Managing permissions for the Serverless Framework user - Managing permissions with your Lambda functions Let's get started! Basic IAM concepts There are three basic concepts you should understand in the world of IAM: users, roles, and permissions. An IAM user is pretty close to what it sounds likea user that is created to interact with AWS. Usually, this is an actual person within your organization who will use the credentials to log into the AWS console. This person often has _access keys_ to programmatically interact with AWS resources. Access keys consist of an \"access key ID\" and a \"secret access key\". Together, they can authenticate a particular user to AWS to access certain resources. You might use them with the AWS CLI or a particular language's SDK, like Boto for Python. An IAM role is similar to an IAM user, but is meant to be assumed by anyone or anything that needs to use it. An IAM user could assume an IAM role for a time, in order to access certain resources. An IAM role could also be assumed by another AWS service, such as an EC instance or a Lambda function. Your Lambda function assuming an IAM role will be important later when we discuss managing permissions with your Lambda functions. Finally, an IAM permission is a statement that grants/blocks an action(s) on a resource or set of resources. An IAM permission contains three elements: _Effect_, _Action_, and _Resource_. (It may optionally include a _Condition_ element, but that's outside the scope of this article.) Effect tells what effect the IAM permission statement haswhether to Allow or Deny access. Generally, an IAM user does not have access to AWS resources. Most IAM permissions have an Effect of \"Allow\" to grant access to a particular resource. Occasionally, you might have an Effect of \"Deny\" to override any other \"Allow\" permissions. Action tells what action an IAM user or role can take as a result of the IAM permission statement. An Action has two parts: a service namespace and the action in that namespace. For example, the Action of `s:GetObject` affects the GetObject action in the s service namespace. You can use wildcards in the Action, such as `ec:` to allow all actions in the EC namespace, or simply `` to allow all actions anywhere. (Hint: Don't do this). Resource tells what resources the permission statement affects. The value is an ARN or list of ARNs to which the statement applies. This lets you give permissions on a more granular basis, such as limiting the ability to query a particular DynamoDB table rather than granting the ability to query _all_ DynamoDB tables in your account. Like the Action element, you can use the wildcard `` to apply the statement to _all_ resources in your account. To make this more concrete, let's see one of these statements in action. Imagine you've created a DynamoDB table named \"my-new-table\", and it has the ARN of `arn:aws:dynamodb:us-west-::table/my-new-table`. Now, you want to create a policy that allows your application to do read & write commands against your table. You would have a policy like:  We see the three permission elements noted above. The Effect is \"Allow\", which grants the listed actions on the listed resources. The Action block contains a list of needed DynamoDB actions, such as GetItem, PutItem, and Query. Notice that it does not include CreateTable and DeleteTablethat is more of an administrative role that your application wouldn't need. Finally, the Resource block has our table's ARN. This limits the scope of the permissions to our table only, so our application wouldn't have the ability to query other tables in our AWS account. IAM permissions can be attached to users or roles (or other things that we won't cover here). This means you can create an AWS user and give it the permission to create DynamoDB tables, view CloudWatch logs, or any of the many other things you can do with AWS. The Two Types of IAM entities with the Serverless Framework When talking about IAM permissions with the Serverless Framework, there are two different entities (users or roles) that you need to worry about: - The IAM user used _by the Framework_ to deploy your Serverless service (the Framework user) - The IAM role used _by a Lambda function_ when it's executed (a function role). To see the distinction, consider the example application in our Express REST API walkthrough. In that example, we deploy an Express application with a DynamoDB table backing it. When we run `sls deploy` to deploy the application, we need to be concerned about the IAM user used by the Framework. This is the user referenced to by the `profile` property in the `provider` block of your `serverless.yml`, or the \"default\" profile if you don't set it. The Framework will look in `~/.aws/credentials` for your access keys, then deploy your application. In deploying your application, your IAM user will need permissions to: - Create an S bucket for your function deployments - Upload your function zip files to that S bucket - Submit a CloudFormation template - Create the log groups for your Lambda functions - Create a REST API in API Gateway - Create a DynamoDB table Once your service is deployed, you have a different set of IAM issues to worry about. The functions handling your HTTP requests have permissions issues of their ownthey need the ability to query the DynamoDB tables and send log messages to CloudWatch. With this understanding in mind, let's walk through how we configure and manage the Framework user and how we manage the IAM permissions for our function roles. Managing permissions for the Serverless Framework user Let's talk about IAM permissions for the Serverless Framework user. This is any permissions that are required when you run a command with the Serverless Framework, such as `sls deploy` or `sls logs`. The Framework is making its calls to AWS using the Node aws-sdk. This means credentials are generally loaded from a file in `~/.aws/credentials` (for Mac/Linux users) or `C:\\Users\\USERNAME\\.aws\\credentials` for Windows users. If you haven't set up permissions before, you'll need to create an IAM user with access keys and the required permissions. There are basically two ways you can approach this: - Fast but risky (aka YOLO): The fastest way to get started with Serverless is to create an IAM user with Administrator Access. This IAM user will have full access to your AWS account and should not be used for your company's production AWS account. The best approach here is to create a new AWS account or a new AWS organization with limited ability to affect other resources. This will give you the widest latitude to experiment with Serverless without getting tangled in a web of IAM permissions. - Slow but safe: If you're using Serverless in production, you'll want more tightly-scoped permissions. With security, you generally want to follow the principle of least privilege. For AWS, this means your Serverless IAM user shouldn't have the ability to alter the Lambda functions and resources of other services in your AWS account. This can be quite difficult but is worth the added security, particularly in a production account. One of our community members has contributed a Yeoman generator template. This generator makes it much easier to create a narrow IAM policy template that will cover many Serverless use cases. To use it, first install Yeoman and the `serverless-policy` generator:  Then run the generator and answer the prompts:  This will create a JSON file in your working directory with permissions scoped to your service. It's not perfect, but it will get you closer. Create an IAM user with that policy fileor ship it to the person in charge of IAM security at your companyand you should be on your way. Managing permissions for your Lambda Functions The second aspect of IAM with Serverless is the permissions for your Lambda functions themselves. If your functions read from a DynamoDB table, write to an SQS queue, or use a KMS key to decrypt a string, they'll need to be given specific permission to do that. You can add these additional permission statements directly in your `serverless.yml`. To add these permissions, use the `iamRoleStatements` section of the `provider` block. Let's use our DynamoDB example from the first section:  This block gives our functions the ability to query, scan, and manipulate items on a particular DynamoDB table. Pro-tip: You can use CloudFormation Intrinsic Functions to make it easier to refer to specific resources. For example, if you've created your DynamoDB table in the `resources` section of your `serverless.yml`, you can use the `Fn::GetAtt` intrinsic function to get the ARN:  Here, we dynamically grab the DynamoDB table ARN by using `Fn::GetAtt`, and pass in the logical Id of the resource `MyDynamoTable` from the `resources` block. Then we request the `Arn` property. You can see which attributes are available for a particular CloudFormation resources by checking the `Return Values` section of the CloudFormation referencesee here for a DynamoDB example. You can also craft custom IAM roles for each function in your `serverless.yml`, but be advised this is an advanced feature. You'll need to make sure to specify _all_ permissions of your functions, including some that Serverless usually handles for you, such as the ability to write to CloudWatch logs. There's a `serverless-puresec-cli` plugin that assists in this process. Puresec scans your functions to see which AWS resources they're accessing and how to automatically create least-privilege roles. It doesn't cover all resources yet, but it is a good start if you're interested. Conclusion IAM permissions are complex, and there's a lot more to learn than what is covered in this article. But this should be a great starting point. Live long and permission. ",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603eb4",
      "title": "Build a Python REST API with Serverless, Lambda, and DynamoDB",
      "content": "Your existing web framework tooling can work seamlessly with the Serverless Framework. Lets go over how to use the Python web framework Flask to deploy a Serverless REST API. In this walk-through, we will: - Deploy a simple API endpoint - Add a DynamoDB table and two endpoints to create and retrieve a User object - Set up path-specific routing for more granular metrics and monitoring - Configure your environment for local development for a faster development experience. If you already have a Flask application that you want to convert to Serverless, skip to the Converting an existing Flask application section below. If you want to skip the walkthrough and just get started with a fully-configured template, check out the Using the Quick Start Template section below. Getting Started To get started, you'll need the Serverless Framework installed. You'll also need your environment configured with AWS credentials. Creating and deploying a single endpoint Let's start by deploying a single endpoint. First, create a new directory with a `package.json` file:  Then, install a few dependencies. We're going to use the `serverless-wsgi` plugin for negotiating the API Gateway event type into the WSGI format that Flask expects. We'll also use the `serverless-python-requirements` plugin for handling our Python packages on deployment.  With our libraries installed, let's write our Flask application. Create a file `app.py` with the following contents:  This is a very simple application that returns `\"Hello World!\"` when a request comes in on the root path `/`. It's the example application shown on Flask's landing page with no modifications. To get this application deployed, create a `serverless.yml` in the working directory:  _Note: a previous version of this post set `dockerizePip: true` instead of `dockerizePip: non-linux`. You'll need `serverless-python-requirements` v.. or higher for this option._ This is a pretty basic configuration. We've created one function, `app`. The handler is `handler` function from the `wsgi` module. Note that this module will be added to our deployment package by the `serverless-wsgi` plugin. We configure our application's entry point in the `custom` block under the `wsgi` section. We declare that our app's entrypoint is `app.app`, which means the `app` object in the `app.py` module. For our function's `events` configuration, we've used a very broad path matching so that all requests on this domain are routed to this function. All of the HTTP routing logic will be done inside the Flask application. The last thing we need to do is handle our Python packages. The `serverless-python-requirements` plugin looks for a `requirements.txt` file in our working directory and installs them into our deployment package. Let's build that `requirements.txt` file. First, create a virtual environment and activate it. I'm using Python in my `serverless.yml`, so I'm specifying that here as well:  Then, install the `Flask` package with `pip`, and save your dependencies in `requirements.txt`:  Now, deploy your function:  After a minute, the console will show your `endpoints` in the `Service Information` section. Navigate to that route in your browser: You did ita real, live application on the Internet! Adding a DynamoDB table with REST-like endpoints Doing a \"Hello World!\" is fun, but your application will need to persist some sort of state to be useful. Let's add a DynamoDB table as our backing store. For this simple example, let's say we're storing Users in a database. We want to store them by `userId`, which is a unique identifier for a particular user. First, we'll need to configure our `serverless.yml` to provision the table. This involves three parts: Provisioning the table in the `resources` section; Adding the proper IAM permissions; and Passing the table name as an environment variable so our functions can use it. Change your `serverless.yml` to look as follows:  We provisioned the table in the `resources` section using CloudFormation syntax. We also added IAM permissions for our functions under the `iamRoleStatements` portion of the `provider` block. Finally, we passed the table name as the environment variable `USERS_TABLE` in the `environment` portion of the `provider` block. Now, let's update our application to use the table. We'll implement two endpoints: `POST /users` to create a new user, and `GET /users/{userId}` to get information on a particular user. Update your `app.py` as follows:  In addition to base \"Hello World\" endpoint, we now have two new endpoints: - `GET /users/:userId` for getting a User - `POST /users` for creating a new User We've added a `boto` dependency, so let's install that into our virtual environment and update our requirements in `requirements.txt`:  Let's deploy the service and test it out!  We'll use `curl` for these examples. Set the `BASE_DOMAIN` variable to your unique domain and base path so it's easier to reuse:  Then, let's create a user:  Nicewe've created a new user! Now, let's retrieve the user with the GET /users/:userId` endpoint:  This isn't a full-fledged REST API, and you'll want to add things like error handling, authentication, and additional business logic. This does give a framework in which you can work to set up those things. Path-specific routing Let's take another look at our function configuration in `serverless.yml`:  We're forwarding all traffic on the domain to our application and letting Flask handle the entirety of the routing logic. There is a benefit to thisyou don't have to manually string up all my routes and functions. You can also limit the impact of cold-starts on lightly-used routes. However, we also lose some of the benefits of the serverless architecture. You can isolate your bits of logic into separate functions and get a decent look at your application from standard metrics. If each route is handled by a different Lambda function, then you can see: - How many times each route is invoked - How many errors you have for each route - How long each route takes (and how much money you could save if you made that route faster) Luckily, you can still get these things if you want them! You can configure your `serverless.yml` so that different routes are routed to different instances of your function. Each function instance will have the same code, but they'll be segmented for metrics purposes:  Now, all requests to `GET /users/:userId` will be handled by the `getUser` instance of your application, and all requests to `POST /users/` will be handled by the `createUser` instance. For any other requests, they'll be handled by the main `app` instance of your function. Again, none of this is required, and it's a bit of an overweight solution; each specific endpoint will include the full application code for your other endpoints. However, it's a good balance between speed of development by using the tools you're used to, along with the per-endpoint granularity that serverless application patterns provide. Local development configuration with Serverless offline plugin When developing an application, it's nice to rapidly iterate by developing and testing locally rather than doing a full deploy between changes. In this section, well cover how to configure your environment for local development. The great thing about the `serverless-wsgi` plugin is that it includes a built-in solution for local development. To start the local server, just run `sls wsgi serve`:  Then navigate to your root page on `localhost:` in your browser: It works! If you make a change in your `app.py` file, it will be updated the next time you hit your endpoint. This rapidly improves development time. While this works easily for a stateless endpoint like \"Hello World!\", it's a little trickier for our `/users` endpoints that interact with a database. Luckily, there's a plugin for doing local development with a local DynamoDB emulator. We'll use the `serverless-dynamodb-local` plugin for this. First, let's install the plugin:  Then, let's add the plugin to our `serverless.yml`. We'll also add some config in the `custom` block so that it locally creates our tables defined in the `resources` block:  Then, run a command to install DynamoDB local:  Finally, we need to make some small changes to our application code. When instantiating our DynamoDB client, we'll add in some special configuration if we're in a local, offline environment. When developing locally, the `serverless-wsgi` plugin sets an environment variable of `IS_OFFLINE` to `true`, so we'll use that to handle our config. Change the beginning of `app.py` to the following:  Now, our DynamoDB client is configured to use DynamoDB local if we're running locally, or use the default options if running in Lambda. Let's see it if works. You'll need two different terminal windows now. In your first window, start up DynamoDB local:  In the second window, start up your local WSGI server:  Let's run our `curl` command from earlier to hit our local endpoint and create a user:  And then retrieve the user:  Yep, it works just like it did on Lambda. This local setup can really speed up your workflow while still allowing you to emulate a close approximation of the Lambda environment. Converting an existing Flask application If you already have an existing Flask application, it's very easy to convert to a Serverless-friendly application. Do the following steps: Install the `serverless-wsgi` and `serverless-python-requirements` packages -- `npm install --save serverless-wsgi serverless-python-requirements` Configure your `serverless.yml`: You should have a `serverless.yml` that looks like the following:  Make sure that the value for `app` under the `custom.wsgi` block is configured for your application. It should be ``, where `module` is the name of the Python file with your Flask instance and `instance` is the name of the variable with your Flask application. Deploy your function with `sls deploy`! Note: if you use other resources (databases, credentials, etc.), you'll need to make sure those make it into your application. Check out our other material on managing secrets & API keys with Serverless. Using the Quick Start Template If you don't have an existing Flask application to convert, but you want a well-structured starting point for an application, you can check out our serverless-flask application template. To use it, you'll need the Serverless Framework installed. You'll also need your environment configured with AWS credentials. With the Framework installed, use the `sls install` command to clone the template project. Then, change into the directory and run a postsetup script to configure it as desired:  Then run `sls deploy` and hit the main web page to see your starter application: You're off and running! What will you build? ",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603eb6",
      "title": "Birth of the NearCloud: on CRDTs and Kuhiro",
      "content": "Kuhir founder Russell Sullivan came to chat with us about the latest hot topic in serverlessCRDTs. CRDTs got several mentions at ServerlessConf in NYC, but not many people fully understand what they do, or what their potential is. (...yet.) Russell's talk will take you through some important concepts. Get up to speed on NearCloud, CRDTs, and Kuhir. Video Serverless Meetups Want to join a meetup in your city and hear cool talks like Russell's? Want to run a meetup in your city and facilitate cool talks like Russell's? Head on over to our Meetups page to join in.",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603eb8",
      "title": "A Game of Throne: hacking a bathroom queue with Serverless and IoT",
      "content": "Tim Growney (along with the rest of the Teespring team) had a problemin their old office, the employee:bathroom ratio was :. Aka, long lines were a daily occurrence. Now, Tim could have accepted this as an inevitable fact of life. He could have settled for an old-fashioned queue, just like the rest of us. But Tim didn't do that. Because Tim...is a serverless engineer.  Watch below to see how he, in the span of a one-day work hackathon, created a serverless queueing system using IoT sensors. Video Serverless Meetups Want to join a meetup in your city and hear cool talks like Tim's? Want to run a meetup in your city and facilitate cool talks like Tim's? Head on over to our Meetups page! Hope to see you there next time.",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603eba",
      "title": "The Ultimate Guide to Serverless Announcements @ AWS re:Invent",
      "content": "Last Updated: // @ :PM PST It's that time of the year, the Christmas-comes-early for cloud developers. Are you ready for AWS re:Invent? This is where AWS Lambda was introduced in , and we're expecting more huge announcements in the serverless realm this year. AWS has gone all-in on serverless, and we can't wait to see the improvements they're making. This post is _the_ place to understand what is announced and why it affects you, the intrepid Serverless developer. We'll be updating throughout the week with all of the latest announcements. If you're attending re:Invent, be sure to check out the Serverless guide to re:Invent . Overview: re:Invent announcements (newest to oldest): - Latest - CloudTrail Logging for Lambda Invocations - Golang support - GB memory - Concurrency controls - VPC integration - Serverless App Repo - Cloud - IoT - AWS Greengrass - IoT Device Defender - IoT Device Management - IoT -Click - Managed services - Kinesis Video Streams - Rekognition Video - Amazon Transcribe - Amazon Translate - Amazon Comprehend - SageMaker - Databases - AWS Neptune - DynamoDB backup and restore - DynamoDB global tables - Serverless Aurora - Containers - AWS Fargate - AWS EKS (Kubernetes-as-a-service) - AWS AppSync (Hosted GraphQL!) - Lambda improvements - Weighted aliases for Lambda - AWS CodeDeploy incremental deployment - Canary management for API Gateway Pre-re:Invent announcements: - Lambda@Edge Improvements - SNS Message Filtering - API Gateway Access Logs Announcements: CloudTrail Logging for Lambda Invocations What: Ability to send CloudTrail events for all Lambda function invocations Why this matters: CloudTrail is a great tool for performing security auditing and compliance for your AWS resources. Previously, you could only create CloudTrail events for more management-like features of Lambda, such as creating or deleting a function. Now, you can get information on function invocations as well. This will enable you to record and react to invocations with much more detail. Golang support What: You can now write your Lambda functions in Go. Why it matters: Like concurrency but hate Node? A typed language without Java? Boom - Golang arrives on Lambda. Use it now!: Here's a template and walkthrough for using Golang with Lambda via the Serverless Framework. GB Memory What: Lambda memory limits upped to GB. Why it matters: More RAM! This can help for heavier processing tasks or anything that would benefit from more memory. As Vlad points out, this also seems to apply to Lambda@Edge: This wasn&;t announced explicitly but Lambda@Edge has GB of RAM as wellhttps://t.co/rAnmTDhqaws lambda serverless reInvent @goserverless pic.twitter.com/bHznNsEIH&mdash; Vlad Holubiev (@vladholubiev) November , Concurrency controls What: Lets you manage invocation concurrency on a per-function basis. Why it matters: AWS has account-wide limits on how many concurrent Lambdas you can have running by default, though you can raise that. It could lead to some of your functions getting throttled if one of them got hammered with a bunch of requests. But now, you can set limits so that one function doesn't result in throttles for other functions. BREAKING: New lambda features!!! reInvent Serverless pic.twitter.com/PezbrFKWa&mdash; Serverless (@goserverless) November , VPC integration What: VPC integration with API Gateway. Why it matters: We'll let Ajay describe why this is important, we couldn't have said it better: VPC integration for API Gateway is massive - front any application with API Gateway, Lambda or otherwise, and start using it to manage authN/AuthZ control, and traffic. Want modern APIs for those on prem deployments while you restructure that monolith? Go for it! Reinvent&mdash; Ajay Nair (@ajaynairthinks) November , Serverless App Repo What: A repository for discovering serverless applications. Preview available here. Cloud What: IDE for writing, running, and debugging code. This is GA today! Why it matters: A developer's editor is a sacred thingfrom terminal junkies to full-fledged IDEs. For those less opinionated or that want to be less tethered to a particular development machine, the Cloud IDE is interesting. Develop directly in the cloud with an always-available environment. Pair programming is interesting, and something we've seen from VSCode and Atom in the last few weeks as well. AWS Greengrass What: Lets you run machine learning at the edge. Why it matters: It makes it easy to use your machine learning models at the edge, even without internet connectivity. You can do all your training in the cloud (maybe with SageMaker!), then push the model to your edge devices for inference. Announcing the preview of Greengrass ML Inference. Run Machine Learning at the edge reInvent pic.twitter.com/EpHzXQ&mdash; AWS re:Invent (@AWSreInvent) November , IoT Device Defender What: Define and audit security policies across fleets of devices. Coming early . IoT Device Management What: Improves security of IoT implementations. Lets you remotely onboard and manage new IoT devices at scale. IoT -Click What: Lets you set up a device to trigger a Lambda with a click. Amazon Transcribe What: Speech to text Amazon Translate What: Real-time language translation Amazon Comprehend What: Natural language processing Rekognition Video What: Vision analysis on video streams Kinesis Video Streams What: Lets you ingest and store video streams. SageMaker What: Service that assist with the heavy-lifting of machine learning. Helps you author, train, and host your models. AWS Neptune What: Fully-managed Graph database. Good for graph relations such as social networks, recommendations, routes, etc. Available in preview. Notes: Probably not serverless-friendly to begin with (e.g. not auto-scaling or pay-per-usage), but it probably won't be too long. Want in? Sign up for the preview here. DynamoDB backup and restore What: Much easier restore operations for DynamoDB. Includes the ability to backup to _any second_ in the last days! DynamoDB global tables What: Multi-region, multi-master tables for having your tables everywhere. For more on why this is important, might want to refresh your memory on Jared Short's ServerlessConf talk. Serverless Aurora What: Aurora is AWS's relational database in the cloud. It's cheaper and faster than MySQL or PostgreSQL databases. Serverless Aurora brings automatic scaling and per-second billing. MySQL available early and PostgreSQL later in . Why this matters: If done right, this is a dream come true. The data layer is an unsolved problem in the Serverless realm. If you want to use a traditional relational database, you're configuring a lot of network rules and VPC configuration -- just what you want to avoid with serverless architectures. You can avoid this pain by using DynamoDB, but the limited query patterns & hidden foot-guns can cause problems down the road. Serverless Aurora could change all of thata relational database that's accessible over HTTP, doesn't require complicated networking configuration, and uses IAM for authentication? Count me in. Announcing Aurora Serverless. All the capabilities of Aurora, but pay only by the second when your database is being used reInvent pic.twitter.com/APRjfRB&mdash; AWS re:Invent (@AWSreInvent) November , AWS Fargate What: Run your containers directly without a cluster. Just pay for your compute resources. Why this matters: This drastically lowers the barrier for running containers. You don't need to set up and maintain a cluster for deploying your containers. It's not serverless -- you're still paying for resources even when they're not actively serving requests -- but it does have some of the similar benefits of serverless architectures. No longer any need to manage servers and clusters for your containers - AWS Fargate - Run ECS and EKS without managing servers reInvent https://t.co/oPsXdvrSAL pic.twitter.com/lZnSCXV&mdash; AWS re:Invent (@AWSreInvent) November , AWS EKS What: Managed Kubernetes on AWS. Why this matters: AWS will run your Kubernetes for you. It's not a serverless solution by any means, but many companies are interested in containers over straight serverless. For running functions-as-a-service on Kubernetes, check out the Kubeless integration with the Serverless Framework. AWS AppSync What: A platform for building data-rich apps with offline functionality. Why this matters: This is an interesting, ambitious offering from AWS. Basically, it lets you set up a managed GraphQL endpoint over a variety of data sources. This endpoint can proxy to DynamoDB, ElasticSearch, or your custom Lambda functions. Further, it provides some nice functionality to keep your device synced when moving between online and offline modes. I'd compare this to an AWS version of Firebase or Realm with the ability to have multiple different backing data sources. That's pretty powerful. One of the drawbacks to Firebase stems from the limitations of its data model. This offering sidesteps those issues. The product page is live at https://aws.amazon.com/appsync/ and you can find docs here. Were launching AWS AppSync as a new service for preview later today! Here are some of its features! @apatel reInvent pic.twitter.com/fGthGsAa&mdash; AWS re:Invent (@AWSreInvent) November , credit: AWSreInvent main account. Weighted aliases for Lambda What: This will let you send some percentage of traffic to one version of a Lambda, the rest to another version. This is GA today, try it out! Why this matters: This will make it easier to confidently push new changes to production. When you push new versions, you can shift a small percentage of users to the new version and monitor for errors, performance metrics, etc. If you're happy with the results, you can gradually ramp up traffic so that all users see the new version. AWS CodeDeploy incremental deployment What: CodeDeploy support for incremental deployment of serverless applications. This is GA today, try it out! Why this matters: This update fits well with the addition of weighted aliases for Lambda. In your CodeDeploy configuration, you can use phased rollouts of your applications. For example, you could have CodeDeploy make a deploy to % of your production traffic at first, then roll it out to the remaining % minutes later. If you discover problems during that minutes, you can rollback the deploy to limit problems to your users. New features coming to lambda breaking serverless reinvent .cc @Ninnir pic.twitter.com/bNjLIZU&mdash; Julien Stanojevic (@GenuineM) November , credit: @GenuineM Canary management for API Gateway What: This will allow you to send some percentage of API Gateway traffic to one source and the rest to another. This is GA today, try it out! Why this matters: Same benefits as the weighted aliases for Lambda above -- more fine-grained rollouts of new code to production. This change is at the API Gateway level, rather than for an individual Lambda function. Canary deployments are a way to safely roll out new changes to customers. Lambda at Edge Improvements What: Lambda@Edge increased memory limits, maximum package size, and function durations. It also allows for dynamic origin selection based on content and the ability to make remote calls in viewer-facing requests. Why this matters: This is a big one. Previously, Lambda@Edge functions allowed you to run limited logic at the edge, such as rewriting headers or redirecting unauthenticated users to a login page. However, the functionality was limited, particularly if you wanted to integrate with other services in your architecture. Now, you can run entire applications at the edge. You can make remote calls to your other services to get dynamic content. You can route requests to different origins based on the request path, making it easier to slowly migrate to Serverless architectures using the strangler pattern. This is a huge deal. It's mind-blowing that this came out _before_ re:Inventthey must have some other amazing announcements in store. SNS Message Filtering What: SNS Subscriptions can add a filter policy where they only receive certain messages rather than all messages published to a topic. Why this matters: This announcement may be underrated, but it makes it much easier to build pub/sub architectures. Previously, you might make a \"fat\" topic with all messages published to it and required annoying filtering logic within your Lambda functions that subscribed to a topic. This would result in wasted Lambda invocations to SNS messages that your function didn't care about. Alternatively, you could create multiple, smaller topics with specific messages types, but that required the complexity of multiple subscriptions for different Lambdas. With this new filter policy, you can use the fat topic pattern while only invoking your function for messages it cares about. This could be based on an `event_type` (e.g., I care about `order_placed` but not `order_shipped`) or other attributes (if a new User is created, trigger me when the `user_type` is `admin`). This can simplify your Lambda logic and lower your costs. API Gateway Access Logs What: You can now enable detailed access logs from API Gateway, just like you could do with Apache, Nginx, or HAProxy. Why this matters: This enables granular analytics on your web requests. You can feed these into your analytics systems or dump them into S to analyze with Athena.",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603ebc",
      "title": "Serverless Aurora: What it means and why it's the future of data",
      "content": "AWS had their annual re:Invent conference last week (missed it? Check out our full recap). AWS Lambda started the Serverless movement by releasing Lambda at re:Invent . But the Lambda releases this year were run-of-the-mill incremental improvementshigher memory limits, concurrency controls, and of course, Golang support (coming soon!). All this to say, there was nothing game-changing in the functions-as-a-service (FaaS) world itself. Well then. Does this mean that AWS is slowing down on serverless? Hardly. We saw AWS asserting that serverless is more than just functions: serverless != functions, FaaS == functions, serverless == on-demand scaling and pricing characteristics (not limited to functions)&mdash; TJ Holowaychuk (@tjholowaychuk) August , In five years when we look back at re:Invent , we won't be talking about the different managed container offerings. We'll be talking about this: Announcing Aurora Serverless. All the capabilities of Aurora, but pay only by the second when your database is being used reInvent pic.twitter.com/APRjfRB&mdash; AWS re:Invent (@AWSreInvent) November , That's right. Serverless Aurora. Why is Serverless Aurora so important? We first need to understand two things: the technology-driven changes in software architectures in the cloud era, and the current state of the data layer in serverless architectures. The Architectural Evolution Earlier this year, Adrian Cockcroft wrote a piece on the Evolution of Business Logic from Monoliths through Microservices, to Functions that blew my mind. It showed how changes in technology are driving changes in development patterns and processes. Adrian has had a front row seat for these changes over the years from his work at eBay, Netflix, and now AWS. A bunch of unrelated technologies combined to drive these changes. Faster networks and better serialization protocols enabled compute that was distributed rather than centralized. This enabled API-driven architecture patterns that used managed services from SaaS providers and broke monoliths into microservices. Chef, Puppet, EC and Docker and eventually Lambda combined to enable and promote ephemeral compute environments that reduced time to value and increased utilization. These tools were combined with the necessary process improvements from the DevOps movement to increase velocity. We're seeing smaller teams deliver features faster with lower costs. These changes have been huge, but the data layer has been lagging. Adrian touched on database improvements, but they aren't as mind-blowing. They have explicit tradeoffs of simple query patterns: The lagging data layer is particularly problematic in Serverless architectures. The Problem of the Serverless Data Layer I spoke on this problem at ServerlessConf NYC in October. In short, there are two approaches you can take with databases with serverless compute: _server-full_ or _serverless_. Server-full databases A server-full approach uses instance-based solutions such as MySQL, Postgres, or MongoDB. I classify them as instance-based when you can tell me how many instances you have running and what their hostnames are. I like Postgres + Mongo because of their popularity, which means data design patterns are well-known and language libraries are mature. However, these instance-based solutions were designed for a pre-serverless world with long-running compute instances. This leads to the following problems: _Connection Limits_ Postgres and MySQL have limits of the number of active connections (e.g. ) you can have at any one time. This can cause problems if you get a spike in traffic which causes a large number of Lambda to fire. _Networking issues_ Your database instances will often have strict firewall rules about which IP addresses can access them. This can be problematic with ephemeral compute -- adding custom network interfaces will add latency to your compute's initialization. _Provisioning issues_ Serverless architectures fit well with defining Infrastructure as Code. This is harder with something like Postgres roles (users). These aren't easily scriptable in your CloudFormation or Terraform, which spreads your configuration out across multiple tools. _Scaling issues_ This is one of the most important problems. Instance-based databases aren't designed to scale up and down quickly. If you have variable traffic during the week, you're likely paying for the database you need at peak rather than adjusting throughout the week. Serverless databases In contrast to server-full, instance-based databases, there is a class of serverless databases. Serverless databases are different in that you're usually paying for _throughput_ rather than a particular number and size of instances. There are a few options for serverless databases, including Firebase and FaunaDB. However, the most common of these databases is DynamoDB from AWS. DynamoDB addresses most of the problems listed above with server-full databases. There are no connection limits, just the general throughput limits from your provisioned capacity. Further, DynamoDB is _mostly_ easy to scale up and down with some caveats. Also, the networking and provisioning issues are mitigated as well. All access is over HTTP and authentication / authorization is done with IAM permissions. This makes it much easier to use in a world with ephemeral compute. However, DynamoDB isn't perfect as a database. You should really read Forrest Brazeal's excellent piece on Why Amazon DynamoDB isn't for everyone. In particular, the query patterns can be very difficult to get correct. DynamoDB is essentially a key-value store, when means you need to configure your data design very closely to your expected query patterns. To me, the biggest problem is the loss of flexibility in moving from a relational database to DynamoDB. With a relational model, it's usually easy to query the data in a new way for a new use case. There isn't that same flexibility for DynamoDB. Developer agility is one of the key benefits of serverless architectures. Having to migrate and rewrite data is a major blocker to this agility. The Future of Data Transition to the cloud: treat servers like cattle, not pets. Transition to serverless cloud architecture: treat servers like roaches&mdash; Ben Kehoe (@benkehoe) March , Ben Kehoe loves to hammer the point that to be truly serverless, your compute should not exist when it's not handling data. This hyper-ephemeral compute requires a new type of database. Highly-scalable, automation-friendly, global, with a flexible data model to boot. Distributed databases are hard. The NoSQL movement, including the Dynamo paper that describes the principles of DynamoDB and influenced its cousins (Apache Cassandra, Riak, etc.), was a first step in the database revolution. The second step is in motion now. AWS announced multi-master Aurora, allowing for your Aurora instances to have masters that accept writes in different Availability Zones. Similarly, they announced DynamoDB Global Tables which syncs data from DynamoDB tables _across different regions_ (!). Writes in So Paulo will be replicated to your copies in Ohio, Dublin, and Tokyo, seamlessly. These manage the difficulty of multi-master global databases. The next step is Serverless Aurora, due sometime in . It checks all the boxes for a serverless database:  Easy scaling.  Pay-per-use.  Accessible over HTTP.  Authentication & authorization over tightly-scoped IAM roles rather than database roles.  A flexible relational data model that most developers know. This is a big deal. We've seen the hints that Amazon recognizes the issues with existing relational solutions in the cloud-native paradigm. They've implemented IAM authentication for MySQL and Aurora MySQL databases already. Further, the Aurora design paper notes how they have changed the relational database for a cloud-native world. I believe this is only the first step in Amazon's plan to push the database further. With the rise of social networks and recommendation engines, graph databases have become more popular. Amazon's new Neptune graph database is an foray into another data area. Graph databases are notoriously hard to shard, so it may be a while before we see a Serverless Neptune. I wouldn't bet against it coming eventually. re:Invent is about the future, and that's why it's my favorite conference of the year. When we look back on re:Invent , I have a feeling the data layer improvements will be the most important of all.",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603ebe",
      "title": "Automating CI/CD workflow for serverless apps with CircleCI",
      "content": "It's pretty easy to set up a simple Serverless app with the Serverless Framework. _But_in real life, the process of continuous integration and deployment (CI/CD) of that application can be much more involved. Never fear! In this post, we're going to take a deep look at the end-to-end workflow of automating a CI/CD process for a serverless application via CircleCI. We will cover: Defining the CI/CD process Creating an app with testable code Preparing the app for automation Integrating with a CI/CD toolchain End-to-end automation for our app Advanced deployment patterns If you already know some CI/CD basics, then you'll probably want to skip straight to the application testing bit. The Basics: CI/CD Overview In an agile development environment, small teams work autonomously and add a lot of churn to the code base. Each developer works on different aspects of the project and commits code frequently. This is a healthy practice, but it comes with some challenges. Without close watch and proper communication about changes, the updates can cause existing code to break. To minimize manual scrutiny and redundant communication across teams, we need to invest in automating CI/CD processes. !The CI/CD Process Flow Figure : The CI/CD Process Flow Continuous Integration The CI process starts with the developer checking code into a code repository. The developer makes their code changes in a local branch, then adds units tests and integration tests. They ensure that the tests don't lower the overall code coverage. It's possible to automate this process by having a common script that can run the unit tests, integration tests and code coverage. Once the code is tested in the context of the local branch, the developer needs to merge the master branch into their local branch, and then run the tests/code coverage again. The above process happens repeatedly for every code commit and thereby continuously integrates the new code changes into the existing software. Continuous Delivery Although the continuous integration process ensures that the code in the master branch is always pristine and well-tested, it cannot help catch usability issues. A QA team and other stakeholders are usually responsible for usability and acceptance testing. A successful exit from the CI process triggers the continuous delivery process and delivers the software system to a QA staging area. The QA environment usually closely resembles the production environment but with less redundancy. The continuous delivery process can have a mixed bag of automated and manual usability/acceptance testing phases. While continuous delivery provides a process to create frequent releases, the releases may not be deployed at all times. Continuous Deployment In case of continuous deployment, every change that is made to the code gets deployed to production, unless tests fail the process. This process is highly automated with new code built, tested, versioned, tagged and deployed to the production environment. In a special scenario, where major bugs and issues are found in a recently deployed version of the software, a \"rollback\" can be initiated. A rollback process takes a previous release version and delivers it to the production environment. This process can be automated but is usually manually triggered. Application Testing Now that we've gone over some basics, let's get started! For this project, we'll use a serverless app, `hello-world-ci`, which I created using the `hello-world` template from the Serverless Framework. We'll keep the app very simple so that we can focus on the CI process. You can install the sample app from the source repo using the Serverless Framework, like so:  Having proper tests in place safeguards against subsequent code updates. We'd like to run tests and code coverage against our code. If the tests pass, we'll deploy our app. It's thisrunning tests against our code whenever new code is committedthat allows for continuous integration. Testable Code We have some tests that we'll run as part of the testing phase. Notice that we have a spec that tests if our function is being called. We are also separating out the actual testable logic of our function into a class:  The `handler.js` code is refactored to use the above `sayHello` method from the `HelloWorld` class:  This makes testing the core logic of the app easy and also decouples it from the provider-specific function signature. Running Tests Now that we've got our tests written up, let's run them locally before we include them as part of our CI/CD process.  The tests results looks like this on the terminal:  The code coverage looks like this in the terminal:  We also get an HTML page with the code coverage results depicted visually, like so: !Visual Code Coverage Results Figure : Visual code coverage results Excluding Testing Artifacts After running the tests, you should see that a `coverage` folder has been created. This holds the files that are generated by Jest. You'll also have a `.circleci` folderthat one is required to enable build automation with CircleCI. When we deploy our serverless app via the Serverless Framework, all the files in your current folder will be zipped up and part of the deployment to AWS. Since the `coverage` and `.circleci` files are not necessary for running our app, let's exclude them from our final deployment by excluding them in our `serverless.yml` file:  Preparing for CI Automation We'll be using CircleCI for automating the CI/CD pipeline for our `hello-world-ci` app. Let's get everything ready to go. Setting up a CircleCI Account Sign up for a CircleCI account if you don't already have one. As part of the sign-up process, we'll authorize CircleCI to access our public Github repo so that it can run builds. Creating an AWS IAM User It is a good practice to have a separate IAM user just for the CI build process. We'll create a new IAM user called `circleci` in the AWS console. Give the user programmatic access and save the AWS credentials, which we'll use later to configure our project in CircleCI. Note: More on setting up IAM users here. Configuring CircleCI with AWS Credentials We have to configure AWS credentials with CircleCI in order to deploy our app to AWS. Go to your project `hello-world-ci` -> Project Settings -> AWS Permissions, and add your AWS credentials for the `circleci` IAM user we created earlier. !Adding AWS credentials Figure : Adding AWS credentials End-to-End Automation Now that we've completed our CircleCI setup, let's work on implementing the CI/CD workflow for our project. Configuration We'll configure CircleCI via a config file named `config.yml` and keep it in the `.circleci` directory. Explanation of how CircleCI works is out of scope for this article, but we'll look at the steps needed to automate our deployments. To keep things simple and get started, we'll use a simple configuration wherein everything we do will be in one job and under one step. CircleCI allows for multiple jobs with multiple steps all orchestrated via a workflow. Here is a snippet of the config file that we'll use:  We have a `job` named `build`, and we have a few `steps`. The `checkout` step will check out the files from the attached source repo. We also have a few `run` steps that just execute bash commands. We'll install the serverless cli and the project dependencies, then run our tests with code coverage enabled, and finally deploy the application. Note: The `save_cache` and `restore_cache` sections in the above config file, allows for caching the `node_modules` between builds, as long as the `package.json` file has not changed. It significantly reduces build times. Note: You can review the full config file for our app. And you can review a full CircleCI sample configuration file with more options as well. Implementing the Workflow To add our app project to CircleCI, do the following: Push the local app from your machine to your Github account or fork the sample project on your Github account. Go to Projects -> Add Projects, and click the 'Setup project' button next to your project. Make sure the 'Show forks' checkbox is checked. Since we have our CircleCI config file already placed at the root of our project, some of the configuration is picked up automatically: Pick 'Linux' as the Operating System. Pick '.' as the Platform. Pick 'Node' as the Language. Skip steps -. Click on 'Start building'. You'll see the system running the build for your project. !Build running on CircleCI Figure : Build running on CircleCI You can drill down to see the steps on the UI that matches our steps in the config file. While it is executing each step, you can see the activity. !Build steps for the project Figure : Build steps for the project You can see the tests running as part of the 'Run tests with code coverage' step. !Running tests for the project Figure : Running tests for the project And finally, you see that our app has been deployed under the 'Deploy application' step. !Deploying the project !Deploying the project Figure : Deploying the project Last but not least, we can copy the endpoint shown in the output onto a browser and see the app run! !Running the app Figure : Running the app Hopefully, the full rundown of the process and its implementation on a CI/CD platform such as CircleCI gives you a better understanding of automating your own applications. Advanced Deployment Patterns In real-life enterprise scenarios, there's a lot of complexity involved in deploying an application. There are concerns about redundancy, high-availability, versioning & rollback, A/B testing and incremental rollouts. All of this needs to be achieved without sacrificing the flexibility & ease of the deployment process, and at the same time keeping the customer happy. In this section, we'll look at some of these concerns and ways the Serverless Framework can help solve it. Multi-region Deployments A popular pattern for introducing redundancy and achieving high-availability is to deploy your application into multiple regions. In our use case, we will deploy our application to two AWS regions - `us-east-` and `us-east-`. Note: It is more complex to do multi-region deployments when databases are involved, as you need to take care of replicating and syncing data across multiple regions. A DNS service like AWS Route with domain mappings would have to be put in place to maintain high availability. Multiple Deploys We can start off with a simple workflow using the Serverless Framework. Using the framework, we can execute multiple `sls deploy` commands targeting a particular region.  Although this method for deploying to multi-region is simple, it poses a challenge. Since there are two different commands, the Serverless Framework packages & deploys the app twice. It's practically impossible to make sure that the exact same copy of the code has been deployed. Separating Packaging and Deployment Keeping the above challenge in mind, the Serverless Framework provides an advanced workflow for deploying to multiple regions. It provides a way to separate the packaging and deploying portions of the overall deploy process. The Serverless Framework provides a `sls package` command to package code and then allows you to use that package in the `sls deploy` command to deploy it.  Let's look at the implementation of this deployment pattern and ways we can automate it. !Separating packaging and deployment Figure : Separating packaging and deployment Here are the details of the workflow: A step in the CI process packages the app code into a package and uploads it to a S bucket. This S bucket could be a generic storage for all deployment packages. The CI process uses the `sls package` command to package the app code. An S event triggers a Lambda function, that takes the package and uploads it to separate S buckets in multiple regions. The S instances in each region, then trigger another Lambda function, that deploys the package to that particular region. The Lambda functions use `sls deploy` passing it the package to deploy. Canary Deployments Another popular deployment pattern is canary deployments. Canary releases are used to reduce risk when releasing new software versions. The pattern lays down a workflow that enables the slow and incremental rollout of new versions by gating it to a small subset of end users. Once the new software version has been tested to be satisfactory for mass consumption, the release is opened to all user traffic. !Canary deployment flow Figure : Canary deployment flow Here are the details of the flow: Current State: Imagine a current state of the environment where an existing version of the software is running in an isolated region. All user traffic is redirected to this environment (denoted by the gray box). Canary State: Next, a new version of the software is ready to be tested and deployed. This new version will be deployed to another isolated region. To test the new version, a subset of user traffic (% in our case) is redirected to the new environment (denoted by the blue box). End State: Once the testing is complete and the results are found to be satisfactory, all user traffic is redirected to the new environment with the new version of the software. Rollback State: Unfortunately, sometimes serious bugs are discovered in the new version and code have to be rolled back. In this scenario, all user traffic is redirected back to the old version of the software (denoted by the gray box). After a certain amount of time that the new version of the software has been live, the old environment with the old version can be taken down. An off-shoot use case of canary deployment pattern is, using it for A/B testing as well. In case of A/B testing scenarios, tweaking the amount of user traffic gives the developers a good idea of the performance and usability of each individual version of the software. Update: AWS announced API Gateway support for canary deployments at AWS re:Invent . Blue/Green Deployments Yet another deployment pattern in use is the blue/green deployment. The blue/green deployment pattern is very similar to canary deploymentsbut instead of gating user traffic, two separate identical environments are used in parallel to mitigate risks of introducing new software versions. One environment is used for go-live, and the other is used for staging new changes. The workflow dictates switching environments back and forth between staging and live. !Blue/Green deployment flow Figure : Blue/Green deployment flow Here are the details of the flow: Live (blue): Initially, the current version of the software is deployed to the blue environment with the all user traffic being redirected to the blue environment. Staging to Live (green): After a new version of the software is developed, it is deployed to the staging environment (green) for testing. After the testing of the new version of the software is considered satisfactory, all traffic is redirected to the green environment and the green environment is considered as live. Live to Staging (blue): Since the green environment is now the live version, the blue environment is considered staging. The new version of the code is deployed to the blue environment for testing. Rollback scenario: Unfortunately, sometimes serious bugs are discovered in the new version (blue) and code have to be rolled back. In this case, all traffic is redirected back to the blue environment. The blue environment is back being the live version. The green environment becomes staging. Routing Mechanics The traffic routing is done by setting up a DNS service (AWS Route ) in front of the API Gateway. In the case of canary deployments, AWS Route is switched from 'simple routing' to 'weighted routing' to achieve a percentage mix of user traffic between environments. In case of blue/green deployments, the 'weighted routing' is toggled between % and % across the blue/green environments. Benefits of Serverless In traditional architectures, the canary and blue/green deployments are used after a lot of consideration and planning. The reason being, there's a high cost to provisioning hardware and maintaining multiple environments required in order to realize the potential of such deployment patterns. The benefits of embracing serverless architectures are immediately evident hereit means no provisioning or maintenance costs for multiple environments. On top of that, the fact that serverless is pay-per-execution reduces execution costs significantly; you never pay for any idle infrastructure, in any environment. Quick summary In this post, we looked at the overall CI/CD process flow, detailing each process step. We then created a serverless application and refactored the code to be testable. We then ran the tests and code coverage locally to make sure our code was working. Once we had our app running locally, we set up an automated CI workflow for our app on CircleCI. At the end of the post, we looked at some of the more complex deployment patterns and how serverless architectures could help make them more cost-effective & feasible. Any comments or questions? Drop them below!",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603ec0",
      "title": "Introducing OpenEvents",
      "content": "In the opening keynote of CloudNativeCon , Dan Kohn, the executive director of the CNCF, announced a small but significant new effort titled OpenEvents. Our company (Serverless Inc.) has been leading the charge on this effort with others within the context of the CNCF. It's early in its development, but its impact is potentially profound. Here's why. !OpenEvents CloudNativeCon Dan Kohn When we talk about events, we are talking about facts. Something actionable. Events are notifications that report when something has happeneda new piece of code was committed, a user created an account, someone started a sentence with Alexa. The world is currently generating more events than ever, largely due to the rise of distributed systems, microservices, platform integrations, IoT sensors and more. Meanwhile, the growth of cloud services and serverless computing (which enable you to process events at any scale, cheaply) are enabling new possibilities for acting on events. Events are powerful. They can enable businesses to make smarter decisions, faster. Think everything from developer benefits (like test automation on new commits) to customer-facing impacts on companies bottom lines (like collecting customer activity to create personalized experiences). To convert events to actions, it is becoming common to transport events across environments: multiple services, cloud vendors, on-premise systems, SaaS products, etc. However publishers of event data tend to describe events differently. The lack of a common way of describing events means developers must constantly re-learn how to receive events. This also limits the potential for libraries, tooling and infrastructure to aide the delivery of event data across environments, like SDKs, event routers or tracing systems. The portability and productivity we can achieve from event data is hindered overall. Enter OpenEvents, a specification for describing event data in a common way. OpenEvents seeks to ease event declaration and delivery across services, platforms and beyond. The current focus of the effort is to define a set of consistent metadata attributes, which can be included with event data to help developers and systems process events more easily. !OpenEvents Logo These attributes include information to help title and categorize events, version the event data so that it can evolve without breaking downstream subscribers, and describe where the event came from and where it might be going. The attributes can be extended for experimental features, and much more. A variety of traditional use-cases can be improved by this, like service communication, SaaS integrations, webhooks, cloud bursting, functions-as-a-service, and improvements to the overall developer experience. We're also excited to see what new use-cases emerge as a result of this. OpenEvents is a new effort and it's still under active development. However, its working group has received a surprising amount of industry interest ranging from major cloud providers to popular SaaS companies. We're excited to keep fostering this effort and what it means to the serverless community. To get involved, go to openevents.io. Serverless and event-driven architectures are on the rise. Standardizing events will accelerate this trend and help us achieve our ultimate goal: empowering developers.",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603ec2",
      "title": "Automate your DynamoDB backups with Serverless in less than  minutes",
      "content": "The good news: AWS announced DynamoDB backups at re:Invent . This will save a lot of unnecessary ops burden. The bad news? You can't schedule and automate your backups. You need to manually click in the console to create your backup. Have no fear, an automated solution is here. Use the power of Serverless to automatically backup your DynamoDB tables on a schedule! Follow the steps below to use our project to backup your DynamoDB tables. Serverless DynamoDB Backups We've created a Serverless project on GitHub to create DynamoDB backups on a schedule. Usage of the project is simple. First, install the Serverless Framework:  You'll need AWS credentials configured in your terminal. Want help with these? Check out our walkthrough. Then, use the Framework's install command to install a project template from a GitHub repo:  Edit the configuration in the `custom` block of `serverless.yml` to match your configuration. This includes setting the `tableName` of your DynamoDB table, the `backupRate` at which you want to create backups, the AWS region where your table is located, and optionally a `slackWebhook` to send Slack notifications. Finally, deploy your Serverless service:  That's it - your service will create DynamoDB backups on your desired schedule! You're an Ops superhero. Setting up a Slack Webhook If you want fancy Slack notifications when a backup succeeds or fails, follow the steps below. In the end, you'll receive notifications like: First, go to the channel you want to send notifications to and click Add an app: In the page that opens, search for `Incoming Webhooks` and click on it. Then click `Add Configuration`. It should show your selected channel in the box. Then click `Add Incoming WebHooks Integration`. Once you've created it, the page will show your Webhook URL: Copy and save this, as you'll need it in your Serverless service. By default, it uses a boring `incoming-webhook` display name and a boring image. I like to customize it a bit: Paste your Webhook URL into the `serverless.yml` as the `slackWebhook`, deploy your service, and you'll start receiving notifications! Additional Notes For the curious, I'll pass along some extra details and troubleshooting tips. Not all DynamoDB tables have backup enabled. For some reason, not all DynamoDB tables are eligible to take a backup. I've found it's my older tables that don't allow backups. If backups aren't enabled for your table, attempting a backup will throw a `ContinuousBackupsUnavailableException`. Backup Names are finicky When creating a backup, you need to specify a backup name. I was using the name of the table plus the ISO format. I kept getting an opaque error of:  I finally discovered that AWS doesn't allow colons in backup names. Cryptic errors aside, I just changed my timestamp to be `YYYYMMDDHHMMSS`. Outdated Botocore To make the API call to create a backup, I'm using the boto library for making AWS API calls in Python. It uses a second library called botocore for understanding the shape of the AWS API. Botocore uses a bunch of JSON files to describe the methods, inputs, outputs, and more of its various services. You can see the whole list here. To use a new operation, such as `create_backup()` for DynamoDB, you need to make sure you have a version of `botocore` with the proper models. `Boto` and `botocore` are packaged into the AWS Lambda environment, which is nice most of the time. It means you don't have to package your own AWS API packages into your Lambda zip files. It's annoying in times like these, right after re:Invent, when the outdated `botocore` dependency means you can use the newest methods. Fortunately, you can upload your own `botocore` data files without packaging your own version of `botocore`! All you need to do is copy the data files for your desired models into your deployment package. You can see I've included the DynamoDB files here. Then, set the `AWS_DATA_PATH` environment variable to the path where your data files are stored. I do it directly in my function handler before importing `boto`:  !The More You Know",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603ec4",
      "title": "The Serverless Champions Program HAS BEGUN!",
      "content": "ServerlessLife We care a lot about open source. Like, a _lot_. You all in the serverless community are the best we've seen. Period. And some of you? Blow our freaking minds with your unrelenting commitment to the future of computing. It was on a day not so long ago, as we were sweeping our mind-fragments off the floor for the dozenth time, in awe at your majesty, that we realized what we had to do. \"These people deserve an award!\" we said. Just like that. These people deserve an award. So we made one. Today, serverless community, we are so, SO proud to announce: The Serverless Champions program. What is a Serverless Champion? A Serverless Champion is a hero in the serverless community. They contribute to open-source serverless projects. They speak at serverless conferences and meetups. They help newcomers get excited. They crawl the forums just looking for questions to answer. They are thought leaders, teachers, and builders. Four times a year, we will select three new Serverless Champions to join the ranks. We are going to put your smiling face on our website and make a Pretty Big Deal about it. We're gonna give you limited-edition swag & access to an invitation-only Slack channel & gosh tons more goodies! But we kind of don't want to spoil it all in this blog post. Just go check out our Serverless Champions page already. And the very first cohort of Serverless Champions are! (drumroll) Takahiro Horike Horike started out as an electrical engineer, but stumbled into programming in his last year of college as he was writing a simulation for visualizing electromagnetic waves. He stuck with software, and was one of the first people to start using Lambda when it was released in . Even so, he found it tricky to get things deployed with Lambda then. A couple months later, when he heard about the Serverless Framework at a conference, he went all in. Horike is a Serverless Framework maintainer, and has authored several open-sourced plug-ins and other resources for the community: - CommandLine Event Args - Step Functions - Serverless Atom Dashboard Twitter | GitHub Marcia Villalba Marcia got into programming at the ripe old age of her favorite game used BASIC to navigate a dog through a maze. She had no idea then that what she was doing was coding, but it certainly stuck, and by her teens she was making websites. When Lambda was first launched, Marcia was building REST backends. She assumed Lambda was mostly an infrastructure tool that, while cool in concept, wasn't super applicable to the work she was doing. And then AWS launched API Gateway support for Lambda. It's a long story from there, but (spoiler alert!) it ends with Marcia and her team rewriting their entire backend to be serverless. Today, Marcia works on game servers, and uses the Framework to rapidly iterate on prototypes. She is an avid serverless educator who regularly posts blogs and YouTube videos to help serverless beginners navigate the waters. Twitter | GitHub | YouTube Ryan Scott Brown Ryan has been building out cloud infrastructures since , and has been working with Lambda since it was in developer preview. He's a Serverless Framework contributor and user who loves building in the open as much as possible. These days, Ryan spends his time focusing on automation tooling and Infrastructure-as-Code (IaC) practices. Early on, he started sharing successes (and failed experiments) on ServerlessCode.com to help others and push for a common set of practices. One of those practices is the Serverless Framework: a common language spec for multiple providers to smooth the way from idea to implementation. Twitter | GitHub | Website More info Huge congrats to our very first cohort of Serverless Champions! But they will certainly not be our last. Check out our Serverless Champions page for more info on the program.",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603ec6",
      "title": "Building & testing an Alexa skill with the Serverless Bespoken plugin",
      "content": "Overview Building an Alexa skill really isnt so hard! And its even easier if you use the Serverless Framework along with the Bespoken Plugin. To show you how, were going to take you through the steps of building your very own Alexa skill, by making a simple guessing game. The game will pull some images from Giphy, then ask users to guess which search term generated those images. In this walkthrough, we will: - Set up an initial project for an Alexa skill - Get started using the Alexa Node.js SDK - Test and debug locally with the Bespoken plugin for Serverless - Deploy the skill to AWS Lambda using Serverless - Talk about advanced steps and further exploration Setting up the environment For starters, youll need a few things: - An Amazon developer account (sign up here) - An AWS account with access to Lambda and DynamoDB (learn how to create your AWS account with IAM roles here) - The Serverless Framework (`npm install serverless -g`) Go ahead and get set up with those items if you have not already. Now, to follow along at home, start by cloning this repo with Alexa sample code: `git clone https://github.com/bespoken/giftionary` This repository contains the example we are going to walk through, already configured with the correct packages and libraries. This includes the Alexa Node.js SDK from Amazon that well leverage for building a skill. Once youve cloned the repository, go to the directory you cloned it into and run `npm install` to setup the project. That will install our dependencies. Just one more thingwhile we are in the terminal, go ahead and run `sls proxy`. That will start the Bespoken Proxy, which we will use in just a bit. You should see output like this if everything is working correctly: Were now ready to start development! Important concepts Before we jump in though, there are three very important concepts we want to review. We will cover each in detail as we go along: - Session - a conversation with a skill - Intents and the Interaction Model - the user interface for the skill - State Machine - the programming pattern the Node.js SDK is built on If you feel like you understand sessions and interaction models pretty well, feel free to jump straight down to State Machine, which is where we begin creating our first skill response. The session A session is a conversation between a user and our skill. It starts when the user invokes the skill by saying something like Alexa, open giftionary. The session remains active as long as the user continues interacting with the skill. This interaction can end for three reasons: The user explicitly ends it (by saying something like quit) The session times out (due to the user not responding to a prompt or question from the skill) The skill explicitly ends itself (because it has completed its work) As long as our skill is in session, it has the chance to share information with the user via voice and visual elements, and/or ask the user questions. In thinking about voice design, its important to keep in mind how to keep the user engaged and keep the session alive so that the skill can do its job. Intents and the interaction model Another very important aspect to understand with Alexa skills are intents and the interaction model. Think of the interaction model as the UI for the skillonly in this case, its not a visual interface, but a vocal one. Our interaction model describes: What the user can say (utterances) What intentions these utterances map to (intents) Intents are essential to the interaction model and Alexa, and to voice- and AI-based programming in general. For example, a user could say any one of the following: Help me, Help, or What is this? Alexa will interpret all of these as being the Help intent. This directly affects which pieces of your code will run. Heres a sample of our interaction model - from the `speechAssets/IntentSchema.json`:  And from `speechAssets/SampleUtterances.txt`:  This is our Play intent. You can see that we have multiple sample utterances defined for itplay, get started, and play a game. All of these show up as the Play intent, which in our code triggers a game to start. Easy, right? Below is another intent, this time containing a slot. Slots are like variables. An intent with a slotrather than just matching what the user says to one of the sample utteranceswill treat the slot as a wildcard. It will pass the value for that wildcard to our code: From `IntentSchema.json`:  From `SampleUtterances.txt`:  In this case, our slot captures the users guesses. Since our search terms are an enumerated set, we provide the list of values possible in speechAssets/TermsSlot.txt. The values are things like cat, dog, plane, etc. Besides being enumerable, slots can also have types, which serve to narrow the values they look for. This helps with speech recognition. Examples of built-in types provided by Amazon are `AMAZON.NUMBER` and `AMAZON.DATE`. Note: You can read more about types here. The State Machine The Alexa Node.js SDK encourages the use of a state machine for building skills. State machines are a natural fit for skills (and other voice-based apps). The basic idea is that your app has states and transitions. Any action a user takes is handled based on the state they are currently in, and part of that handling may include transitioning to another state. This pattern pushes us to think about the discreet states the user will go through in interacting with our app, and how, for each of these scenarios, we will handle what a user might say. This mindset gets us thinking about providing appropriate, contextualized responses to the user, an important part of voice/conversational interfaces. To see this in action in our skill, lets take a look at our state-handlers:  For the guessing game were building, we have the following states: - New Session - Start Game - Guessing - Play Again - Help Heres how we handle new sessionsthe user first interacting with our skill:  Whenever a user starts a new session (by saying something like Alexa, open giftionary), this method will be called. Our code then checks to see if this is a new user or not. We leverage the built-in support for DynamoDB that is part of the Node.js SDK for this; user-level data is automatically stored on the attributes property. Additionally, these attributes are passed back and forth to our skill as long as it remains in session, and then persisted to DynamoDB when the session ends. For new users, we change their state to HELP mode. This explains to them how the game works and asks them if they would like to start playing. For existing users, we skip straight to the PLAY mode. Responding via voice Now that weve worked through the basics of our state machine, how do we actually reply to the user when the game starts? Easy! Here is our code that does that:  The key thing to look at here is the speak method: Take a look at this image What is the search term for it? This is what we are actually saying to the user. And the part in the middle that looks like XML is actually an SSML tag (Speech Synthesis Markup Language). SSML is a very helpful tool for working with voiceit allows us to insert pauses into speech, change the vocal tone, emphasize certain words, or even bleep things out. Feel free to read up a bit on SSML here. The `listen` part means that this response is going to wait on the user to respond. If they do not, it will reprompt them with the specified text. As for the `cardRenderer` callcards are images and text that can accompany the skill. In our case, the card displays the image that the user is asked to guess about. We set this image on the card, and also provide additional instructions to the user on what to do (i.e., guess what the image is) - here is a screenshot of how the cards look in the Alexa app on the phone: Testing Locally Now that we have a basic working skill, lets test it out locally! To do this, were going to run our proxy via the Bespoken Serverless plugin. This plugin allows us to interact with a skill on our laptop without deploying it to Lambda. It saves a lot of development time; even though Serverless makes deployments a lot easier, it still takes time to upload our Lambda file with all its dependencies to AWS. Setting up a new skill in the developer console Before we go further, we need to set up our Alexa skill in the developer console. In the Developer console home, click on Alexa: Then Get Started under Alexa Skills Kit: And finally click Add New Skill in the top right: Here, youll need to enter the Skill Information as shown: - Name: Giftionary - Invocation Name: Giftionary Then click Next: Copy the contents of `speechAssets/IntentSchema.json` into the Intent Schema: Under Custom Slot Types -> Enter Type, put Term. Then copy the contents of `speechAssets/TermsSlot.txt` to the Enter Values section, and then click Add. Copy the contents of `speechAssets/SampleUtterances.txt` to the Sample Utterances section. And with that, youve set up the Interaction Model! Now click Next; it will probably take a moment to process: Select HTTPS for Service Endpoint Type. We are going to use Lambda for deployment, but for local testing were using the Bespoken endpoint. Remember the terminal window we opened earlier, where we ran `sls proxy`? Copy the URL that is printed out there and enter it in the Default field: Select No for Provide Geographical Endpoints, and then click Next: Under Certificate for DEFAULT Endpoint, select My development endpoint is a sub-domain of a domain that has a wildcard certificate from a certificate authority. Then click Next: Now we can test! Running our first test Were going to start with a test that uses the Service Simulator. Type play in the Enter Utterance field and then click Ask Giftionary: Now take a look at your console where `sls proxy` is running: The same request and response from the Service Simulator is now in our terminal window. Neat, huh? And we are not limited to testing via the Service Simulator. At this point, any Echo device can be used for testing. Tools like Echosim.io and the Reverb app will also work. Now that were completely set up, lets do a real deployment! Deployment with Serverless This step is easy; just open up your terminal and type `sls deploy`. Once that completes, well need to update our skill configuration. Remember, its still pointing at our testing URL. We want to point it to our new Lambda instead. To update it, get your Lambda ARNgo to the command-line and enter `sls info -v`. You should see something like this: Copy the ARN and enter it on the configuration screen, then click Save, like so: Now our skill is all set to be used by others! Recap - Weve accomplished a lot here, so lets do a quick review: - We discussed the basic concepts behind how skills (and the Node.js SDK) work - We set up our own first skill - We tested it locally on our machine - We configured it in the skill development console - We deployed the AWS Lambda using Serverless Further reading From here, there are lots of areas to explore further: - Set up Skill Beta Testing - Publish the Skill so the world can use it - Integrate longform audio or video into your skill - Integrate display capabilities for devices such as the Echo Show I'm John from Bespoken, so feel free to reach out if you have any questions. Happy skill development!",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603ec8",
      "title": "How to build a static Serverless site with Netlify",
      "content": "The Serverless.com site is (quite obviously, we hope) a serverless site. The benefits to a serverless website should be pretty well-known in this crowd: cheap to run, scalable out of the box, hands-off administration. But our site isnt just serverlessits a statically hosted serverless site. Why make it static? WELL, let us count the ways: - Its ultra fast (everything served from a CDN) - Provides a state-of-the-art UX - Works offline (a la PWA standards) - More secure - Easier to reason about, maintain, & share code across teams And people notice: The @goserverless docs are the slickest, smoothest-running SPA I&;ve ever used on the web, I think ever. https://t.co/UrOcOjT&mdash; Jerome Leclanche (@Adys) December , In sum, from one engineering team to another, we effing love this website. We want to tell you all about how we built it so that you, too, can have a website you effing love. On we go. How do we statically host our site? We (shocker) use the Serverless Framework for the serverless bit. But what about the static hosting? There are two ways (as of this posting) to statically host a site: Build a lot of custom stuff yourself Use Netlify We chose the latter. Netlify has been getting tons of press and adoption, and its pretty clear why when you realize how powerful it is. We use it for automating CI/CD & https setup, static site redirects (via `_redirects` file), proxied url handling, deployment notifications, and lots of other stuff we wont go into right now because all great lists must come to an end somewhere. Deploying a new site with Netlify For starters, youll need a static site generator. To help you choose one, here is a great list of open-source static site generators. We use Phenomic. It isnt at the top of that list, but its written in React and we really like it. Once youve done that, heres how easy it is to deploy a new site with Netlify. First off, connect it to your GitHub repo: In the Netlify console, click Create a new site: Then OAuth with Git: Search for the repository you want to deploy: Build it with `npm run build`: AND! (jazz hands) Your site is deployed! Merging pull requests: builds and previews As you submit new pull requests and update your site, Netlify can automatically trigger new builds on your behalf. On the Serverless.com site, when we create a new branch, Netlify pings us with a GitHub comment on that branch to generate a build preview URL for us: We use these build previews a lot to share in-progress designs and content across teams. Once its good to go, we merge to master and Netlify triggers a new build. Setting up deployment notifications We have a dedicated Slack channel for Netlify build notifications. This way, we can quickly see when builds succeed or fail, and also access all our latest deploy previews in one spot: Nowbuild your own static serverless site! Are you a true believer now? Do you want to build your own statically hosted serverless site? Boy, do we have the resources for you! We wrote a previous blog post about why and how we built a fast, secure, scalable static site with React, so you can check that out for more background. Weve also open-sourced everything for the Serverless.com site: - Heres our frontend code - Heres our backend code (complete with a host of slick serverless functions you can peruse) Maybe were biased, but we think static serverless sites are the best. Give em a try, and happy building!",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603eca",
      "title": "The state of serverless observability—why we built Thundra",
      "content": "For a very long time, monitoring tools were simple. They were mainly used as external pings. But monitoring tools have significantly evolved in recent years. They provide things like time series traces, metrics, and logs. This kind of monitoring is called whitebox monitoringa subcategory of monitoring, based on information derived from the internals of systems. Its what people have come to expect, and frankly a turn that is much-needed as companies continue to embrace microservice architectures. What we noticed at OpsGenie was that the serverless world had some monitoring tools, but none that were powerful enough. As we began the process of turning our monolith into a microservice architecture, and deploying various services with AWS Lambda, we felt the pain. We couldnt see anything. We wanted observability. So we got to work. The end result of this work is Thundra, which we hope will solve all of your serverless pain points in the same way it did ours. The road to observability Observability is the dev trendword of . As explained in Cindy Sridharans blog post, observability is a superset of monitoring. It aims to provide insights into the behavior of systems along with rich context, which is perfect for debugging purposes. In general, there are three pillars of observability: Traces provide end-to-end visibility into requests throughout the entire chain. Traces can be used for identifying which parts of the system have performance bottlenecks, detecting which components of the system lead to errors, and debugging the whole request flow for domain-level bugs. Metrics provide measured or calculated information (mostly numbers) about a particular process or activity in the system over intervals of timein other words, a time series. A metric can be application/environment specific (CPU metrics, memory metrics), module/layer specific (cache metrics, DynamoDB metrics) or domain specific (user metrics). Logs are an immutable and verbose representation of discrete events that happened over time. Logs are used for debugging, auditing, and analyzing system behavior. The current state of serverless observability At OpsGenie, wed been implementing new applications as microservices and splitting our existing monolithic architecture into microservices (more on that here). We were using AWS Lambda as FaaS for deploying and running our microservices. There are all kinds of frameworks and tools for monitoring typical web applications. But for Lambda? Almost nothing. Especially for Java, which was our language of choice. AWS does provide you with some metrics about how long an invocation took. But it doesnt say much about what is going on under the hood. X-Ray added more detail about calls to external services, but it didnt do enough to expose internal metrics automatically. We spent a lot of time on figuring out what our monitoring setup should be. We could have gone with a regular APM tool, or the Lambda-specific new products, but all of them had at least one of the following issues/drawbacks: They didnt consider the nature of a Lambda environment. Publishing data synchronously is an anti-pattern here because it increases the request duration. Besides, receiver-side data monitoring might not be available all the time. This meant wed need to either retry sending the request, or just skip it silently. Since Lambda functions should be stateless and the container itself can be destroyed at any time, monitoring data should not be saved on local storage (in memory or on disk). Publishing with background threads is not a good idea either, because when there is no request handled by the container, the container is in a frozen state. No CPU resource/slot is assigned so there will be no running background threads to publish monitor data. They didnt support automated instrumentation. Updating our code by injecting instrumentation logic would complicate things. That approach was error-prone and didnt support instrumenting rd party libraries because they dont relinquish their source code for you to play with. They didnt have metrics and logs. Collecting trace data is good, but for gaining better visibility into our application, we also needed metrics and logs. And they needed to be correlated. Logs wouldnt be helpful if we couldnt associate them with the trace data of the request itself, where the logs were printed. Enter: Thundra We ended up implementing an in-house monitoring product, Thundra, to bring deeper observability to our Lambdas in deployment. AWS X-ray already did a great job with end-to-end observability, so we piggybacked on X-ray for visibility. We added a way to enrich X-Ray with trace IDs, so we could view our traces there directly. We also incorporated several additional metrics & logging features into Thundra, to pick up where X-Ray left off. Integration with AWS X-Ray Most modern systems interact with each other, either providing or consuming services to/from other systems. Even for a single request, there might be a flow-through system. So distributed tracing is an essential requirement. Fortunately, AWS has X-Ray, which is a distributed tracing service integrated with AWS Lambda. X-Ray provides you an end-to-end view of requests as they move through your systems. With X-Ray, you can analyze how your Lambda functions and their connected services are performing. You can identify and troubleshoot the root cause of performance issues and errors, and see a map of your applications underlying components. X-Ray is good for the end-to-end visibility, but it instruments at a high level. For low-level instrumentation, you still need to manage X-Rays sub-segments yourself. We integrated our tracing infrastructure with X-Ray. When a span starts, a mapped X-Ray sub-segment is automatically created. This way, we can monitor and query our local traces on X-Ray. This integration puts us on the road to full observability, by combining distributed tracing with local tracing: !xray-integration Asynchronous publishing Thundra publishes all data through AWS CloudWatch in an asynchronous way, as described as a best practice in AWSs white paper. Sending data this way allows us to eliminate the concerns mentioned above, and come up with a solution which provides zero overhead: Our functions run as fast as they can; there is no additional latency caused by sending monitoring data. Monitoring does not cost us additional money because of request latency. Our functions send monitoring data in a reliable way, making sure we dont miss any critical information along the way. Heres how all that works at a high level: !monitoring-architecture Trace, metric and log data are written in a structured format as JSON, and sent to CloudWatch asynchronously via `com.amazonaws.services.lambda.runtime.LambdaLogger`. We also have another Lambda functionlet's call it monitor lambdawhich subscribes to log groups of monitored Lambda functions with a subscription filter that is triggered by monitor data. The monitor lambda then forwards the data to ElasticSearch, either directly or indirectly through Kinesis or Firehose stream, where it can be queried and analyzed later. Correlating traces, metrics, and logs To have full system observability, you not only need all the trace, metric, and log datayou need them to be correlated. You should be able to answer these key questions: - Which metrics were calculated in which trace? For a specific request, what were the cache metrics, DB access metrics, etc? - Which logs were printed in which trace? For a specific request, what were the application logs that let you analyze activity during the request? - If I find a trace that was much slower than other, what were the metrics and logs flowing from that trace? I need to investigate from the general (slow request) to the specific (a cache miss that resulted in a slow database call). - If I find some metrics that are abnormal for a set of requests, how can I find their source traces to see the related metrics and logs? We modeled our metric and log data to be able to reference the current trace. Check out the OpenTracing specification and data model documentation for the Span and Trace concepts if youd like a bit more info on how we structured things. !trace-and-log Ability to instrument without messing up your code To use existing monitoring tools on AWS Lambda, you need to instrument the code by inserting custom spans (contexts). Before accessing the database, for example, youd need to start a span and finish it after the operation to measure the duration.  This requires changing your application code, which will ultimately require more testing and maintenance. Its also error-prone, as anyone whos ever forgotten to end a span can attest. Plus, you cant trace external libraries without rebuilding themnot really feasible for most. For our team, we wanted Thundra to support automated instrumentation as a cross-cutting operation. Automated instrumentation is good, but it is not always enough. Sometimes you might need to start/end custom spans with custom attributes, even in a single method. So tracing should support both automatic and manual instrumentation. We took these issues into consideration and implemented our own JVM agent, which does bytecode level instrumentation. The agent can be dynamically attached to the JVM at runtime, which was necessary since theres no way to give a JVM argument to a Lambda function for the agent. With our agent, traced methods can be marked with class- or method-level annotations. For rd-party libraries, you cant put annotations on them because you dont have their source code. That means youll need to specify/configure them declaratively somehow, such as by configuration files or system properties. To overcome such limitations, as stated before, our instrumentation infrastructure also supports declaring methods/classes to be traced as regular expression definitions by environment variable, which is given to the Lambda function configuration or by configuration files included in the uploaded artifact/jar. Thundras instrumentation agent has the following supports: - Trace methods and classes programatically using annotations - Trace method arguments, return values and thrown errors - Measure execution time for each line - Track local variables states for debugging - Execute actions before/after a method call, or when an error is thrown - Take action when a method call exceeds specified limits Heres an example. We can instrument all public methods of a class named `UserService` and trace it by annotation as follows: !trace-by-annotation Detecting long run and taking consequent action You cant instrument everything. And really, you shouldnt. If you instrument everything, most of the time there will be redundant tracing data. This will ultimately cause CPU and memory overhead in the system. And also, lets say you do instrument every methodyou may not know which part of the method body takes up the majority of the time. Detailed tracing should be kicked in just as it is needed. If the method call exceeds its predefined time limit, then that call is considered as a long-run. In other words, when the method call takes so long that it hits its long-run limit, you can trigger detailed tracing. When it is detected that the current method call is a long run call, it is too late for instrumentationthe method call is already active. In this case, method level CPU profiling provides us useful metrics about CPU consumption percentages of methods. By using this information, we can focus on the problematic method to find and understand the bottleneck. Here is a typical example of how our CPU profiler points us that regex related methods are the most CPU consuming operations:  Interested in Thundra? Cool! Were launching it very soon as a private beta. If you want to give it a try, sign up here for early access. Further reading: See how we monitor AWS Lambda at OpsGenie, and check out our engineering blog to keep up with Thundras progress.",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603ecc",
      "title": "Build a Github webhook handler with Serverless & AWS Lambda",
      "content": "One of the great applications for Serverless is using it as glue code between different services. You can spin up an endpoint to handle a webhook in seconds without bugging your company's Ops department. Github has a very mature webhook integration where you can be notified of a wide range of events. You can run a linter when a pull request is opened, send a notification when an issue is created, or trigger a deploy when a pull request is merged. In this tutorial, we'll show how to handle Github webhooks. We'll create a webhook that fires whenever our open-source repository is starred. Our handler for this event will post a celebratory message in a Slack channel. The end result will look like this: Let's get started! Before you start To complete this tutorial, you'll need: - The Serverless Framework installed with an AWS account set up; - A Github account, plus a repo where you have admin or owner permissions; and - A Slack account where you have the ability to create apps. Setting up your Slack incoming webhook The first thing we'll do is set up an Incoming Webhook for a Slack channel. This will give us an HTTP endpoint to post messages that will be displayed in our Slack channel. First, create a new channel in Slack where you want the messages to go. You probably don't want to spam your whole team in `general`. In your new channel, click the link to `Add an app`: Search for the \"Incoming Webhook\" application, and create a new one for your channel: After you create it, it will display a Slack webhook URL. This is the URL where you will post data to show in the channel: Finally, you can customize the webhook display so it's nicer when it posts in the channel. Here, I change the name to \"Github Stars\" and use a star emoji as the icon: Deploying our Serverless webhook handler Now, let's move on to setting up our webhook handler. In our handler, we'll want to parse the given event for the information we want, then send a formatted message to Slack using our webhook URL from the previous step. First, let's create a new directory for our Serverless service and initialize it with a `package.json`:  We're going to be using the `WatchEvent` from Github to get notifications of our repository being starred. We want to post a message that looks as follows: For this message, we'll need: - the repository being starred; - the total number of stars for the repository; - the username starring our repository; and - the URL to the user's Github profile. Github includes an example event structure, which is very useful. A truncated version is below:  In our service directory, let's create a `handler.js` with our handler code: ```js 'use strict'; const request = require('sync-request'); const WEBHOOK_URL = process.env.WEBHOOK_URL; module.exports.stargazer = (event, context, callback) => { const body = JSON.parse(event.body) const { repository, sender } = body; const repo = repository.name; const stars = repository.stargazers_count; const username = sender.login; const url = sender.html_url; try { sendToSlack(repo, stars, username, url); } catch (err) { console.log(err); callback(err); } const response = { statusCode: , body: JSON.stringify({ message: \"Event processed\" }), }; callback(null, response); }; const sendToSlack = (repo, stars, username, url) => { const text = [ `New Github star for _${repo}_ repo!.`, `The ${repo} repo now has ${stars} stars! :tada:.`, `Your new fan is ` ].join('\\n'); const resp = request('POST', WEBHOOK_URL, { json: { text } }); // Use getBody to check if there was an error. resp.getBody(); } ``` Let's walk through this handler code. The exported `stargazer` function is the handler for our Lambda. This is what will be called when our function is triggered. In that function, we pull out the necessary elements of the webhook event, then use our `sendToSlack` function to assemble the message and post it to Slack. We should look at two other things before moving on. First, notice how we're getting the webhook URL from a WEBHOOK_URL environment variable. This is something we'll need to inject with our `serverless.yml`. Second, we're using a third-party NPM package, `sync-request` (yes, I use sync-request because I don't like messing with callbacks ). We'll need to install this package locally with NPM, and then Serverless will include it in our deployment package. Let's install that now:  With our handler code written, let's move on to our `serverless.yml`. This is our \"infrastructure-as-code\", where we configure our different functions, events, and additional configuration:  In our `functions` block, we configure a single function, `stargazer`. We provide a path to our handler file and the name of the function to be triggered. Then, we set up an http event to be triggered on a POST request to `/stargazer`. Notice in the `provider` block that we've added the `WEBHOOK_URL` environment variable under the `environment` section. This matches with our handler code, which required our Slack webhook URL in the environment. Make sure to update that value with your URL from setting up your webhook. Let's deploy our service! Run `sls deploy` to send it to the cloud : Serverless will print out a URL for your function to be accessed. Copy that URL as you will use it to configure your Github webhook. Setting up your Github webhook We're ready for the last step -- creating the Github webhook to send to our function endpoint. Navigate to a repository where you're an owner or an admin. Click \"Settings\" at the top, then the \"Webhooks\" tab on the left hand side. Then, click the \"Add webhook button\": In the Add webhook screen, enter your function endpoint into the Payload URL input box and choose `application/json` as the Content Type: Then, go to the section to choose which events trigger the webhook. Click the \"Let me select individual events\" option, then choose the \"Watch\" event at the bottom of the list. Click the \"Add webhook\" button, and you're ready to go! Github will immediately send a test event to your endpoint. There will be a section showing \"Recent Deliveries\" and the status code, so you can see if you're having any failures. If you are, check the logs in your console with `sls logs -f stargazer` to find the errors. Conclusion You did it! Quick and fun notifications of anytime a user stars your Github project. If you want the code used in this application, check it out here Check out the full range of Github webhook events -- you can implement some really powerful workflows with webhooks + Serverless. Let us know what you build!",
      "__v": 0
    },
    {
      "_id": "64e0891eb72e199dda603ece",
      "title": "How To Manage Your Alexa Skills With Serverless",
      "content": "Introduction Masashi here, creator of the Serverless Alexa plug-in. Serverless and IoT go hand in hand, and it's easy to use the Serverless Framework to develop AWS Lambda functions for Alexa Skills. Unfortunately, you can't control Alexa Skills with the Framework, which was a bummer to me because I found the Alexa Skills Kit webapp and ask-cli didn't have the simplicity I'd come to love with the Serverless Framework. But! Luckily, the Serverless Framework has a great plugin system. I decided to solve this little problem with the power of the community! Overview The Serverless Alexa Skills Plugin lets you integrate Alexa Skills into the Serverless Framework. We can now control the manifest and interaction model of Alexa Skills using `sls` command and `serverless.yml`! How to use it Installation The plugin is hosted by npm:  Get your credentials `Login with Amazon` is an OAuth. single sign-on (SSO) system using your Amazon.com account. To get your credentials, log in to the Amazon Developer Console, go to `Login with Amazon` from `APPS & SERVICES`, and then `Create a New Security Profile`: :  Then, you can check the model like so:  That's it! There are a few more steps needed in order to completely publish skills, so I'm planning to do further integrations with the Alexa Skills Kit in the future. It's still pretty great to be able to integrate manifests and models, since we update those many times as we develop. All the better if we can manage them with the source code of our Lambda functions! Summary Now, we can completely manage our Lambda Functions and Alexa Skills with Serverless Framework + Serverless Alexa Skills Plugin! If you have any comments or feedback, please create an issue or send a pull request. I always welcome them ",
      "__v": 0
    },
    {
      "_id": "64e0891fb72e199dda603ed0",
      "title": "From chef to Serverless developer in  years",
      "content": "I started out my software development career in a funny wayas a chef. See, I fell in love with cooking and baking in high school. I was fascinated with the way I could experiment by mixing different ingredients to try and make something delicious...sometimes frankenstein-ish. I got a job in my home town in Ireland, my first time actually in a kitchen and supplying food to people, along with washing an insane amount of dishes. I got really into baking when one of the chefs there showed me how to make Banoffee Pie. It was so simple and tasty that it actually blew my mind. I started baking over the summer to make some extra money while living at home, making cupcakes, cheesecakes and other little desserts. Thats when I moved out with my long-time school friend, and Id often cook for us, trying to make the most with what little we had. But with our new place, it was harder to travel to and fro from the restaurant I worked in and our home in the city. I decided to work part time there and got another job as a barista. Jumping from job to job got exhausting at times, on top of the baking I did on the side in the hopes of starting a little bakery business. Id have bad days doing the morning shift in the restaurant and the closing shift in the coffee shop. When Id make my way home, my friend would cook to help out while also trying to learn how to cook himself. Hed always ask me what could he make with the ingredients we had lying around, I have this, this and this. What can I do with it?! Instead of texting or calling and trying to think on the spot all the time, I thoughthuh, maybe I could make a mobile app! Something he could throw a few ingredients into and a recipe would pop out. I mean, how hard could it be? So off I went to learn how to build an app. From chef to dev It was summer that I quickly realized I had no idea where to even start. What language should I learn? What platform should I do it on? Web or Mobile? What is a front and back end? Ah! I turned to the same place many people do: Code Academy. I started learning some web development and tried to put myself more and more into a developer mindset. It opened my perception of everything! (Ok, maybe not the wonders of the universe, but you get what I mean.) Messing around and trying to build static websites made we wonder about mobile apps and how they were built. I invested in a Udemy online course to learn Java, and used that knowledge to start working on Android. I found it really hard at the start, to the point where getting the screen to say `Hello World` was a huge accomplishment. But I really wanted this. I really wanted to learn how to code. So I powered through. After finishing my Java course and constantly working on Android, I built my first app and delivered it to the Play Store. (Funnily enough, it wasnt the app for my friend, ha!) I felt there was so much more out there to learn and didnt want to stop at just mobile. Questions went through my head of How do I connect to other devices?, What are servers and what do they do?, What is the cloud.and why is it called the cloud? All these questions were a Google away. So I tasked myself to learn about servers and how to code for them. I started building silly backend apps in PHP that took data from mobile test apps, web forms and curl commands. This was when I started to feel like I was in the wrong profession. The more I learned and built things, the less excited I felt about cooking. I wanted to be a developer. I wanted to create things that made a difference and were innovative. But I didnt have a degree in Computer Science, or a degree full stop (didnt finish college, dropped out in my second year). But I truly wanted to work as a developer. I invested some money in taking the Java certification, in hopes of getting my CV taken seriously. I failed it the first time, but passed it the my second time around. I was so happy; this could be my ticket into being an actual developer! Thus began the job hunt I put it into my CV and sent it everywhere I could find that was hiring developers. But nothing We are looking for someone with a degree, someone with experience, someone with a masters. I started to think it was more just someone else. While feeling a bit down, thinking I would never get a job because I dont have a degree or the money to start one, I decided to take more courses on Udemy. I added everything I got from Udemy to my CV. I was proud of the work I did, so why not have it there? It must have paid off because I got an interview for an IT Consultancy here in Ireland. Dream come true! Feeling excited and a bit scared, I was brought into a different world. But god I wanted to do well in it. I always asked to do more and be a part of more projects. I was given charge of migrating PHP applications from on-premise to the cloud. Some learning curve! The apps had to be updated to PHP, we needed to implement DevOps and I needed to understand AWS on top of it all. I bought a book called The Phoenix Project to understand DevOps and bought ACloudGurus Developer Associate course to learn AWS. I loved the cloud and DevOps. It was so much fun! After putting in a lot of time to learn all about the tooling, the project became much easier, and ultimately a success. We managed to not only migrate our apps, but also to create a skeleton for us to repeat this process in the future by following the steps we laid out during migration. This opened up the start of our Innovation team in the office. We wanted to build innovative ideas to make the office smarter and all of our lives easier. I was given charge to gather the stack needed for our idea of a smart desk booking service: a live data feed of our office that would show staff the best times to bring clients in,the best times for the staff themselves to come in, or if they should stay home and avoid the commute. When building the backend, I found wed need to spend a lot of time creating listeners for data changes and using streams. We planned to use Lambdas and the connection to them through API Gateway. So, off I went looking for time-saving frameworks to use. Lo and behold, Serverless came into play! Transitioning to Severless development I fell in love with it in an instant. I got to concentrate on the code and to get the app out fastboth for people to use, and for us to continually develop and grow what we had in mind. I watched and rewatched everything the YouTube Channel FooBar had to offer; I couldnt recommend it more to get up to scratch with Serverless. I took in every bit of knowledge Marcia had, and I found I grew comfortable building what I needed to. In the end, I came up with the simple little architecture below: Using Serverless, I was able to create a full flow for users to claim desks and see changes in real time: staff in/out of the office and desks available. All while keeping full control of our data, having a scalable model which practically takes care of itself, and without having to worry about our infrastructure and just focus on making something great for people to use. And now? It took years of teaching myself a whole new profession. Long evenings of making my way through Code Academy, studying OOP concepts for exams and just creating things I thought were really cool and fun to see working on the phone in my hand. Ive been learning Serverless for the last months, and Im now planning to bring Serverless into my joba service to help clients build and migrate their applications. Along with building a chatbot side business using Serverless as my platform to do so! It was a very busy years! From cooking for people, baking cakes for events and growing an addiction to coffee, I never felt Id be creating things like serverless text adventure chatbots and mobile apps that teach you about our solar system. Maybe someday I will finish off my recipe maker app ",
      "__v": 0
    },
    {
      "_id": "64e0891fb72e199dda603ed2",
      "title": "How to use AWS Fargate and Lambda for long-running processes in a Serverless app",
      "content": "AWS dropped so many serverless announcements at re:Invent, the community is still scrambling to make sense of them all. This post is all about AWS Fargate. In this article, I will show you how to create an end-to-end serverless application that extracts thumbnails from video files. But, oh no, processing video files is a long-running process! Whatever will we do? This is where Fargate comes in. TL;DR A Docker container does the processing -> The container extracts the thumbnail and uploads the image to an S bucket -> The container is managed by AWS Fargate. All functionality is triggered from AWS Lambda functions and contained within a serverless application written with the Serverless Framework. Excited? Me too! Read on. !Fargate + Lambda + Serverless Framework = Bliss Some background & overview With the execution time limits of the AWS Lambda platform, there are a lot of use cases involving long-running processes that are hard to implement. On the flip side, serverless computing offers benefits (like zero-administration, pay-per-execution, and auto-scaling capabilities) that are hard to ignore. AWS recently announced AWS Fargate to bridge that gap. Fargate technology works on Amazon ECS & Amazon EKS, and allows users to run containers without having to manage servers or clusters. Amazon ECS and Amazon EKS have two modes or launch types: Fargate and EC. With Fargate, all you have to do is package your application in containers, specify the CPU and memory requirements, define networking and IAM policies, and launch the application. You can define an ECS Task and call it from your Lambda function. In this article, we will look at a serverless application built using Fargate and Lambda. Most importantly: I have built the application so it's generic enough to be used as a reference template for any long-running processes that need to be executed as part of a serverless application. Let's dig in! What we'll cover: Building a container image to extract thumbnails Setting up ECS using Fargate Defining an ECS Fargate Task Setting up IAM roles & policies Running the task Creating a serverless application Configuration and settings Triggering a Lambda function to run the ECS Fargate Task Triggering a Lambda function when thumbnail is generated Workflow The diagram below and the steps that follow describe the overall workflow of the the application we're building. !Architecture Diagram for processing video to generate thumbnail in AWS ECS using Fargate Upload video to S S triggers a Lambda function when the video is uploaded The Lambda function runs the ECS Fargate Task with appropriate parameters. The ECS Fargate Task executes the Docker container: that processes the video file to extract thumbnail, and uploads the thumbnail image to S S triggers another Lambda function when the thumbnail is uploaded The Lambda function writes the url of the thumbnail to the log. Note: If you are not interested in learning how the container image was built or how to test extracting the thumbnail locally, please skip ahead to the Setting up ECS (Fargate) section. I have the Docker container rupakg/docker-ffmpeg-thumb shared on Dockerhub for you to use. Building the Container Image We're encapsulating the core functionalityextracting a thumbnail from a video filein a container. We'll be using `ffmpeg` to manipulate the video, and the AWS CLI to upload the thumbnail to S. The container image accepts parameters to customize the functionality. Let's look at the Dockerfile and then see how we would execute it. Dockerfile Here is the Dockerfile:  I use the base image jrottenberg/ffmpeg for `ffmpeg` and then install the AWS CLI. Then I execute `ffmpeg`, passing it the parameters and then executing the `copy_thumbs.sh` script. The `copy_thumbs.sh` script uploads the extracted thumbnail to S. The ENTRYPOINT describes the command I used to extract a frame from the video file. Note: `ffmpeg` has many options to optimize what we're doing and is out of this article's scope. Check out ffmpeg docs for details. Now that we have looked at the Dockerfile, let's download the latest code from the repo to our machine and build the image:  Note: Check out the docker-ffmpeg-thumb Github repo for the latest code. For your convenience, I have the Docker container rupakg/docker-ffmpeg-thumb shared on Dockerhub for you to use. Running the Container Locally Let's run the container image we created by passing it the required parameters:  The parameters are pretty intuitive. Additionally, pass in the AWS credentials in the above command, and you should have a thumbnail in the specified S `mybucket/myfolder` path. With the functionality working using the container image, let's look at using it in a ECS (Fargate) task and build it into a serverless application. Setting up ECS using Fargate If you have already worked with ECS, you might have some or all of these steps completed. But I'm going to walk you through the steps of setting up ECS with Fargate, assuming that you have not done it before. AWS provides a First Run Wizard that is an excellent resource to start playing with ECS using Fargate. The wizard does a few things for us, as shown below: !AWS ECS Fargate First Run Wizard Figure : AWS ECS Fargate First Run Wizard Container Definition First, we will create the container definition. Note that AWS provides us a couple of pre-defined container definitions (as shown below), but we will select the 'custom' option and create a new container definition from scratch. !Container Definition Figure : Container definition Click the 'Configure' button. In the resulting pop-up screen, fill in the 'Container Name' and the 'Image': !Configure Container Figure : Configure container Note: The image for the container specified by `rupakg/docker-ffmpeg-thumb:latest` is the image that we built earlier. In this case, note that I am using the Docker Hub registry url pointing to the Docker image I built and uploaded. If you created your own image, specify that Docker image url here. You are welcome to use my Docker image as well. Next, open up the section 'Advanced container configuration', and in the 'ENVIRONMENT' section, add the following 'Env Variables':  !Advanced container configuration: Environment Figure : Advanced container configuration: Environment Leave the 'NETWORK SETTINGS' section empty. Next, in the 'STORAGE AND LOGGING' add the following 'Log configuration' key/value pairs:  !Advanced container configuration: Storage and Logging Figure : Advanced container configuration: Storage and Logging Leave the 'RESOURCE LIMITS' and the 'DOCKER LABELS' sections empty. Click the 'Update' button. Task Definition Next, we will edit the 'Task Definition', by clicking on the 'Edit' button: !Task definition Figure : Task definition Update the 'Task definition name' to `ffmpeg-thumb-task-definition`, and then click the 'Save' button: !Update Task definition Figure : Update Task definition The updates we made above to the Container and Task definition sections are shown below: !Updated Container and Task Definition Figure : Updated Container and Task definition Service Definition For our application we really don't need a 'service', but during the service definition creation step, AWS automatically creates a 'security group'. This is useful. So accept all the default values and click the 'Next' button: !Create Service Figure : Create Service Configuring the Cluster This brings us to the last step: 'Cluster Configuration'. Accept the default values of 'Cluster name' as `default`, and note that AWS will automatically create a VPC and subnets as needed. Click the 'Next' button: !Cluster Configuration Figure : Cluster Configuration Review Let's review what we just configured. You can see all the settings in the screenshot below: !Review Figure : Review Click the 'Create' button to create the container definition, task definition, service and the cluster. That should display a screen with all the items showing as 'complete'. You can then click on the 'View Service' button to view the service that was created: !Setup Complete Figure : Setup Complete Now, if you go back to the 'Task Definitions' menu item, you will see the new task that we just created. You can click on the task, and review the settings that we added. You can also create a new revision if you choose to at a later date: !New Task Figure : New Task You can take a look at the JSON version of the task definition as shown below: !Task JSON Figure : Task JSON Pre-requisite Resources To be able to run our task and support the application, we need to create a couple of resources before-hand. We need a S bucket where we will be uploading our videos for processing. The bucket in the setting `\"bucket\": \"\"` will be created automatically when we deploy our app. We need to set an IAM Bucket Policy for the bucket to give it `public` permission to get objects. Use the following policy and add it to the 'Bucket Policy' for the bucket, via the AWS S console:  Under that bucket, we need a folder where the thumbnails extracted from the video file will be uploaded. We will need to manually create a folder with the name we have in the setting `\"thumbnails_folder\": \"\"`. Test the above changes by uploading a file to this bucket and then accessing it by its public url from a browser. If you can view the file, you are all set. IAM Roles & Policies For our task to access the S bucket/folder we specified from our account, we need to give it specific permissions. Since the container is executing inside the ECS context, we can add another role that will have the specific S access policies. When we created our task definition, AWS created a role `ecsTaskExecutionRole` for us which gave access to run the task. Instead of updating that role directly, we will attach a new role for S access. Let's create a new role. Go to the AWS IAM Console and switch to the 'Roles' tab: !ECS Task Execution Role Figure : ECS Task Execution Role Click on the `ecsTaskExecutionRole` role. On the resulting page, click on the 'Add inline policy' link: !Add inline policy Figure : Add inline policy On the 'Create Policy' screen, click on the JSON tab. I have already created JSON fragment that encapsulates the policy we need. Paste the following JSON in the text area:  Basically, the above policy allows our task to list our bucket and also allows to put an object into the bucket/folder path. Click the 'Review policy' button, and give the new role a name `UploadToSRolePolicy`. Click the 'Create policy' button to create the policy and attach it to the `ecsTaskExecutionRole` role. You can see the resulting screen below: !Attach new role Figure : Attach new role With all the pre-requisite setup completed, we can now initiate running the task we created! Initiating the Task Execution Now that we have a task set up, let's run it. We will start by running the task from the AWS Console. Check the box next to the task, click on the 'Actions' dropdown menu, and select the 'Run Task' item: !Initiate Run Task Figure : Initiate Run Task In the following screen, we need to specify some settings that are required by the task to run: !Task Run Settings Figure : Task Run settings Here are a couple of things that are going on: Pick 'FARGATE' as 'Launch type' Pick 'default' as 'Cluster' Assign '' for 'Number of tasks' For 'Cluster VPC', pick one with the non-internal IP i.e. .x.x.x Only then you will see the 'Subnets' dropdown populate. Pick any two Pick the default 'Security group' that AWS created Pick 'ENABLED' for 'Auto-assign public IP' Leave the 'Task Overrides' section under 'Advanced Options' as-is. Note that a 'Task Execution Role' `currentecsTaskExecutionRole` has been automatically created and assigned to the task. This IAM role gives permission to run/start the task. The 'Container Overrides' section gives us an opportunity to override any settings for the container we created. It should be populated with the settings that we added while creating the task definition. If you choose to change any of these settings, you can do so now in this section: !Container Overrides Figure : Container Overrides Once you are satisfied with your changes, click the 'Run Task' button. Running the Task After the task is run, you can see an instance of the task being created on the resulting page: !Task Created Figure : Task Created Let's click on the task under the column 'Task'. We can see the details of the task in the following screen: !Running Task Details Figure : Running Task Details Once the 'Last status' field changes to 'STOPPED', you can switch to the 'Logs' tab to see the logs of the execution output: !Log Output Figure : Log output As you can see in the output, the container we created generated a thumbnail from the video file it was passed, and uploaded it to the S bucket/folder we specified. Side note: you can also use the AWS CLI to run/start/stop the task.  Now that we have executed the task from the AWS comsole, let's create a serverless app that will run the task based on parameters we pass along. This is where things get cool. Creating the App To recap, we are trying to extract a thumbnail from a video file based on a frame position that we specify. Here is the workflow: Upload video to S S triggers a Lambda function when the video is uploaded Get the S object related metadata from the event The Lambda function runs the ECS Fargate Task with appropriate parameters The ECS Fargate Task executes the Docker container: that processes the video file to extract thumbnail, and uploads the thumbnail image to S S triggers another Lambda function when the thumbnail is uploaded The Lambda function writes the url of the thumbnail to the log Let's start by creating a new serverless app using the Serverless Framework's boilerplate template `aws-nodejs`. We will name the app `ffmpeg-video-thumb`. To follow along, please download or clone the code for the app ffmpeg-video-thumb, and open it in an editor. Configuration The source code for the application is pretty generic, but you will need to supply it configuration settings that apply to your needs. Some of these configuration items include custom settings, plugins, IAM roles, and event rules for functions. Custom Settings We need the following settings that are defined in the `custom` section of the `serverless.yml` file. To make it easy, I have included a `config.dev.example.json` file with those entries.  Copy the `config.dev.example.json` to `config.dev.json`, and then supply the values for the entries. The `serverless.yml` loads the settings from the `config.dev.json` at the time of deployment. Note: The `config.dev.json` has been added to `.gitignore` file, so it will not be added to your git repo. Plugins There are cases, like one below, where you need the AWS CloudFormation Pseudo parameters in your configuration `serverless.yml` file.  Luckily, there is a Serverless plugin for that! Check out the awesome serverless-pseudo-parameters plugin written by Sander van de Graaf. You can now use `{AWS::AccountId}`, `{AWS::Region}`, etc. in any of your config strings, and this plugin replaces those values with the proper pseudo parameter Fn::Sub CloudFormation function. To install and add to the package.json, simply run:  and in the `serverless.yml` file include it under the `plugins` block:  IAM Roles To allow the Lambda functions in our serverless app to do certain actions, we need to set permissions with AWS. We need the following permissions:  The above IAM Role settings in the `serverless.yml` file allows the Lambda functions to run ECS Tasks, assumes the role defined in the `execRoleArn` setting and allows getting S objects from the bucket we defined. Event Rules for Functions We have two Lambda functions. Each of them are configured with certain rules that trigger them.  The above configuration sets a rule for function `triggerOnUploadVideo` to be triggered based on an event from S when a object with `suffix` '.mp' is created in the `bucket` we specified.  This configuration sets a rule for function `triggerOnThumbnailCreation ` to be triggered based on an event from S when a object with `suffix` '.png' is created in the `thumbnails_folder` we specified. Application Code We looked at pre-requisites and then some configuration needed by our serverless application. Now let's look at the function code that implements the features we need. Lambda Function to Process Video The Lambda function `triggerOnUploadVideo` is responsible for processing the video. It runs an ECS Task using Fargate, passing along appropriate parameters. This function is triggered when a video file with the `.mp` extension is uploaded to the specified S bucket. Let's take a quick look at the code at `handler.js`: ```javascript module.exports.triggerOnUploadVideo = (event, context, callback) => { const bucket = event.Records[].s.bucket.name; const key = event.Records[].s.object.key; ... const s_video_url = `https://s.amazonaws.com/${bucket}/${key}`; const thumbnail_file = key.substring(, key.indexOf('_')) + '.png'; const frame_pos = key.substring(key.indexOf('_')+, key.indexOf('.')).replace('-',':'); ... runThumbnailGenerateTask(s_video_url, thumbnail_file, frame_pos); ... }; ``` When the function is triggered by an S event, metadata about the bucket and the key for the object is received. I use that data to populate the parameters that the `runThumbnailGenerateTask` function needs. Note: I do a small hack to name the video file so that the position for the frame we want to extract as a thumbnail is part of the filename. So a file named test_-.mp will assume the frame position to be :. The code for the function `runThumbnailGenerateTask` is below: ``` var runThumbnailGenerateTask = (s_video_url, thumbnail_file, frame_pos) => { // run an ECS Fargate task const params = { cluster: `${ECS_CLUSTER_NAME}`, launchType: 'FARGATE', taskDefinition: `${ECS_TASK_DEFINITION}`, ... overrides: { containerOverrides: [ { name: 'ffmpeg-thumb', environment: [ { name: 'INPUT_VIDEO_FILE_URL', value: `${s_video_url}` }, ... ecsApi.runECSTask(params); } ``` Most of the function code is creating the `params` json structure that is passed to the helper function `ecsApi.runECSTask` to execute the ECS task using Fargate on AWS. You can customize any of these parameters via configuration. The `containerOverrides` array has custom values for the environment vars we setup for our container image. Lambda Function to Notify Thumbnail Creation The Lambda function `triggerOnThumbnailCreation` is triggered when a '.png' file is uploaded to S. It prints out the name of the thumbnail file into the logs. Deploying the App Now that we looked at the code, let's deploy and run the application!  The app is deployed. Let's use it. Upload an .mp video file in the AWS S bucket you configured. It has to be named something like `test_-.mp`, where the `-` is the frame position of your thumbnail. Running the App Open up two terminal windows and let's tail the logs of our two functions. Here is what we see for the `triggerOnUploadVideo` function. I will break the output up into sections and explain.  Above, you can see that the function was triggered. Then the `event` JSON was written out, and the two log messages with the metadata for processing the video file.  Here we see that soon after the ECS task is called by the function, the ECS task is run. You can see the data structure for the task that is passed. Since the ECS task goes off in an async fashion, as far as the Lambda function is concernedit has ended. Let's tail the logs for the `triggerOnThumbnailCreation` function and wait for the thumbnail to be generated. In a few moments, the thumbnail is generated and the logs shows the message:  At this point, you can point your browser at the url printed in your logs and view the thumbnail. Cost I was curious what kind of cost I incurred while I was writing the article. I tested, created many tasks, and executed them many times over a reasonably long time scale. Since it only charged me when the ECS tasks and Lambda functions were executed, the cost incurred is negligible. Again, reminding us why serverless is the way to go...  !Cost Analysis Extending the App We used a simple hack to pass the frame position for the input video file, but in real life we might need more detailed data for processing our file. In that case, a solution that uses AWS Kinesis to store metadata for the video, while S is used only for storage purposes, could be a viable option. Summary We looked at a possible solution to execute long-running processes in ECS using Fargate, but with tight integration with AWS Lambda. We built a Docker container to encapsulate the long-running process. We set up ECS using Fargate and created an ECS task to execute our container. We tested the functionality of extracting a thumbnail from a video file. Even better, we built and deployed a serverless application from scratch, written with the Serverless Framework. I know the article is a long one (to be honest it threw me for a loop just to write it!), but I wanted to capture what it takes to build such an application end-to-end. I'd love to hear from you about other use cases you solved with the reference app we built, or any you have in mind. Any questions or feedback, please leave it below!",
      "__v": 0
    },
    {
      "_id": "64e0891fb72e199dda603ed4",
      "title": "How to monitor AWS account activity with Cloudtrail, Cloudwatch Events and Serverless",
      "content": "CloudTrail and CloudWatch Events are two powerful services from AWS that allow you to monitor and react to activity in your accountincluding changes in resources or attempted API calls. This can be useful for audit logging or real-time notifications of suspicious or undesirable activity. In this tutorial, we'll set up two examples to work with CloudWatch Events and CloudTrail. The first will use standard CloudWatch Events to watch for changes in Parameter Store (SSM) and send notifications to a Slack channel. The second will use custom CloudWatch Events via CloudTrail to monitor for actions to create DynamoDB tables and send notifications. Setting up Before we begin, you'll need the Serverless Framework installed with an AWS account set up. The examples below will be in Python, but the logic is pretty straightforward. You can rewrite in any language you prefer. If you want to trigger on custom events using CloudTrail, you'll need to set up a CloudTrail. In the AWS console, navigate to the CloudTrail service. Click \"Create trail\" and configure a trail for \"write-only\" management events: !CloudTrail write-only events Have your trail write to a Cloudwatch Logs log group so you can subscribe to notifications: Both examples above post notifications to Slack via the Incoming Webhook app. You'll need to set up an Incoming Webhook app if you want this to work. First, create or navigate to the Slack channel where you want to post messages. Click \"Add an app\": In the app search page, search for \"Incoming Webhook\" and choose to add one. Make sure it's the room you want. After you click \"Add Incoming Webhooks Integration\", it will show your Webhook URL. This is what you will use in your `serverless.yml` files for the `SLACK_URL` variable. If you want to, you can customize the name and icon of your webhook to make the messages look nicer. Below, I've used the \"rotating-light\" emoji and named my webhook \"AWS Alerts\": With that all set up, let's build our first integration! Monitoring Parameter Store Changes The first example we'll do will post notifications of from AWS Parameter Store into our Slack channel. Big shout-out to Eric Hammond for inspiring this idea; he's an AWS expert and a great follow on Twitter: awswishlist Ability to trigger AWS Lambda function when an SSM Parameter Store value changes.That could then run CloudFormation update for stacks that use the parameter&mdash; Eric Hammond (@esh) December , Parameter Store (also called SSM, for Simple Systems Manager) is a way to centrally store configuration, such as API keys, resource identifiers, or other config. (Check out our previous post on using Parameter Store in your Serverless applications.) SSM integrates directly with CloudWatch Events to expose certain events when they occur. You can see the full list of CloudWatch Events here. In this example, we are interested in the `SSM Parameter Store Change` event, which is fired whenever an SSM parameter is changed. CloudWatch Event subscriptions work by providing a filter pattern to match certain events. If the pattern matches, your subscription will send the matched event to your target. In this case, our target will be a Lambda function. Here's an example SSM Parameter Store Event:  We need to specify which elements of the Event are important to match for our subscription. There are two elements important here. First, we want the `source` to equal `aws.ssm`. Second, we want the `detail-type` to equal `Parameter Store Change`. This is narrow enough to exclude events we don't care about, while still capturing all of the events by not specifying filters on the other fields. The Serverless Framework makes it really easy to subscribe to CloudWatch Events. For the function we want to trigger, we create a `cloudWatchEvent` event type with a mapping of our filter requirements. Here's an example of our `serverless.yml`:  Notice that the `functions` block includes our filter from above. There are two other items to note: We injected our Slack webhook URL into our environment as `SLACK_URL`. Make sure you update this with your actual webhook URL if you're following along. We added an IAM statement that gives us access to run the DescribeParameters command in SSM. This will let us enrich the changed parameter event by showing what version of the parameter we're on and who changed it mostly recently. It _does not_ provide permissions to read the parameter value, so it's safe to give access to parameters with sensitive keys. Our `serverless.yml` says that our function is defined in a `handler.py` module with a function name of parameter. Let's implement that now. Put this into your `handler.py` file:  This function takes the incoming event and assembles it into a format expected by Slack for its webhook. Then, it posts the message to Slack. Let's deploy our service:  Then, let's alter a parameter in SSM to trigger the event:  Note: Make sure you're running the `put-parameter` command in the same region that your service is deployed in. After a few minutes, you should get a notification in your Slack channel: !SSM Create Slack alert Awesome! Monitoring new DynamoDB tables with CloudTrail In the previous example, we subscribed to SSM Parameter Store events. These events are already provided directly by CloudWatch Events. However, not all AWS API events are provided by CloudWatch Events. To get access to a broader range of AWS events, we can use CloudTrail. Before you can use CloudTrail events in CloudWatch Event subscriptions, you'll need to set up CloudTrail to write a CloudWatch log group. If you need help with this, it's covered above in the setting up section. Once you're set up, you can see the huge list of events supported by CloudTrail event history. Generally, an event will be supported if it meets both of the following requirements: It is a _state-changing_ event, rather than a read-only event. Think `CreateTable` or `DeleteTable` for DynamoDB, but not `DescribeTable`. It is a _management-level_ event, rather than a data-level event. For S, this means `CreateBucket` or `PutBucketPolicy` but not `PutObject` or `DeleteObject`. Note: You can enable data-level events for S and Lambda in your CloudTrail configuration if desired. This will trigger many more events, so use carefully. When configuring a CloudWatch Events subscription for an AWS API call, your pattern will always look something like this:  There will be a `source` key that will match the particular AWS service you're tracking. The `detail-type` will be `AWS API Call via CloudTrail`. Finally, there will be an array of `eventName` in the `detail` key that lists or more event names you want to match. Pro-tip: Use the CloudWatch Rules console to help configure your items the first few times. You can point and click different options and it will show the subscription pattern: Let's insert our DynamoDB CreateTable pattern into our `serverless.yml`:  Very similar to the previous examplewe're setting up our CloudWatch Event subscription and passing in our Slack webhook URL to be used by our function. Then, implement our function logic in `handler.py`:  Again, pretty similar to the last examplewe're taking the event, assembling it into a format for Slack messages, then posting to Slack. Let's deploy this one:  And then trigger an event by creating a DynamoDB table via the AWS CLI:  Wait a few moments, and you should get a notification in Slack: !Slack DynamoDB CreateTable alert Aw yeah. You could implement some really cool functionality around this, including calculating and displaying the monthly price of the table based on the provisioned throughput, or making sure all infrastructure provisioning is handled through a particular IAM user (e.g. the credentials used with your CI/CD workflows). Also, make sure you delete the table so you don't get charged for it:  Conclusion There's a ton of potential for CloudWatch Events, from triggering notifications on suspicious events to performing maintenance work when a new resource is created. In a future post, I'd like to explore saving all of this CloudTrail events to S to allow for efficient querying on historical data\"Who spun up EC instance i-afkjjfk?\" or \"Who allowed .../ ingress in our database security group?\" If you use this tutorial to do something cool, drop it in the comments!",
      "__v": 0
    },
    {
      "_id": "64e0891fb72e199dda603ed6",
      "title": "Your CORS and API Gateway survival guide",
      "content": "Building web API backends is one of the most popular use cases for Serverless applications. You get the benefit of a simple, scalable backend without the operations overhead. However, if you have a web page that's making calls to a backend API, you'll have to deal with the dreaded Cross-Origin Resource Sharing, or CORS. If your web page makes an HTTP request to a _different domain_ than you're currently on, it needs to be CORS-friendly. If you've ever found yourself with the following error: then this page is for you! In this post, we'll cover all you need to know about Serverless + CORS. If you don't care about the specifics, hit the TL;DR section below. Otherwise, we'll cover: - Preflight requests - Response headers - CORS with custom authorizers - CORS with cookie credentials Let's get started! TL;DR If you want the quick and dirty way to solve CORS in your Serverless application, do this. To handle preflight requests, add the `cors: true` flag to _each HTTP endpoint_ in your `serverless.yml`:  If you're using a custom authorizer, you'll need to add the following CloudFormation in your `resources` block of `serverless.yml`:  CORS Preflight Requests If you're not making a \"simple request\", your browser will send a preflight request to the resource using the `OPTIONS` method. The resource you're requesting will return with methods that are safe to send to the resource and may optionally return the headers that are valid to send across. Let's break that down. When does my browser send a preflight request? Your browser will send a preflight request on almost all cross-origin requests. (The exceptions are \"simple requests\", but it's a pretty narrow subset of requests.) Basically, a simple request is only a `GET` request or a `POST` request with form data that has _no authentication_. If you're outside of that, it will need a preflight. If you use a `PUT` or `DELETE` request, it will send a preflight. If you use a `Content-Type` header outside of `application/x-www-form-urlencoded`, `multipart/form-data`, or `text/plain`, it will send a preflight. If you include any headers outside some very basic ones, such as Authentication headers, it will send a preflight. What's in the response to the preflight request? The response to a preflight request includes the domains it allows to access the resources and the methods it allows at that resource, such as `GET`, `POST`, `PUT`, etc. It may also include headers that are allowed at that resource, such as `Authentication`. How do I handle preflight requests with Serverless? To set up the preflight response, you'll need to configure an `OPTIONS` method handler at your endpoint in API Gateway. Fortunately, this is very simple with the Serverless Framework. Simply add `cors: true` to _each endpoint_ in your `serverless.yml`:  Note how the `response` object has a `headers` property, which contains an object with `Access-Control-Allow-Origin` and `Access-Control-Allow-Credentials`. It can be a real pain to add these headers everywhere in your function, particularly if you have multiple logical paths. Luckily, there are some nice tools to help with this! If you use Javascript, check out the Middy middleware engine for use with Lambda. It has a lot of nice middlewares that handle the boring boilerplate of your Lambda functions. One is the `cors` middleware, which automatically adds CORS headers to your functions. A basic example looks like this:  Perfectautomatic CORS headers! Check out the whole Middy library for lots of other nice utilities. If you're a Pythonista, Daniel Schep has made a nice `lambda-decorators` library with the same goals as Middyreplacing Lambda boilerplate. Here's an example of using it in your Python functions:  Note: Daniel is the creator of the `serverless-python-requirements` package, which you should absolutely be using if you're writing Lambda functions in Python. Check out our previous blog post on Python packaging. CORS with custom authorizers Custom authorizers allow you to protect your Lambda endpoints with a function that is responsible for handling authorization. If the authorization is successful, it will forward the request onto the Lambda handler. If it's unsuccessful, it will reject the request and return to the user. The CORS difficulty lies in the second scenarioif you reject an authorization request, you don't have the ability to specify the CORS headers in your response. This can make it difficult for the client browser to understand the response. To handle this, you'll need to add a custom GatewayResponse to your API Gateway. You'll add this in the `resources` block of your `serverless.yml`:  This will ensure that the proper response headers are returned from your custom authorizer rejecting an authorization request. CORS with Cookie credentials _Note: This section was added on January , thanks to a request from Alex Rudenko. Hat tip to Martin Splitt for a great article on this issue._ In the examples above, we've given a wildcard \"\" as the value for the `Access-Control-Allow-Origin` header. However, if you're making a request using credentials, the wildcard value is not allowed. For your browser to make use of the response, the `Access-Control-Allow-Origin` response headers _must_ include the specific origin that made the request. There are two ways you can handle this. First, if you only have one origin website that's making the request, you can just hardcode that into your Lambda function's response:  In this example, we check if the `origin` header matches one of our allowed headers. If so, we include the specific origin in our `Access-Control-Allow-Origin` header, and we state that `Access-Control-Allow-Credentials` are allowed. If the `origin` is not one of our allowed origins, we include the standard headers which will be rejected if the origin attempts a credentialed request. Conclusion CORS can be a pain, but there are a few straightforward steps you can take to make it much easier to deal with. You know what that means. Goodbye forever, inexplicable `No 'Access-Control-Allow-Origin' header is present on the requested resource` error. ",
      "__v": 0
    },
    {
      "_id": "64e0891fb72e199dda603ed8",
      "title": "Serverless Framework example for Golang and Lambda",
      "content": "Everyone, the day has come. AWS Lambda is finally. Compatible. With Golang. Here's how you can start using Go with the Serverless Framework RIGHT NOW and deploy Lambdas to your heart's content. Get Started First things first, you'll be needing the Serverless Framework installed, and an AWS account. (If it's your first time using the Serverless Framework, our first time deployment post has a quick setup guide. Takes like minutes, we promise.) Use the Go template The Framework will configure AWS for Go on your behalf. There are a couple Go templates already included with the Framework as of v.`aws-go` for a basic service with two functions, and `aws-go-dep` for the basic service using the `dep` dependency management tool. Let's try the `aws-go-dep` template. You will need `dep` installed. Make sure you're in your `${GOPATH}/src` directory, then run:  Change into your new service directory and compile the function:  The default command in the included Makefile will gather your dependencies and build the proper binaries for your functions. You can deploy now:  Finally, invoke your function:  Nice! Building a Web API with Go + Lambda The basic example is nice, but let's try something a little more useful. Lambda + API Gateway is awesome for quickly spinning up endpoints to retrieve or ingest data. So we're going to build an example endpoint. For our friends coming from interpreted, dynamically-typed languages (looking at you, Pythonistas & Javascript-lovers!), the Golang approach is a little different. You have to be a more intentional about the input & output of your functions. Don't worry, we'll take it slow. We're going to make an HTTP endpoint that accepts a POST request at the path `/echo`, logs the POST body, and echoes the body back to the client. First, let's fix our `serverless.yml` to attach an HTTP event:  We'll need to update our function in `hello/main.go`. Remember, Golang is a compiled, statically-typed language, so we need to define the `event` object that's coming into our function. Fortunately, AWS has provided a number of event types in a Github repo. We can just use those. Update your `hello/main.go` to have the following code:  Our `Handler()` function now takes an `APIGatewayProxyRequest` object and returns a `APIGatewayProxyResponse` object. In our function code, we're printing the request body, then returning a response with the request body. Recompile & deploy again:  Notice that you now have an `endpoint` listed in your Service Information output. Let's use `curl` to hit your endpoint and get a response:  Great! This should get you started on a web API. Feel free to check out the other Lambda events in Golang. Why use Go for your Lambdas? Golang support for Lambda has been one of the most anticipated releases. The crowd at re:Invent was ecstatic when Werner announced Golang support was coming soon. Why do people care about Golang so much? Simple: the combination of safety + speed. As we saw above, Golang is a compiled, statically-typed language. This can help catch simple errors and maintain correctness as your application grows. This safety is really useful for production environments. However, we've had Java and C support in Lambda for years. These are both compiled, static languages as well. What's the difference? Java and C both have notoriously slow cold-start time, in terms of multiple seconds. With Go, the cold-start time is much lower. In my haphazard testing, I was seeing cold-starts in the -ms range, which is much closer to Python and Javascript. Speed _and_ safety. A pretty nice combo. A Gateway to all the runtimes There's one final note about the Golang implementation on Lambda that's really interesting. The `main()` function which is the entrypoint to our Golang binary _isn't_ our Lambda handler function. It's a little RPC server that wraps our handler function:  Under the covers, it looks like Lambda starts up your executable on a coldstart. The executable listens on a given port, receives input via JSON, and sends a response via JSON. This opens up a lot of possibilities to bring other runtimes into Lambda. You just need to pull in an executable that implements the desired RPC interface. Erica Windisch, CTO at IOpipes, is already making progress on this by pulling NodeJS into Lambda: NodeJS running natively on AWS Lambda... &mdash; Erica Windisch (@ewindisch) January , This is really exciting; can't wait to see what the serverless community builds!",
      "__v": 0
    },
    {
      "_id": "64e0891fb72e199dda603eda",
      "title": "Where to start: the most popular Framework plugins",
      "content": "\"Don't build it again if someone else already open-sourced it for you.\" Signed, every developer ever. In other words, if you haven't yet checked out this huge list of Serverless Framework plugins, you're missing out. Because they are slick. And useful. And ready to use right now. Fortunately and unfortunately, there are also close to of them. Yew! Where do you even begin? HERE. That's where. We hereby present the most popular plugins for the Serverless Framework (plus a goodie bag of our own team's favorites). The community's most-loved Framework plugins (by GitHub stars) . Headless Chrome Plugin, by Marco Lthy The Headless Chrome Plugin bundles the serverless-chrome/lambda package and ensures that Headless Chrome is running when your function handler is invoked. Install:  Click here for Serverless Chrome examples. . Serverless Webpack, by Serverless Heaven If you want to use the latest Javascript, TypeScript, Elm, CoffeeScript (and more!) with Babel, then Serverless Webpack is for you. The plugin offers enhanced dependency management, and packages only the external libraries that are really used by your code. Install:  A cool walkthrough that uses it: - Creating a Serverless GraphQL gateway on top of a rd party REST API Plus: click here for an Elm demo. . Python Requirements Plugin, by United Income Pythonistas, this one's for you. This plugin automatically bundles dependencies from `requirements.txt` and makes them available in your PYTHONPATH. Install:  A cool walkthrough that uses it: - Build a Python REST API with Serverless, Lambda, and DynamoDB . Typescript Plugin, by Graphcool Zero-config Typescript supportyes please. Don't need to install any other compiler or plugins; it just works right out of the box. Install:  Click here for an example. Top plugin for Alexa development Bespoken Plugin, by Bespoken Test your Lambdas during development without having to deploy. The plugin generates a local server that is a attached to a proxy online so you can use that url to access the functionality from your laptop. A cool walkthrough that uses it: - Building & testing an Alexa skill with the Serverless Bespoken plugin . Domain Manager, by Amplify Manage custom domains with API Gateway. Use this plugin to create custom domain names that your Lambda can deploy to. Domain Manager also supports base path mapping for deploys and domain name deletion. TWO cool walkthroughs that use it: - How to set up a custom domain name for Lambda & API Gateway with Serverless - How to deploy multiple micro-services under one API domain with Serverless . Serverless WSGI, by Logan Raarup Build your deploy Python WSGI apps using Serverlesscompatible with Flask, Django, Pyramid and more. Also has a `wsgi serve` command that serves your application locally during development. . AWS Alias, by Frank Schmid This plugin lets you use AWS aliases on Lambda functions. Each alias creates a CloudFormation stack that is dependent on the stage stack; this approach makes for easy removal of any alias deployment, and protects the aliased function versions.  Have a plugin that's missing from our repo? Add it! Just submit it to the Community-contributed plugins repo. Want to make your own plugin? Please do. Every plugin is a glimmering gem in our heart-est of hearts. Here are some resources to get you started: - How To Write Your First Plugin For The Serverless Framework - Part - How To Write Your First Plugin For The Serverless Framework - Part - Advanced Plugin Development - Extending The Serverless Core Lifecycle - Advanced Plugin Development Part - Command Alises & Delegates, Enhanced Logging",
      "__v": 0
    },
    {
      "_id": "64e0891fb72e199dda603edc",
      "title": "How to apply design thinking to lean startup software development",
      "content": "When we say Design Thinking, were talking about the capital-D capital-T Design Thinkingthe one evangelized by Tim Brown at IDEO and changing organizations across the globe. More productivity (!) it promises, better connection to your users, higher rates of innovation! Tl;dr seems legit, right? So we tried it. Oh there were stridesand pain, and pitfalls. Plus some things you might want to repeat, and other things you wont. If youre thinking of adopting Design Thinking, or just thinking about your product process in general, read on for a peek into ours. What does Design Thinking look like in a tech company? In a nutshell, Design Thinking is a process not for how you execute, but for how you determine what you should be building in the first place. It focuses on a tight, user-driven feedback loop that, in theory, allows you to validate new feature ideas as quickly as possible before writing a single line of production code: . Its a classic problemno system is without its flaws. But at least we have a system. In sum We like Design Thinking and will probably keep using it, with some additional experiments and tweaks. What about all of you? Have you found any edits to Design Thinking that make it work a little better for startups? Wed love to see how others are using it!",
      "__v": 0
    },
    {
      "_id": "64e0891fb72e199dda603ede",
      "title": "Build a multi-region, multi-master application with Serverless and DynamoDB Global Tables",
      "content": "To see how DynamoDB compares to MongoDB, Cassandra, or BigTable, see here. AWS gives devs some powerful building blocks for making amazing applications. In this walkthrough, we're going to create a multi-region, multi-master, geo-routed applicationall in about minutes. We'll have compute located on two different continents writing to databases in their own region, and the written data will be replicated to the database in the other region. Users will be directed to the closest region according to their location. Prefer the video walkthrough? The webinar was recorded live, and is available on Cloud Academy; give it a watch! Now, on to the word-wise walkthrough. Setting up your app Before we begin, you'll need the Serverless Framework installed with an AWS account set up. Your version of Serverless must be . or higher to take advantage of regional endpoints. I'm going to write this in Python because I love Python. But you can do it in any language you want. Create a new Serverless project from a template using the `sls create` command and change into that directory:  Basic functionality Our application will be a web API over a key-value store, where users can submit and retrieve values for keys. The path in the URL will be the key name, and the payload for a POST request will be the value. To handle the basic functionality with our application, we'll need three things: - a DynamoDB table to store our keys and values, - a `set_key` function to allow users to store the value for a key - a `get_key` function to allow users to retrieve the value for a key Edit your `serverless.yml` so that it looks as follows:  Let's walk through this a bit. In the `resources` section, we've used CloudFormation to define our DynamoDB table. We've see it has a HASH key named \"key\"this is the primary access pattern for our table. In the `provider` section, we've added `iamRoleStatements` that give our functions GetItem and PutItem permissions to our table. We've also injected the name of the table into our environment via the `KEY_TABLE` variable. Also, note that we're using a regional endpoint with `endpointType: regional`. This was added in the . release of the Serverless Framework, and it allows us to have our endpoint in a particular region, rather than at all CloudFront edge locations. Finally, in the `functions` section, we have our two functions: `getKey` and `setKey`. The function handlers are defined in the `handlers` directory in the `get_key` and `set_key` modules. Let's add our handlers. First, remove the templated `handler.py` and add a `handlers` directory:  Add the code for your `setKey` function in `handlers/set_key.py`:  We will parse the key name from the URL path parameter and the value from the JSON body in the POST request. Then we'll save a DynamoDB Item with the key, value, and region it was written from. Then, we'll add the code for our `getKey` function in `handlers/get_key.py`:  It takes the key from the path, retrieves it from DynamoDB, and returns it to the user. Note that it returns both the region it was last written from _and_ the region that it's currently being read from. This will be useful later on. Great, let's deploy it. I'm going to use the `us-west-` region:  If your deploy was successful, it will show your base url in the `endpoints` block of your Service Information. Let's test our endpoints. Export your url into a variable called BASE_URL, then run a curl command to set a key:  And now, let's retrieve it:  Awesome, we retrieved our key. Note that it says it was last written in the `us-west-` region and that we're currently reading from the `us-west-` region. Adding a custom domain Now, let's add a custom domain to our endpoint. This means: (a) we won't have a funky endpoint like `rbmzj.execute-api.us-west-.amazonaws.com`; (b) it'll be easier to geo-route later. To follow along here, you will need to own a domain registered in Route. We're going to use the serverless-domain-manager plugin to simplify this. Note: If you want more details on this, read our post on setting up your custom domain with Serverless & API Gateway. This usage will be a little bit different since we're using a new feature to set up regional domains. My base domain is `serverlessteam.com`. I want my app to generally be accessible at `keyvalues.serverlessteam.com`. First, let's create an SSL certificate for our domain using Amazon Certificate Manager (ACM). Navigate to Amazon Certificate Manager _in the region you want to deploy your endpoint_. Click \"Request a certificate\" and enter your domain. I'll create a certificate at `keyvalue.serverlessteam.com`. Hit 'Next', and AWS will send your domain manager an email to confirm the certificate. While you're here, you should create one for your other region as well. I created mine in `eu-central-` (Frankfurt) using the same process. Next, install the `serverless-domain-manager` plugin using `npm`:  Configure your `serverless.yml` by adding the plugin to `plugins` and adding config in the `custom` block:  This will help us provision a custom domain in API Gateway. Note that we're provisioning a `regional` endpointType, and the domain name will use the region. We won't have it create our Route record as we'll create that separately. Go on and create your domain:  Because it can take up to minutes to provision a domain, let's provision our other region while we're here:  And now, deploy our two services:  Both our services are deployed. Let's set up the Route Latency records to our endpoints. Latency-based routing is a really interesting feature of Route. It allows you to create multiple DNS records for the same resource. Each DNS record points to IP addresses in different regions. When the user makes a DNS query, Route will return the record that offers the lowest latency based on the requesting user's location. Neat. We'll create two records for `keyvalue..com`one in us-west- and one in eu-central-. Use the script below, modifying the value of `DOMAIN` in the first line to match your domain: ```bash $ DOMAIN=serverlessteam.com SUBDOMAIN=keyvalue.${DOMAIN} STACKNAME=serverless-keyvalue-dev HOSTEDZONE=$(aws route list-hosted-zones --query 'HostedZones[?Name==`'${DOMAIN}'.`].Id' --output text) USDOMAIN=$(aws cloudformation describe-stacks --stack-name ${STACKNAME} --region us-west- --query 'Stacks[].Outputs[?OutputKey==`DomainName`].OutputValue' --output text) EUROPEDOMAIN=$(aws cloudformation describe-stacks --stack-name ${STACKNAME} --region eu-central- --query 'Stacks[].Outputs[?OutputKey==`DomainName`].OutputValue' --output text) aws route change-resource-record-sets \\ --hosted-zone-id ${HOSTEDZONE} \\ --change-batch '{ \"Comment\": \"optional comment about the changes in this change batch request\", \"Changes\": [ { \"Action\": \"UPSERT\", \"ResourceRecordSet\": { \"Name\": \"'${SUBDOMAIN}'\", \"Type\": \"CNAME\", \"TTL\": , \"SetIdentifier\": \"us-west-\", \"Region\": \"us-west-\", \"ResourceRecords\": [ { \"Value\": \"'${USDOMAIN}'\" } ] } }, { \"Action\": \"UPSERT\", \"ResourceRecordSet\": { \"Name\": \"'${SUBDOMAIN}'\", \"Type\": \"CNAME\", \"TTL\": , \"SetIdentifier\": \"eu-central-\", \"Region\": \"eu-central-\", \"ResourceRecords\": [ { \"Value\": \"'${EUROPEDOMAIN}'\" } ] } } ] }' bash $ curl -X POST ${USDOMAIN}/testing -d '{\"value\": \"It worked\"}' {\"key\": \"testing\", \"value\": \"It worked\", \"region\": \"us-west-\"} bash $ curl -X GET ${USDOMAIN}/testing { \"key\": \"testing\", \"value\": \"It worked\", \"writeRegion\": \"us-west-\", \"readRegion\": \"us-west-\" } bash $ curl -X GET ${EUROPEDOMAIN}/testing {\"error\": \"Sorry, an error occurred while retrieving your key.\"} bash $ aws dynamodb scan --table-name keyvalues --region us-west- | \\ jq -c '.Items[] | { key } ' | tr '\\n' '\\' | \\ xargs - -n -t aws dynamodb delete-item --table-name keyvalues --region us-west- --key bash $ aws dynamodb create-global-table \\ --global-table-name keyvalues \\ --replication-group RegionName=us-west- RegionName=eu-central- \\ --region us-west- bash { \"GlobalTableDescription\": { \"GlobalTableStatus\": \"CREATING\", \"GlobalTableName\": \"keyvalues\", \"ReplicationGroup\": [ { \"RegionName\": \"us-west-\" }, { \"RegionName\": \"eu-central-\" } ], \"CreationDateTime\": ., \"GlobalTableArn\": \"arn:aws:dynamodb:::global-table/keyvalues\" } } bash $ curl -X POST ${USDOMAIN}/testing -d '{\"value\": \"It worked\"}' {\"key\": \"testing\", \"value\": \"It worked\", \"region\": \"us-west-\"} bash $ curl -X GET ${EUROPEDOMAIN}/testing { \"key\": \"testing\", \"value\": \"It worked\", \"writeRegion\": \"us-west-\", \"readRegion\": \"eu-central-\" } ``` Nice, it worked. We can read the key in a different region. Note that the response from the `getKey` method includes the region in which the key was written (`us-west-`) as well as the region from which we're currently reading (`eu-central-`). We can also use our custom domain to retrieve the key from whichever region is closest:  This read from the `us-west-` region as I'm located in the US, but it would read from the `eu-central-` region for those closer to that region. Frontend client To this point, we've been using `curl` and the terminal for calling our API. If you want a more visual approach, Alex Casalboni created a visual frontend for interacting with the backend. It allows you to write to and read from specific regions, while also showing the latency of your requests: Check it out and run it yourself! Miss the DynamoDB global tables webinar? Follow @goserverless on twitter or sign up for our bi-monthly newsletter (via the lefthand menu) to stay on top of serverless industry news.",
      "__v": 0
    },
    {
      "_id": "64e0891fb72e199dda603ee0",
      "title": "Serverless style - my journey from fashion to tech",
      "content": "I am an artist. A fashionista. And Im going to be real with youI never imagined I would be here, writing a blog post, from my desk at a San Francisco tech startup. Lets just get this out of the way now. If anything, I was once anti-tech. (Emphasis on the was, obviously.) I thought tech wasnt cool. I thought as an industry it was too focused on disruption and not focused enough on people. But as you can see, I now work for Serverless. How I ended up here, and what led me to find passion in this industry, is a journey with many twists and turns. Ill do my best to not give you motion sickness with this story. Tech? Um, no I grew up one mile from Apple headquarters in Cupertino, California. Emerging tech was all around me, but I could have cared less about how it was made. In , Steve Jobs came to my school preaching the gospel about the new Macintosh SE and Macintosh II personal computers. He was wearing Tevas with socks and I remember thinking, Ew. I daydreamed through his entire spiel. I didnt think, I want to know how that works! or I could build this. But I do know my rd and th grade years were filled with hours of Oregon Trail on that machine. In high school, I dove straight into all kinds of artistic pursuits. I was super involved in the mid-s Bay Area music scene, was a radio DJ at KSCU . FM and went to school at UC Irvine for Studio Art and German. I moved to Cologne, Germany, where I worked as an English teacher and Gallery Assistant. I eventually moved back to the Bay Area and got a job at SFMOMA. I thought I would end up being a curator specializing in video art, and even looked into going back to school for my Masters in Art History. But after two years at SFMOMA I became extremely unhappy; I hated being poor and unappreciated (pros of working in a non-profit environment). So I left. Thats when I met Sam. From non-profit art to fast-paced fashion I met Sam at a mutual friends party and we couldnt. Shut. Up. About fashion. Two months later, we were signing business partnership papers in San Francisco. Our fashion sales agency, Varjak, was born. Sam and I spent several years together, attending fashion weeks in New York, London and Paris, representing up and coming designers in wholesale, meeting fashion VIPs including U.S. Vogue Editor in Chief, Queen Bee herself, Anna Wintour. Thenwell, you all are smart. You know what comes next. Everything started going digital. Online shopping became the big moneymaker. Retail all over the world was suffering, and our buyers were reporting weak sales numbers. Varjak was taking major hits. Tech, yet again, was creeping its way back into my life. So I sold my half of the business to Sam and sat there, wondering what on earth I was going to do next. Enter: a random tech recruiter I randomly met a tech recruiter at a happy hour in SF and we started talking about work. You seem like such a passionate and motivated person, she told me, Youd be great in tech. What??! was really all I could say to that. Oh, no no no I thought. Not me, not tech. I grew up around it, I said. I wasnt interested. But she persuaded me to send in my resume. For some reason, I did. Almost as quickly as Id met the recruiter, she called me with a position. I have something that could be the right culture fit for you, she told me. The company is pretty techy in terms of what they are doing, but the people seem great; I think someone like you could be a good addition to their team. Maybe shake them up a little with your flare and sass (or rather SaaS)? I was reluctant, but also intrigued. Ultimately, I thought: hey, why not? I was down to try anything once. I said yes, and started my contract position as an Executive Assistant at Serverless that same week. I was nervous. But also, I LOVE the unknown and have never been one to shy away from challenges. They make sure I learn rapidly and continue growing. That first week The first thing I noticed when I started at Serverless was how genuine and nice everyone was. Austen Collins, our CEO and founder, started his career in Hollywood and ended up creating this Framework that thousands of developers use every day. He came from a creative industry and found a solution for a technical problem. I got to know the others and found a great group of funny, brilliant, multi-talented individuals who all had other interests outside of tech, but were passionate about Serverless. As a company, they were focused on community and had a mission to empower others. They wanted to make it easier for everyone to make software. The more I got to know about the company and the people working here, the more I found myself imagining all the ways I wanted to make Serverless stand out and shine in this tech-saturated world. The future is serverless Tech doesnt have to be stoic or unfeeling. Its a lot like fashion, to be honest. It requires having creative solutions, being fast-paced, being on the cutting edge, and engaging with the community (& the fans). I think a lot of modern tech companies struggle to bring true humanity into this industry. But they are starting to take in views from people outside of tech, like me, and they are becoming more diverse. And heres the thing: I was drawn to Serverless not only because of the people, but because of what they are trying to do as a company. They are trying to democratize software development. They want to make it easier to do and accessible for anyone to learn. Maybe even me. Unlike that Steve Jobs presentation in , Im excited now to see where my ideas will take me. I want to know how things work. I want to build them. From my fashion roots, I have spent a lot of time creating my own style language. Every day, I choose my outfits with the intention of communicating a strong, visual message. And now, I want to help Serverless become classic blackthat enduring look everyone needs. Watch out world... Here comes my Serverless style.",
      "__v": 0
    },
    {
      "_id": "64e0891fb72e199dda603ee2",
      "title": "Using the Serverless Dashboard plugin for Atom",
      "content": "Many developers love and use the Serverless Framework for writing their serverless applications. In the spirit of making it even easier to manage the serverless applications using the Serverless Framework, Takahiro Horike, created the Serverless Dashboard package for the Atom editor. In this post, I will give you an overview of the Serverless Dashboard package. We'll cover: Installing the plugin in Atom Visualizing the serverless.yml file Using the Atom plugin to manage a serverless app Installation Let's start by installing the Atom package or plugin. If you don't have Atom, you'll need that first! If you don't have the Serverless Framework installed, you'll need that too. Installing the Serverless Framework is a breeze:  You can search for the Serverless Dashboard package on the Atom site: !Search for the package Search for the package To install it, do the following: Launch Atom Open 'Settings View' using Cmd+, on macOS or Ctrl+, on other platforms Click the 'Install' tab on the left side Enter `serverless-dashboard` in the search box and press Enter Click the 'Install' button that appears !Install the package Install the package Visualizing the serverless.yml file Let's create a simple app named `helloatom` using the boilerplate template provided by the Serverless Framework.  Open the app files in atom. To use the Serverless Dashboard plugin, locate the 'Serverless Dashboard' item on the 'Packages' menu list as shown below: !Post install Post installation Click on the 'Open your serverless.yml' and choose the `serverless.yml` for the project. A new pane will open with the Serverless Dashboard showing a visual representation of the `serverless.yml` file. !Serverless Dashboard Serverless Dashboard Managing a serverless app The Serverless Dashboard plugin not only lets you visualize the `serverless.yml` file, but also helps you easily manage a few things inside the Atom editor pane. Deploy a service You can easily deploy the service by clicking on the 'Deploy Service' button. The service will be deployed to the stage and the region as specified. Note that you can change those settings directly from the pane. !Update deploy settings Update deploy settings Let's see what the deployment looks like: !Deployment Deployment Nothing new here if you're already familiar with Serverless Framework. The output of the deployment is exactly what you would see in the terminal if you'd deployed using `sls deploy`. Updating the serverless.yml file Let's add a new function, `byeWorld`, to the serverless.yml file, like so: ```yaml byeWorld: handler: handler.byeWorld The `events` block defines how to trigger the handler.byeWorld code events: - http: path: bye-world method: get cors: true js console.log(` From helloWorld: \\n Event: JSON.stringify(event) \\n\\n`); ``` Since we just updated our function code, let's just deploy the `helloWorld` function alone. Select the 'Deploy Function' and click on the 'Apply' button as shown below: !Deploying a function Deploying a function Invoking a function To invoke a function, click on the dropdown next to the function name. Select 'Invoke', and then hit 'Apply': !Invoking a function Invoking a function Logs for a function To view the logs for a function, click on the dropdown next to the function name and select 'Logs', and then hit 'Apply' as shown below: !Logs for a function Logs for a function Removing a service Last but not least, you can remove the service right from the pane: !Removing a service Removing a service Summary The Serverless Dashboard is a convenient package that you can install in Atom to easily access various commands for the Serverless Framework within the Atom editor. It does not support all the commands that are available via the CLI, but it covers the most commonly used commands for managing a service. Kudos to the Serverless Champion, Takahiro Horike, for writing this package and helping the community.",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603ee4",
      "title": "Implement real-time updates using Lambda and Websockets via IoT",
      "content": "Would your frontend application benefit from having access to updated data in real-time, but you can't seem to find a 'serverless' way to do it? Well, that was me months ago. I had a goal to make a serverless chat app, but no clue how I would implement real-time communication between my backend and the frontend client. If you search AWS for 'websockets serverless' you will unfortunately find nothing (at least at the time of writing this article). So how did I do it? Read on, my friends! (Or, just check out the repository to see what I've done.) Some background Before we dive into the real-time updates, Id like to explain how I've structured my application in general. This is a (very) rudimentary diagram I've created to show how the whole app fits together. !AWS Diagram The frontend is hosted in an S bucket as a static site. I'm using VueJS for my frontend framework. That bucket is behind a CloudFront distribution with an SSL certificate from Amazon Certificate Manager, and my Route domain points `awschat.net` (my app) to that distribution. The backend is your standard serverless API. API Gateway routes incoming requests to specific Lambdas, which then access DynamoDB and the IoT message broker. (We'll dive more into that later.) I also use a Cognito User Pool authorizer to ensure that all of my requests are coming from users of my application. I'll dive more into that in the next section. Note: If you want to check out the finished version of my application before reading on, head over to https://awschat.net Setting up Cognito To set up Cognito User Pools with your API and frontend authentication, youll need a bit of CloudFormation. Here are the Cognito-related entries in the resources section of my `serverless.yml`:  One thing to note is: when you want to add a Cognito User Pool Authorizer to an endpoint, the Serverless Framework doesnt support using a user pool that gets created in the same stack. That means we have to define every endpoint-authorizer attachment manually in CloudFormation, like this:  We also have to use the normalized name of our API Gateway method. (Check here for normalized names of serverless-created resources.) Setting up the IoT message broker configuration Step : AWS This part is a little bit more complex. In order to allow a front end user to use our message broker, we first have to set up an Identity Pool that our users can authenticate with. Note that, in my application, I've left privileges a little more loose than they should be in a serious production app. In fact, I technically leave all IoT users unauthenticated. This is because adding logged-in credentials is a bit more complex. ButI only give them permission to subscribe, receive, and connect. So no one can maliciously send a bunch of messages from their client. The CloudFormation to set that up is right here:  We will also need to add a policy to IoT that allows all actions to be executed. To change your IoT policy, you must go to the IoT console and click 'Get started'. From there, click 'Secure' on the left side menu, and then 'Policies'. You will want to create a new policy that looks like the one below:  Note: Again, this is probably too broad of a policy for a production application. You can narrow it down once you've established your application's IoT access patterns. Step : Accessing IoT from Lambda Say a user sends an API request to add a message to a chat. We update our database, and then return the message to the user who sent it, but how do we notify the other user that a message has been added? That's where the AWS SDK comes in. My example uses the JavaScript SDK. Youll need to get your IoT endpoint, which you can do by running `aws iot describe-endpoint` in your console if you have the AWS CLI. Or you can find it by going to the IoT console, clicking 'getting started' and then clicking on 'settings' in the bottom left corner. ```javascript const AWS = require('aws-sdk'); const iotData = new AWS.IotData({endpoint: 'YOUR_IOT_ENDPOINT HERE'}); exports.handler = (event, context, callback) => { const iotParams = { payload: JSON.stringify({ message: 'Hello!'}) topic: `/my-app/${event.receiverId}` } iotData.publish(iotParams, (err, data) => { if (err) { // handle error here } callback(null, { success: true }) }) } ``` The example Lambda above takes in a user id (`event.receiverId`) and broadcasts to that user's channel in our app. We have a prefix in our topic, `my-app/` which will let us use the same IoT endpoint for multiple applications. The last thing we need to do is set up our frontend application to use the IoT message broker so that we can actually receive the publish event. Step : Connecting in your front end Luckily, someone has already come up with a library that lets us listen to the IoT mqtt broker over Websockets! You can find it here. Here is an example of how to use it in a webpack build (taken from the library mentioned above): ```javascript import AWS from 'aws-sdk/global' import AWSMqtt from 'aws-mqtt' AWS.config.region = 'us-east-' // your region AWS.config.credentials = new AWS.CognitoIdentityCredentials({ IdentityPoolId: '...' // your identity pool id }) const client = AWSMqtt.connect({ WebSocket: window.WebSocket, region: AWS.config.region, credentials: AWS.config.credentials, endpoint: '...iot.us-east-.amazonaws.com', // NOTE: get this value with `aws iot describe-endpoint` clientId: 'mqtt-client-' + (Math.floor((Math.random() ) + )), // clientId to register with MQTT broker. Need to be unique per client }) client.on('connect', () => { client.subscribe('/myTopic') }) client.on('message', (topic, message) => { // this is where you will handle the messages you send from Lambda console.log(topic, message) }) client.on('close', () => { // ... }) client.on('offline', () => { // ... }) ``` Conclusion That's it! Now you know how to implement real-time communication between your frontend application and serverless backend. The source code for the entire project can be found here: - Serverless chat app backend - Serverless chat app frontend",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603ee6",
      "title": "Communication strategies for remote teams",
      "content": "In case you didn't know, the Serverless team brings new meaning to the word \"distributed\". All in all, we span time zones, and our overlapping waking hours are slight. Getting communication right is hard for teams who are all co-located, and at least x more difficult for remote teams like us. It's easy to feel disconnected when you can't interact in personfrom not only your co-workers, but from the company vision, too. One way we tackle this problem is by having bi-annual team retreats. Our most recent retreat was only a couple weeks ago, and communication was our key topic of discussion. How could we make it better? So we got everyone in a room, and dug in. It made for some tough conversations, but gave us a huge amount of clarity and a stronger foundation for moving forward. Heres what we learned that might be applicable for you other remote teams out there. Radical Candor and empathy Radical Candor was required reading for everyone before this retreat. One of the core tenets of Radical Candor is that you need to meet people where they are, not where you assume them to be. This can mean lots of things, from asking people about their lives, to communicating the honest (hard) stuff in a way that can be best heard by that person. In fact, Radical Candor requires you to say the hard stuff. The framework is all about honest and caring communication that will spur the company forward. When it came to Serverless as a company, this meant we needed to answer a basic question: how do we all actually prefer to communicate? Thus began the experiment We chose to answer this question in a decidedly physical way: we drew a line across the floor. The line spanned our entire meeting room. One end represented people who prefer asynchronous, written communication; the other represented those who prefer in-person meetings. We then stood on the line, each team member placing themselves roughly where they felt they sit in terms of the their communication preferences. We each had a chance to say a few words about why we had chosen that specific place on the spectrum. The results were amazing and intensely personal and humanizing. Hearing everyone explain exactly why they like a certain communication styleor dontwas eye opening. So, too, was the sheer number of communication tools that we as as team are using (close to on an initial count). This exercise brought to a light a few epiphanies for us a team. methods for better communication Communicate about which tools you use (or don't). Don't check the team channel on Slack ever? It's ok, but say it. This doesnt mean you have to be draconian and land on or tools, just recognize that everyone communicates a little differently. Assume positive intent (as our VP of Engineering Ganesh put it). Email go unanswered? Assume your colleague didnt see it because they prefer a different communication mode, and dont take it personally. Assume they really do want to hear what you have to say. Communication tools = silos. Its easy to assume that with every newfangled communication tool were all getting closer to communication nirvana. Slack will save the world! Confluence will make us organize our work more efficiently! Asana is flexible enough for all of our needs! What came out for us was the exact opposite: each communication tool is an opportunity, surebut an opportunity to create yet another silo that will add more friction. Your colleague who never replies to your emails? Theyre a Slack fiend. Feel like youre in an echo chamber on Github, with your PRs going unnoticed? Its because your teammate turned off notifications going to their email. Enact a two-way SLA. Along with positive intent, it can be useful to assume an unofficial two-way SLA policy. This means that if you ask a question, or post a comment and it goes unanswered, follow up in at least one different channel. We too often use our communication tools to push out content, and we need to remember that they can be really efficient pull tools as well. Hey, would you mind answering this question I asked? can go a long way toward healthier communication and happier people all around.",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603ee8",
      "title": "Fantastic Serverless security risks, and where to find them",
      "content": "With the rising hype around all things Serverless, Ive been getting regularly asked one simple question: My response is always the same. How can you guarantee anything? Im just as scared about my EC instance getting breached as I am about my Lambdas. Most server vulnerabilities are due to programmer error. That one line of code that does a tiny bit more than it should. That one app secret you misplaced. Those files you forgot to encrypt. There are numerous things we, the developers, can do to write better software. That said, the distributed nature of Serverless Architectures gives a malicious attacker more room to maneuver. The greatest asset of serverless is also its most dangerous foe; it gives attackers significantly more points of entry. This had me genuinely worried, so I started digging for answers. That's when I came across Puresec's study, The Top Ten Most Critical Security Risks in Serverless Architectures. I read it without lifting my eyes from my screen. So many things became crystal clear. The path to better Serverless Security Sadly, theres still a common bad practice among developers: we focus on security once the software were building is already up and running. In a nutshell, the getting around to security...eventually mentality is what kills us. The top serverless vulnerabilities are remarkably similar to the top vulnerabilities, period. Read on for the takeaways from the Puresec security study, and for measures you can take right now to strengthen the security of your application. TL;DR __Note__: I strongly suggest you read the whole Puresec study. Its freaking awesome. If you want a quick recap of the risks take a look at the TL;DR below. Or just jump to the section you are interested in. - Event injection  Solved with input validation and predefined database layer logic, such as an ORM or stored procedures. - Broken authentication  Solved with built-in authentication/authorization solutions and avoiding dangerous deployment settings. - Insecure deployment settings  Solved with never using public read ACLs and keeping files encrypted. - Misuse of permissions and roles  Solved with the least privilege principle. - Insufficient logging  Solved with rd party tools such as Dashbird or becoming well versed in using CloudWatch. - Insecure storing of app secrets  Solved by using AWS KMS to encrypt your application secrets. - DoS attacks and financial exhaustion  Solved with writing efficient code, using timeouts and throttling. - Improper exception handling  Solved by logging stack traces only to the console or dedicated log files. Never send stack traces back to the end user. Getting risky with it This is probably obvious to most of youseveral steps in improving security lie in the quality of our application structure as a whole. The way we architect our software, and our level of attention to detail, will ultimately lead to a robust and secure software product. Thats enough of my yapping. Lets get started with the risks! Event injection It's common sense to always validate input. So why am I even talking about this? Because we often forget about the edge cases. Do you regularly make sure the input is of the data type you are expecting? I tend to forget from time to time, so I imagine most people do. How about the several different event types that can trigger a serverless function? The events dont necessarily need to be HTTP requests. You may only have checked to make sure the input from an HTTP event is validated. Dont forget to check for the case when the event is not what you expect it to be. And please, use a firewall. Its easy to set up and makes a huge difference. Moving on from the actual events, try to use predefined logic for database interaction. This will reduce the risk of injections. Especially if you make sure to run all the code with the minimum OS privileges required to get the job done. Broken authentication Use built-in solutions for authenticating users and authorizing their access to resources. This is pretty straightforward with authorizers or AWS Cognito. Using them is a no-brainer. Related: We have a post on setting up robust IAM permissions, check it out. You can rest assured using stateless authentication with Auth or JWT is perfectly fine. The real issue is not the actual authentication method, but instead insecure deployment settings containing components with public read access. Well talk more about this in the next section. Some of you maybe dont like stateless authentication, and thats okay. You can use sessions just fine, but not running on the Lambdas themselves. Every serverless function is stateless by nature, so we cant store persistent data on them. Hence, for sessions, we can use a dedicated Redis server. AWS has Elasticache, which is a great Redis and Memcached service. You only have to make sure to have the Lambda and Elasticache running in the same VPC. (Heres a quick tutorial for getting that set up.) Once youve done that you can add the `AWSLambdaVPCAccessExecutionRole` to your Lambdas IAM statements and be good to go. With all the talk about actual authentication principles, thats not the real issue here. Your application layer authentication may work flawlessly, but that doesnt stop a malicious attacker from accessing S buckets with public read access. Please, never enable public read access, unless you are using a bucket for storing images or a static website. In which case, you only keep those files in that bucket, nothing else! Insecure deployment settings If youre even the slightest bit worried about the privacy of your files, enable all the encryption methods you possibly can. Luckily AWS has both client-side encryption for encrypting a file before its sent through the wire, as well as server-side encryption once its added to an S bucket. But, none of this makes any sense if your buckets have public read access enabled. Keep track of your S ACLs and make sure the access levels are not littered with unnecessary permissions. You can enable server-side encryption (SSE) for protecting data in your buckets with both SSE-S and SSE-KMS. Pick whichever you feel will work best with your use case. Id also encourage you to use client-side encryption with the AWS SDK. Heres a nice explanation for you to check out. Over-privileged function permissions and roles Developers are lazy most of the time (and Im no exception)we set a single permission level for a whole service containing tons of functions. Even though this maybe makes sense at first, it can be very dangerous. Functions should only have those permissions that they need to fulfill their purpose. A function that fetches some data from DynamoDB should not have permission to add data to DynamoDB, for instance. The same logic applies to adding images and retrieving them from S. It doesnt make sense for a function doing the GetObject operation to have permission to do a PutObject operation, now does it? If youre used to working with the Serverless Framework, you can easily configure the IAM Role Statements on a per-function basis (or just use this plugin). Make sure to always follow the least privilege principle. One function, one use case, one permission model. Insufficient logging and monitoring Here comes the difficult bit! Crappy logs equal missed error reports. If you miss critical errors, your users will suffer from greater downtime just because you werent notified properly in order to fix them. But its just as dangerous the other way around. Make sure never to log out info containing sensitive data. To get a grip on this, you either need to become a master parser of CloudWatch logs, or use a rd party tool such as Dashbird. From my experience with Dashbird, Ive enjoyed that they have live monitoring and error reporting, timeout monitoring, live tailing, price calculations and many other features Ive still not had the need to use. It gives you a birds eye perspective on your Serverless app, pretty much simulating what a regular old-school server application would look like. It can also send error reports to a Slack channel. (We all know how much developers love Slack.) Dashbird has a free plan, so you can go ahead and try it out if you think it'll be useful for you. Keep application secrets encrypted Even though you dont push environment variables to GitHub, malicious attackers can still access the values if they gain access to the system where your code is running. Hence, the need to use KMS to encrypt environment variables. Theres a plugin for the Serverless Framework that makes it easy. DoS and financial exhaustion I love the principle of Lambda, where you pay for the amount of time your code is running. This pushes the developer to write efficient code. Efficient code is less error prone and you can anticipate it will not run for ridiculously long periods. This makes it much easier to add timeouts. The default timeout for a function when using the Serverless Framework is seconds. Its more than enough for pretty much any production-level HTTP request. The default memory usage is set at MB and thats often more than enough. If you ever worry about DoS or some hacker invoking your Lambdas for a stupid large about of time, you can always throttle incoming API calls. This goes through API Gateway, and its as simple as setting a few fields to limit the amount of requests per second. Using the Serverless Framework is also helpful here, because it enables setting a monthly cap on the number of invocations a particular API can have. Incredibly convenient. Improper exception handling and verbose error messages Debugging serverless architectures is still an issue. Handling this is best done with sound programming practices. Write unit tests. Write readable code. Emulate the AWS environment locally, so you can run all the code locally before deploying to the cloud. Stack traces should only ever be logged to the console or log files; never send stack traces to back to the client. Make sure to only send vague messages in error responses. Risk-vana...? Many of the issues I mentioned here apply to general coding practices, regardless of whether you're using traditional servers and Serverless Architectures. Writing clean code, keeping secrets safe, doing input validation and error handling, are universal concepts we as developers swear an oath to uphold. The real issues come with deployment settings, per-function permissions, bad logging, insufficient error reporting, and financial exhaustion. These issues are still manageable, youre just not used to solving them yet. Serverless is still a young paradigm that we need time to get used to. The decentralized nature of using serverless pushes us to look for ways of grouping resources into logical groups. Its biggest advantage is also the largest drawback. Wrapping up This article has showed you the basics of Serverless security, what to watch out for, and how you can patch as many vulnerabilities as possible. Hopefully, this has helped you gain more insight of the inner workings of Serverless Architectures. If you want to take a look at Puresecs guide, check it out here. If you want to read my latest articles, head over here, or you can always hit me up on Twitter. Until next time, be curious, have fun, and feel free to utilize the comments below.",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603eea",
      "title": "Running a scalable & reliable GraphQL endpoint with Serverless",
      "content": "New to AppSync? Check out this Ultimate Guide to AWS AppSync - Part : GraphQL endpoints with API Gateway + AWS Lambda (this post) - Part : AppSync Backend: AWS Managed GraphQL Service - Part : AppSync Frontend: AWS Managed GraphQL Service Introduction Over the last four years, I've been exploring the world of big data, building real-time and batch systems at scale. For the last couple of months, I've been developing products with serverless architectures here at Glassdoor. Given the intersection of serverless and big data, there have been a few questions on everyone's mind: ) How can we build low latency APIs to serve complex, high dimensional and big datasets? ) Using a single query, can we construct a nested response from multiple data sources? ) Can we build an endpoint which can securely aggregate and paginate through data and with high performance? ) And is there a way we can do all of that at scale, paying only for each query execution, and not for idle CPU time? The answer for us ended up largely being GraphQL. This post aims to show you how you too can streamline your existing workflow and handle complexity with ease. While I won't be digging deep into specific things Glassdoor was working on, I will be showing you a pretty related example that utilizes a mini Twitter clone I made. Ready to talk about creating Serverless GraphQL endpoints using DynamoDB, RDS and the Twitter REST API? Ready to see a sweet performance comparison? Ready to hear some solid techniques on how you can convince the backend team that using GraphQL is a great idea? Awesome. Let's go. Note For the GraphQL and Serverless primer, keep reading. Or click here to go straight to the code walkthrough What is GraphQL? Im going to start this off by stating a fact: The way we currently build APIs, as a collection of micro-services that are all split up and maintained separately, isnt optimal. If you're a fellow back-end or front-end engineer, you're probably familiar with this struggle. Luckily for us, the tech horizon is ever-expanding. We have options. And we should use them. GraphQL lets you shrink your multitude of APIs down into a single HTTP endpoint, which you can use to fetch data from multiple data sources. In short, it lets you: Reduce network costs and get better query efficiency. Know exactly what your response will look like and ensure you're never sending more or less than the client needs. Describe your API with types that map your schema to existing backends. Thousands of companies are now using GraphQL in production with the help of open source frameworks built by Facebook, Apollo, and Graphcool. Starbucks uses it to power their store locator. When I read that, it made my morning coffee taste even better. . Hence, on API Gateway the network latency is approximately between - ms, which can be further reduced by caching. You might ask, \"Why do we need API Gateway? Can't we just use Lambda to fetch the GraphQL response?\" Selling GraphQL in your organization Ready to switch everything over, but not sure about how to convince the backend team? Well, heres how Ive seen this play out several times, with success. First, the frontend team would wrap their existing REST APIs in a serverless GraphQL endpoint. It added some latency, but they were able to experiment with product changes way faster and could fetch only what was needed. Then, they would use this superior workflow to gain even more buy-in. They would back up this buy-in by showing the backend team that nothing had broken so far. Now Im not saying you should do that, but also, if you wanted to, there it is for your consideration. My lips are sealed. Special thanks! First of all, I would like to thank Nik Graf, Philipp Mns and Austen Collins for kickstarting open source initiatives to help people build GraphQL endpoints easily on Serverless platforms. I have personally learned a lot during my work with you guys! I would also like to give a shout to our open source committers - Jon, Lo Pradel, Tim, Justin, Dan Kreiger and others. Thanks Andrea and Drake Costa for reviewing the final draft of this post and Rich for helping me out with questions. Last but not the least, I would like to thank Steven for introducing me to GraphQL. I hope you guys liked my first blog post! Feel free to reach out and let me know what you think. Siddharth Gupta Github | LinkedIn | Twitter",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603eec",
      "title": "ETL job processing with Serverless, Lambda, and AWS Redshift",
      "content": "One of the big use cases of using serverless is ETL job processing: dumping data into a database, and possibily visualizing the data. In this post, I'll go over the process step by step. We'll build a serverless ETL job service that will fetch data from a public API endpoint and dump it into an AWS Redshift database. The service will be scheduled to run every hour, and we'll visualize the data using Chart.io. At Serverless Inc., we use ETL jobs just like this for tracking metrics across an array of data points (though feel free to modify the one I'll show here to fit your own needs). Let's get to it. What we will cover: Prerequisites Creating the ETL job service Deploying and scheduling the job Visualizing the data Prerequisites Before we begin, you'll need the Serverless Framework installed with an AWS account set up. Your version of Serverless must be . or higher to take advantage of all the latest updates. AWS Redshift Setting up AWS Redshift is out of the scope of this post, but you'll need one set up to dump data into it from our ETL job. Once you have it set up and configured, keep the cluster endpoint in Redshift handy, as we will need it later to configure the database connection string. Redshift cluster endpoint: `.xxxxxxxxxxxx..redshift.amazonaws.com:` DB connection string: `postgres://:@:/` where `:` is the cluster endpoint. Building the ETL job service My previous projects were in Node.js, but I'm going to write this in Python because I have recently started to play with Python. And, (to the chagrin ofmy colleague Alex DeBrie I love it! But you can write this in any language you want. Let's create a new Serverless project from a template. Use the `sls create` command and change into that directory:  Next, let's install some required dependencies. We'll use the `serverless-python-requirements` plugin for handling our Python packages on deployment:  Note: If you want a deeper dive on the `serverless-python-requirements` plugin, check out our previous post on handling Python packaging with Serverless. With the dependencies out of the way, let's get started. Replace the `serverless.yml` file contents with the following `yaml` code:  Let's review the above `serverless.yml` configuration for our service. We need to specify the connection string to the Redshift postgres database. You'll notice I have used the AWS Parameter Store (SSM) to store the connection string for obvious security reasons:  Note: For a detailed explanation of secrets management strategies, check out our previous post on managing secrets, API keys and more with Serverless. Since Redshift is secured by running under a VPC, you'll need to supply the appropriate security groups, subnets, and IAM roles like so:  Using a Public API To keep it simple, I wanted to use a free, public API without any authentication. We'll be using the CoinMarketCap API. Bitcoin is in peak hype, and I thought it would be interesting to see the metrics over time. Fetch data and stash into Redshift Now that we have our configuration set, let's replace the `handler.py` file with the following code:  Let's review the code. We start by importing the required libraries, and specifically we are using the SqlAlchemy library to work with postgres. The `get_data()` method accesses the API to fetch the data for bitcoin. The `cleanup()` method is a helper method to filter out only the fields from the API that we need. In the following code segment, we define the schema for the `sample_coinmarketcap`:  In the `init_tables()` method, we first drop the table if it exists, and then create the table, if it does not exist. We are dropping the table each time because we want to store the latest set of data every time we process. If instead you want to append data to the table, do not drop the table. Finally, the `main()` method brings it all together by fetching data and inserting the data into the database. You can review the full source code at the serverless-etl Github repo. Test the job service Before we deploy the service, let's test the service to see if our code is functioning properly. Since we're mainly concerned with testing whether or not our API call gets us the required data correctly, we'll comment out the database-related code for now. Comment out the code in the `main()` method as shown below:  To test locally, simply do:  The output is shown below:  Now that all the data we want looks good, we can deploy our service. Deploy and schedule the job Before we deploy, remember to uncomment the database code that we commented above. Once that's done, deploy the ETL job service:  This will set up our ETL job service in AWS to run as per the specified schedule. In our case, the job will run every hour. Visualizing the data I thought it would be useful to show the fact that you can visualize that data easily via services like Chart.io or Metabase.io. Without going into too much detail about these services, you can see the visualization below: !Chart.io graph Summary We saw how easy it is to create an ETL job service in Serverless, fetch data via an API, and store it in a database like Redshift. The service can be deployed on AWS and executed based on a schedule. The cost savings of running this kind of service with serverless is huge. Redshift instances are pretty expensive; with serverless you'll only pay when the job executes. No need for dedicated infrastructure. If you have other use cases you've implemented or have any questions, please leave a comment below. I'd be happy to discuss it with you.",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603eee",
      "title": "The serverless path to building better software",
      "content": "We, as the software tooling industry, are failing to empower developers to build better software. Dont get me wrong, the way we develop software has changed dramatically over the last years. Weve made tremendous progress in terms of the technologies and tools available to us. But what have these developments actually done for our productivity as software developers and creators? Have they made it easier to build software? Cheaper? Faster? Ive spent the last years of my career building tools aimed at making developers more productive (first with my own startup, then CircleCI, and now Serverless), and in searching for the answers to these questions Ive mostly found them to be no. While there have certainly been tools developed that have helped us to build much more powerful software, they have only incrementally contributed to our productivity. As an industry, were still faced with the same underlying problem: building software is extremely difficult and expensive. The proof is in the cloud Lets take a look at the cloud as a very macro example. The cloud lets somebody else manage our servers, letting us (the developers) spend more time focusing on code. This is an incredible value proposition. This is the value proposition that has resulted in an explosion of cloud services over the past years as well the healthy $ billion market capitalization that Amazon enjoys as Im writing this. But while the cloud has delivered on its promise of freeing software developers from having to deal with physical servers, it has failed to eliminate a lot of the complexity that made dealing with servers a pain in the first place: provisioning, scaling, maintaining, debugging, etc. This same paradox holds true for a lot of the technologies that we as an industry (myself included) have helped create. CI/CD platforms help make testing and deploying software easier, especially among teams, but they still require writing tests, configuring the environment, and dealing with faulty build containers. GitHub provides us with a much better user experience for collaborating on software, but it hasnt fundamentally eliminated the problems teams face when they collaborate on a complex code base. In general, most of the progress weve made as an industry has been incremental. Weve failed to make software significantly easier, cheaper, and faster to develop, which is the end state that we all want. The path to building better software My short answer: serverless. Or rather, continuing to invest in the robustness of serverless technology. Serverless, which is a movement to abstract infrastructure away from application development as much as possible, is currently manifested mostly in the form of function-as-a-service (FaaS) offerings, such as AWS Lambda. These offerings, and the wider serverless movement itself, are interesting because they have the potential to manifest dramatic increases in productivity. Namely: democratizing the ability to create heavily customized tooling, eliminating work redundancy by letting us reuse code, and making it much easier for us to access data. Making it easy to customize tooling When it comes to software development tooling, we face the same issues any potential consumer of SaaS faces. Build or buy? Invest in building a tool to exactly meet our needs, and then get stuck maintaining it? Or adopt a third-party offering, which inevitably comes with some baked-in opinions and workflows that wont work the way we need? Neither are ideal; both result in tool stacks that are often brittle, expensive, error-prone, and dont help us efficiently produce high-quality software. Ideally, wed be able to adopt a tool that has a strong open-source community and/or commercial company behind itbut is easily, and drastically, customizable. This answer might seem obvious, but we just arent there. The growing proliferation of open APIs did some of this groundwork, but its still far from easy to customize most tooling. Serverless, though, has the potential to make this ideal state a reality. If your source management, CI/CD, and project management system all exposed events in a uniform fashion, which you could easily react to with a serverless function deployed to any platform of your choicewell, Id say thats pretty ideal. We have a ways to go to achieve this, but projects like Auths Extend, CNCFs CloudEvents, and many more are making progress towards this goal. Making it easy to reuse code As developers, theres nothing more disheartening than writing code that weve written a hundred times. How amazing would it be if we could compose the bulk of our applications out of pieces of code weve already written, and then focus our coding time on the business logic that delivers unique value? Serverless architectures today are essentially microservice architectures: groups of functions that have common functionality. They are deployed together, and share infrastructure resources such as an API gateway. I believe that as serverless tooling evolves, these services will become much smaller, much more specific, and will cover a much wider set of use cases (e.g. a function that deploys and configures a specific piece of infrastructure, or performs a load-test). As these services become smaller and more specific, they also become more easily shared, configured, and consumed by people other than the original developer. This will eventually take us to a future in which micro-services can be easily shared, deployed, and reusedboth publicly, and privately within an organization. This would dramatically reduce the amount of time we spend re-inventing the wheel and allow us to focus more on unique functionality and business value. There are currently efforts along these lines underway, such as Standard Lib. Itll be really exciting to see where these go in the future. Making it easy to access data All digital businesses today generate and collect massive amounts of data. This data is a resource that could be extremely valuable. Notably, we could use it to build even more powerful features for our users. The problem that comes along with this massive amount of data is that it tends to live in a lot of different places in a lot of different formats, making it difficult to utilize. Event-driven architectures, which serverless architecture are typically built upon, have the ability to expose all data in the form of events. These events are then directly utilizable with serverless functions. While this general architectural pattern is still young, if we could reach a state where all data exists in the form of events, and can be reacted to with functions hosted anywhere, we could solve a lot of data portability and access challenges we face today. The result: far more productive development. In sum This is an exciting time to be a developer. Were likely to see radical and unprecedented improvements in the tools we have available. Its up to each of us to continue to demand and drive that change.",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603ef0",
      "title": "Best tools for serverless observability",
      "content": "We admit it. In the serverless realm, getting the observability you need can be really frustrating. In his series on serverless observability, Yan Cui has stated the challenges, and the reasons behind them, incredibly well. But there is hope. There is a constant onslaught of new tools, new features, and loud voices demanding change. At this point, were truly at the cusp of serverless observability being not just passable, but great. In this post, we are compiling resources that you can use to have top notch insight into your functions. We will update this as new information becomes available, so it can serve as an observability tools guide for you, the intrepid serverless developer. Read on for the best tools and best practices. The tools - AWS CloudWatch - AWS X-ray - Dashbird - IOpipe - Thundra - OpenTracing - Epsagon AWS CloudWatch CloudWatch is the native AWS logging tool. Its primarily for logging, monitoring, and alerts. Benefits: - Tracing & profiling to investigate performance and cold starts - Monitoring and error logs - Customizable alerts - For Lambda users, works out of the box - A lot of people use it, which means there are a lot of plugins and other resources widely available Drawbacks: - Metrics have up to one minute delay (not real-time) - No customizable events - Will probably need to use a separate log aggregator for centralized logging Metrics: Cloudwatch comes with easy Lambda metrics; no setup. Logs: Logs from your Lambda function, plus general status logs, are sent directly to Cloudwatch Logs. Further reading: - Using CloudWatch metrics and alarms - Using CloudWatch logs AWS X-ray X-ray is a distributed tracing system you can use for debugging across various AWS systems. Its usage is not mutually exclusive with another tool, like IOpipe or CloudWatch, and most people use X-ray in conjunction with another monitoring tool. Further reading: - X-ray and Lambda: the good, the bad, the ugly Dashbird Ever used the native CloudWatch interface? Not always touted as the most user-friendly UI. Dashbird sits on top of CloudWatch and provides a more navigable user experience, plus a few additional features. Benefits: - Tracing & profiling to investigate performance and cold starts - Monitoring and error logs for debugging your serverless functions - Doesnt require additional code to implement - Customizable alerts - Lambda cost-analysis (per-function basis) Drawbacks: - Metrics have up to one minute delay (not real-time) Performance metrics: includes extras like Lambda cost analysis. Architecture metrics: track account-level stats across your entire architecture (individual microservice views also available). Further reading: - Log-based monitoring for AWS Lambda with Dashbird IOpipe IOpipe works with AWS Lambda functions written in Node.js, Python, and Java. It provides tracing, profiling, monitoring, alerts, and real-time metrics. Benefits: - Tracing & profiling to investigate performance and cold starts - Monitoring & customizable events for granular error logs and debugging your serverless functions - Real-time metrics - Customizable alerts - Really easy to install and get running Drawbacks: - You have to use a wrapper for each function, which can result in performance delays (about ms) Real-time metrics: Monitor invocations, duration, memory usage, and errors in one place. Search functionality: You can add multiple rules to find invocations that match. The example below looks for long-running invocations over ms, but you can search for errors, cold starts, or even custom metric values (e.g., userId = ). Further reading: - X-ray and IOpipe: better together - IOpipe Serverless Plugin Thundra Thundra has not yet hit general availability, but you can sign up for beta access here. Much like IOpipe, it promises to provide tracing, profiling, monitoring, alerts, and metrics. Thunda will differ from IOpipe in a couple ways. They plan to focus on Java rather than Node.js or Python. They are also attempting to avoid latency by keeping data-sending separate from the Lambda function itself. Instead, theyll first write their metrics to logs, and an out-of-band log processor will send those metrics to the Thundra backend. Further reading: - The state of serverless observabilitywhy we built Thundra OpenTracing OpenTracing, is a vendor-neutral open standard for distributed tracing that is supported by the CNCF. Libraries are available in languages: Go, JavaScript, Java, Python, Ruby, PHP, Objective-C, C++, and C. Note that this is a standard, and not a tool. Youll have to set up your own collector and interface, or you can use a paid tool such as LightStep. Benefits: - You can use it with any cloud provider, not just AWS Drawbacks: - Takes some set-up Further Reading: - Supported Tracer Implementations - Distributed Tracing in Minutes - Towards Turnkey Distributed Tracing - OpenTracing: An Open Standard for Distributed Tracing Epsagon Epsagon is a serverless monitoring and observability tool that automatically detects full transactions throughout a companys system, calculates costs and provides aggregated numbers around cost and performance across the most critical business functions. Using distributed tracing and AI technologies, Epsagon helps companies significantly reduce downtime and cost by providing end-to-end observability and application performance monitoring at the application level. Troubleshooting using distributed tracing: automatic instrumentation provides full traces. Application performance and cost monitoring: complete dashboard for the health of the serverless application. Benefits: - Automatic tracing and monitoring of the entire application, including distributed tracing. - AI-based prediction and alerting of issues before they happen. Drawbacks: - Automatic instrumentation adds a few milliseconds to the running time of the code. Further Reading: - Epsagon emerges from stealth - Epsagon Launch - Why We Started Epsagon - Ways to Gain Serverless Observability Did we miss anything? Feel free to leave comments, and/or submit a PR against this post to leave us suggestions.",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603ef2",
      "title": "Serverless Workarounds for CloudFormation's  Resource Limit",
      "content": "Developing with Serverless is microservice friendly, but sometimes you don't want microservices. Perhaps you like the comfort of keeping all your application logic in one place. That's great, until you hit the oh-so-common error:  That's rightCloudFormation has a limit of resources per stack. In this post, I'll give you some background on the CloudFormation limit and why it's so easy to hit. Then, I'll follow up with a few tips on how to avoid hitting the limit, including: - Break your web API into microservices - Handle routing in your application logic - Using plugins to split your service into multiple stacks or nested stacks - Pestering your AWS rep to get the CloudFormation limit increased Let's begin! Background on the resource limit Before we get too far, let's understand the background on this issue and why it's so easy to hit. When you run `serverless deploy` on a Serverless service that's using AWS as a provider, a few things are happening under the hood: The Serverless Framework packages your functions into zip files in the format expected by Lambda The zip files are uploaded to S A CloudFormation stack is deployed that includes your Lambda function, IAM permissions, Cloudwatch log configuration, event source mappings, and a whole bunch of other undifferentiated heavy lifting that you shouldn't care about The problem arises when you hit the aforementioned limit of resources in a single CloudFormation stack. Unlike other service limits, this is a hard limit that AWS will not raise in a support request. Now you may be saying \"But I only have functions in my servicehow does this equal resources?\" A single function requires more than one CloudFormation resource. For every function you add, there are at least three resources: An `AWS::Lambda::Function` resource, representing your actual function An `AWS::Lambda::Version` resource, representing a particular version of your function (this allows for fast & easy rollbacks) An `AWS::Logs::LogGroup` resource, allowing your function to log to CloudWatch logs If you wire up an event source such as `http` for API Gateway, you'll be adding a few more resources: `AWS::Lambda::Permission`, allowing API Gateway to invoke your function; `AWS::ApiGateway::Resource`, configuring the resource path for your endpoint; and `AWS:ApiGateway::Method`, configuring the HTTP method for your endpoint. For each `http` event you configured, you end up creating six (!) CloudFormation resources, in addition to shared resources like `AWS::ApiGateway::RestApi` and `AWS::IAM::Role`. Given this, you'll start to run into that limit around - HTTP functions. If this sounds like you, keep reading to see how you can avoid this problem. Break your Web API into microservices The most common place we see people run into the resource limit is with web APIs. This can be perfectly RESTful APIs, RPC-like endpoints, or something in between. Users often want to put a bunch of HTTP endpoints on the same domain. By default, the Serverless Framework creates a new API Gateway domain for each service. However, there are two ways you can manage to put endpoints from different services in the same domain. The first way, and my preferred way, is to map your API Gateway domains to a custom domain that you own. When you create an API Gateway in AWS, it will give you a nonsense domain such as `https://nbenfjn.execute-api.us-east-.amazonaws.com`. However, you can map over this domain using a custom domain that you own, such as `https://api.mycompany.com`. This is much cleaner, plus it won't change if you remove and redeploy your service -- much more reliable for clients that you can't change. Further, if you use a custom domain, you can also utilize base path mappings to segment your services and deploy multiple to the same domain. For example, if you have routes, of which are user-related and of which are product-related, you can split them into two different services. The first, with all of your user-related routes, will have a base path mapping of \"users\", which will prefix all routes with `/users`. The second, with your product-related routes, will prefix your routes with `/products`. Aside: Interested in using a custom domain with base path mapping? Check out our two posts on the subject: How to set up a custom domain with Serverless and How to deploy multiple micro-services under one domain. A second approach is to use the `apiGateway` property object in your `serverless.yml`. This was added in the `v.` release of the Serverless Framework. It allows you to re-use an existing API Gateway REST API resource. You'll have the nonsense domain (`https://nbenfjn.execute-api.us-east-.amazonaws.com`), but it won't require you to shell out the $ for a custom domain of your own. Check out the docs on the new `apiGateway` property here. Handle routing in your application logic Warning: The following advice is considered heresy in certain serverless circles. Use at your own risk. If you don't want to split up your logic into multiple services, you can try an alternative routestuffing all of your logic into a single function! Here's how it works. Rather than setting up specific HTTP endpoints that map to specific function handlers, you set up a single route that catches _all_ HTTP paths. In your `serverless.yml`, it will look like this:  The first event matches any method request on `/`, and the second event matches any method request on any other path. All requests will get sent to `myHandler.main`. From there, your logic should inspect the HTTP method and path to see what handler it needs to invoke, then forward the request to that handler within your function. Conceptually, this is very similar to how it works with the web frameworks of old, such as Express for Nodejs and Flask for Python. API Gateway is similar to Nginx or Apache -- a reverse proxy that forwards HTTP events to your application. Then Express or Flask would take those events from Nginx or Apache, figure out the relevant route, and send it to the proper function. It's very easy to use these existing web frameworks with Serverless. You can check our prior posts for using Express with Serverless or deploying a Flask REST API with Serverless. Even if you don't want to use existing web frameworks, you can build your own routing layer inside your Lambda. Our good friends at Trek built a `lambda-router` package that you can look at, and there are a number of other options available as well. If you're thinking of taking this route, I strongly suggest reading Yan Cui's (aka theburningmonk) post on monolithic vs multi-purpose functions. As always, Yan has great insight on some deep serverless topics. Split your stacks with plugins If you've gotten this far, you're a hold out. You don't want to split your services. You don't want a mono-function. But you still have over resources. It's time to explore using multiple CloudFormation stacks. There are a few ways we can do this. First, you can simply move certain parts of your application into a different CloudFormation stack, even if it's managed in the same service. Examples of this would be to put your slow-changing infrastructure, such as VPCs, Subnets, Security Groups, Databases, etc. in one stack, then have your more dynamic infrastructure like Lambda functions, event subscriptions, etc. in a different CloudFormation stack. For most deploys, you'll only be deploying the dynamic stack. Occasionally, you'll want to deploy the slow-changing stack. If this sounds good to you, check out the `serverless-plugin-additional-stacks` plugin by the folks at SC. The second approach is to use Nested Stacks with CloudFormation. You can use Nested Stacks to create a hierarchy of stacks. The Stacks are linked together, but each one gets to use the full resource limit. Warning: Nested Stacks are a pretty advanced area of CloudFormation, and they're not for the faint of heart. Make sure you know what you're doing. If Nested Stacks sound like the solution for you, check out these two plugins: - `serverless-nested-stack`, which splits your LogGroups and Roles into one Stack, and all other resources into another, and - `serverless-plugin-split-stacks`, by the great Doug Moscrop, creator of the `serverless-http` plugin and many others. Bug your AWS contacts You know what to do. Send out a tweet with `awswishlist` or ping your AWS support rep and let them know you'd like the resource limit raised. Conclusion The resource limit in CloudFormation can be an annoyance, but luckily there are a few workarounds. Let us know if you have other methods for getting around this limit.",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603ef4",
      "title": "Serverless by the numbers:  report",
      "content": "When it comes to how people use serverless, there are plenty of anecdotes out there. I consolidated all my APIs down into a single serverless GraphQL endpoint. I used serverless to power my machine learning instance. What is a server, again? No idea what that is. I know what Lambda is though, is it like Lambda? (Disclaimer: I made that last one up.) But the point is, those are just anecdotes. What do the cold hard numbers have to say about it? Eternal questions, my friend. Which this post will dare to answer. With charts. Event sources To address the biggest question of the day: just what are developers putting inside all these services theyre deploying? APIs dominate The answer: http. So, a bit of useful background before we dig in. Services with only one event type make up % of all services. % of all services utilize exactly two event types, and % of all services contain three or more. To make the data a bit easier to ingest, were going to break this out a few different ways. First, heres a chart showing only the single event-source services (remember, this is % of the whole): All services with a single event type, broken out by event type The majority of services with two event types (% of all services) have http as one of themhttp + cron, http + sns, you get the idea. Theres a pretty big other bucket, but thats mostly because there were a lot of permutations to represent and we kept it to the most popular: All services with exactly two event types, broken out by event type It gets a little ridiculous to break out all the permutations of services with + event sources, but those make up a little over % of all deployed services. How many functions per service? Lets take a look at how many functions developers are cramming into each service. Note that this chart only includes services that have seen development activity on distinct days, as an attempt to exclude \"Hello, World\" apps. Services, bucketed by number of functions Top languages Which runtimes are serverless developers gravitating to? Overall Node.js is the clear front-runner, followed by Python. Percentage of services deployed, by language. Fastest-growing Lets look at all language prevalence over time (excluding Node ., which otherwise completely dominates): Languages used on AWS (minus Node .), in percentage of services Python . is the most steadily-growing language overall in the past eight months, with an especially big breakout this year. However, the relatively long time scale on the chart above reduces Go to a blip in the lower right corner. This is because Go has only been supported by Lambda since January (or about months, at the time of writing). Golang adoption curve Golang support was one of the most-talked about re:Invent announcements (that, and Serverless Aurora). But it didnt become GA until January . So, how quickly has Go picked up speed? Heres a breakout of each language (versions combined), directly comparing August to February : Languages used on AWS, in percentage of services In a mere six weeks, Go is already at about half of Java usage. Also worth noting that Node.js prevalence fell three percentage points from August to February. Just for fun, lets see what the experimentation curve looks like. For that, were going to take a week over week look at the number of deployments for services written in Go since January th. (Note that this isnt number of services, but number of deployments; in other words, how often are people playing around with Go.) Golang usage on AWS, in percentage of all deployments Yeah. It has tripled already. Thats some steady growth. Itll be really interesting to see what the services chart looks like next year. When do devs do their deployments? At :am! Ok not really. Peak service deployment hours, it turns out, are between lunch time and get-off-work time: Tl;dr - Python . is growing fast, but Go isnt doing bad either - APIs are dominating serverless use cases - Developers hate mornings",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603ef6",
      "title": "How to contribute to Serverless open source projects",
      "content": "Interested in contributing to Serverless? Awesome. Check this guide for everything you need to know. Contributing to open source While not specific to Serverless, if youve never contributed to open source before, you might want to check out the GitHub Open Source Guides for a general primer. One of the most important takeaways from the guide is that you dont have to write code to contribute: (Quote taken from the GitHub Open Source Guide) This is certainly true for Serverless as well, and well be getting into all the ways you can helpwith or without code. Contributing to Serverless Our Serverless Framework GitHub repo contains a contributors guide where we lay out the basics, and we have a similar contributors guide for Event Gateway. Ways you can contribute First of all, Serverless is more than just the Framework. We also have the Event Gateway, plus plugins and examples repositories. We have user-contributed plugins that do everything from enabling canary deployments to mitigating cold starts. We have detailed examples for Go runtime usage and GraphQL implementations. Writing a cool new Serverless plugin or example is immensely beneficial to the community. In terms of non-code ways you can pitch in: Review someone elses code. Add an existing plugin or example to the list if it isnt already there. We love all of these things. Sometimes our docs have typos or need updates; if you see an opportunity for improvement, then click the edit button. We love that too. (Just as an FYI, you can even do that on our blog!) Perhaps most importantly, new people try serverless all the time, and they have questions as they figure things out. Visit the forums and answer a question today. Youll be directly giving back to the community, and perhaps even learning some new things yourself in the process. Issues and Pull Requests See something in the Framework or Event Gateway that you want to fix or change? Create an issue first. That way, we can talk to you about solutions before you spend a lot of time on a PR. Understanding labels We use labels to categorize our issues and PRs, and you can filter by these as you search: - Help wanted: issues or pull requests that need special attention - Good first issue: special areas where first-time contributors can jump in - Needs feedback: if you want to contribute by providing feedback Milestones Want to know when the newest Framework is being released? It all depends on the milestones. Here, for example, is our . milestone: v. gets released when all the issues are closed and pull requests are reviewed and merged. Get started! - Check out Framework issues - Check out Event Gateway issues - Add a plugin - Add an example - Answer a forum question Other helpful resources: - How to write your first plugin for the Serverless Framework - The most popular Framework plugins",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603ef8",
      "title": "When (and why) not to go serverless",
      "content": "There are a lot of people out there championing the serverless movement. Serverless lowers administrative overhead. It takes server maintenance off developers plates forever and cuts server costs. The benefits are real. But so are the drawbacks. If youre considering serverless, read on. Observability is more difficult Its probably the biggest critique of serverless right now: you just lose some amount of critical insight into your functions. Serverless encourages event-based architectures, which a lot of people arent familiar with. Add to that, that serverless is a new enough space that the available tooling is relatively immature. It can be hard to do things as simple as stack traces. The observability talks have not just been practically useful, but also somewhat reassuring that there are still problems to solve with microservice/serverless architectures and it&;s not just me missing something obvious!&mdash; Matthew Jones (@matt_rhys_jones) March , In the past year, logging and monitoring platforms such as Dashbird, IOpipe, and X-ray have vastly improved their options. Within the next one or two years, serverless observability should be much closer to parity. But there may always be the caveat that, by their very design, serverless functions are stateless. It makes them hard to debug in production by using anything except logs. While there is tooling that keeps developers from flying blind, there is a lot of room for improvement in the serverless observability space. Latency Serverless functions mean youll be dealing with cold starts. Small caveat to say that there is a fairly simple workaround that many serverless developers use: keeping functions warm by hitting them at regular intervals. But this is mostly effective for smaller functions. Things get a lot more complicated when you have larger functions or relatively complicated workflows. To minimize cold start times, here are some things you should keep in mind: - Application architecture: keep your serverless functions small and focused; cold start times increase linearly with memory and code size - Choice of language: Python & Go can considerably lower cold start times, whereas C & Java notoriously have the highest cold start times. - VPCs: cold start times increase due to extra overhead of provisioning networking resources Heavier reliance on vendor ecosystems With serverless, you dont manage the server. That also means you lose control over server hardware, runtimes and runtime updates (at the time of writing, Node.js is out but AWS is still on Node.js ). The provider also imposes concurrency and resource limits. The specifics of your application architecture can suddenly become determined by the provider youre using. If you go serverless with AWS Lambda, for example, the only serverless-esque databases you can use are DynamoDB or Serverless Aurora. (Though you can also, say, attach a Lambda to a VPC and access other databases like RDS, ElastiCache, and ElasticSearch instances inside it.) Were talking here about vendor lock-in. There are a lot of discussions out there about the long-term impacts of going all-in on a single provider, with a wide disparity in opinions: Instead of trying to avoid vendor lock-in, concentrate on switching cost. How easy is a solution to adopt now; and migrate away from later?&mdash; Kelsey Hightower (@kelseyhightower) April , The CNCF is also actively working to initiate standardization across platforms, in order to make it easier to migrate applications and mitigate vendor lock-in in general. Its harder to hire A lot of developers dont know what severless is. And even if they do, its a hazy enough concept that applicants can have a hard time imagining what their job would entail. Having serverless in a job title has a real chance of shrinking the size of your candidate pool, in a market where finding qualified people is already hard enough. Even if youre willing to take developers without specific serverless experience, they may be too intimidated to apply. On the flip sideto a smaller group of experimenters and fast-paced environment lovers, up-and-coming technology stacks are a huge selling point. All that saidwhy use serverless? If there are drawbacks to serverless, then why are people using it? Well, overall it can add a lot of efficiency into application development and workflow. These are the four main reasons people switch to serverless: - it scales with demand automatically - it significantly reduces server cost (-%), because you dont pay for idle - it eliminates server maintenance - it frees up developer resources to take on projects that directly drive business value (versus spending that time on maintenance) I have had every argument thrown at me. I then throw back: &quot;I hardly have to manage anything and it scales and costs a lot less&quot;. win&mdash; Paul Johnston (@PaulDJohnston) August , There are some use cases for serverless which, despite any possible downsides, are especially hard to argue against. Serverless APIs are workhorses. Along those lines, the number of digital businesses not just utilizing, but going fully serverless is increasing: As of today @bustle has fully adopted serverless. Were down to ec instances mostly comprised of self-managed HA Redis. We serve upwards of a billion requests to million people using SSR preact and react a month. We are a thriving example of modern JavaScript at scale.&mdash; Tyler Love (@tyleralove) March , Our own website is a static, serverless site built using Lambda, the Serverless Framework, and Netlify. Its never gone down and we spend zero hours a week maintaining it. TL;DR As with all things in life, there are tradeoffs. Serverless means you gain efficiency, and trade some control & visibility. Further reading on serverless architectures - Serverless architectures primer - How we migrated our startup to serverless - Why we switched from Docker to Serverless - Serverless (FaaS) vs. Containers - when to pick which? Thanks to @hotzgaspacho for adding this to the post.",
      "__v": 0
    },
    {
      "_id": "64e08920b72e199dda603efa",
      "title": "How we raised over $ for charity in an hour",
      "content": "As part of my initiative to get to know my new coworkers when I started at Serverless last December, I set up : meetings with each person. One of those people was Felix, Product Designer and PM at Serverless. It was around the holidays, and Felix mentioned hed like to find a way for Serverless to give back. I was really excited to do something community-oriented myself, and the two of us got to brainstorming. What we ended up with was a simple fundraiser that took us only an hour or two to run, and raised enough cash to provide almost meals to Bay Area soup kitchens. If youre looking for a simple, high-impact way to get your organization involved in giving, then read on! How the bar cart idea was hatched For context, Serverless works out of the HeavyBit officesa co-working space where lots of developer tool startups have desk space. We wondered if there was a way to involve not just Serverless, but the entire building. It turns out both of our significant others do what Ill call a donation cart at work: one team picks a charity and a drink (both with and without alcohol), puts the mixings on a cart with a tip jar, and goes around the office giving out drinks in exchange for donations. It sounded simple, low effort, and high impact. I loved the idea. Felix and I met the first week of December, and since we knew people would be out around the holidays, we decided to move fast. Within a week, the donation cart was ready to go! Cost basis and fundraising goals Growing up, I have a lot of fond memories of drinking hot chocolate with candy canes; it was a special treat that I only had in winter. As an adult, I still get filled with nostalgia when I have peppermint and hot chocolate. That ended up inspiring our choice of beverage: hot chocolate with candy canes and whipped cream (with some optional Baileys on the side). One trip to the store later, and Felix and I had: - enough hot chocolate, candy canes, whipped cream, and Baileys for cups. (I opted for a slightly higher-priced hot chocolate mix, since good cocoa can make or break a cup.) - two Santa hats In total, Serverless invested $ on supplies. The charity we settled on was GLIDEs meal program. GLIDE serves , meals a day, and $ sponsors an entire meal. Being someone who loves metrics, I thought having a baseline number like $ would encourage people to donate more than just $ or $. Solidifying the details The day before the event, we sent out a Slack message to the entire office building to let everyone know the details of the event, and make sure the date/time wouldnt be disrupting any important meetings: We felt confident about setting a goal of $, which is about $. per person in a - person office. Dana Oshiro, one of the founders of HeavyBit, challenged people by saying she would match the first $! This was great motivation for some people, and really helped to create some initial momentum. The day of the event, Felix and I both added our own flare to the cart (which, conveniently, we were able to borrow directly from HeavyBit)I brought winter fabric to use as a tablecloth, and Felix played holiday music from his phone. We pulled out mugs, spoons, and filled hot water containers. In no time, we were ready to raise some money for GLIDE and spread cheer around the office. How it went At :pm, we put on our santa hats and made our way around the rd floor. The music and hats were a great way to catch everyones attention! We would introduce ourselves to each person and exchange names, occasionally asking about each others company or roles. People would ask how much to donate, we would say, As much as youd like, but $ sponsors an entire meal. Most of the time, people gave $- and thanked us for being so thoughtful. By the time we wrapped up with one team, another was glancing over to see if wed come their way. Previously, I thought $ would be a stretch but when we left the rd floor we were already starting to blow past it! The second floor was the same. People were eager to donate and excited about our drink options. I was caught off guard by how into it people were. Not only were people really generous, but people who missed the cart due to meetings found us later to donatesome even sent money via Venmo later that evening just to chip in. Although we had a tip jar for cash, it was really helpful to have a Venmo account ready to go. We used my personal account and I donated the sum online the next day. End result In the end, we raised $ and had an % participation rate. All in less than an hour. This equated to full meals. I guess instead of a goal of $, we should have had a goal of meals. We were pretty close! Bonus I feel there are two types of people who work in co-working spaces: those who know everyone, and those who only know their team. Being an introvert, I am the latter. Going up to strangers and saying Hi, Im Molly takes a lot of energy for me. But pushing around the donation cart with a coworker gave me an easy way to meet almost the entire office. Because of this realization, weve decided to incorporate volunteering into Serverless new employee orientation. Each orientation group will pick a charity and event to give back to the community and start off their Serverless experience with a bang. All in all, this was a super easy way to give back, and Id highly encourage anyone whos looking for a simple afternoon community project with great purpose to consider it.",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603efc",
      "title": "AWS Lambda Node.js  support: what it changes for serverless developers",
      "content": "Node support for AWS Lambda is here! If you're a serverless developer on Lambda, read on for what you need to know about Node . Namely: speed, Async/Await, object rest and spread, and NPX. Speed Node is faster. (YES.) Specifically: about % reduction in runtime and % reduction in render time (if you're doing server-side rendering in your Lambda function). You can see the full performance benchmarks in this great post by David Gilbertson. (Though we must say, if you want maximum speed, Go is still faster.) Async/Await With Node , if you wanted to execute asynchronous code, you were probably using promises. Honestly, we kind of like promises; they're just straightforward and readable. Except when you hit a situation where you need to nest rows of them together. With Async/Await, those nested promises can be no more. Just use keyword `await` to await a function:  Simplify your build If you are using webpack to build your functions to polyfill async/await, you can simply use the native functionality now and simplify your build. Simplicity for the win! It's native in Node : ```js export const hello = async (event, context, callback) => { const response = { statusCode: , body: JSON.stringify({ message: `Go Serverless v.! ${(await message({ time: , copy: 'Your function executed successfully!'}))}`, input: event, }), }; callback(null, response); }; ``` (Code snippet from the `serverless-nodejs-starter` example.) Big shoutout to the serverless webpack plugin for supporting this long before lambda runtime. Object rest and spread You can now spread in parameters into your function, and combine objects together more easily. Spread can be used instead of `Object.assign` and/or lodash assign/extend. Both rest and spread help to create a more readable codebase. Object spread example:  Remember yolo === `true`. Live it up! For more on Object rest and spread, checkout this post NPX If you're running node locally, it comes shipped with NPM version ., which includes NPX: Npx allows you to run serverless without installing it globally: `npx serverless deploy` If this is your `package.json`:  You can simply run `npm run deploy` to deploy the function to Lambda; no need to have anything installed globally on your machine. Check this video for a full rundown. Further reading: - AWS Node . migration guide - Deploy a REST API using Serverless, Express, and Node.js",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603efe",
      "title": "Strategies for working with remote and distributed teams",
      "content": "Recruiting in San Francisco is a full contact sport, and the data seems to suggest that finding great talent is only going to get more difficult. The demand for software developers is slated to increase % over the next four years, and there's a mass exodus of San Francisco Bay Area residents (due mostly to the high cost of housing). In , Venture Beat identified Seattle, Portland, and San Francisco as the cities with the most demand for agile developers, but the markets that have the most agile developers are San Jose, Chicago, New York, Houston, Atlanta, Austin, and Dallas. Mismatch much? This desire to leave the Bay Area is a challenge we're facing at Serverless.com right now. Several of our SF-based employees plan to move out of the area in the next year in search of a higher quality of life. So, as head of ops at Serverless, I've been asking myself: What's a company to do if most of the talent it's looking for doesn't live in its city? This is exactly what PeopleTech and Github were trying to answer when they hosted their Exploring Strategies for Managing Remote and Distributed Workforces event. I attended, and learned a lot. If you're facing the same pain points as me and managing a distributed team that's only going to become moreso, then I've compiled tons of helpful info for you in this post. Some background on the panel companies and their distributed models The experts The panel was star-studded: - Merrit Anderson, VP, Employee Experience and Engagement at Github - Shelby Wolpa, VP of People Operations at InvisionAPP - Leah Sutton, VP Global HR at Elastic - Matt Mullenweg, CEO at Automattic They all sat down to discuss how their respective companies are approaching distributed workforces. The different approaches to distributed teams Each of the companies on the panel had been approaching distributed teams in a different way. Automattic started off with an office in San Franciscobut upon realizing that the most people weren't even coming into their \"very expensive\" office, founder and CEO Matt Mullenweg decided to move to a % distributed model. This would allow employees to live and work wherever they wanted. Github sees itself as \"distributed first\", but also has a beautiful flagship office in San Francisco and branch offices around the world where employees can work. A talented facilities design team makes sure that all Github offices embody the Github culture, and that those working from home offices also receive design elements that match the company offices. InvisionApp has been % distributed from day one and now has about employees in countries around the world. Elastic.co has over employees distributed across two main hubs, one in Amsterdam, The Netherlands and the other in Mountain View, California, USA and throughout regional offices in London, Berlin, Phoenix, Hong Kong, and Sydney as well as team members who are % distributed and don't work out of any office regularly. Now: what are the takeaways from the panel? Time zone management At Serverless, we're currently using a hybrid approach. Half of our team works from San Francisco, and the other half are distributed globally. To be forthright, this has caused some difficult time zone spreads. One of our project teams has an engineer in San Francisco, Germany, and Thailand. In order to meet together, the San Francisco engineer has to wake up incredibly early, and the engineer in Thailand has to work late. The few hours of overlap they have each day is usually eaten up by product meetings, which leaves little to no time for true collaboration. Knowing this, I wondered how other, even more distributed teams made things work. It turns out that having such a large time shift is an absolute deal-breaker for Leah Sutton from Elastic. She is hyper-aware of the experience that team members will have when she makes hiring decisions. When one engineering team (that had all of its team members located in a spread between the United States and Europe) wanted to hire a talented engineer in Australia, Leah said no. \"Adding one person who is significantly outside of the spread of the rest of the team will lead to that person feeling very lonely, and will create a negative work experience for everybody,\" she said. Matt Mullenweg of Automattic pointed out that while a large spread like this could work for some teamslike those that function on asyncronous communication, or need continuous coverage in different regions (like support)when building and creating something new, teams need to able to work together and communicate in real time. Automattic even created an app called \"The Lonliest Team Member\" to keep tabs on team member locations and easily group them together on projects. InvisionApp has created a universal work time for the entire company by stating work hours are from AM to PM Eastern Time. It's unclear if this affects the regions in which they are willing to hire talent. Attracting the best talent By opening up their recruitment pipeline to locations all over the world, the companies on the panel have been able to find high quality employees for all aspects of their businesses. All panel members agreed that being very intentional when writing job descriptions that highlight the company's distributed culture helps attract the types of employees who work well in that environment. Merritt Anderson also requires that Github recruiters \"meet [potential candidates] where they're at\" by attending regional meetups and conferences. When asked about how distributed companies can compete with the perks more tradional employers (like Google or Facebook) may provide, Matt Mullenweg said, \"[At Automattic] we hire adults We're not going to give you a hair cut, but we'll pay you a great salary and you can get your hair cut wherever you want.\" It's this focus on paying employees well, and giving them the freedom to manage their time as they see fit, that has given these companies a definite recruiting advantage when looking for talent. The distributed team may also prove more inclusive for a variety of employees. \"Distributed teams make it easier to pick up kids from school, or balance work life with the caretaking of a loved one,\" said Mullenweg. Integrating cultural values globally So when the world is your hiring pipeline, how is a company supposed to attract talent that fits your cultural values, and ensure that every new hire is integrated into those values? Small startups with limited budgets could follow the InvisionApp onboarding processa week of onboarding that is completed entirely on the video conferencing app, Zoom. \"Our team members almost have a more intimate understanding of each other's lives. Everything is done over video chat, so we see the band poster hanging in the background, or the tail of the cat weaving in and out of the [video] frame,\" said Wolpa. Because all of the team meetings are done over video chat, InvisionApp is able to use in-person retreats to focus on team bonding and having fun. Recently InvisionApp brought all employees to Los Angeles for three days of culture, team bonding, and celebration. Team members who had worked together for years were super excited to finally hang out in person. Some of the companies take an in-person approach to onboarding. Every two months, Elastic brings in a cohort of their most recent recruits for a week of onboarding. The new employees complete a day of service, and several days of culture and training. \"People feel really connected to their fellow cohort members even after they all return to their respective teams in different locations,\" Sutton said. Github does three days of in-person onboarding in their San Francisco office, with coordinating game nights and industry meetups, to get new team members plugged in. Every employee deploys to the Github repo on their first day. Deciding whether to go distributed or not Choosing to go distributed definitely has its benefits, but it also has some significant challenges. A company would need to incorporate distributed values into its core DNA, with clear strategies around keeping employees engaged and communicating well. The teams on the panel seemed to have created company policies that address a lot of this. InvisionApp does all meetings via Zoom, and Automattic is very clear that they are a Slack first company who eschews traditional email. \"That's why it takes me a month to respond to emails,\" said Mullenweg. \"They're all from people outside of the company.\" But if the alternative is competing for the ever dwindling pool of talent in San Francisco, and having to pay astronomical wages to combat the insane cost of living here, then choosing to go distributed may become less of a choice and more of a necessity. Photo credits to: Annie Spratt and Farzad Nazifi on Unsplash",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f00",
      "title": "A DynamoDB-backed CRUDL example using Golang",
      "content": "This post is going to revolve around my adventure of building a working CRUDL serverless example in Golang. But first, some quick background about how and why I got here. Why I decided to experiment with Golang Let's just sayfor a software engineer, I'm kinda old. Not punchcard old, but audio-cassettes-as-storage old. It's no surprise, then, that I've used a lot of different languages: BASIC, Pascal, COBOL, assembler, C, C++, LISP, Smalltalk, Java, Javascript/Node, a little bit of Python/PHP, and an alphabet soup of .NET variants, to be exact. I did my first Lambda project in Java, because I didn't want to have to get proficient in a new language while I was also learning a new platform. Even though the cold start issues didn't plague me too muchall of my processing was backend batch data collection and massagingit was pretty clear that Java had limitations for user-facing Lambda projects. So for my second Lambda project, I used Node.js. Like many others, I had a love/hate relationship with Node. And while I loved how easy it was to find and use new packages with _npm_, I really couldn't get past all the hoops I had to jump through to deal with callback hell. While Node is great for other tasks, it just doesn't make sense to deal with concurrency in some form when building stateless functions that almost never need it. And then came Golang So when () AWS announced Golang support for Lambda, () my friends at the Serverless Framework added support for it almost immediately, and () some really good performance numbers for Golang Lambda functions started to get published, I was intrigued. There's not a \"best language for serverless\" winner yet, and given the limitations I ran into my first two legitimate tries, I thought it was worth my while to give Golang a test run. Getting started I started by working through Maciej Winnicki's initial Golang example. After that, it seemed like a good next step was to build upon it, and produce a full CRUDL example that backed the functions with DynamoDB. AWS had a nice example of using Golang to interact with DynamoDB, so all I did was repurpose that code so that it was called from within Lambda functions. The application structure The overview below will be general, but feel free to check out all the code on GitHub: `go-sls-crud`. Here's the basic structure. I put each function in its own `.go` file. Then, I centralized all the DynamoDB code in its own file, to isolate it in case I wanted to swap in a different data store later. That gave me a comfortable separation of powers: the function code dealt with the interaction with API Gateway objects, and the DAO file handled data. I'm not entirely convinced that I got the file structure right, but it's functional, and this more complete example gave me a decent view into the good and bad of Golang. Golang: the good and the bad It was nice to have a compiler back after spending a few years with interpretive languages. I knew I'd make syntactical mistakes, and it was comforting to know that the compiler message gives you precision without the overhead of spinning up your whole binary first. I really like how the Golang compiler considers an import you don't need to be an error, helping reduce the size of your eventual upload to Lambda. In order for Golang to scale for me, though, I'd have to be smarter about the structure of the `makefile`. For noob level development like I was doing, having it compile every function every time was fine. But back in my C++ days, it was sure handy to have a `makefile` that was smart enough to only recompile things that changed. Productivity got a lot better when I switched from IntelliJ's Golang plugin to Atom's. I found the linter to be a bit more powerful in Atom. If I were to continue with Golang, though, I'd spend some time figuring out how to get it to compile upon change using my `makefile` instead of its default install behavior. The hardest part of this early Golang learning curve was figuring out how to segment code into different files. And frankly, I'm still not certain I'm doing it right. I wanted to put all the DAO-like code that interacted with DynamoDB into one place so that it would be easier to swap it out for a different data store in the future. The path structure was difficult to follow, and I had trouble finding good examples. But I did eventually get it functional. In sum? Overall, I really liked how Golang minimized the amount of code I had to write. Once I overcame the path structure issue and got used to the syntax, progress came quickly. Stuff I still need to learn, & what's next I stopped short of working in unit or system tests for this little CRUDL example, but those are the obvious next steps in the march towards a full-blown CI/CD toolchain example. As someone whose career started before test-driven development was a thing, I tend to favor system testing over unit testing; it tells you more about the production readiness of your code given the full interaction you get from all your components. If I were to continue with this project, I'd build some sort of endpoint testing suite (like the one I started to build for Node over a year ago). Alternatively, I was really impressed with Siddharth Gupta's GraphQL example, and think it would be fun to try to build a Golang, GraphQL, serverless CRUDL example (and win Buzzword Bingo in the process ). That would provide a nice foundation for the larger killer app example I think the serverless community is missing. Something like a serverless, GraphQL version of WordPress or Discourse. This would provide a bridge between an application most people understand, and a new way of architecting it with serverlessto both lower costs and make easier to iterate over. I'd love to hear some thoughts or suggestions on what might make sense, as the serverless revolution continues to gain ground.",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f02",
      "title": "Introducing our first cohort of  Serverless Champions",
      "content": "Perhaps you remember, late last year, when we announced the beginnings of the Serverless Champions program. Wellwe are back with the first round of winners for ! What is a Serverless Champion? A Serverless Champion is a hero in the serverless community. They contribute to open-source serverless projects. They speak at serverless conferences and meetups. They help newcomers get excited. They crawl the forums just looking for questions to answer. Aside: If you, too, want to contribute to open-source, check out our handy guide. Announcing: Our first round of Serverless Champions for We conducted interviews with all three winners, and were including some of our favorite excerpts here. To see the interviews in their entirety (which you should!), head on over to our Champions page. Alex Casalboni Alex is the author of the aws-lambda-power-tuning plugin. He co-organizes ServerlessDays (formerly JeffConf), frequently speaks and conferences, gives webinars, and has created many examples for the serverless community, including this multi-region application with DynamoDB global tables. Q: Alex, you have been a contributor and an evangelist for the Serverless Framework for a long time. When did you start using the Serverless Framework? AC: I started using the Serverless Framework as soon as we realized that a system with more than a handful of Functions would never scale without proper tooling. It was May and I wrote my first review of the framework v.. The project was still early stage (only contributors, now there are almost !) and the serverless ecosystem was so immature that nobody could clearly define \"serverless\", at least not without making a few enemies (well, that still happens, especially on Twitter!). Then I met Austen Collins at the first ServerlessConf in New York a few days later and I finally realized the scope of what the team was going to build in the upcoming months. Read the full interview, and find Alex on: Twitter | GitHub | site Frank Schmid Frank is a core maintainer of the Serverless Framework, is incredibly proactive about stepping in on GitHub issues, and joining discussions on the forums and in Serverless Slack channels. In addition to using the Serverless Framework daily for his job at Stashimi, he is the author & maintainer of the serverless-webpack plugin. Q: What are your thoughts on what is keeping serverless architectures to be adopted widely by organizations? FS: There are some major key points that make a long-term transition inevitable. There is the delegation of fixed administration and running costs to the provideryou pay as you go. The costs scale with the actual load of the system. Additionally the provider costs tend to decrease over time (at least from what I see). Then, serverless based systems scale better and can cover load spikes more easily. Architecture-wise, the microservices nature of serverless architectures are modeled more cleanly. They stay extensible, separated and are maintainable in a much better way. Also, new services are coming out all the time, things like AWS Lex and AWS Comprehend. Services like this take something that would be very hard for developers to maintain themselves, and offer a much better experience. Of course, adopting serverless architectures is not an ad-hoc thing. It requires a change in people's minds, which can be a very slow process; you might even have to wait for a new generation of software engineers to occupy senior-level positions. It is also most likely no binary decision where you just state \"now we will be serverless\"same as with someone who tries to introduce agile methodologies ad-hoc and for everything, even making coffee. ;-) That would obviously not work, and only be food for objectors who are against changes at any cost. I'm sure that we'll continue to see a large shift towards serverless systems over time. Even present-day objectors will most likely follow as the serverless architecture in the end (IMO) leads to a market advantage for the implementers. Read the full interview, and find Frank on GitHub. Rowan Udell Rowan has been involved with the Serverless Framework since it was called JAWS. In addition to speaking on AWS and Serverless all across Sydney, he maintains a blog where he teaches others worldwide about serverless. Hes even created his own serverless chatbot course. Q: Talk about a service or an application that satisfactorily made you believe that serverless architectures is the way to go. What did you learn from that experience, and what do you think is the biggest challenge in developing serverless applications today? RU: I never really needed convincing. For me [building a] chatbot application was one of the best examples of a good fit for a serverless and event-driven application, which is why I chose it as the subject of the video course I made. Other than that, I've found serverless really suited to automating operational tasks, since it reduces the overhead for intermittent, but important, maintenance jobs. I think the biggest challenges for developers new to serverless are in the management of state: You can no longer assume that the machine that executed the code last time will do it again this time. What makes it more confusing is that it might be the same machine! Understanding the value and implementation nuances of idempotent activities (which are key to a robust, distributed system) is another thing which has a steep learning curve for developers new to serverless. Read the full interview, and find Rowan on: Twitter | GitHub | site",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f04",
      "title": "How to create a REST API in Java using DynamoDB and Serverless",
      "content": "In this walkthough, we will build a `products-api` serverless service that will implement a REST API for products. We will be using Java as our language of choice. The data will be stored in a DynamoDB table, and the service will be deployed to AWS. !image What we will cover: Pre-requisites Creating the REST API service Deep dive into the Java code Deploying the service Calling the API Install Pre-requisites Before we begin, you'll need the following: Install `node` and `npm` Install the Serverless Framework installed with an AWS account set up. Install Oracle JDK and NOT Java JRE. Set the following: `export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-.jdk/Contents/Home` Install Apache Maven. After downloading and installing Apache Maven, add the `apache-maven-x.x.x` folder to the `PATH` environment variable. Testing Pre-requisites Test Java installation:  Test Maven installation:  Create the Serverless project Let's create a project named `products-api`, using the `aws-java-maven` boilerplate template provided by the Serverless Framework, as shown below:  After a successful build, we should have an artifact at `aws-java-products-api/target/products-api-dev.jar` that we will use in our deployment step. Let's deploy the service to the cloud:  On a successful deployment, we will have our four API endpoints listed as shown above. Calling the API Now that we have a fully functional REST API deployed to the cloud, let's call the API endpoints. Create Product  Now, we'll make a few calls to add some products. List Products  Here's the `java-products-dev` DynamoDB table listing our products: !image No Product(s) Found:  Get Product  Product Not Found:  Delete Product  Product Not Found:  View the CloudWatch Logs We have used the `logj.Logger` in our Java code to log relevant info and errors to the logs. In case of AWS, the logs can be retrieved from CloudWatch. Let's do a GET call and then take a look at the logs from our terminal:   Notice the lines about the database connection being open/closed, the request data structure going to DynamoDB and then the response coming back, and finally the response data structure that is being returned by our API code. Removing the service At any point in time, if you want to remove the service from the cloud you can do the following:  It will cleanup all the resources including IAM roles, the deployment bucket, the Lambda functions and will also delete the DynamoDB table. Summary To recap, we used Java to create a serverless REST API service, built it and then deployed it to AWS. We took a deep dive into the DAL code that handles the backend data mapping and access to the DynamoDB table. We also looked at the mapping between events, the API endpoints and the lambda function handlers in the service, all described intuitively in the `serverless.yml` file. By now, you should have an end-to-end implementation of a serverless REST API service written in Java and deployed to AWS. Hope you liked the post, and feel free to give me your feedback or ask any questions, in the comments below.",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f06",
      "title": "How to manage canary deployments on Lambda via the Serverless Framework",
      "content": "When we update Lambda functions in serverless applications, we take a lot of precautions. We build tests to be confident that we are not introducing bugs or breaking anything. We publish the update in different stages to check how it behaves in the cloud. And still, a tingling runs down our spines every time we release to production. We are never % sure that we won't bump into an integration error, or that we didn't overlook any edge cases. If that sounds like you, then fear no more! The Canary Deployments Plugin is your safety net. I built this plugin for the Serverless Framework to let you manage canary deployments with ease. Below, I'll dive into how the plugin works and show you an example of the Canary Deployments Plugin in action. Ready? Awesome. Background: Lambda Weighted Aliases + CodeDeploy = Peace of mind The AWS team recently introduced traffic shifting for Lambda function aliases, which basically means that we can now split the traffic of our functions between two different versions. We just have to specify the percentage of incoming traffic we want to direct to the new release, and Lambda will automatically load balance requests between versions when we invoke the alias. So, instead of completely replacing our function for another version, we could take a more conservative approach making the new code coexist with the old stable one and checking how it performs. All this sounds great, but let's be honest, changing aliases weights and checking how new functions behave by ourselves is not going to make our lives easier. Fortunately, AWS has the service we need: CodeDeploy. CodeDeploy is capable of automatically updating our functions' aliases weights according to our preferences. Even better, it will also roll back automatically if it notices that something goes wrong. Basically, our Lambda function deployments will be on autopilot. Canary deployments: before and after Now let's say we wanted to implement all this Weighted Alias + CodeDeploy stuff into our serverless application. To do this, we'd need to create some AWS resources. We'd need to first create a CodeDeploy application, a DeploymentGroup and an Alias for each function, plus some new permissions here and there, then replace all the event sources to trigger the aliases instead of `$Latest`...yeah. Not actually that straightforward. But that was then, and this is now. Now, the Canary Deployments Plugin takes care of all those things for you! It makes gradual deployments simply a matter of configuration. Integrating canary deployments into your serverless app In the next sections, we'll walk through the different options we can configure with the Canary Deployments Plugin. Setting up the environment We'll create a simple Serverless service with one function exposed through API Gateway, making sure that we install the plugin. Our `serverless.yml` should look like this:  Our function will simply return a message:  Which we should get by calling the endpoint after deploying our service:  Deploying the function gradually This is where the fun starts. We'll tell the plugin to split the traffic between the last two versions of our function during the next deploy, and gradually shift more traffic to the new one until it receives all the load. There are three different fashions of gradual deployments: Canary: a specific amount of traffic is shifted to the new version during a certain period of time. When that time elapses, all the traffic goes to the new version. Linear: traffic is shifted to the new version incrementally in intervals until it gets all of it. All-at-once: the least gradual of all of them, all the traffic is shifted to the new version straight away. All we need now is to specify the parameters and type of deployment, by choosing any of the CodeDeploy's deployment preference presets: `CanaryPercentMinutes`, `CanaryPercentMinutes`, `CanaryPercentMinutes`, `CanaryPercentMinutes`, `LinearPercentEveryMinutes`, `LinearPercentEveryMinute`, `LinearPercentEveryMinutes`, `LinearPercentEveryMinutes` or `AllAtOnce`. We'll pick `LinearPercentEveryMinute`, so that the new version of our function gets is incremented by % every minute, until it reaches %. For that, we only have to set `type` and `alias` (the name of the alias we want to create) under `deploymentSettings` in our function:  If we now update and deploy the function:  We'll notice that the requests are being load balanced between the two latest versions:  Making sure you don't break anything Now our function is not deployed in a single flip, woohoo! But, if we have to ensure that the whole system is behaving correctly by ourselves, we didn't really achieve anything impressive, did we? We can add another AWS service to the mix to avoid this: CloudWatch Alarms Note: read more on this in our CloudWatch alarms post. We can provide CodeDeploy with a list of alarms to track during the deployment process, then cancel it and shift all the traffic to the old version if any of them turns into the `ALARM` state. In this example, we'll monitor that our function doesn't have any invocation error. We'll be using A Cloud Guru's Alerts Plugin for this:  The Canary Deployments plugin expects the logical ID of the CloudWatch alarms, which the Alerts plugin builds by concatenating the function name, the alarm name and the string \"Alarm\" in Pascal case. End-to-end testing By gradually deploying our function and being able to track CloudWatch metrics, we have all the tools we need to minimize the impact of a potential bug in our code. However, we could even avoid invoking a function version with errors by running CodeDeploy Hooks first. Hooks are simply Lambda functions triggered by CodeDeploy before and after traffic shifting takes place. It expects to get notified about the success or failure of the hook, only continuing to the next step if it succeeded. They are perfect for running end-to-end or integration tests and checking that all the pieces fit together in the cloud, since it'll automatically roll back upon failure. This is how we can configure hooks:  Notice that we need to grant our functions access to CodeDeploy, so that we can use its SDK in the hooks. It should look something like this:  Hooks are well-suited for running tests, but we could actually execute whatever else we need to happen before or after our function deployment too. We only have to keep in mind that we must notify CodeDeploy about the result of the hook; otherwise, it'll assume that it failed if it doesn't get any response within one hour. Protip: we don't really need to notify CodeDeploy in the Lambda function hook. We could trigger a background job that runs anywhere else and report the result from there. Conclusion CodeDeploy and Lambda Weighted Aliases take the deployment process of our Serverless functions to the next level. They significantly reduce the chances of releasing buggy code, and react automatically if anything goes wrong. Instead of publishing a new function version that gets all the invokations straight away, the deployment goes through different stages: The before traffic hook is executed. Traffic is shifted gradually to the new version and the provided CloudWatch alarms are monitored, canceling and rolling back if any of them is triggered. The after hook is executed. All those stages are executed in order. If any of them fails, the deployment will be considered as failed and the whole system will roll back to the previous state. You can get the code in the above example here, and if you're interested in the details of the underliying technology, check out this post. Happy safe deployments!",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f08",
      "title": "What are Serverless Components, and how do I use them?",
      "content": " So, whats the goal with Serverless Components? We want to make it easier for you, our developer community, to assemble cloud applications. Plain and simple. Composing a modern application means youre plucking SaaS and managed (i.e. serverless) services from all over and combining them to create solutions. This is great, because it's faster and has lower overhead. But it's also highly complex. Theres a lot of manual work you have to do to assemble these pieces into an outcome, and not a lot of tooling to help you build and manage that outcome. Enter: Serverless Components Serverless Components aims to change all of that. Components presents a single experience for you to provision infrastructure and code across all cloud and SaaS vendors. Think of them like building blocks which you can use to build applications more easily. Serverless Components will also form an ecosystem driven by community contributions, which you can browse through and utilize. The net result is that you save development time. Dont build your own image processing API from scratchuse the existing component and tweak it. How Serverless Components work Lets take a look at how you can use Serverless Components to create an entire serverless application. A uniform experience Any cloud service can be packaged as a Serverless Component. Within each Component is the provisioning, rollback, and removal functionality for that service, which you can run via the Serverless Components CLI. Components expose minimal configuration with sane defaults so that you can configure the resource it contains more easily. To do this, add the Component you wish to provision in a components property within a serverless.yml file. Run `components deploy` to provision the resource. Composing components into higher-order components Say you want to write a serverless image processing API that pulls images from S and modifies them. To do so, you might create an AWS API Gateway endpoint to call an AWS Lambda function, which then pulls an image from the AWS S bucket and modifies it. We currently offer Components for all of these services. Each with simple configuration inputs so that you can configure their underlying resources easily and deploy them quickly. All Serverless Components can be composed together and nested in a larger Component. Well combine these three infrastructure-level Components to create our serverless image processing API, which will become its own, higher-order Component. Again, all of this is declared in your `serverless.yml` file. Heres why this is important. When you create this image processing API, you will do a lot of initial work to configure everything. Youll configure the Lambda function, the REST API endpoint, the S bucket, etc. But you can abstract a lot of that away by nesting those infrastructure-level Components in a higher-order Component. That higher-order Component can use sane defaults and expose simpler configuration. Now, you can reuse this higher-order Serverless Component somewhere else. Or, another developer can use it in their own application by simply tweaking some aspects of the configurationfor instance, maybe they just want to specify their own S bucket which contains images. These higher-order Components essentially become use-cases, which can be easily used and reused by anyone. Composing components to form an entire application Lets zoom out from our image processing API. When you think about building an entire application, it has needs across the board. You need to have user management, plus lots of other data models and API endpoints to work with them. Fortunately, you can continue to nest Serverless Components. Simply take the higher-order Component in the previous example and compose them together into even higher Components. Like so, entire applications can be built by continuing to nest Serverless Components. Like all Components, people can easily reuse your application Component if it exposes simple configuration. Most importantly: Serverless is about outcomes, not infrastructure. We believe weve made a solution that makes infrastructure more invisible, enhances developers ability to focus on outcomes, and fosters a community to share and reuse outcomes. Ready to try Serverless Components? Great! We have a full working example ready to go. Head on over to the Serverless Components repo on GitHub and check out our templates. Weve authored several infrastructure-level Components you can use to create higher-order outcomes in our temporary registry. If you'd like a really comprehensive walkthrough, here's how to set up a landing page using the Serverless Netlify and Lambda Components. Wed love to hear any and all feedback from you, our developer community. So try it out and tell us what you think! Whats next for Serverless Components? We believe that Components represents the ideal developer experience for the future of serverless development, and we plan to integrate it into the Serverless Framework. However, the implementation is not yet mature so we have decided to incubate it as a standalone project for now. We have plans for a public registry, but for the time being were keeping all of the Components in the Github repo.",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f0a",
      "title": "Automating a CI workflow for a Python serverless app with CircleCI",
      "content": "I had previously written a post that defined the CI/CD process, discussed various deployment patterns, created a NodeJS app, and automated the end-to-end CI flow. In this post, I will be doing that same thing for Python. We'll build a Python app and go over the end-to-end process for automating the CI flow. We will cover: Creating a Python app Writing testable code Preparing for CI automation Implementing the CI workflow But first, I'll do a quick overview of the CI/CD process. If you already know some CI/CD basics, then you'll probably want to skip straight to creating the app. CI/CD Overview In an agile development environment, small teams work autonomously and add a lot of churn to the code base. Each developer works on different aspects of the project and frequently commits code. This is a healthy practice, but it comes with some challenges. Without close watch and proper communication about changes, the updates can cause existing code to break. To minimize manual scrutiny and redundant communication across teams, we need to invest in automating CI/CD processes. !The CI/CD Process Flow The CI/CD Process Flow Continuous Integration The CI process starts with the developer checking code into a code repository. The developer makes their code changes in a local branch, then adds units tests and integration tests. They ensure that the tests don't lower the overall code coverage. It's possible to automate this process by having a common script that can run the unit tests, integration tests, and code coverage. Once the code is tested in the context of the local branch, the developer needs to merge the master branch into their local branch, and then run the tests/code coverage again. The above process happens repeatedly for every code commit, and thereby continuously integrates the new code changes into the existing codebase. Creating the app Now that we've gone over some basics, let's get started! Note: We won't be covering the basics of creating a serverless Python app, but you can get an idea by reading this post about ETL job processing. Let's cut to the chase and install the sample app from the source repo using the Serverless Framework like so:  Having proper tests in place safeguards against subsequent code updates. We'd like to run tests and code coverage against our code. If the tests pass, we'll deploy our app. It's thisrunning tests against our code whenever new code is committedthat allows for continuous integration. Testable Code We have some tests that we'll run as part of the testing phase. Notice that we have a test that tests if our function is being called. We are also separating out the actual testable logic of our function into a class:  The `handler.py` code is refactored to use the above `say_hello` method from the `HelloWorld` class:  This makes testing the core logic of the app easy, and also decouples it from the provider-specific function signature. Running Tests Now that our tests are written up, let's run them locally before we include them as part of our CI/CD process. For running tests we will use nose. We add the `--with-coverage` flag to run the test with code coverage. (Remember, you will need coverage.js installed to use coverage.)  The tests results look like this in the terminal:  Alternatively, you could also run code coverage with the `--cover-html` flag to get a nice visual chart of the code coverage as shown below:  The above command creates an html representation of the coverage metrics in the default folder `cover`, and looks like so: !Coverage Excluding Testing Artifacts After running the tests, you should see that a `.coverage` folder has been created. If you run the visual code coverage command, the `cover` folder will be created as well. You'll also have a `.circleci` folderthat one is required to enable build automation with CircleCI. When we deploy our serverless app via the Serverless Framework, all the files in your current folder will be zipped up and be part of the deployment to AWS. Since the `coverage`, `cover`, and `.circleci` files are not necessary for running our app, let's exclude them from our final deployment by excluding them in our `serverless.yml` file:  Note: See more details on packaging options with the Serverless Framework. Preparing for CI Automation We'll be using CircleCI for automating the CI/CD pipeline for our `python-ci` app. Let's get everything ready to go. Setting up a CircleCI Account Sign up for a CircleCI account if you don't already have one. As part of the sign-up process, we'll authorize CircleCI to access our public Github repo so that it can run builds. Creating an AWS IAM User It is a good practice to have a separate IAM user just for the CI build process. We'll create a new IAM user called `circleci` in the AWS console. Give the user programmatic access and save the AWS credentials, which we'll use later to configure our project in CircleCI. Note: More on setting up IAM users here. Configuring CircleCI with AWS Credentials We have to configure AWS credentials with CircleCI in order to deploy our app to AWS. Go to your project `python-ci` -> Project Settings -> AWS Permissions, and add your AWS credentials for the `circleci` IAM user we created earlier. !Adding AWS credentials End-to-End Automation Now that we've completed our CircleCI setup, let's work on implementing the CI/CD workflow for our project. Configuration We'll configure CircleCI via a config file named `config.yml` and keep it in the `.circleci` directory. Explanation of how CircleCI works is out of scope for this article, but we'll look at the steps needed to automate our deployments. Note: If you want some further reading, CircleCI introduces concepts of Jobs, Steps and Workflows. CircleCI allows for multiple jobs with multiple steps, all orchestrated via a workflow. But to keep things simple, we're going to keep everything within one job and one step. Here is a snippet of the config file that we'll use:  We have a `job` named `build`, and we have a few `steps`. The `checkout` step will check out the files from the attached source repo. We also have a few `run` steps that just execute bash commands. We'll install the serverless CLI and the project dependencies, run our tests with code coverage enabled, and finally deploy the application. Note : The `save_cache` and `restore_cache` sections in the above config file allow for caching the `node_modules` between builds, as long as the `package.json` file has not changed. It significantly reduces build times. Note : You can review the full config file for our app. And you can review a full CircleCI sample configuration file with more options as well. Implementing the Workflow To add our app project to CircleCI, do the following: Push the local app from your machine to your Github account, or fork the sample project on your Github account. Go to Projects -> Add Projects, and click the 'Setup project' button next to your project. Make sure the 'Show forks' checkbox is checked. Since we have our CircleCI config file already placed at the root of our project, some of the configuration is picked up automatically: Pick 'Linux' as the Operating System. Pick '.' as the Platform. Pick 'Node' as the Language. Skip steps -. Click on 'Start building'. You'll see the system running the build for your project: !Build running on CircleCI You can drill down to see the steps on the UI that matches our steps in the config file. While it is executing each step, you can see the activity: !Build steps for the project You can see the tests running as part of the 'Run tests with code coverage' step: !Running tests for the project And finally, you see that our app has been deployed under the 'Deploy application' step: !Deploying the project !Deploying the project Last but not least, we can copy the endpoint shown in the output onto a browser and see the app run! !Running the app Hopefully, the full rundown of the process and its implementation on a CI/CD platform such as CircleCI gives you a better understanding of automating your own applications. Summary In this post, we looked at the overall CI/CD process flow, and created a serverless application in Python. We refactored the code to be testable, then ran the tests and code coverage locally to make sure our code was working. Once we had our app running locally, we set up an automated CI workflow for our app on CircleCI. Any comments or questions? Drop them below!",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f0c",
      "title": "Fn Project brings containerized, cloud-agnostic functions to a cloud near you",
      "content": "Back in October, Oracle first announced the open source Fn Project at the JavaOne conference. Since then, the team here at Serverless has kept a keen eye on it. More function providers means more freedom for FaaS users & less vendor lock-in. This is one of our driving principles behind the Serverless Framework. So, it was only natural to bring the Fn Project into the growing list of Serverless function providers About Fn The Fn Project uses a vendor agnostic approach, leveraging containers to allow organizations run Fn either on premise or in their cloud of choice. Another driving force behind the project is: a clear separation of serverless & container orchestration is important. So, whether you're on Kubernetes or any other provider, Fn is agnostic when it comes to your container orchestration of choice. Use any containerized code as a function Watch this minute video for a quick overview on the project, and see a sweet demo of their Fn Flow component: The platform is container-native, enabling users to write functions in any programming language, with excellent support for local development and testing. Fn even allows devs to bring their own Dockerfile such that ANY containerized code can be used as a function. The Fn Project consists of major components: Fn Server is the Functions-as-a-Service system that allows developers to easily build, deploy, and scale their functions into a multi-cloud environment The Fn Load Balancer (Fn LB) allows operators to deploy clusters of Fn servers and route traffic to them intelligently. Most importantly, it will route traffic to nodes where hot functions are running (to ensure optimal performance), as well as distribute load if traffic to a specific function increases. Fn FDKs (Function Development Kits) are aimed at quickly bootstrapping functions in all languages, providing a data binding model for function inputs, making it easier to test your functions, and laying the foundation for building more complex serverless applications. Fn Flow allows developers to build and orchestrate higher-level workflows of functions, all inside their programming language of choice. It makes it easy to use parallelism, sequencing/chaining, error handling, fan in/out, etc., without learning complicated external models built with long JSON or YAML templates. For more information on the Fn Project and why they built it, I highly recommend checking out this post. Fn + Serverless Framework Starting today, you can now deploy your Fn functions using the Serverless Framework and the familiar `serverless.yml` config we have all grown to know and love. Getting Started with Serverless & Fn Make sure you have the Serverless Framework installed on your machine:  Then, create a new service with the `sls create` command and supply the newly added `fn-nodejs` or `fn-go` templates:  Fn functions run in Docker containers, so you need a running fn service in order to run it. See the guide on installing Fn to finish setup. Project Structure The Fn Project structure is similar to all other Serverless Framework providers, with one tiny difference. Instead of a `handler` property pointing to where the code lives, the function code location is driven by convention. !fn-sls-code-ref-structure The function key will reference the folder path, and inside that folder path it will look for a `func.[Your Runtime]` (a.k.a `func.js` or `func.go`). So, the below serverless.yml is looking for a `./hello/func.js` file when `sls deploy` is run: ```yml The `service` block is the name of the service service: name: hello-world The `provider` block defines where your service will be deployed provider: name: fn plugins: - serverless-fn The `functions` block defines what code to deploy functions: hello: ",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f0e",
      "title": "React to any cloud event with hosted Event Gateway",
      "content": "Last year, we released the Event Gateway project: an open source serverless communication fabric that allowed developers to react to any event, with any function, on any provider. When we did that, we were looking to solve a lot of common problems serverless developers experience. We wanted to make it easier to build decoupled APIs backed by FaaS. We wanted to enable an event-driven future, make it easier to share events across teams and services, and give teams the ability to build a reactive, truly pay-per-use infrastructure. The Serverless Event Gateway is pretty dope. React to any event, with any function, on any provider. https://t.co/TMtPoXWUja&mdash; Kelsey Hightower (@kelseyhightower) October , But that was only step . This post talks about some exciting updates for Event Gateway, and also where were going next. Note: If you just want to get straight to seeing Event Gateway in action, you can watch Austen Collins demo it at CloudNativeCon below: Now, on to the news! Event Gateway as a hosted service In its initial iteration at launch, Event Gateway was a piece of software that you had to start up and run. You needed servers to interact with it. And while a lot of people loved Event Gateway and all the problems it solved, they (shocker!) didnt love that it forced them to deal with infrastructure. So today, were releasing a public beta of Event Gateway as a hosted service! In other words, Event Gateway goes serverless! You can get up and running with it right now, no infrastructure required. Sign up here, and then check out our getting started guide to hit the ground running. The getting started example takes only a few minutes. Configurable connectors Lots of FaaS usage is essentially glue code between two systems. A frontend client sends a data payload to an endpoint thats sent to another system for analytics processing. Or, you catch an emitted event and put it in SQS for integrating with legacy infrastructure. But part of the serverless mantra is: Write Less Code. Dont write boilerplate code where you dont need it, and configuration is better than code wherever possible. Focus on your business logic, and outsource code to others. Thats why weve added the notion of connectors into Event Gateway. These are bits of logic which take your event and send it to another systemFirehose, Kinesis, or SQS for instancewithout having to write the boilerplate integration code into a Lambda function. These connectors are configurable. You specify the end system you want to send your event to (such as a Kinesis stream or SQS queue), and the Event Gateway will handle forwarding the event to that system. No more glue code in Lambda just to pipe data from one system into another. Write configuration, not code. Plugin for the Serverless Framework We made a plugin for the Serverless Framework that makes it really easy to deploy your Lambda functions with Event Gateway. The plugin enables you to: - register API endpoints - put endpoints from different services on the same domain very easily, even if those services are in different AWS accounts - hook up custom events, so that if one of your services emits an event, another function in a different service can subscribe to it - set up connectors in the Event Gateway to route events to external systems. We have a getting started example for using the Event Gateway plugin with our hosted Event Gateway. Check back next week for an in-depth walkthrough tutorial on using the different features of Event Gateway with the plugin. CloudEvents integration The Cloud Native Computing Foundation (CNCF) has been working on CloudEvents, a specification for describing event data in a common way across providers. We at Serverless have taken a leading role in this effort, as we believe strongly in the importance of standards and interoperability in this new event-driven world. That's why we have made the Event Gateway CloudEvents-compatible. All functions receive a CloudEvents payload describing the event received. The CloudEvents spec recently hit an . release, and the CNCF Serverless Working Group is pushing hard toward the . milestone. The future of Event Gateway: where were going Our path here is threefold: we want to continue adding more functionality into the user experience, integrate a wider range of available function types, and make it even easier to pull in different events. User experience and diagnostics An event-driven system is inherently harder to debug than a more synchronous, request-response setup. You have to be careful to understand that your function didnt trigger on a given event and then debug the reason. Was the event emitted? Was the subscription configured incorrectly? Did the function fail? Were working to add better diagnosability so that you know that your system is working as it should and how to debug it when its not. Function Types We want to increase the range of function types you can use to include additional FaaS providers, such as Azure Functions, Kubeless, OpenFaaS, and others. We also want to include more built-in, configurable functions to handle boilerplate logic: authorization, validation, transformation, enrichment, and connecting other systems. Feeding events into the Event Gateway We want to make it easier for serverless application developers to integrate diverse event types, from a variety of cloud sources and SaaS applications, into Event Gateway. For instance, pulling data from event stores like Kafka and Kinesis, so that developers can seamlessly react to those events with functions in a unified way. This, we believe, is how developers will finally realize the true power of events combined with FaaS. Get started! For a really nice example that takes you through API Gateway usage and custom event usage in only a few minutes, go straight to the getting started guide. If you want to sign up for the hosted version, heres where you can make an account. You can also check out the open source Event Gateway project on GitHub. More examples and resources - Kelsey Hightower used Event Gateway to power his demo at KubeCon. See his Github repo here, or watch the live demo. - Austen Collins demos Event Gateway triggering different cloud providers in his CloudNativeCon talk. - You can find a couple more in-depth examples in the Event Gateway examples repo.",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f10",
      "title": "How to create a REST API with pre-written Serverless Components",
      "content": " Introduction You might have already heard about our new project, Serverless Components. Our goal was to encapsulate common functionality into so-called \"components\", which could then be easily re-used, extended and shared with other developers and other serverless applications. In this post, I'm going to show you how to compose a fully-fledged, REST API-powered application, all by using several pre-built components from the component registry. Excited? Let's go! Wait, what are components again? I'm going to start with a quick refresher on Serverless Components. If you already know all about them, then jump straight to install or building the app. In essence, components are logical units which encapsulate code to perform certain actions. Components expose an interface so that they can be re-used easily by others without them having to know the inner workings. They can be orchestrated and nested to build higher-order functionalities, like so: To take a look at a simple example, let's imagine that we want to deploy multiple AWS Lambda functions to AWS. Doing this the old way Taking a deeper look into AWS Lambdas internals, we see that every Lambda function needs an IAM role. Further, we need to configure parameters such as \"Memory size\" or \"Timeout\". We need to provide the zipped source code and ship it alongside our AWS Lambda function. We could accomplish our task by manually creating an IAM role, configuring the (\"Memory size\" and \"Timeout\"), and zipping and uploading the code. But doing this manually is cumbersome and error-prone. And, there's no way to re-use common logic to create other AWS Lambda functions in the future. Doing this with components Enter: Serverless Components! The components concept provides an easy way to abstract away common functionality, making it easier to re-use that functionality in multiple places. In our case, we would componentize the AWS IAM role into one component, which takes a `name` and the `service` as inputs, creates an IAM Role, and returns the `arn` as an output: We could similarly componetize the AWS Lambda function into another component, which takes the `name`, `memorySize`, `timeout`, `code` and `iamRole` as inputs, creates the AWS Lambda function, and returns the `arn` as an output: Then, we'd be able re-use our two components to create dozens of AWS Lambda functions and their corresponding roles, without the need to manually create Lambda functions or IAM roles ever again. via GIPHY Write once, use everywhere. Installing Serverless Components First up, you'll need to get Serverless Components installed on your machine. Serverless Components is a CLI tool written in JavaScript that helps in deploying, testing, and removing our component-based applications. To install, simply run: `npm install --global serverless-components`. Note: Right now Serverless Components needs Node.js or greater. Compatibility with older Node.js versions is already in the making. Components we're using to build this API Let's take a quick look at the different components we'll use throughout this tutorial to build our REST API-powered application. `aws-lambda` The `aws-lambda` component gives us a convenient way to deploy Lambda functions to AWS. When using it, we'll need to supply `memory`, `timeout`, and `handler` properties. All other configurations (such as the function `name`) are optional. The Lambda component will even auto-generate and automatically manage an IAM role for us, if we don't specify one. You can find the documentation and some examples in the AWS Lambda component registry entry. `aws-dynamodb` The `aws-dynamodb` component makes it possible to create and manage DynamoDB tables. The only configurations necessary for this component are () the `region`, in which the table should be created; () an array called `tables`, which includes the different DynamoDB-specific table definitions. The components documentation shows some example uses. `rest-api` The `rest-api` component creates a REST API according to a config consisting of a `gateway` and `routes` property. `gateway` config The `gateway` config property determines where the REST API should be created. The component currently supports `aws-apigateway` to setup a REST API on AWS using the API Gateway, and `eventgateway` to setup a REST API using the hosted version of Event Gateway. In this post, we'll be using the `aws-apigateway` configuration to tell the Framework that we want to set up our REST API on AWS using the API Gateway service. `routes` config The `routes` config property is used to specify the routes with its paths and methods and maps them to the Lambda functions which should be invoked when accessing those routes. If for example, you want to implement a \"Products\" API you'd create routes like this: That's it. When deploying the component to AWS, the Framework will automatically create a REST API using the API Gateway service and return the URLs we can use to perform the above operations. You can find the documentation and some examples in the REST API component registry entry. Building our application Enough theory. Let's dive right into the code and build an application! Creating a component project Let's start by creating a new components project. A components project is simply a directory containing a `serverless.yml` file. To start, create a new directory called `products-rest-api` by running `mkdir products-rest-api`. After `cd`ing into it, we'll need to create an empty `serverless.yml` file by running `touch serverless.yml`. Then, open this directory with your favorite code editor. In order to tell Serverless that we have a components project, we need to add the following lines of code to our `serverless.yml` file:  The `type` property tells the Framework that our application is called `rest-api-app`. If you compare this information with `serverless.yml` files from components in the registry, you might see that they too have a `type` property at the root level. This is because our `rest-api-app` application is itself a component, which could be re-used by other components or projects. There is no distinction between an application or a component. Both are components at the end of the day. Adding a `products` DB table Since our REST API will be used to store and retrieve product data, we'll need to have a database backend to persist such products. The `aws-dynamodb` component makes it easy for us to use and manage AWS DynamoDB tables. Let's add this component to our application. Add the following code into your projects `serverless.yml` file:  With this code snippet, we've officially added the first component to our project! Components can be added and configured in the `components` section, and configured via `inputs`. In our example, we've added a component we called `productsDb` with the `type` `aws-dynamodb`. We then created a new DynamoDB table called `products` in the `us-east-` region. Our database schema is defined with the `schema` property, and defines the products properties `id`, `name`, `description` and `price`. Adding our Lambda functions The next thing we need to do is add AWS Lambda functions to create and list our products. We'll store our Lambda code in a file called `products.js` which is located in a directory called `code` in the projects root directory (`code/products.js`). Let's start with the `createProduct` function, which will insert a new product into the database. `createProduct` Paste the following code into the `products.js` file:  Here, we imported the AWS SDK, which is required to make AWS SDK calls. Then, we created a new DynamoDB instance called `dynamo`. We fetched the `table` name from the function's environment variables and defined our `create` function, which includes the logic to insert a new product record into the database. Note that we're already using an event shape which corresponds to the AWS API Gateway event definitions. Next up, we need to add an AWS Lambda component to our `serverless.yml` file:  Here, we're adding our `createProcuct` component which is of `type` `aws-lambda`. We've defined the function's configuration with the help of the component's `inputs` section. Note that we're specifying the path to our function's code with the help of the `root` property and the environment variables via `env`. `getProduct` and `listProducts` Let's add the functionality to fetch a single product (`getProduct`), as well as all products (`listProducts`), from the database. Add the following JavaScript code to the `products.js` file:  With this code, we simply defined and exported two functions, called `get` and `list`, which will query our DynamoDB database and return the result formatted in a way the API Gateway understands. Let's add two new AWS Lambda components to our `serverless.yml`one component for the `get` function and one for the `list` function:  Here, we declared two function components called `getProduct` and `listProducts`, which are of `type` `aws-lambda`. Both functions were configured via `inputs`, and refer to the same code location as our `createProcuct` function. We also passed in the corresponding environment variables via `env` to ensure that our Lamdba functions had access to our DynamoDB table name. That's it from an AWS Lamdba functions perspective! Adding the `rest-api` components The last missing piece is the `rest-api` component, which ties everything together and makes it possible to interact with our Products application end-to-end. Adding and configuring our REST API is as easy as adding the corresponding component configuration to our `serverless.yml` file:  Here, we created our `productsApi` REST API using the component with the `type` `rest-api`. Via `inputs`, we configured it to use the `aws-apigateway` component under the hood as our `gateway` of choice. Then, we defined our `routes`. Generally speaking, we defined routes, one for each function. We made `/products` accessible via `GET` and `POST`, and specific products accessible at `/products/{id}` via `GET`. A speciality in this component definition is the use of Serverless Variables via `${}`. Serverless Variables makes it easy for us to reference different values from different sources: environment variables, component outputs, or the `serverless.yml` file itself. In our case, we're referencing the AWS Lambda components by name and passing those functions into our `rest-api` component. This way, the component knows how to configure the REST API so that the corresponding function is called when a request is sent to the endpoint. Enabling `cors` for our endpoints is as easy as adding the `cors: true` configuration to our route. Deploy and testing That's it! We wrote all the necessary code to set up and configure our fully-fledged REST API! We now have a REST API which is accessible via different API endpoints. This REST API will trigger our AWS Lambda functions, which in turn reach out to DynamoDB to query our products. Let's Deploy and test our application! Deploying is as easy as running `components deploy`. We can see our API Gateway endpoints at the end of the deployment logs. It should look something like this:  Let's insert a new product. We can `curl` our `/products` endpoint via `POST`:  Great! Now we should be able to get our inserted product via the following `GET` request:  A list of all products can be requested like this:  Nice! Feel free to continue playing around with your REST API. Once done, you can remove the application via `components remove`. Conclusion We've just created our very first product REST API via Serverless Components. And we did it by using three different pre-built components from the Serverless Components registry (`aws-lambda`, `aws-dynamodb` and `rest-api`). Setting everything up was as easy as adding the function logic and the corresponding component configurations in our `serverless.yml` file. I hope that you've enjoyed this tutorial, and got a feeling for how powerful Serverless Components are. You could enhance this project further by adding a static website interface with the help of the `static-website` component. The `examples` section in our Serverless Components repository is another great resource to get some inspiration what else you can build with the Serverless Components framework. More Components posts - What are Serverless Components, and how do I use them? - How to create a static landing page with Serverless Components - How to create a blog using pre-built Serverless Components and Hugo",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f12",
      "title": "Create a blog using pre-built Serverless Components and Hugo",
      "content": " Most of us blog, and a very common dilemma is deciding how to host the blog site. You want something easy to use, produce content with, and maintain. Bonus points if it's also easy to port elsewhere in case you ever want to move it. Static site generators are a good option in this regard; they help keep the authoring part simple. They use Markdown as a document format, spruce up the look & feel with themes, and provide a simple workflow for a fully deployable HTML/CSS/JS-based blog site. The deployment part is, however, up to you. In this post, we will create a serverless, static blog site. We'll generate it with Hugo, deploy it with pre-built Serverless Components, and host it on AWS. Why serverless? Hosting static websites with serverless is a key use case: not only easy to deploy, but also very cost-effective. Here is what we will cover: Generate a static blog site Deploy the site using Serverless Components Deep dive into configuration and implementation Generating a static blog site Although we will be using Hugo to generate our blog site, you can use your favorite static generator. As long as you can build the final site into a local folder, you are good to go. We'll see how that can be done with Hugo. Note: Since working with Hugo is well documented on their site, I'll leave the exercise of creating the site to you. However, to get you started, I've created a sample blog site and shared it below. You can use that with the solution I present here. First, make sure to get Hugo installed and working. Then, run the following commands on your terminal to get going:  Note: The sample sls-blog site code is on Github. If you are following along, you should have a working blog site that you previwed locally at `localhost://`, and you should have the static files ready to be deployed in the `site` folder on your machine. Wrapping it up in a component You might have already heard about our latest project, Serverless Components, and how you can use them. Our goal was to encapsulate common functionality into so-called \"components\", which could then be easily re-used, extended and shared with other developers and other serverless applications. We built all the functionality needed to take a set of static files, host it on S with appropriate permissions & configuration, set it up on a CDN, map a domain name, and finally deploy it to AWS. The blog application The blog application references the `static website` component that encapsulates all the functionality we need. Here's the `serverless.yml` file:  The `type` identifies the application. The `components` block is the gut of the application. It simply references the `static-website` component using the `type` attribute. It used the `inputs` block to supply the input parameters required by the `static-website` component to customize it behavior. The `contentPath` specifies the path where the content of the site belongs. In the above section, we generated the static files for our blog site using Hugo. This location is what we specify here. Although the `static-website` component can make use of `templateValues` using Mustache templates, our blog site does not use it. The `hostingDomain` and `aliasDomain` attributes are used to configure the CDN and map it to a domain name. You will notice the usage of the variable `${self.serviceId}` in the above configuration. The `serviceId` is an unqiue, autogenerated, random identifier that you can use to force uniqueness. In our case, you can copy the same application over and create as many instances of blog sites you want. In all practical purposes, you will probably replace the `hostingDomain` and `aliasDomain` attribute values with your own specific domain names. Deploy Once you have the `serverless.yml` set and file path for your static files ready, you can simply `deploy` the blog application. Here's how:  This will detect the component dependencies, run the deployment logic for each, and finally deploy the blog application to AWS. Here's what you get:  :boom: You have a blog site at:  !image Figure: _Blog site built with Hugo and deployed with Serverless Components_ Note: If you put in a real domain name, you can access the site via the domain as well. Give CloudFront and Route about - mins to finish the configuration. You can also get the information about the resources that were deployed by running:  And you can always cleanup the resources by running:  The Aha Moment Let's just think for a moment about what we just did there. You have a blog site, based on your theme, optimised using a CDN, on your domain, using serverless, on AWS. In...really not that much time. And no servers to maintain! No excusesget on with those articles you have been meaning to write! Now that I have piqued your interest in Serverless Components, I wanted to also express that you don't have to be highly technical to use it. The Serverless Components abstracts away a lot of inner workings, and exposes a simplistic view of the behavior you seek. You, as a front-end developer or a full-stack engineer, can benefit from using the serverless technologies without getting into the nuts and bolts of things. For the curious, let's get into the details and take a peek behind the curtain of how these components work. The Static Website Component The blog site we created uses the `static-website` component. Let's walk through the `static-website` component that wraps up the functionality to deploy a static website on AWS S. It not only configures S to host a website, but also configures a CDN using CloudFront and maps a custom domain via Route (DNS). Yes, that's a lot of moving parts, but that's the beauty of encapsulating all that complexity in a reusable and sharable Serverless Component. The `static-website` component is composed of several other smaller components. The idea is to build up small, independent blocks of functionality and encapsulate them into reusable chunks. The `static-website` component is shared via the registry on Github. Configuration Components are declared and customized by its configuration file (i.e. `serverless.yml`). They are identified by a `type` and take input parameters to customize it's behavior. The input parameters are described by an `inputTypes` block in the component's configuration file. Components can be made up of other Components. The 'composition', or the component's dependencies, are specified in the `components` block. Let's take a look at the `serverless.yml` file for the `static-website` component. Type and Metadata  The `type` attribute is used to reference the component when used from another application or component.  All of the above attributes are pretty self-explanatory, and are metadata about the component. Input Parameters The `inputTypes` block is the specification for inputs that the component exposes. The specs allow for specifying if a parameter is required or not and it's default values. The system will validate the inputs based on these specs.  Composition The `components` block lists the dependencies that make up the top-level component or an application. In the case of the `static-website` component, we have several smaller components that build up the functionality: `mustache`: provides Mustache templating capabilities `aws-s-bucket`: manages a S bucket `s-policy`: manages S bucket policy `s-sync`: sync a local folder to a S bucket `s-website-config`: configures a S bucket for website hosting `aws-cloudfront`: configures and manages CloudFront distribution `aws-route`: configures and manages Route mappings Note: You can find more details about these components and look at the code in the Components registry. Each one of these components are independent of each other, but they can be weaved together into a higher-order compomnent.  Note: Take a look at the entire `serverless.yml` file here. Input Variables Child components can use parent's `inputs` as input for themselves. This allows sharing of input data and also signifies a dependency. Here is an example:  Here the `siteCloudFrontConfig ` component needs the hosting domain name and the alias domain name to configure the CloudFront distribution. So, it passes the `input.hostingDomain` to its `originId` parameter, and `input.aliasDomain` to its `aliasDomain` domain parameter. Recall that the blog site (parent application in our case) had the `inputs` block as follows:  Output Variables Components can take input parameters to customize their behavior, but components can also expose output variables. The output variables expose output values that are generated inside the component as part of their implementation. Here is an example:  The `dnsName` input parameter for the `aws-route` component is provided the output from the `aws-cloudfront` component. You will notice that the component's instance name, `siteCloudFrontConfig`, is used to reference the output variable. Dependency Components can have dependencies amongst each other; it can be fairly cumbersome for a component or application author to keep track of. To aid with that, the system keeps track of the dependency tree for you, based on the use of input variables and output variables. For example, since the `siteRouteConfig` component uses the output variable from the `siteCloudFrontConfig` component, the `siteRouteConfig` component waits for the `siteCloudFrontConfig` component to finish, and then uses its output. The set of components that do not have any dependencies can execute in parallel, thus improving performance. Component Behavior We looked at the configuration for the `static-website` component. Now, let's look at the code that drives the behavior. The behavior or implementation of the `static-website` component is placed in the `index.js` file, as shown below: ```js index.js const { not, isEmpty } = require('ramda') const deploy = async (inputs, context) => { let outputs = context.state const surl = `http://${inputs.hostingDomain}.s-website-${inputs.hostingRegion}.amazonaws.com` if (!context.state.name && inputs.name) { context.log(`Creating Site: '${inputs.name}'`) outputs = { url: surl } } else if (!inputs.name && context.state.name) { context.log(`Removing Site: '${context.state.name}'`) outputs = { url: null } } else if (context.state.name !== inputs.name) { context.log(`Removing old Site: '${context.state.name}'`) context.log(`Creating new Site: '${inputs.name}'`) outputs = { url: surl } } context.saveState({ ...inputs, ...outputs }) return outputs } const remove = async (inputs, context) => { if (!context.state.name) return {} context.log(`Removing Site: '${context.state.name}'`) context.saveState({}) return {} } const info = (inputs, context) => { let message if (not(isEmpty(context.state))) { message = [ 'Static Website resources:', ` ${context.state.url}` ].join('\\n') } else { message = 'No Static Website state information available. Have you deployed it?' } context.log(message) } module.exports = { deploy, remove, info } ``` Let's walk through the code. First, note the three methods that provide all the functionality: `deploy`, `remove`, and `info`. At the end of the code block, you will notice that these methods are exported so that they are publicly accessible from outside. At minimum, all components should follow this pattern and implement these three methods. The core system will build the dependency tree of child components and call these methods down the chain. However, it is not necessary to provide an implementation via the `index.js` file if you don't need it. You can see that the `blog-app` application that we created does not provide any `index.js` file. It just describes the composition of it's child components via the `serverless.yml` configuration file. The `deploy` method The `deploy` method is used to encapsulate the deployment behavior of the component. It inspects & validates input parameters, calls appropriate code to deploy the necessary resources, and saves state in the `state.json` on disk. The `static-website` component completely relies on its child component implementations, and so the `deploy` method only prints out a message that includes the site url. The system calls the `deploy` method of the child components down the dependency chain. The `remove` method The `remove` method is used to encapsulate the cleanup behavior of the component. It reverses the effect and cleans up resources created via the `deploy` method. In this case, a message is printed stating that the site has been removed. The component's state in the `state.json` file is also cleared. The system calls the `remove` method of the child components down the dependency chain. The `info` method The `info` method is used to print any resources that were deployed. In this case, the static website url is printed if the component has been deployed. Summary We saw how easy and simple it is to use Serverless Components to build and deploy applications, such as the blog site we created. Components gives us the flexibility to compose higher-order applications by combining reusable pieces of code. What will you build with components? Let us know in the comments below. More on Components - What are Serverless Components, and how do I use them? - Create a REST API with pre-written Serverless Components - Create a landing page with pre-written Serverless Components",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f14",
      "title": "Serverless Platform updates - Install Components from url, Fn Project support, CloudEvents",
      "content": "Well everyone, we now have a few different product updates to give. We're welcoming Serverless Components and Event Gateway into the product updates family! We are combining all of these into Serverless Platform updates. Jump straight into the section you want to read about: - Components v.: Install components from url, core version locking, Node support - Framework v.: Fn Project support, fixes for the variables system, support for AWS GovCloud and China regions - Event Gateway: Hosted beta is ready to use, Serverless Framework plugin, CloudEvents support Changes in Components v. The new components release contains a bunch of goodies. Install components from URL In version ., you can now reference a component type using a url as a source. This allows for reusable components by uploading them to a URL first, giving teams a way to share components. !component type url Package command Alongside the ability to use Components from a url, we have also added a package command. This command will pack up a component directory into a zip for reuse. !component package command Core version locking This feature gives developers a way of declaring which version of Components core their component is compatible with. If a component is included that is incompatible with the current version of core, an error is thrown. !core version locking !core version locking error Programmatic Usage API We've added methods for using the components package programmatically. The documentation for these new methods can be found here. Support for Node - We've moved to using babel compilation for components, so the components package now supports version node and greater. Changes in Framework v. Framework v. introduces Fn support, fixes for the variables system, and support for AWS GovCloud and China regions. You can find a complete list of all the updates in the CHANGELOG.md file. Fn support The Fn Project is an open-source, container-native serverless platform that runs on any cloud or on-prem. And now there is an integration with the Serverless Framework. Read here for more details and some examples. Fixes for the variables system We introduced several improvements and fixes, including pre-population of service and region values, a \"PromiseTracker\" class, and more. Read up on the whole list of changes! Support for AWS GovCloud and China regions Just what it says on the tin. Check it out here! Contributors This release contains lots of hard work from our beloved community, and wouldn't have been possible without passionate people who decided to spend their time contributing back to make the Serverless Framework better. Huge round of applause to all of the contributors who submitted changes for this release! Changes in Event Gateway As we recently announced, the open-source Event Gateway project now also exists as a hosted (read: fully serverless) service. Plugin for the Serverless Framework The Event Gateway plugin for the Serverless Framework makes it dead simple to deploy your Lambda functions with Event Gateway. The plugin enables you to: - register API endpoints - put endpoints from different services on the same domain very easily, even if those services are in different AWS accounts - hook up custom events, so that if one of your services emits an event, another function in a different service can subscribe to it - set up connectors in the Event Gateway to route events to external systems. Check out the plugin here or try the get started example. CloudEvents support The Cloud Native Computing Foundation (CNCF) has been working on CloudEvents, a specification for describing event data in a common way across providers. At Serverless, we've taken a leading role in this effort; we believe strongly in the importance of standards and interoperability in this new event-driven world. As such, Event Gateway CloudEvents-compatible. All functions receive a CloudEvents payload describing the event received. Configurable connectors Connectors let you Write Less Code in the true serverless way. Configuration is better than code wherever possible. In that spirit, Connectors in Event Gateway are bits of logic which take your event and send it to another systemFirehose, Kinesis, or SQS for instancewithout having to write the boilerplate integration code into a Lambda function. Try an example here. Upcoming releases & contributions If there's something you want to change about the Serverless Framework, Event Gateway or Components, open an Issue! We even have a quick & easy guide on contributing to Serverless open source projects. PR reviews are also highly welcomed, as they greatly speed up the time-to-merge. Serverless Examples The Serverless Examples Repository is an excellent resource if you want to explore some real world examples, or learn more about the Serverless Framework and serverless architectures in general. We have examples for Components and Event Gateway, too. Serverless Plugins Serverless provides a completely customizable and pluggable codebase. Our community has written a vast amount of awesome plugins you can use to further enhance the capabilities of the Framework. You can find the full list at our Serverless Plugins Repository. Don't hestitate to open up a PR over there if you have a new Framework plugin to submit!",
      "__v": 0
    },
    {
      "_id": "64e08921b72e199dda603f16",
      "title": "How to Create a Dynamic Site with Pre-built Serverless Components",
      "content": " In previous posts, weve explored how to create a static website and a REST API with Serverless Components. This post is going to build on top of that knowledge to create a dynamic website powered by a database. The example dynamic website were going to put together is a retail app. Just a simple eCommerce website that lists the products in your database via a REST API. Getting started First, a quick overview of the Components we are using, and how you'll need to set up your environment. Components were going to use To assemble our dynamic site, were gonna use the following lower-level components: - static-website: For the frontend logic of our application - aws-lambda: For the backend logic of our application - aws-apigateway: For the REST API endpoints - aws-dynamodb: For our products database You can find all those components in our registry. Overall, our entire app architecture looks like this: Ready to start? Let's go. Install Serverless Components If you havent already, install Serverless Components with the following command:  Youll also need to have an AWS account to host pretty much everything. Alright! Now that we have everything set up, lets starting building our retail app! Setting up the frontend Well start out by setting up the frontend via S. All you have to do is create a `serverless.yml` file, use the `static-website` component, and pass in the required config. Add the following config to a `serverless.yml` file:  Couple of things to notice here. We're referencing the `self` object, which includes some useful information about your new component, like the `path`, and the `serviceId`. The `path` is the absolute path of your component, and `serviceId` is an auto-generated id for your component/service, which you can utilize to make sure your child components have unique ids that are associated with each other. Using this `path` property, we're referencing our frontend code on our file system, which lives in a `frontend` directory. There are a lot of files in there, so to keep this post clean, you can check this directory in our examples directory and add it to your filesystem. It should be pretty straightforward. You'll also notice that we're referring to a REST API component using the variable syntax `${productsApi.url}`. So we'll need to add that component. Let's do that! Setting up the REST API You can add a REST API by adding the `rest-api` component. This component supports both AWS API Gateway and Serverless Event Gateway. In this example, we're going to use the API Gateway. Let's add this component to our `serverless.yml` file by adding this config:  Sweet! Now we have a REST API set in place. As you can see, this REST API needs function backends to process your API requests. So we'll need to add Lambda function components. We're going to do that next. Setting up the backend code Just add `aws-lambda` components to your config along with the actual logic. To do that, you can add the following config:  You'll notice that we're referencing to our code base with the `root: ${self.path}/code` property. So we'll need to add that before moving forward. Just create a `code` directory in the root directory of your component (where `serverless.yml` lives) and add an `index.js` file inside that looks like this:  We won't get into the details of the code above, but it should be pretty self explanatory. Essentially, you'll be exporting the Lambda handlers needed by your REST API. Wait a minute, our Lambda codebase is referring to a DynamoDB database table to query. So we'll need to add this final piece of the puzzle. Let's do it! Setting up the database We're going to use the `aws-dynamodb` component to provision this products database. To do that, just add the following config to your `serverless.yml`:  Awesome! We just assembled the final piece of our new component. It's a lot like LEGO, isn't it? If you deploy your component, your entire app should be deployed. However, for the sake of demonstration, let's add a final step to seed some product data in the database so that you can view it on the website. BONUS: Seeding data Remember that, at the end of the day, you're creating a brand new component using lower-level components. You can add logic specific to this new higher-order component just like you do with other components. In our use case, we need some logic to seed data into our database. We can do that by adding an `index.js` file with the following logic: ```js const { readFile } = require('fs') module.exports = { async deploy(inputs, context) { const productsDb = await context.children.productsDb const products = await new Promise((resolve, reject) => readFile('data/products.json', (err, data) => { if (err) { reject(err) } else { resolve(JSON.parse(data)) } }) ) if (products.length > ) { const tablename = `products-${context.serviceId}` context.log(`Seeding ${products.length} items into table ${tablename}.`) const insertItem = (triesLeft, wait) => (product) => productsDb.fns .insert(productsDb.inputs, { log: context.log, state: productsDb.state, options: { tablename, itemdata: product } }) .catch(async (error) => { if (triesLeft > ) { return new Promise((resolve, reject) => { setTimeout(() => { const doInsert = insertItem(triesLeft - , wait)(product) doInsert.then(resolve, reject) }, wait) }) } throw error }) const insertions = products.map(JSON.stringify).map(insertItem(, )) await Promise.all(insertions) } } } ``` Did you notice our reference to the `serviceId`? Just like with the config above, you can access the `serviceId` along with other useful information under the passed in `context` object. It's a lot like the `self` object above, but on steroids! We're also loading a `products.json` file from a `data` directory. So let's create a `data` directory in our component root and add this `products.json` file in there. It should look something like this:  Visit your new website Alright! Now we're ready to deploy! Just run the following command:  At this point, all your components are being provisioned, and your seed data will be inserted into the newly created database. After deployment is complete, you should see the following output:  Now you can see your REST API endpoints, along with the root url of your newly created dynamic website. Just copy this website and open it in your browser. Voila! Your dynamic website is live! Notice all the frontcode weve added, along with the seeded products from the database getting queried by the Lambda functions via our REST API. Summary In a nutshell, what we've done is added the following components, along with their required config & code: - `static-website`: Hosted on AWS S to hold our frontend codebase - `rest-api`: Using AWS API Gateway to provision our RESTful API endpoints - `aws-lambda`: We added of those as backend for our API endpoints - `aws-dynamodb`: Our products database, and seeded some fixture data into it And this is how you can create a dynamic website with Serverless Components! You can extend this app by adding even more frontend code and routes and do more queries to our backend. You can also add more backend logic and infrastructure, all with nothing but Serverless Components. Hope you found this walkthrough useful. You can check the entire example app in our components directory. Happy hacking!",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f18",
      "title": "How to write your first Serverless Component",
      "content": "The open-source Serverless Components project makes it easy for anyone to author their own application components. We have several previous posts. But what if you want to try building a brand new reusable component of your own? In this post, we will cover how to build your very first Serverless Component. Specifically, we'll be making an AWS CloudWatch Metric Alarm component from scratch. The component will provision and encapsulate functionality for managing a CloudWatch metric alarms. We'll then use this component in an application that tracks AWS billing and sets alarms for estimated charges. The goal here is two-fold: streamline the complexity of provisioning metric alarms, and provide an intuitive interface via well-defined inputs and outputs that can be reused to build applications and higher-order components. Ready? Awesome. What we will cover: Defining the reasoning for the component Defining the configuration Defining the input and output parameters Implementing the component Using the component in an example application Running the application The CloudWatch Metric Alarm Component We will build a `aws-cloudwatch-metric-alarm` component to create AWS CloudWatch alarms on metrics based on various conditions that are supported by AWS. A Serverless Component consists of two files: `serverless.yml` for configuration and `index.js` for implementation code. Configuration To start off, let's create a `serverless.yml` file:  Open the file in your favorite editor. We will start by specifying some basic metadata about the component, like so:  Let's go over what all of this does. The `type` attribute identifies the component. This will be used later in our example app to reference the component. The `version` attribute is the current version of our component following semantic versioning. The `core` specifies the release version of Serverless Components we are using. It is helpful to fix a version against which our component has been built and tested. The next block is quite intuitive and describes some other metadata for the component. Since I will merge this component to the main Serverless Component registry, I have the `author` and `repository` attributes set accordingly. Input Parameters To create a component that can provide functionality in a flexible manner, Serverless Components has a concept of declaring `inputTypes`. This defines the spec for the component's input parameters. When we write our example app that uses this component, the system will validate the user-supplied input parameters against this spec. We will see the validation at work later in the post. In our case, we will be using the AWS CloudWatch API to create/update alarms, list alarms and delete alarms. So let's start by defining our `inputTypes` that match the CloudWatch APIs. Append the following block to the `serverless.yml` file: ```yml serverless.yml ... inputTypes: alarmName: type: string required: true description: \"The name for the alarm. This name must be unique within the AWS account.\" alarmDescription: type: string description: \"The description for the alarm.\" comparisonOperator: type: string required: true description: \"The arithmetic operation to use when comparing the specified statistic and threshold. Valid values are: `GreaterThanOrEqualToThreshold` | `GreaterThanThreshold` | `LessThanThreshold` | `LessThanOrEqualToThreshold`\" threshold: type: number required: true description: \"The value against which the specified statistic is compared.\" metricName: type: string required: true description: \"The name for the metric associated with the alarm.\" namespace: type: string required: true description: \"The namespace for the metric associated with the alarm.\" dimensions: type: object[] required: true description: \"The dimensions for the metric associated with the alarm.\" period: type: integer required: true description: \"The period, in seconds, over which the specified statistic is applied. Valid values are , , and any multiple of , max: .)\" evaluationPeriods: type: integer required: true description: \"The number of periods over which data is compared to the specified threshold.\" actionsEnabled: type: boolean description: \"Indicates whether actions should be executed during any changes to the alarm state.\" okActions: type: string[] description: \"The actions to execute when this alarm transitions to an `OK` state from any other state. Each action is specified as an Amazon Resource Name (ARN).\" alarmActions: type: string[] description: \"The actions to execute when this alarm transitions to the `ALARM` state from any other state. Each action is specified as an Amazon Resource Name (ARN).\" insufficientDataActions: type: string[] description: \"The actions to execute when this alarm transitions to the `INSUFFICIENT_DATA` state from any other state. Each action is specified as an Amazon Resource Name (ARN).\" statistic: type: string description: \"The statistic for the metric associated with the alarm, other than percentile. Valid values are: `SampleCount` | `Average` | `Sum` | `Minimum` | `Maximum`. Either `statistic` or `extendedStatistic`, but not both.\" extendedStatistic: type: string description: \"The percentile statistic for the metric associated with the alarm. Specify a value between p. and p. Either `statistic` or `extendedStatistic`, but not both.\" unit: type: string description: \"The unit of measure for the statistic. Valid values are: `Seconds` | `Microseconds` | `Milliseconds` | `Bytes` | `Kilobytes` | `Megabytes` | `Gigabytes` | `Terabytes` | `Bits` | `Kilobits` | `Megabits` | `Gigabits` | `Terabits` | `Percent` | `Count` | `Bytes/Second` | `Kilobytes/Second` | `Megabytes/Second` | `Gigabytes/Second` | `Terabytes/Second` | `Bits/Second` | `Kilobits/Second` | `Megabits/Second` | `Gigabits/Second` | `Terabits/Second` | `Count/Second` | `None`\" datapointsToAlarm: type: int description: \"The number of datapoints that must be breaching to trigger the alarm.\" treatMissingData: type: string description: \"Sets how this alarm is to handle missing data points. Valid values are: `breaching` | `notBreaching` | `ignore` | `missing`\" evaluateLowSampleCountPercentile: type: string description: \"Used only for alarms based on percentiles. Valid values are: `evaluate` | `ignore`\" ``` Ya I know, it is quite verbose, but that's how many parameters are in the API. Output Parameters Just like `inputTypes`, Serverless Components also have a concept of declaring `outputTypes`. This is the spec for the component's outputs that it exposes. Let's start by defining our `outputTypes` that match some of the important and relevant outputs from the calls to the CloudWatch APIs. The outputs that you expose from your component is totally your choice. The point to keep in mind is that the component will be used later to build higher-order components and applications. So choose to expose outputs that will make sense later on. So for example, we expose the `alarmArn` output parameter, that can be passed into another component to do something with the alarm. Append the following block to the `serverless.yml` file:  Notice that we use names for the input and output parameters that match the AWS API. The only subtle difference is that the AWS API uses CamelUpperCase whereas Serverless Components use camelCase. However this small transformation can be easily automated in code. Documentation Notice that the `inputTypes` and `outputTypes` have a nice format where it is easy to understand each parameter and is self-documenting. The system will use this information to automatically generate documentation for the README file. Speaking of which, let's create a README.md file. For auto-generating documentation for the input/output parameters, paste the following markers into your README file:  The auto-doc generation capability is invoked by running `npm run docs` from the root of the Serverless Components project folder. Note: Since our component is outside the Serverless Components project structure, you will have to temporarily copy your component under the `registry` folder of the Serverless Components project repo, and then run:  This will generate the required documentation and place them between the markers we added in the README.ms file. Check out the README.md file for the final documentation. Implementation Now that we have defined the input/output interface, let's look at the implementation for our component. Serverless Components lay down a contract for implementing provisioning, listing status and cleanup logic. Commands The provisioning functionality is implemented via the `deploy` command while the cleanup functionality is implemented via the `remove` command. The `info` command is for listing out resources or output parameters after provisioning. The underlying system is set to call the `info` command automatically after the `deploy` command is executed. When there are higher-order components or applications that are composed of other components, the `deploy`, `info` and `remove` commands are called down the full dependency chain. Apart from these three commands, a component author can include other commands. These are JS methods that are exported and available to the user as commands from the CLI. The implementation code will live in the `index.js` file. Let's create the file in our project folder:  Open the file in your favorite editor. Let's build these three commands for our component. Provision Alarm We will add the provisioning logic for the CloudWatch Metric Alarm in the `deploy` method. In the `index.js` file, let's create a method named `deploy`. Add the following code to the file: ```js index.js ... const deploy = async (inputs, context) => { let outputs = context.state if (!context.state.alarmName && inputs.alarmName) { context.log(`Creating CloudWatch Metrics Alarm: '${inputs.alarmName}'`) await putMetricAlarm(inputs) outputs = await describeAlarmsForMetric(inputs) } context.saveState({ ...inputs, ...outputs }) return outputs } ... ``` Let's look at the signature of the `deploy` method. It takes two parameters, `inputs` and `context`. The system packs the input parameters from the `serverless.yml` and passes them via the `inputs` parameter. The system packs some other valuable information via the `context` parameter. We will go over `context` later in the post. The above command calls two helper methods: `putMetricAlarm(inputs)` and `describeAlarmsForMetric(inputs)`. The `putMetricAlarm(inputs)` method calls the CloudWatch putMetricAlarm API method. It creates or updates an existing metric alarm. This API call does not return any information about the alarm that is provisioned. Next, the `describeAlarmsForMetric(inputs)` method calls the CloudWatch describeAlarms API method. This method returns information about the alarm provisioned by the earlier call. The call to `context.saveState({ ...inputs, ...outputs })` saves the output as state. We will look at state management in detail later in the post. List Alarm We will add the code for listing the alarm that we just created, in the `info` method. In the `index.js` file, create a method named `info`. Add the following code to the file: ```js index.js ... const info = async (inputs, context) => { if (!context.state.alarmName) return {} let outputs = context.state outputs = await describeAlarmsForMetric(inputs) context.saveState({ ...inputs, ...outputs }) context.log(`Listing CloudWatch Metrics Alarm for '${context.state.alarmName}'`) console.log( '-' ) console.log(`Metric Name: ${context.state.metricName}`) console.log(`Namespace: ${context.state.namespace}`) console.log(`Alarm Name: ${outputs.alarm.alarmName}`) console.log(`Alarm Arn: ${outputs.alarm.alarmArn}`) console.log(`Alarm State: ${outputs.alarm.stateValue}`) console.log( '-' ) return outputs } ... ``` The above `info` command calls the `describeAlarmsForMetric(inputs)` which in turn calls the describeAlarms API to get information about the alarm that was provisioned in the `deploy` command. The data is then written out to the log as output, and printed on the terminal. Remove Alarm We will add the code for deleting the alarm that we created, and cleanup any resources, in the `remove` method. In the `index.js` file, create a method named `remove`. Add the following code to the file: ```js index.js ... const remove = async (inputs, context) => { if (!context.state.alarmName) return {} try { context.log(`Removing CloudWatch Metrics Alarm: '${context.state.alarmName}'`) await deleteAlarm(context.state.alarmName) } catch (e) { if (!e.message.includes('Invalid Metric Alarm name specified')) { throw new Error(e) } } context.saveState() return {} } ... ``` The `remove` command calls the `deleteAlarm(context.state.alarmName)` which in turn calls the CloudWatch deleteAlarms API method, to delete the alarm that was provisioned earlier. Note: You can view the full `index.js` file here. Context Any method that is exported as a command is sent a `context` parameter for the method implementing the command. The `deploy()`, `remove()` and `info()` methods are such examples. The system packs some valuable information via the `context` parameter, the most important ones being: Data structures: `state`, `options`, `children` and `archive` Functions: `load` and `saveState` Let's look at each of these one at a time. State Management The system allows state management for components. The component author is in control of what gets saved as state. The command methods can access the data stored in state via the `state` data structure. The component can look at the state of data and decide to execute different paths in logic. For instance, the `deploy` method can decide if the component needs to call the 'create' or the 'update' logic. The `saveState` method You will notice that each command calls `context.saveState()` and optionally passes in some data that gets saved as state for the component. The system saves state for a component in the `state.json` file. We will look at the `state.json` file for our application later in the post. In our component, the `deploy` command saves the relevant outputs to the state by calling the `context.saveState()` method. Options The CLI allows for passing in any command-line options and passes it in the `options` data structure, via the `context` to the command methods. For e.g. the `aws-dynamodb` component exposes an `insert` command that takes a few command-line options to insert data into a DynamoDB table.  Children Higher-order components or applications can be composed of child components. The `context` gives access to the child components via the `children` object. The `children` object can be used to call commands exposed by the child components. For example, the retail-app's `deploy` command accesses the child component `aws-dynamodb`'s `insert` command to insert seed data.  Also, the system automatically figures out the dependency tree for your components' children. It has also built-in safeguards against accidental circular-dependency errors. You can see how powerful and flexible Serverless Components can be. The `load` method The `context` also exposes a `load` method, which allows you to load a component programatically inside another component or an application. For instance, the `aws-lambda` component's `deploy` command, loads the child component `aws-iam-role` programmatically to provision an IAM role. ```js index.js async function deploy(inputs, context) { ... ... const defaultRoleComponent = await context.load('aws-iam-role', 'defaultRole', { name: `${inputs.name}-execution-role`, service: 'lambda.amazonaws.com' }) ... ... ``` Although the component we created here was all declarative, but you can see how powerful Serverless Components can be considering that a higher-order component or an application can be entirely written programmatically. Hope this leaves you with enough information to create your own components! Now that we have written our component, let's use it to build an application. Note: You can find the full code for the `aws-cloudwatch-metric-alarm` component here. The Billing Alarm Application We will use the `aws-cloudwatch-metric-alarm` component we just built to create a serverless application, `cw-billing-alarm-app`, that will trigger an alarm for AWS Billing based on our configured settings. It will also send us an email when the alarm is triggered. The application is declaratively composed using the `cw-billing-alarm-app` component, and has no implementation of its own. Hence, we only have a `serverless.yml` configuration file to specify the component and its inputs. File System Components Serverless Components that are sourced from the main registry, can be referenced by their `type` name. By default, the component will be loaded from the registry. To make local development easy, Serverless Components have a feature of specifying a local folder as a value to the `type` attribute. In that case, the component will be loaded from the file system. In our application's `serverless.yml` file, we reference the `aws-cloudwatch-metric-alarm` component from the local folder like so:  Inputs Based on the `inputTypes` defined in the `aws-cloudwatch-metric-alarm` component, we will add `inputs` to the component in our application's `serverless.yml` file. Let's create a `serverless.yml` file in the application folder and add the following code to it:   The `${self.serviceId}` is an unique identifier autogenerated by the system. You can always use your own value, but with this identifier in place you can create as many apps as you want by copying the configuration. Input/Output Parameter Validation If you inspect the `inputTypes` that we defined for our `aws-cloudwatch-metric-alarm` component, you will notice that there are several `required` input parameters. To safe-guard against non-compliance with the requrements, the system will automatically validate the applications `inputs` against the spec for the given component. Notice that we have only included the `alarmName` parameter under the `inputs` section. If you run the `deploy` command you will see the validation in action: ``` $ components deploy Error: Type error in component \"billingAlarm\" - Input comparisonOperator has `undefined` value of \"undefined\" but expected the following: required true. - Input threshold has `undefined` value of \"undefined\" but expected the following: required true. - Input metricName has `undefined` value of \"undefined\" but expected the following: required true. - Input namespace has `undefined` value of \"undefined\" but expected the following: required true. - Input dimensions has `undefined` value of \"undefined\" but expected the following: required true. - Input period has `undefined` value of \"undefined\" but expected the following: required true. - Input evaluationPeriods has `undefined` value of \"undefined\" but expected the following: required true. ... ``` Let's quickly fix that by passing in input parameters that will remove the errors. Replace the entries in the `serverless.yml` file with the following code:  The `alarmActions` attribute points to a pre-configured SNS topic that points to an email address of your choosing. Running the Application To deploy the app, run the following in the terminal:  As expected, the application calls the child components `deploy`, which in turn provisions the CloudWatch Metric Alarm. Then, it lists the alarms that it just provisioned. The `info` command can be run anytime to list the alarms and get information about it. To list the alarms, run the following in the terminal:  You can see the `outputs` in action. The `output` parameters are used to print the information about the alarm. And, finally to clean up and remove all resources provisioned by the `deploy` command, run the following in the terminal:  In the deployed state of the application, whenever the AWS Billing estimated charges go beyond the configured amount, you will get an email as configured under the SNS topic. Application State We discussed that components manage their own state. Since our application is just a component in itself, it stores it's state in the `state.json` file, as shown below:  You will notice that our application does not have any state of its own, but includes the state of its child component `aws-cloudwatch-metric-alarm`. If it's relevant for an application to store its own state, it can do so as well. Note: You can find the full source code for the `cw-billing-alarm-app` application here. Summary We created a Serverless component from scratch, defined the spec for it's input and output parameters, and implemented the functionality to create commands that provision, list and cleanup CloudWatch Metric Alarms. We looked at some of the features that Serverless Components provide to make it easy to develop reusable components, that can be used to create higher-order components or applications, simply by composing other components. At the end, we built an application that will trigger an alarm for AWS Billing estimated charges based on our configured settings. It will also send us an email when the alarm is triggered. Hope you enjoyed creating your first Serverless Component and an example application to go with it. If you have any follow-up questions, please hit me via the comments below. And, above all, I would like to hear what cool components and applications you built! More Serverless Components tutorials - What are Serverless Components, and how do I use them? - Create a REST API with pre-written Serverless Components - Create a static landing page with pre-written Serverless Components - Create a dynamic retail site with pre-written Serverless Components - Create a blog site with pre-written Serverless Components and Hugo Roadmap We are continuously adding features to Serverless Components, so be on the lookout for future updates via our newsletter, articles on our log and watch the Github repo. Examples of features are being added on a regular basis.",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f1a",
      "title": "Build a Serverless GeoSearch GraphQL API using AWS AppSync & Elasticsearch",
      "content": "In this tutorial, we're going to build an Elasticsearch-backed GraphQL API on AWS AppSync. All using the Serverless Framework. AppSync offers the ability to create serverless GraphQL APIs with much less backend code than previously possible. We will take advantage of this to create our own geo search service (similar to the one used by AirBnB), which will allow users to search for items within a km radius of a given location. Let's get started! Setup Deploy Elasticsearch on AWS Elasticsearch Geo Mappings Define GraphQL Schema for API Appsync Mapping Template GraphQL Resolvers Deploy GraphQL Appsync API Teardown Setup Go ahead and install the Serverless Framework CLI and create a new directory for our project:  Deploy Elasticsearch on AWS First we need to provision our elasticsearch deployment on AWS. Create a `serverless.yml` file with the following contents: ```yaml service: appsync-placesearch frameworkVersion: \">=.. Teardown We could easily add support for realtime updates to our API at this point, but we've already done a lot of work today. So let's destroy our API and leave that for another tutorial. In order to update or delete our API, we'll need to feed our serverless config its `apiId` and `apiKey`. Let's go ahead and add these to our `serverless.yml` instructing it to pick them up via environment variables: ```Yaml{-} service: appsync-placesearch frameworkVersion: \">=.. ",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f1c",
      "title": "Giving back to the community with a TeenTech coding workshop",
      "content": "If youre a company looking for ways to give back to your local community, youre not alone! Its something we try to focus on at Serverless, too. You might remember our post about putting together a holiday bar cart that raised over $ for homeless services in the San Francisco Bay Area; after that event I was immediately asked, What are we doing next? So we put it to a vote. I asked the team to send me ideas on how we could give back, and then I threw them all into a Slack poll: We decided to work with teens in the area who were interested in coding. TeenTechSF A team member passed along contact information for an organization called TeenTechSF. TeenTech organizes events for high school students to visit tech companies, go on tours, and learn coding skills through speakers and workshops. The organization itself is run by student leaders (with adult support), and their mission is to provide free events to enable teens of diverse backgrounds to get into coding. It sounded great to us! We got in touch right away. Planning the event Our team created curricula for both a beginners and an intermediate/advanced workshop. The beginners class would focus on entry-level Javascript skills, while the advanced workshop would take students through deploying a basic application. Although this event included almost all of the San Francisco team, we wanted to figure out a way to include our remote team as well. (For those of you who dont know, we have team members all across the US, as well as in Asia and Europe.) So we created a Slack channel where remote team members could pitch in and help out with questions from the students. We were also able to use it to share pre-event information, and students were able to have a platform for asking us questions leading up to the event. It was a great idea! A couple days before the event I had one last meeting with TeenTechSF leadership to get a final headcount ( students), go over the pre-event email, and cover any remaining questions. I placed a pizza order, and we ready to go. The workshops The day of the event, students showed up; we learned the hard way that a weekday night is perhaps not the best for high schoolers. We were all a little bummed at first, but actually, on hindsight it ended up being great because the ratio allowed us all to work with a student one on one. Every teen who came was grinning ear to ear by the end of the night. You could see their faces light up as everything clicked, and they were writing functional code. The students also got the opportunity to ask some really in-depth questions, and in the advanced workshop, they even had time to show off personal projects they had been working on. Knowing this helped us see that, for future events, we perhaps do want to keep the teacher:student ratio small. Lexi Ross (software engineer at Aspen, and one of our teachers), remembers in particular one young woman who was on the TeenTech leadership team. She was initially shy and described herself as non-technical, but was able to get up and running with a basic serverless project that served some HTML to users in less than an hour! Another student had previously built a project that turned text into Yoda speak, and he was thrilled to find out that he could use Lambda to easy publish his project to an external URL that anyone could access. Nickk Gottlieb was also really impressed with how quickly the students learned. \"The teens we worked with didnt have a lot of experience with coding or software, but they were hungry to build, and once they realized how the code turned in to an actual web page, something tangible, they were off to the races.\" Lessons learned, and things wed change If we were to plan another TeenTech event, I would hold it on a weekend. I believe this would have helped a lot with attendance. A couple students didnt bring laptops, and we werent prepared for that in advance (several of us handed over our own personal laptops). Another quirk of the planning phase was, since this organization provides a unique leadership opportunity to high school students, that means youre communicating with students on their schedule. That is to say, in the evenings after school. It made me glad that we took about a month to plan the event, since quick communication during the day was not an option. How you, too, can teach teens how to code! If youre in the San Francisco Bay Area, I had a great experience working with TeenTech. But as I said earlier, you might want to host your workshop over a weekend. It appears that TeenTech operates in other cities as well. You can contact them to find out if theyre in your city. If youve been doing creative things to give back to your local community, Id love to hear about it in the comments!",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f1e",
      "title": "On serverless, data lock-in and vendor choice",
      "content": "Ive spent the last + years, first at CircleCI and now at Serverless, talking with technology decision makers about their cloud adoption strategies. The number one concern that I hear today, that I rarely heard five years ago, is the organizational requirement for vendor choice. In the early days of the cloud, the primary concern was vendor lock-in. Many organizations saw relying on third parties for their application infrastructure as a relatively new, and overly risky, pattern. But the world has changed quite a bit since those early days. Now we have a highly digitized economy that values speed. We have a multitude of developer servicesAWS, Google, Microsoft, Stripe, Auth, Cloudflarewho are releasing new products and features all the time. These products and features promise even more speed, even faster iteration. The serverless movement itself has produced countless success stories showing that small teams, using highly abstracted services like AWS Lambda, can innovate and ship software at an incredible rate. I'm seeing greater interest than ever in speed and efficiency, even at the expense of lock-in. So no, the question is no longer about vendor lock-in. It is about vendor choice. How do organizations remain agile and fast, remain able to utilize the best developer tooling to accelerate past what was previously thought possible? Vendor choice is one of the most important things IT decision-makers could be thinking about today. And the path to achieving it, I believe, is data portability. How vendor choice is different than lock-in Organizations that favor speed and efficiency above all else are increasingly adopting a serverless approach to their application architecture. And in tandem, theyre thinking about the need to optimize for vendor choice. This may seem the same as avoiding lock-in, but the mindset is actually quite different. Vendor choice means achieving a state in which engineers (and the organization as a whole) have as much flexibility as possible in choosing the tools they need to solve the problem at hand. Many serverless architectures are not simply AWS Lambda in tandem with some other AWS services, but a mesh of other servicesStripe for handling payments, Okta for handling user authentication, and Twilio for sending SMS. Different tools are more appropriate for different use cases, and vendor choice is about having the flexibility to select these tools on a use case by use case basis. Failing to achieve vendor choice and its costs In my experience, any approach that limits an organization to a single platform has high costs, particularly in regards to speed and efficiency. The market simply moves too fast to be limited by a single platform. Technology leaders from companies like Expedia, JPMorgan and Coca-Cola have all talked openly about their pursuit of vendor choice, and the flexibility and agility that it gives them. I predict that these stories will become more common in the coming years, and further, that technology leaders who are able to achieve vendor choice will put their organization in a position to win. They will get software to market faster, deliver greater levels of innovation, reduce operational costs, and retain better talent. Acknowledging the real problem: data lock-in The true danger with lock-in, especially with serverless, is the potential for data lock-in. Data has gravity. It accumulates. Data is economically disincentivized to leave, by way of platform pricing. This is the single biggest threat to vendor choice. Lambda, for example, provides a ton of value to developers, but it only integrates natively with AWS-specific storage options like DynamoDB and S. Auth gives you powerful user authentication out of the box, but results in your user data being stuck in a closed system. Stripe lets you accept digital payments in minutes, but results in very valuable payment and customer information tethered to a paid service. Being locked in to these services and their ecosystems is a byproduct of the fact that our data is locked in to these services. The path to vendor choice The serverless movement is an important step toward the ultimate dreama world in which developers can focus only on honing and improving their secret sauce, while rd party services take care of the rest. But how can organizations move toward this, while also maintaining vendor choice? The answer lies in data portability. Data portability means that organizations can move their data anywhere they want, whenever they want. This allows them to switch out their managed services at any time, or use several different competing services in tandem. Its a best of both worlds scenario: organizations get the increased efficiency they want from managed services, while also maintaining autonomy over their own data. Obviously, however, most vendors are not going to be incentivized to make their data portable. Lock-in benefits them (at least in the short term) by helping them retain customers. To achieve true data portability, and therefore enable vendor choice, we need to champion external solutions. Open source may be the solution There are many open source efforts underway that have the specific goal of granting more flexibility and choice when it comes to data. One such effort is CloudEvents, which is currently incubated within the CNCF. The goal of CloudEvents is to create an open spec for describing event data. This effort will make data, in the form of events, much more portable between cloud providers. ",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f20",
      "title": "Serverless updates - SQS events, private endpoints, Event Gateway open source",
      "content": "The work on Serverless never stops. We just launched Serverless Framework v. (with SQS event support), and have some exciting updates to Event Gateway open source! Jump straight into the section you want to read about: - Framework v.: SQS events, API Gateway private endpoints, API Gateway resource policies - Event Gateway: simpler subscriptions and authorization Serverless Framework v. Serverless Framework v. contains all sorts of new goodiesSQS event support, private endpoints for API Gateway, and so much more. Read on for the highlights, or see the complete list of updates in the CHANGELOG.md. And, we didnt say this, but if you try `sls login` in Framework v., you might find an easter egg. ;) SQS event support On the heels of Lambda sliding SQS events support in under the radar a couple weeks ago, the Serverless Framework now supports SQS events as well! This feature has been on the community wish list for a long time. In addition to being a much-needed component of a serverless workflow, SQS event migration is an especially great onramp to serverless technologies. Already have EC worker instances processing SQS queues? Port it over to Lambda! Read our full post on the SQS integration, which covers using SQS with the Serverless Framework, batch size and error handling, and protecting your downstream services with concurrency control. Also feel free to check out the PR. Private API Gateway endpoints Private API Gateway endpoints let you do things like support your product with backend APIs that are not exposed to the public internet but are accessible from within your VPC. Now you can create your API Gateway private endpoints using the Serverless Framework. Read more on the AWS blog, or see the PR. API Gateway resource policies API Gateway resource policies let you easily provision API controls. Users from different AWS accounts can be provisioned to securely access your API, much like youve been able to do with S for a while. See the PR. Custom stack & endpoint names Now you can customize your CloudFormation stack name and AWS API Gateway API name. More info in the PR. And thats not all Even more cool changes are listed in the CHANGELOG.md. Event Gateway The Event Gateway is an open-source event router which enables you to connect your existing data and workloads to serverless compute via an event-driven pattern. You can use all of the new features below via our hosted Event Gateway. Follow these docs to get started. Subscription simplicity & power Weve overhauled how Subscriptions work in the Event Gateway to make them easier and more powerful. A Subscription binds a single Event to a single serverless Function (-to-). Like this:  Our new updates enable Subscriptions to call serverless Functions two simple ways: asynchronously or synchronously. An asynchronous Subscription means when you send an Event to the Event Gateway and it routes the Event to a Function, the Event Gateway will not send the Function's response back to the origin/client that made the request. You can use async Subscriptions to create a traditional pub/sub pattern. For example, if you want to perform multiple actions when a user signs up for your application, create multiple Subscriptions:  Here's what this looks like in Serverless Framework V. using the Event Gateway plugin:  A synchronous Subscription means the Event Gateway will wait for a serverless function to process an event and then return the response to the origin/client that published the Event. You can use this to create a traditional request/response experience, even though its powered by an event-driven model. You can use synchronous Subscriptions along with the `path` and `method` setting on the Subscription, to create a single REST API route:  Here's what this looks like using Serverless Framework V. using the Event Gateway plugin:  What's even cooler is that you can ditch the `path`, `method` and thinking about endpoints entirely and simply use the Event Gateway SDK to publish synchronous Events from the client-side of your application. Here's an example of what that looks like using Serverless Framework V. with the Event Gateway plugin as well as the Event Gateway SDK:   The great thing about this pattern is that you dont have to worry about paths, methods or the general location of the Function thats receiving this. The experience is utterly simple. Keep in mind, you can also publish Events asynchronously too from your client to do all types of user activity tracking, error logging and more. Lastly, you can combine both synchronous and asynchronous Subscriptions on a single Event. Here's an example using Serverless Framework V. and the Event Gateway plugin, which synchronously processes an HTTP request to create a user, while asynchronously processing the HTTP request for analytics purposes and storing it in an Event log:  We hope this simple Subscription experience gives you the flexibility you need to power many types of workflows, while enabling you to take greater advantage of serverless compute. All of this is available now in the hosted Event Gateway and you can learn more about this in the hosted Event Gateway documentation. Authorization In this release, we also dramatically simplified how the Event Gateway does Authorization. You can now set custom authorization at the Event level. This means you can block Events from entering the Event Gateway (on any path and method) if the Event does not first pass authorization. Heres how it works: When using the Event Gateway, you must first register the Events you want to use within the Event Gateway. When registering those Events, you're able to specify a serverless Function that contains your authorization logic, which will process the Event first and determine if the Event Gateway should pass it to the Function containing your business logic. Here's an example of this with Serverless Framework V. using the Event Gateway plugin:  The `createUser` Function will not be called unless the `authorize` Function allows the `user.create.request` Event to be accepted. Upcoming releases & contributions If there's something you want to change about the Serverless Framework, Event Gateway or Components, open an Issue! We even have a quick & easy guide on contributing to Serverless open source projects. PR reviews for the Serverless Framework, Components as well as Event Gateway are also highly welcomed, as they greatly speed up the time-to-merge. Serverless Examples The Serverless Examples Repository is an excellent resource if you want to explore some real world examples, or learn more about the Serverless Framework and serverless architectures in general. We have examples for Components and Event Gateway too. Serverless Plugins Serverless provides a completely customizable and pluggable codebase. Our community has written a vast amount of awesome plugins you can use to further enhance the capabilities of the Framework. You can find the full list at our Serverless Plugins Repository. Don't hesitate to open up a PR over there if you have a new Framework plugin to submit!",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f22",
      "title": "Using SQS with AWS Lambda and Serverless",
      "content": "At long last, the wait is over. AWS recently announced that Simple Queue Service (SQS) is available as a Lambda event source. This has been a highly-requested feature for a while, and the AWS team took the time to make sure it was implemented correctly. Why the fuss about SQS? In my opinion, SQS is the third leg in a trifecta of core integrations for Lambda. The first leg was API Gateway, which allowed developers to quickly deploy REST APIs and other HTTP-accessible business logic. The second leg was S triggers, which let you asynchronously process data blobs, whether it be log file processing or the canonical example of creating image thumbnails. The third and final leg is message processing via SQS, allowing you to offload tasks that are time- or resource-intensive into a background process for a faster, more resilient system. The SQS integration is also a great on-ramp for users looking to test the waters with Lambda and Serverless. If you manage a fleet of EC worker instances that are processing from SQS queues, porting that logic to Lambda should be pretty straight-forward. Quit thinking about auto-scaling, resource utilization, and reserved instances and get back to focusing on your business logic. What we'll cover in this post In this post, I'll cover a few practical notes about working with SQS and Lambda. In particular, this post discusses: Using SQS with the Serverless Framework. Go here if you just want the TL;DR version. Batch Size and Error Handling with the SQS integration. Protecting your downstream services with concurrency control. Let's dig in! Using SQS with the Serverless Framework As of the v. release of the Serverless Framework, SQS is a supported event source! Using the SQS integration is pretty straightforward. You'll register an event of type `sqs` and provide the ARN of your SQS queue:  Note that the queue must existthe Framework will not create it for you. If you want to create the SQS queue within your service, you can do that in the resources block of your `serverless.yml`. You can then reference that resource in your `sqs` event as follows:  Here, we've created an SQS queue called \"MyQueue\", and we've referenced it in our `sqs` event for the `hello` function. Batch Size and Error Handling with the SQS integration When setting up the SQS event integration, you may configure a `batchSize` property. This specifies the _maximum_ number of SQS messages that AWS will send to your Lambda function on a single trigger. This is an interesting and powerful property, but you need to be careful to make sure it's properly tuned to fit your needs. Sending batches of messages into a single invocation can reduce your costs and speed up your processing of messages. If your Lambda function needs to do a costly operation each time it spins up, such as initializing a database connection or downloading a dataset to enrich your messages, you can save time by processing multiple messages in a single batch. You're effectively amortizing that costly operation over a larger number of messages, rather than paying the cost for each message that comes through. However, you need to understand how batches work with the SQS integration. SQS is a traditional messaging system. Messages are placed into a queue for processing. A worker will read a message off a queue and work it. If the work is successful, the worker will remove the message from the queue and retrieve a new message for processing. With the SQS / Lambda integration, a batch of messages succeeds or fails together. This is an important point. Let's say you have your `batchSize` set to messages, which is the default. If your function is invoked with messages, and your function returns an error while processing the th message, all messages will remain in the queue to be processed by a different Lambda function. AWS will only delete the messages from the queue if your function returned successfully without any errors. If it's possible that one of your messages could fail with others succeed, you need to plan for resiliency in your architecture. You could handle this a few different ways, including: Using a `batchSize` of , so that messages succeed or fail on their own. Making sure your processing is idempotent, so reprocessing a message isn't harmful, outside of the extra processing cost. Handle errors within your function code, perhaps by catching them and sending the message to a dead letter queue for further processing. Calling the DeleteMessage API manually within your function after successfully processing a message. The approach you choose depends on the needs of your architecture. Protecting your downstream services with concurrency control One of the common architectural reasons for using a queue is to limit the pressure on a different part of your architecture. This could mean preventing overloading a database or avoiding rate-limits on a third-party API when processing a large batch of messages. The combination of Lambda's auto-scaling nature plus a large volume of messages in your SQS queue could lead to some serious issues with your downstream services. This is where Lambda's concurrency controls are useful. With concurrency controls, you can specify the maximum number of instances of a function you have running at a particular time. For example, imagine your SQS processing logic needs to connect to a database, and you want to limit your workers to have no more than open connections to your database at a time. Without concurrency control, a large new batch of messages could easily overwhelm your database. With concurrency control, you can set proper limits to keep your architecture up. With the Serverless Framework, you can set concurrency control with the `reservedConcurrency` property on a particular function. For our example above, we could limit our function to no more than concurrent invocation as follows: ```yml serverless.yml functions: hello: handler: handler.hello reservedConcurrency: ",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f24",
      "title": "Build and deploy a Serverless Node.js app powered by Cosmic JS",
      "content": "Serverless application development is growing in popularity. And it's no surprise. Build apps, not infrastructure is an appealing proposition. With serverless, you don't have to manage server infrastructure, you get infinite scalability, and you never have to pay for idle server time. And the Serverless Framework does one better: it further abstracts building serverless apps into a delightful developer experience. It helps you get a cloud function-powered app up and running, fast. In this tutorial, I'm going to show you how to get started with the Cosmic JS Serverless Starter app in a matter of minutes. And as a bonus, I'll show you the steps to transform any Node.js app into an infinitely scalable Node.js Serverless App! Let's get started. TL;DR Check out the Cosmic JS Serverless Starter on GitHub. Install the Cosmic JS Serverless Starter To install the Cosmic JS Serverless Starter, run the following commands:  These commands do the following: Install the Cosmic CLI Login to your Cosmic JS Account (you may need to create a new Bucket, if so, it'll walk you through this ) Download the Serverless Starter and import the demo content to your selected Bucket. Once it's downloaded, you'll be prompted to start the app with the following commands:  !Serverless Node.js App Get ready for lift off Now that we have our application running locally, let's get ready to launch it to AWS cloud. First, we'll need to connect our AWS credentials. (Here's how you can find them.) After you've retrieved your AWS keys, run the following commands:  We're all set to launch our Serverless Starter to AWS. Run the following command to log in to the Serverless service and deploy your app:  The `serverless deploy` command does all the heavy lifting for us. It performs some magic behind the scenes with our AWS account, then once our app is available, it will provide the endpoints to find the deployed app. Adding the environment variable, `COSMIC_BUCKET`, connects our deployed app to our Cosmic JS Bucket via the Cosmic JS API. This enables us to manage content for our newly deployed serverless app from the Cosmic JS Admin Dashboard. !Serverless Output From the output above, you can see the available endpoints where our Serverless app is now deployed. Make any Node.js app Serverless-ready So, we ran a few commands and pushed the Cosmic JS Serverless Starter app to AWS. Cool. But let's go a bit deeper. We're going to take a look at () how we transformed the Node Starter app, and () how we can convert almost any Node.js app into a serverless app. Edit `serverless.yml` To transform the Node Starter to a Serverless app, we had to create a serverless.yml file and set the config to handle dynamic routes. This is accomplished in the `function > app > events` area to indicate the dynamic http events. It looks like this:  Edit `app.js` Next, we needed to edit the entry point for the app, the `app.js` file, to export the main handler function. We also included the serverless-http node module to wrap our Express app for Serverless use. Here's the `app.js` file converted to serverless:  In Conclusion With a few edits, you can quickly transform almost any Node.js app into a serverless app. The Serverless Framework abstracts the complexity of AWS account configuration, and gives you a delightful development experience. Add Cosmic JS-powered content, and you have an infinitely scalable serverless app with content managed via the Cosmic JS API. No servers to manage, no CMS to maintain. Building apps, not infrastructure, is the future! If you have any comments or questions about building serverless apps with Cosmic JS, reach out to us on Twitter and join the conversation with the Cosmic JS Community on Slack.",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f26",
      "title": "Get in where you fit in: inclusion and diversity in tech",
      "content": "Along with all the other companies in the Lightspeed portfolio, we at Serverless recently signed an agreement to focus on diversity and inclusion. But even before signing that agreement, diversity and inclusion has been incredibly important to me, and to the entire operations team here at Serverless. If youre thinking about ways to improve your own company diversity, heres my own experience entering the tech industry as both a woman and person of color, and what I, and we, at Serverless are trying to do to make things better. I think this is an incredibly important time for organizations to be educating themselves about why diversity matters. Consider this your starter kit, straight from me. My own experience: why diversity matters I am not only a woman in tech, but a first generation Korean woman born to immigrant Korean parents. Growing up, there was a lot of cultural bridging I had to do between my Korean side and American side. I involuntarily grew up speaking languages because of the community I was around most of the time. At home I spoke Korean with my parents, at school I spoke English, and my free time was spent hanging out with a large Mexican friend group, where I learned and spoke Spanish with them and their families. Career-wise, I came from the fashion industry. For those who dont know or cant imagine, fashion had a drastically different demographic than the tech industrylots of women and gay men who were incredibly driven, cultured, open minded and diverse in their own personalities. All this to say, multiculturalism has been a big part of my life. So when I joined Serverless and started going to tech functions all over the Bay Area, the most shocking thing to me about the tech industry (even though I knew it already) was how similar everybody seemed. As a woman and person of color, I felt isolated. Adjusting to this new paradigm I saw from my time in the fashion industry how the incredible diversity of opinion there actually fostered more creativity and innovation. And while I was in fashion, I always felt like my ideas and feedback were taken as seriously as anyone elses. In tech, however, I felt like I had to prove myself to be taken seriously. For example, I come from fashion and I like to look on point. I dress up when I come to work; I dont wear t-shirts or casual clothing like most colleagues in this field do. Ive found that, because of the way I dress, its easy for some people think Im too much or just seem out of place. They take me less seriously, and Ive had to work double hard to fit in. But nothing stops me from understanding how things function. Another example of stereotyping: a fellow attendee at a tech event started speaking Chinese to me and I told him I was Korean. He still went on speaking Chinese, and thought I would be impressed. The entire situation was uncomfortable. I told him I spoke German, Korean and Spanish but no Chinese. When he wouldnt listen, I asked point blank in German, Kannst du mich verstehen? (Can you understand me?) Things got quiet. Recognize your own bias Tech is supposed to be one of the most innovative, disruptive industries around. Which means you have to be open-minded to thrive. That doesnt stop at product thinking; it has to extend into the way you think about people. Stop yourself often and think: why do I think that, why did I do that? Is it correct, or is it a default reaction from my point of view? The key to really opening up conversations around implicit bias is that you have to be vulnerable. You have to talk about your personal experience, while leaving yourself totally open to soaking in others experiences. Also, ask questions. Dont EVER feel like any question is stupid. Ive recently had a few white friends ask me questions about my race and identity, and it was incredibly refreshing. They admitted that they will never know what its like to experience the world from my point of view, so they wanted to learn more. It was as simple as that. The data behind implicit bias What youve seen up until now is one personal story from one person. But if you want to see how small acts of implicit bias add up to create large-scale bias, Facebook actually has a fantastic training on it, in which they cite an array of studies that show how even a % bias in a certain direction can have a huge impact on the entire organization in only a handful of years. Furthermore, people who self-reported that they were aware of their own bias did worse in bias testing. Yeah, Dunning-Kruger will get you every time. The numbers tell a stark and compelling story, and they offer some great advice for how to overcome implicit bias during the interview process. Everyone should watch this training. Its free to the public, and it will change the way you think about others, and change the way you see potential in others. Some statistics, because why not: There are half as many African Americans and Hispanics in tech as in the rest of the private sector % of tech executives are white More than % of employees at Apple and Google are still white Unfair treatment and turnover costs companies $ billion per year Diversity efforts could net the IT industry an extra $ billion in revenue each year (Source: Tech Repubic) Its a hard problem. But we can work at addressing it. How Im integrating myself into tech And it goes both ways. There are things Im actively doing, too, to learn more about the tech world, and especially Serverless. Im teaching myself to code, which is frankly something Im scared to do. Its intimidating. But this is the best way to learn where people in this industry are coming from and what their day-to-day is like. I know from my previous experience that learning new languages opened up so much more of the truth, and enabled me to create space for empathy. I do my best to inject a culture of empathy at Serverless because I know what its like to be misunderstood. At the end of the day, dont we all just want to be comfortable and understood in our own skin? I ask that you all make an effort to go beyond your reservations and fears. Engage with different types of people and try to start learning their languages. Learn new social code, ask questions if you dont know the answers, and always leave yourself vulnerable. Ive been asking tons of questions on why my Python or Javascript code isnt running properly and Ive had nothing but amazing and patient support from my peers. Youd be surprised at the reception youd receive when asking for help in understanding something you dont have answers to. Where we, and you, fit in We are working hard on these issues here at Serverless. We are scaling at a fast pace and excited to see what kind of great work comes out of our team. We are doing our best to ensure our group is well-rounded and that many types of people are present. We believe that our successes will be more impactful when we have a strong team that is empathetic, collaborative, and open to diverse ideas and opinions. We want to discuss uncomfortable issues and find solutions together. Just as people code in various languages to achieve the best results, we need to be open to learning and engaging with the multiple types of people that are underrepresented; you may be overlooking a great potential language that could open up your path to success and beyond. If diversity is something you're working on, let's connect! All photos taken from the amazing WOCintech project",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f28",
      "title": "Unit testing for Node.js Serverless projects with Jest",
      "content": "Have you recently found yourself wondering how to write unit tests for your Serverless project? Well, good news. I'm here to talk about just that. As the size and complexity of your Serverless project grows, automated testing becomes the key to creating clean abstractions, getting fast feedback, and maintaining the sanity of your team. In this post, we will cover the basics of creating unit tests for Node.js projects using the Serverless Framework. We will also show you how to run those tests on CI and provide some tips on writing good unit tests for your Serverless project. Note: The example project is available on GitHub here. We'll also cover some resources to check out for next steps at the bottom of the post. Ready? Let's go. Choosing your test framework I'm a huge Jest fan when it comes to testing frameworks. Why? ) there's zero configuration needed to get started ) it includes a good test runner ) has built-in functionality for mocks, stubs, and spies ) and has built-in code coverage reporting To add Jest to your project, run `yarn add --dev jest`, and you should be good to go. Setting up the project We decided to start with a fresh copy of the `aws-node-simple-http-endpoint` example in this section:  The default endpoint in that example is quite simple, which is great for our case: ``` handler.js 'use strict'; module.exports.endpoint = (event, context, callback) => { const response = { statusCode: , body: JSON.stringify({ message: `Hello, the current time is ${new Date().toTimeString()}.`, }), }; callback(null, response); };  handler.js function getLocalGreeting(language) { switch(language) { case \"en\": return \"Hello!\"; case \"es\": return \"Hola!\"; case \"ru\": return \"!\"; default: return \"\"; } } function pickLocale() { const languages = [\"en\", \"es\", \"cn\", \"fr\", \"ru\"]; We miss Python's random.choice return languages [Math.floor(Math.random() languages.length)]; }  handler.js module.exports.endpoint = (event, context, callback) => { const response = { statusCode: , body: JSON.stringify({ message: getLocalGreeting(pickLocale()), }), }; callback(null, response); };  $ sls invoke local -f localGreeting { \"statusCode\": , \"body\": \"{\\\"message\\\":\\\"!\\\"}\" } ``` Creating tests To make sure that our local greeting generation is working as expected, we decided to create a Jest unit test for the `getLocalGreeting` function. For your tests to be picked up by Jest automatically, they either need to be placed in the `__tests__` directory of your project, or include the word `test` or `spec` in the filename. This can be configured easily in Jest options if you prefer a different layout. Let's go ahead and creat the `__tests__` directory and add a `handler.test.js` file in it. The overall structure looks like this:  To be able to reference functions from `handler.js` in the test file, we need to export the function we're about to test:  In the handler test file, we load the `handler.js` file and add two assertions for the local greeting function. One of those assertions is explicitly incorrect, so that we check if errors actually display correctly:  Running tests We can run tests for the first time by running `yarn run jest` in the root directory of the project with no parameters supplied. We should get the expected failure back:  After replacing `` with `` in the tests assertion, all the tests pass:  Nice! Running unit tests on CI Getting the tests running on CI is straightforwardjust invoke `jest` the same way you would in development. On CI services, you also generally need to add extra configuration for things like installing and caching of dependencies, and execution controls. We tested our function on CircleCI with the following config:  Test folder layout As more test cases get added to the project, it is important to keep a consistent file and folder structure within our `__tests__` folder. I advocate keeping the structure in the test directory as close as possible to the application file layout. If we were to extract the `getLocalGreeting` function into its own `greeting.js` file, we would also extract the tests for it into `__tests__/greeting.test.js`. Had we decided to add folders in our project specific to models, views, or controllers, we would also make sure to place the tests accordingly:  Unit test recommendations for Serverless projects To make sure that the unit tests for your Serverless project are adding value to your development process and not being an annoyance for your team, I recommend following the unit testing best practices. Keep the unit tests fast and constrained The best unit tests are the ones that cover a specific component of the system. This ensures that each individual test runs fast enough to be executed on developer machines during the development process and on CI. Use mocking where necessary Mocking is a powerful tool (which Jest provides good functionality for). For parts of your Serverless project that interact with external databases like DynamoDB or third-party systems like Stripe, I recommend mocking out the external requests to ensure that your test suite does not depend on the third-party services being available and to reduce the latency of the test runs. Keep in mind that mocking out an external API might hide the changes in that external API. Make sure to regularly validate the mocks against the recent third-party APIs if you decide to mock out important parts of the project in tests. Unit tests are not a full test suite by themselves Unit tests are best used as guidelines for adequate component design and to validate the correctness of individual components. In your Serverless project, however, you will likely need to check whether different components of your project work correctly together via integration tests. I'd recommend using Jest for integration testing as well. Conclusion Unit tests are only one part of a successful testing strategy. In addition to unit tests, writing integration tests, end-to-end tests, and performing manual validation will help you ensure the quality of your Serverless applications. In this post, we talked about why pick Jest as the unit testing framework for your Serverless projects, how to write tests, how to structure the test files and how to execute tests on CI. I hope this will help you get started with testing your Serverless projects! I'd also highly encourage you to check out the resources below, in order to learn more about Jest and other JavaScript testing frameworks. And while you're at it, share your own testing tips with the community! Please drop a comment with any feedbackI'd love to hear from you! Resources - Serverless HTTP endpoint example with the Jest test - Jest documentation - CircleCI documentation reference",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f2a",
      "title": "Serverless Community Survey: huge growth in serverless usage",
      "content": "We at Serverless Inc ran an open-to-all serverless community survey. Our goal was to tease out some cool insights that we could then share back with the community. Were pretty beside ourselves with how the space has grown since our first survey in . Key takeaways: wow at that spike in mission critical serverless workloads, multi-cloud usage is on the rise, and the biggest cha Well, we wont give it all away. ;) The full report is below; check it out! Quick disclaimer: This survey was created and (primarily) distributed by us at Serverless Inc. While we tried to cast the net as widely as we could, the majority of people who answered the survey were probably Serverless Framework users, and as such you can expect some bias in the results. Just want to make that clear! We will also reference our survey results in this report, and the same caveat applies there. Now, on to the data. Serverless is moving from fringe to critical workloads In , % of respondents said they were using serverless at work in some capacity. In , % of respondents answered that they used it at work (nearly double!), and over half said that serverless was critical for the work they did at their jobs: We hear a lot that companies who had never used public cloud before got their start with serverless. And indeed, almost a quarter of respondents said that they had limited to zero public cloud experience before serverless: Even more significant% of the people who had limited/no cloud experience prior to serverless now say that serverless is either critical or important to the job that they do. Thats huge, and goes to show how serverless is rapidly changing the ways people develop software today: Multi-cloud is growing We asked respondents which cloud providers they were using, and left it open-ended so they could list as many as they wanted. What we found was that multi-cloud scenarios are on the rise: About % of survey-takers were using or more cloud providers; nearly double the number from last year. More people are facing challenges around operations We asked people what the biggest roadblocks were for them when they considered using serverless architectures. Three of the top four answers had to do with operationalizing serverlessmanaging and enforcing best practices, lack of tooling, & lack of knowledge on their team: When we asked people a subtly different question about the biggest challenges they faced with serverless, the answers followed right in step with adoption roadblocks. Notably, the top three pain points were debugging, monitoring, & testing; all things that clearly point back to a lack of tooling: The serverless enterprise is growing This year, just over % of respondents said they worked in a company with more than people. Thats a % increase in a single year, in a sector of companies not typically known to embrace rapid change: Whats more, % of enterprise respondents said that serverless was either critical or important for the work they did at their jobs. These arent just light workloads or side projects. That daily serverless development life Go has officially edged past Java in terms of usage: Though, fun fact: while Node.js is the most popular runtime overall, it has a bit less sway in the enterprise, and % of respondents (x the overall number) in + employee companies are using Java: As for what people are using serverless to do: Things people have built with serverless tech We got some great answers to this, and its hard to publish them all individually, so we made a nice little word cloud: Those instances of company you see are people telling us they built their companies on serverless. The fly is people saying on the fly, as in, they built an app that would let them (e.g.) process images on the fly. And a surprising number of people are generating or manipulating PDFs? Here is just a sample of some great answers we got here, covering everything from critical workflows to fun side projects: A complete data pipeline from MongoDB to Redshift that also handles data transformations High-availability alerting platform ETL tool extracting and translating exif data from photos into a heat map Member management system using facial-recognition (and then similarly) A serverless front-door controller to allow access into a building from a Slack slash command Customer authentication and purchasing system, Dynamics CRM integration Fermentation temperature monitoring with a Raspberry Pi Content publishing pipeline. I'm also currently working on an IoT platform which automates car parks using machine learning. (Wow, we applaud your breadth!) A developer portal for third-party components for our ETL platform Media transcoding service integrated with automated transcription, translation, and captioning services A todo app that lets me procrastinate better (Thanks for speaking the truth for all of us, anonymous survey-taker.) Key takeaways from the data - Serverless is growing, and fast. Several key adoption metrics are x what they were last year. And not just with smaller companies; the enterprise is adopting serverless technologies for critical workloads just as rapidly. - Operationalizing serverless is the biggest obstacle to wider serverless adoption right now. - Many more people are using multiple cloud providers than there were last year, which only increases the importance of projects such as CloudEvents and other initiatives that enable vendor choice. - For a lot of people, serverless is their first exposure to the public cloud. Meaning: serverless is already shifting the ways developers work and improving accessibility to the cloud.",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f2c",
      "title": "Efficient APIs with GraphQL and Serverless",
      "content": "GraphQL can be a tool for building enlightened APIs, but it can also be a source of mystery for developers accustomed to REST. In this post, I'll talk about the motivations that might lead you to choose GraphQL, and how to serve a GraphQL API that will let you really take advantage of its benefits. Here's what we'll be covering: - REST API design - A GraphQL approach - Making your GraphQL endpoint serverless REST API design First, lets talk about some situations that arise in REST APIs; this directly segues into when and why you would want to use GraphQL. Let's say you have a REST resource to represent the products that your business offers:  And also a resource for orders by customers:  The order refers to the product by its ID. If the client needs to display the product information in the context of the order, it makes two requests: one to get the order record, and one to get the details for the product specified on the order. You might try to improve this API in a couple ways. One would be to offer a way to retrieve the product details directly from the order number, so that you can make the two requests in parallel.  If you consistently need all the product information with the order, you might just decide to include the product information in the order resource:  Now youve solved the issue of having to make multiple requests, but youve polluted the order object with properties from another resource, which makes it harder to use and evolve. You can fix this by keeping all product information under a single property:  That looks pretty good! It has the benefit that any code that was written to work with the `/products` response will also work with \"product\" property from `/orders`. The drawback of including the product information on the order is how it affects the backend. This may require a more expensive multi-table query with SQL, or a second query under NoSQL, or else a denormalized table that records product information with the order. It also increases the size of the response body, which can become a real problem as the REST API gets more mature and the response includes more information for different purposes. A slightly inelegant solution is to allow the request to flag whether it wants the product information:  This is not very clean, but it does allow the backend to avoid doing the extra work when product information isnt necessary, at the cost of increased code complexity. If you repeat this design struggle many times over the lifetime of your API, you may end up with a lot of flags for different properties. If you reach this point in your API design, then congratulations! You have partially re-invented GraphQL. Enter GraphQL Lets see how a GraphQL API answers the same questions. Youd begin by creating types for products and orders. You dont start the API by tying things together with foreign keys, as you did with REST. Instead, the order type contains a product field as we eventually decided on in the above example:  You create query fields to get orders and products:  If the client needs to get an order and the relevant product details, you only need a single query. Since the query contains an exact statement of all the properties that it expects, the service knows by design whether it needs to fetch product information. This allows you to write a backend that minimizes database and compute time. For example, suppose you want to know the customer name, delivery address, order quantity, product name, and product price:  This query would return only the requested properties:  Making your GraphQL endpoint serverless There are even deeper advantages to having a serverless GraphQL endpoint, which you can read more about here. The tl;dr is that when you use GraphQL, you are relying on only one HTTP endpoint; and when you have one HTTP endpoint to connect all your clients to your backend services, you want that endpoint to be performant, reliable, and auto-scaling. Building a GraphQL endpoint with the Serverless Framework So, how do we build this with the Serverless Framework? Were going to target AWS Lambda with Node in this example, and the code should be easily adaptable to other FaaS providers. You can download the code for this example here. Using the GraphQL reference implementation in JS, we can easily create our GraphQL schema from the type declarations. First import the utilities we need from the `graphql` library:  Now you can use GraphQL schema language to specify the schema: ```js const schema = buildSchema(` type Product { id: String! name: String! price: String! } type Order { id: String! customerName: String! deliveryAddress: String! product: Product! quantity: Int! } type Query { product(id: String!): Product order(id: String!): Order } `) ``` Next, we create resolvers so that queries can access our data. This is also where we make sure the resolver isnt doing any more work than necessary. The `graphql` library is very flexible. Resolvers can exist for individual fields, and a resolver can either be a constant value, a function, a promise, or an asynchronous function. Functions have access to any arguments for the field via a single object parameter. We want the database record for the product information to be retrieved only when requested, so we make the resolver for that field a function:  The methods `database.products.get()` and `database.orders.get()` are both asynchronous functions, returning promises. The resolver for product simply calls through to the database. You do not need to worry about manually removing extraneous fields, since graphql-js does that for you. The resolver for order is more complex. It uses async/await syntax to fetch the order record before returning. This allows us to get the productId for use in the resolver for the product field. Since the resolver for the product field is a function, it wont be invoked unless the product field is actually included in the query. All that remains is to create a handler for Lambda. Using the newer asynchronous syntax introduced by Node for Lambda, this is very simple.  Since all of the set-up logic for the GraphQL schema is outside of the handler, this will only be executed when Lambda needs to spin up a new instance to serve requests. To enable us to query by POST request, we have to include the following in `serverless.yml`:  Thats it. After a quick `sls deploy`, we can curl our new GraphQL endpoint to test the query:   Conclusion Youve now got a working GraphQL endpoint built with Serverless that scales automatically with increased traffic! In this example, we went with a single-Lambda approach. If you want infrastructural microservices, you can also use the flexibility of resolvers to have a primary Lambda that invokes other lambdas to resolve different query fields. If you want a more in-depth solution that uses GraphQL from top to bottom, you can use schema stitching to combine multiple GraphQL APIs into one. Other Serverless + GraphQL resources - How to make a Serverless GraphQL API using Lambda and DynamoDB - Running a scalable & reliable GraphQL endpoint with Serverless",
      "__v": 0
    },
    {
      "_id": "64e08922b72e199dda603f2e",
      "title": "Using TensorFlow and the Serverless Framework for deep learning and image recognition",
      "content": "Deep and machine learning is becoming essential for a lot of businesses, be it for internal projects or external ones. The data-driven approach allows companies to build analytics tools based on their data, without constructing complicated deterministic algorithms. Deep learning allows them to use more raw data than a machine learning approach, making it applicable to a larger number of use cases. Also, by using pre-trained neural networks, companies can start using state of the art applications like image captioning, segmentation and text analysiswithout significant investment into data science team. But one of the main issues companies face with deep/machine learning is finding the right way to deploy these models. I wholeheartedly recommend a serverless approach. Why? Because serverless provides a cheap, scalable and reliable architecture for deep learning models. In this post, well cover how to build your first deep learning API using the Serverless Framework, TensorFlow, AWS Lambda and API Gateway. We will cover the following: - Using serverless for deep learning - standard ways of deploying deep learning applications, and how a serverless approach can be beneficial. - Hello world code - a basic Lambda function with only lines of code. There is no API here, well start with the simplest possible example. - Code decomposition - looking through the configuration file, and the python code for handling the model, to understand how the whole example works. - API example - get a working API for image recognition on top of our example. If you want to skip the background about what TensorFlow is and why youd want to use serverless for machine learning, the actual example starts here. Why Serverless + TensorFlow? First of all, lets briefly cover what TensorFlow is: an open source library that allows developers to easily create, train and deploy neural networks. Its currently the most popular framework for deep learning, and is adored by both novices and experts. Currently, the way to deploy pre-trained TensorFlow model is to use a cluster of instances. So to make deep learning API, we would need stack like this: (Image from AWS.) The main pain points in this infrastructure is that: - you have to manage the cluster - its size, type and logic for scaling - you have to pay for unused server power - you have to manage the container logic - logging, handling of multiple requests, etc With AWS Lambda, we can make the stack significantly easier and use simpler architecture: The difference in both approaches First of all, a serverless serverless approach is very scalable. It can scale up to k concurrent requests without writing any additional logic. Its perfect for handling random high loads, as it doesnt take any additional time to scale. Second, you dont have to pay for unused server time. Serverless architectures have pay-as-you-go model. Meaning, if you have k requests per month, you will only pay for k requests. And not only does it make pricing completely transparent, its just a lot cheaper. For the example TensorFlow model well cover in this post, it costs $ for about k requests. A similar cluster would cost a lot more, and youd only achieve pricing parity once you hit M requests. Third, infrastructure itself becomes a lot easier. You dont have to handle Docker containers, logic for multiple requests, or cluster orchestration. Bottom line: you dont have to hire someone to do devops for this, as any backend developer can easily handle it. As well see in a minute, deploying a serverless deep/machine learning infrastructure can be done with as little as lines of code. That said, when wouldnt you go with a serverless approach? There are some limitations to be aware of: - Lambda has strict limits in terms of processing time and used memory, youll want to make sure you wont be hitting those - As mentioned above, clusters are more cost effective after a certain number of requests. In cases where you dont have peak loads and the number of requests is really high (I mean M per month high), a cluster will actually save you money. - Lambda has a small, but certain, startup time. TensorFlow also has to download the model from S to start up. For the example in this post, a cold execution will take . seconds and a warm execution will take seconds. It may not be critical for some applications, but if you are focused on real-time execution then a cluster will be more responsive. The basic line example Lets get started with our serverless deep learning API! For this example, Im using a pretty popular application of neural networks: image recognition. Our application will take an image as input, and return a description of the object in it. These kinds of applications are commonly used to filter visual content or classify stacks of images in certain groups. Our app will try to recognize this picture of a panda: Note: The model and example are also available here. Well use the following stack: - API Gateway for managing requests - AWS Lambda for processing - Serverless framework for handling deployment and configuration Hello world code To get started, youll need to have the Serverless Framework installed. Create an empty folder and run following commands in the CLI:  Youll receive the following response:  As you can see, our application successfully recognized this picture of a panda (. score). Thats it. Youve just successfully deployed to AWS Lambda with TensorFlow, using the Inception-v model for image recognition! Code decomposition - breaking down the model Lets start with serverless YAML file. Nothing uncommon herewere using a pretty standard deployment method:  To test the link, we can just open it in the browser: https://.execute-api.us-east-.amazonaws.com/dev/handler Or run curl:  We will receive:  Conclusion Weve created a TensorFlow endpoint on AWS Lambda via the Serverless Framework. Setting everything up was extremely easy, and saved us a lot of time over the more traditional approach. By modifying the serverless YAML file, you can connect SQS and, say, create a deep learning pipeline, or even connect it to a chatbot via AWS Lex. As a hobby, I port a lot of libraries to make the serverless friendly. You can look at them here. They all have an MIT license, so feel free to modify and use them for your project. The libraries include the following examples: - Machine learning libraries (Scikit, LightGBM) - Computer vision libraries (Skimage, OpenCV, PIL) - OCR libraries (Tesseract) - NLP libraries (Spacy) - Web scraping libraries (Selenium, PhantomJS, lxml) - Load testing libraries (WRK, pyrestest) Im excited to see how others are using serverless to empower their development. Feel free to drop me a line in the comments, and happy developing!",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f30",
      "title": "Google Cloud Functions goes GA: what it means for Serverless",
      "content": "Yesterday at Google Cloud Next, Google announced general availability for their serverless functions offering, Google Cloud Functions. This is a solid step forward in making it easier for developers to use Googles innovative services with minimal friction. Its also a major step forward for FaaSnow all four major cloud providers offer FaaS compute. Serverless, Inc. has a Component available to use for Google Cloud Functions _right now_. The Serverless Framework has supported Google Cloud Functions for over a year now, and we are already moving to release an update to work with Googles new APIs. Heres everything you, as a serverless developer, need to know about GCF and its impact on serverless development. Why it matters for Google Google is one of the big four public cloud providers, and while they currently lag behind in cloud market share, they have some amazing technology and the potential to grow significantly. For Google and their Cloud Platform, valuable technology has never been the challenge; theyve innovated technologies like Kubernetes, TensorFlow, and BigTable. Their challenge, and often the challenge in offering a compelling cloud platform in general, is presenting developers with a unified platform of compelling and easy-to-adopt services. Google Cloud Functions is a good step in that direction. Working with functions is something that is intuitive for developers, and offers a great onboarding story for to the wider Google Cloud community. Want to take advantage of Googles machine learning or networking services? Just write a function that taps into those services. While the integration story with Google Cloud Functions and the rest of their services is certainly not complete, this is good progress. If youre a developer who wants to take advantage of more Google services, this announcement should mean you are very happy right now. Why it matters for the Serverless Movement With the general availability of Google Cloud Functions, all four major public cloud providers now offer a serverless FaaS compute option. This helps solidify FaaS as a preferred building block for cloud applications, and is a hopeful step toward having interoperability between these different FaaS compute options (see: CloudEvents). Why this matters for Serverless Framework users The Serverless Framework is cloud-agnostic development framework that makes it easy for developers to build serverless applications on any FaaS provider. While the Framework has had an integration with Google Cloud Functions for over a year, we will soon be releasing an update to work with Googles updated APIs. Weve also authored a Serverless Component that makes it simple and easy to deploy a Google Cloud Function. Note: If you havent already checked out the Serverless Components project, its aimed at offering and easy, open, and composable packaging mechanisms for serverless logic. In sum If youre a developer who likes Googles innovative suite of managed services, you should be really happy about Google Cloud Functions. This will give you the power to utilize those services much more easily. If youre enthused about the serverless movement and/or a user of the Serverless Framework, you should be excited to see that all four major cloud providers are embracing FaaS and pushing serverless compute forward. Other posts about Google Cloud - Building an image recognition endpoint with Serverless and Google Cloud Functions",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f32",
      "title": "The new Serverless Platform Beta: everything teams need to operationalize serverless development",
      "content": "Today, were excited to announce the Serverless Platform Betaa single toolkit that provides everything teams need to operationalize serverless development. We plan to expand active development on the Serverless Platform with the $M Series A funding we raised from Lightspeed Venture Partners and Trinity Ventures, which we are also announcing today. Read on for a full feature breakdown, or watch the minute video: Serverless adoption is growing, and fast In , we created a project called the Serverless Framework. Our mission was to make serverless development easy. By leveraging new cloud infrastructure that auto-scales and charges only when its used, we believed developers could build software with remarkably low overhead. At that time, we had no idea what the Framework would become. A community rallied behind it and the broader serverless movement, contributing to the open source core, offering their opinions and insights, and sharing their passion. By , companies like Coca-Cola were uttering two simple words: \"Serverless first\". Every new greenfield project or innovation effort within their organization was to be built on a serverless architecture. Organizations that had never embraced the public cloud before saw serverless as their gateway in; they too began to adopt serverless. Serverless moved from a fringe buzzword to a mainstream business decision. As the adoption of serverless has grown, so too have the needs of todays serverless teams. In addition to tooling that simplifies the development of serverless applications, these teams need tools to simplify operations across their teams and entire organization. They need logs, they need team collaboration, they need ways to integrate with legacy systems, and so much more. This is exactly what we built the Serverless Platform were announcing today to do. The Serverless Platform lets you build, operate & integrate The Serverless Framework solves several problems in the build phase, and has soared in robustness thanks to a passionate open-source community. Now, the Serverless Platform extends its focus to two other phases of serverless application lifecycle management: operating and integrating. Operating with the Serverless Dashboard When you ask a serverless developer what their top three pain points are, they will tell you unequivocally: debugging, monitoring and testing. There are ways to build serverless applications, but no good way to operate them. This is exactly what the Dashboard does. It gives you architectural views of your serverless applications, exposes logs for metrics, alarms and debugging, and lets you collaborate easily with teammates on development. The Dashboard, just like our Framework, is vendor-agnostic. Use any provider you want, and use multiple providers at once. Well expose everything in a single place. Consolidated view & oversight for your serverless applications Instead of trying to hold all the pieces of your serverless application in your head, you can see them visually in the Dashboard. Teammates and executives can have oversight over all of your serverless applications. View all of the functions, event subscriptions and resources your serverless application contains. Inspect the configuration of each function and subscription. Review all cloud infrastructure resources. Check deployment history and collaborate on services with teammates To date, working on a serverless service has put developers in a silo. There was no easy way to work on a service with others, check who deployed last and when, or to see exactly what changed. Now, you can collaborate on Framework deployments and services by inviting team members to the project: And you get easy access to the deployment history to see the deployment dates and changes: Integrating with the Event Gateway Also included in the Platform is a hosted version of the Event Gateway, a powerful event router, capable of routing event data to serverless functions and other services across clouds. Its the answer to how organizations can integrate serverless into their existing services. What is the Event Gateway? The Event Gateway lets you react to _any_ event, from anywhereany cloud provider or SaaS, containers and legacy infrastructure. With the Event Gateway, you can do things like: easily build FaaS-backed APIs, use configurable connectors to react to events from data stores like Kafka or Kinesis, and utilize multiple cloud providers in a single serverless application. Some real world examples: Kelsey Hightower from Google, in his recent CloudNativeCon keynote, ran the Event Gateway on Kubernetes, on Google Cloud; an S event on AWS was sent through the Event Gateway and routed to a Google Cloud Function: Austen Collins from Serverless, Inc., with the help of various Cloud Native Computing Foundation members, uses the Event Gateway to trigger different cloud providers: Use it to build APIs or react to custom events. Monitor events and function invocations with real-time logs Once you publish events into the Event Gateway, you can see them in the Serverless Dashboard via real-time logs: You can self-host the Event Gateway in your own AWS account, or use our hosted (serverless) option, included in the Dashboard. Try the Serverless Platform Beta If your goals are to deliver fast, increase innovation and reduce overhead, then there is no better option today than a serverless architecture. With the Serverless Platform Beta, we want to help everyone build more and manage less through serveless technologies. With our Series A funding from Lightspeed Venture Partners and Trinity Ventures, we plan to double down on our efforts to expand the features of the Serverless Platform and continue actively contributing to our Framework, Components and Event Gateway open source projects. We cant wait to hear what you think about the Serverless Platform, and especially the new observability tools it adds to the Serverless Dashboard. Try the Serverless Platform Beta here.",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f34",
      "title": "ServerlessConf  San Francisco: key takeaways for the future of serverless",
      "content": "Last updated: Aug @ :pm PT It's been amazing to see the lightning fast transformation happening in the serverless space. ServerlessConf was the conference of serverless projects. By the conference in NYC, the community had already been building a lot of projects. The new problem was tooling, and tooling discussions dominated the stage last year. So what about ServerlessConf in SF? It's been the conference of two things: () big@$$ companies talking about their large-scale, production serverless architectures; and () (from a drastically different perspective) non-engineers talking about how serverless technologies empowered them to begin developing their own apps, without a coding background. We'll be updating this live all day during day , so stay tuned! Serverless adoption and architecture in large-scale organizations The talks in this category had three main types: Serverless as the basis for rapid development Developers looking to minimize time to value will automatically gravitate toward serverless. Leslie Pajuelo from Walmart just ran a POC in which she rebuilt their high performance orchestration layer. It's a use case we hear about all the timea developer goes, \"we want to try this serverless thing,\" and they build it out with a or person team. It does so well that the organization then moves to expand usage. Testing, debugging and monitoring of production serverless apps This has been an undercurrent of serverless adoption discussions for a long time, and it's been dominating the table discussions here. Every presentation starts to touch on the ways teams at Verizon, CapitalOne and Nordstrom are handling all of their operations with a smattering of tool sets. We'd be remiss if we didn't mention that this is something we've been passionate about here at Serverless, Inc, and as such we just launched a new Serverless Platform Beta to help teams operationalize serverless across their entire organization. Structured models and practices to design and analyze Serverless architecture Rob Gruhl from Nordstrom talked about the best ways to scale serverless: Partition for horizontal scalability Embrace eventual consistency Idempotency throughput Stateless(ish) compute Understand your bottlenecks And it's easy to see why this matters. Verizon is here at ServerlessConf talking about serverless in the enterprise. There was a case study from Box. Capital One and Netflix presented on their own serverless architecture. Fender Digital (yeah, the guitar company) is all-in on serverless right now. And? They're transitioning everything to Go. And speaking of which Wow, Go was everywhere Our community survey showed that Go usage was increasing, and had already edged past Java. But even the ServerlessConf stage had several mentions of companies who were using Go, and cloud providers who were moving to adopt it. .@AzureFunctions moving to support Golang? ServerlessConf pic.twitter.com/KTmnlldJZ&mdash; Serverless (@goserverless) July , People are really thinking about security One thing we noticed in every \"here's how we're using serverless\" presentation this year, which was largely missing in previous years, was the security component. How are serverless organizations handling security, what are their best practices? Mark Nunnikhoven insistedyou're better off out of the gate with serverless security. You can't dig into a single function running on, say, Lambda, and poke into other parts of the system. And ultimately, security is about the _people_. It isn't about just securing the functions, you need to have a robust system that does what it's intended to do, and only what it's intended to do. Chris Munns built on what others had been saying to offer insight into securing Lambdas specifically. Do not use `` in your IAM policies. Dependency management is key; keep track of package dependencies and apply security updates. Don't use a VPC; putting your functions inside of a VPC provides little extra security benefit. Stop doing stupid stuff with secrets. Really great pragmatic guidance and tips from @chrismunns on writing secure lambda functions for @awscloud. serverless serverlessconf pic.twitter.com/TXdooEvYig&mdash; Tony Pujals (@tonypujals) August , DiffOps (Credit to Ben Kehoe for the 'DiffOps' term.) When the serverless movement was gaining momentum, there was a lot of buzz about how it would remove the need for DevOps. Not so, says everybody at ServerlessConf . With all of these companies giving presentations about how they're running serverless at scale, a big undercurrent has been, \"this is how we do our serverless operations.\" There are still ops. And we're all still defining and discovering what those ops are. Sam Kroonenberg was adamant that Serverless amplifies the need for good development practices. For example, you have to automate. And you need robust unit testing. Ben Kehoe from iRobot, for the record, had a fantastic in-depth talk on gaps in the serverless mesh, covering cross-service blue/green deployments. It's hard to distill in a single recap post, but everyone should watch the video when it goes live on serverlessconf.io. We'll link to it here also. Best practices all around Yochay Kiriaty of Microsoft Azure provided a great \"don't do this\" bucket list for anyone who's getting started with serverless: - Functions logic should be stateless - Functions should be idempotent - One task per function (\"do one thing\") - Functions should finish as quickly as possible - Avoid recursions - Concurrency limitations and rate limits Erica Windisch of IOpipes talked about serverless observability cornerstones. The serverless culture is about not building when you can buy, building as little as possible in general, and doing it with minimal complexity. Meaning: in a serverless world, business performance and metrics are more important than infrastructure metrics. Know your KPIs. Infrastructure performance? BORING. Great talk on observability by @ewindisch. ServerlessConf pic.twitter.com/nyrNoH&mdash; Linda Nichols (@lynnaloo) August , Jared Short and Forrest Brazeal had some advice of another type: how to successfully go home from ServerlessConf and successfully convince all your co-workers that it's a _great_ idea. - Leaders want to hear that it significantly reduces compute costs, they will be far from alone when it comes to adoption, and you can get started with few development resources - Architects want to hear that cloud lock-in is an acceptable risk (and a bit of a myth), they can trust but verify with load testing tools, serverless is \"getting more and more boring\" (read: safe) by the day - SysAdmin want to hear that they still have knobs to turn, they can partner with developers, and they can still monitor code if not infra. There are opportunities for them to start taking on more and more serverless DevOps. - Other developers want to hear that it's even faster to test in the cloud anyway, there are all kinds of cool new frameworks they can use, and they can start focusing more on writing code that matters Persona : the disrupted developer has a lot of questions &amp; might be intimidated pic.twitter.com/zGyMZIin&mdash; Serverless (@goserverless) August , Bringing software development to the non-developer developers What does that even mean? It appears there is a trend among people with non-engineering backgrounds using serverless. With a lot of the tricky administration cut out of the mix, a vast ecosystem of beginner-oriented tutorials, and pre-existing code repositories like NPM, it's never been easier for inexperienced newbies to get started with their own coding projects. Our own Andrea Passwater does Growth at Serverless, Inc, and has started to deploy her own marketing-based automation tooling. In her own words: \"Serverless significantly lowers the barrier to entry for anyone who wants to automate parts of their workflow. And if I could automate away the boring things in my life, then why wouldnt I?!\" Her first serverless app was Serverless Ipsum, but she has since moved on to other things, like a Slack bot that pings her coworkers about their blog post deadlines. And she's not the only person preaching about the newfound accessibility of coding. Keith Horwood is speaking on Stdlib, an API platform that could make developing APIs more like using Zapier. There are more engineers having open discussions about how to make coding more accessible to everyone, and more people at ServerlessConf from non-engineering backgrounds who are here to learn. .@goserverless @sogrady @monkchips theme from @Serverlessconf seems to be &quot;Knowledge workers and other non-developers are the newest Kingmakers&quot; serverless&mdash; Val Bercovici (@valb) July , That's a wrap See you all at ServerlessConf !",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f36",
      "title": "Deploy a Serverless Frontend with the Serverless Finch Plugin",
      "content": " If you've already been using the Serverless Framework for your development projects on AWS, you may have realized that deploying the frontend portion of your applications isn't built in. Fortunately for you, the Serverless Framework is extensible. Meaning, there's a plugin for that: Serverless Finch. In this post, I'll show you how you can use the Serverless Finch plugin to deploy your own static website assets in a way that is compatible with most modern frontend frameworks or less-complex static sites. Requirements First off, if you haven't already, you'll need to: install the Serverless Framework configure AWS API keys that at least give you broad S permissions (If you've done any work with the Serverless Framework, you probably already have these) install Node.js and npm (I'm using node v.., but more modern versions should work too) prepare some static website files and put them in a folder called `client/dist` in the same folder as your Serverless Framework project create an `index.html` and an `error.html` file to deploy to S as a default landing page and error page Steps and are very customizable so I'll explain more about them in the advanced features section below. -- Install, configure, deploy, done. After you've completed the initial steps above, the deployment process should be quick and easy: From within an existing Serverless Framework project directory, install the Serverless Finch plugin with `npm install --save serverless-finch`. Then, update your `serverless.yml` file by adding this:  Warning step: Make sure that you have configured an S bucket purely dedicated to storing the files for this frontend. The plugin will delete and overwrite the current contents of the bucket. Now, warning step completed, run `serverless client deploy` and see the magic happen! Remember that _by default_ the plugin looks for your already-built frontend code in `./client/dist`. Don't have an `index.html` or other website files yet? Here's a quick script you can run before `serverless client deploy` just to give yourself a barebones page and see how things work:  Because Serverless Finch can take potentially destructive actions (overwriting existing files, changing bucket policies, etc.) the plugin should now display a confirmation prompt prior to taking any action. After entering `Y` for yes you should then see something like this:  After that, congrats! You've just deployed your static website. Go ahead and visit the domain shown to confirm your site was deployed. Advanced Features Okay, so the basic tutorial was nice and all, but what if you want to set a custom distribution folder? Custom index and error documents? Or maybe use this with a custom domain? Here are a few common questions and answers for power users. Can I make the distribution folder something other than `client/dist`? Yes. Just add a distributionFolder path value to the `serverless.yml` file this:  Keep in mind that this path is relative to the location of `serverless.yml`. Can I set custom index and error documents? Yes! Just add a line to serverless.yml for each:  Keep in mind that you will need to put these inside whatever distribution folder you configure though! How does Serverless Finch integrate with custom domains? Great question! We're currently considering additional functionality to help manage custom domains. Currently, you can register your own domain on Route and then point that domain to the S bucket using an AWS A record alias (this requires the bucket to share the name with the custom domain). You can also create a CloudFront Distribution to cache the files in your S bucket, and then point your custom domain to that CloudFront distribution with a Route A record alias. The benefit of CloudFront is that you can also get a free HTTPS certificate and you can name your S bucket whatever you want. Either way, I'd recommend you determine how to setup your custom domain based on what's best for your needs, and then use Serverless Finch to update and deploy the changes inside your S bucket (which should update your site regardless of the option you choose). What other customizations can I make? Lots! For a full list of features, you can view the project on GitHub. Don't see something that you think would be a useful feature? We are happy to add in your contributions! Final Thoughts About a year ago, I adopted and republished a broken Serverless Framework plugin that turned into Serverless Finch. Over the last year, the community has added a bunch of new features with more on the way. If you're interested in becoming a contributor or maintainer please open a PR or get in touch.",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f38",
      "title": "'How to use the Serverless Event Gateway: build a REST API and react to custom events'",
      "content": " Last week, we announced the Serverless Platform Beta, a single toolkit that provides everything teams need to operationalize serverless development. The Serverless Platform is made of three componentsthe Serverless Framework for deploying applications, the Serverless Dashboard for visualizing your application, and the Event Gateway for routing events to compute. I'm going to focus on the Event Gateway for this post. We'll learn about two use cases of the Event Gateway: Building REST APIs Reacting to custom events Let's get started! Event Gateway: when you should use it We built the Event Gateway to be the centralized event hub for the serverless ecosystem. Rather than scattered point-to-point integrations across your infrastructure, all events will flow into a single Event Gateway. You can then configure the Event Gateway to react to these events as you want, in multiple different ways. For example, if a new user signs up and a `user.created` event is emitted into the Event Gateway, your marketing team may have a subscribed function that sends them a welcome email, while your development team may have a function that inserts the new user into Elasticsearch to power search functionality in your application. Getting set up with the Serverless Platform The Event Gateway is an open-source project that you can run yourself on your own infrastructure. We also provide a hosted version of the Event Gateway with the Serverless Platform Beta for quickly getting started with the Event Gateway. Note: This section assumes you have already installed the Serverless Framework and have configured your terminal with AWS credentials. If this is not true for you, check out our longer guide to getting started with the Serverless Platform. You will need to sign up for a Serverless Platform account. If you already have the Serverless Framework installed, you can create a Serverless Platform account by entering the following command into your terminal:  This will open a browser window to create a Serverless Platform account. Once your account is created, the proper credentials will be returned to your terminal for easy command-line deploys. Once you've signed up for an account, create a new application in the Dashboard. You can do this by clicking the red `+ App` button. When you create a new application, you will get an application URL in the form of `-.slsgateway.com`. This is the URL to which you will emit your Event Gateway events. Make sure you have your tenant name, app name, and application URL available as you will need them in the following sections. Creating and deploying your first service We will be using the code in the Event Gateway Getting Started repo. You can clone this service into your terminal with the following command:  Then change into your service directory and install the dependencies:  Open the `serverless.yml` file and edit the `tenant` and `app` values to be your Tenant and App name from the Serverless Platform:  Then, deploy your application:  With your application deployed, it's time to interact with your service. Using Event Gateway for a REST API One of the more popular use cases for the Event Gateway is to create a REST API. REST APIs have grown in popularity recently with the advent of single-page apps and microservices. Functions-as-a-service (FaaS) providers like AWS Lambda, Google Cloud Functions, and Azure Function have made it easier than ever to deploy compute to production. With the Event Gateway, you can connect an HTTP path and method to a serverless function for your REST API. To see how this is done, look at your `serverless.yml`. There is a `functions` block that describes the functions your service will deploy to AWS Lambda. Look at the top two functions in particular, as shown below:  For both `getUser` and `createUser`, the function's handler is defined. Then, the _events_ to which we want these functions described are listed. You can think of it like \"If this, then that\"\"If an HTTP POST request arrives on the `/users` path, then call the createUser function in the Event Gateway.\" You can test that your REST API works by issuing an HTTP request with `curl` in your terminal. The `curl` request should go to your application URL that you configured in your terminal, in the format of `https://-.slsgateway.com`. Run the following command to issue a POST request to `/users` to create a new user:  You should get a response returning the user that was created. Your AWS Lambda function will receive an event object in CloudEvents format. CloudEvents is a CNCF hosted project for describing event data in a common way. In our request above, the `event` received by our Lambda function will look similar to the following:  In the `data` object, you can see all the parameters of our HTTP request, including the `headers`, `body`, and `path`. You can use this information in the business logic of your Lambda function. You can then retrieve that user by issuing a GET request to the `/users/` path:  As you can see, the Event Gateway can easily be used to handle REST API workloads with pay-per-use, infinitely scalable serverless functions. Using the Event Gateway with custom events The REST API use case is a pattern with which most developers are familiar, and it's a great way to get started with the Event Gateway. However, my favorite part of the Event Gateway is using custom events to asynchronously react to events that are happening across my application architecture. First, let's see an example in action. Then we'll talk about why it's so important. In the previous section, we created a new user by making a POST request to our `/users` endpoint. If you look at the `createUser` function in the `handler.js`, you'll see the following snippet of code after the user is saved to the database:  In this snippet, the function is using the `emit()` function in the Event Gateway SDK to emit a `user.created` event into our Event Gateway. This event contains the details of the user that was created. Now take a look at the `functions` block of your `serverless.yml` again. The third function listed looks as follows:  The `emailUser` function is subscribed to the `user.created` event type and reacts in an `async` manner. In this simple example, this could be your on-boarding team sending a \"Welcome!\" email to your application's new user. This is a powerful pattern. One portion of your application emits events indicating that something happeneda new user was created, an IoT light went on, or a video was viewedand other parts of your application can receive those events and react as needed, by sending an email, storing to an analytics database, or alerting an on-call engineer. Event Gateway use cases We're seeing more and more of this reactive, event-driven pattern using asynchronous communication. Microservices have allowed for teams to independently develop and scale their application components, while tools like Kafka help with asynchronous communication between microservices. For those of you that are front-end engineers, you might see Event Gateway custom events as similar to Redux actions and reducers but used in your application backend. While the `user.created` event only has one subscription in this example, you can add any number of additional subscriptions. Do you want to insert this newly-created user into Elasticsearch to power discoverability in your application? Does your sales team want to get notified if the new user works for an enterprise company? You can add additional subscriptions without affecting existing subscriptions and without updating code for the original producer. The Event Gateway is designed to democratize the accessibility of these events by connecting them to serverless functions. Just like FaaS democratized compute, the Event Gateway democratizes events. How you can contribute The Event Gateway is in active development, and we're looking to make it easier to import events from your existing infrastructure, such as Kafka, Kinesis, RabbitMQ, and more. If you're interested, check out the Event Gateway repository and get involved!",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f3a",
      "title": "Deploy a REST API using Serverless, Django and Python",
      "content": "I started using Django seriously years ago, and I think it's an exceptional framework. In addition to its core strength, Django has a vast list of add-ons and supporting libraries. One of those gems is called the Django Rest Framework (or DRF for short), a library that gives developers an easy-to-use, out-of-the-box REST functionality that plugs seamlessly with Djangos ORM functionality. But what if you want to do this serverless-ly? In this post, I'll talk about deploying serverless Django apps with the Serverless Framework! Django: the SQL beast Django is powerful, but its also heavily dependent on a SQL database like MySql or Postgresql. No matter how hard I tried, I couldnt find any Django DB engine that is able to work on top of AWS DynamoDB. The solution I'm suggesting here is built from components: Using RDS. RDS is a managed SQL service, but not completely serverless; you pay for idle, and it does not scale automatically. Using a VPC. When using RDS, this is a necessary step for security. When adding VPC into the mix, your Lambda must also run inside the VPC, which leads to slow starts and a complicated configuration. But, all that is too complicated for my demo. I wanted something quick and dirty. Using SQLite SQLite here I come! Ok, so SQLite is actually not that dirty. It's the right tool for constrained environments (like mobile), or when you don't need to save a lot of data and you want to keep everything in memory. Global shared configuration might be a good idea. Have a look at the following diagram: !sqlite You have a lambda function that requires configuration in order to function, the configuration is saved in a SQLite DB located in S bucket. The Lambda pulls the SQLite on startup and does its magic. On the other end, you have a management console that does something similar, it pulls the SQLite DB, changes it and puts it back Pay attention that only a single writer is allowed here, otherwise things will get out of sync. How long will it take us to develop this? None. We can use this one from Zappa. Let's call it Serverless SQLite, or 'SSQL' for short. Let's get this thing started Lets define what we're building here: It's going to be a Django app with the appropriate Django admin for our models You should be able to log into the admin and add or change configuration. The user should be able to call a REST API created by DRF to read configuration details, something very similar to to this Python rest API. You can find all the code for the demo here. I'm assuming you already know how to create a Django app, so well skip the boring stuff and concentrate on the extra steps required to set up this app. WSGI configuration Its something small, but thats whats doing the magic. In `serverless.yml`, the wsgi configuration points to the `wsgi` app that Django exposes. SSQL configuration Under `settings.py` a configuration was added which loads the SSQL DB driver:  But when testing locally, I do not want to connect to any S bucket. It slows down the operation. Therefore, we'll make a check to verify whether we are running a Lambda environment or not. If not, then we'll load the regular SQLite driver: `IS_OFFLINE = os.environ.get(LAMBDA_TASK_ROOT) is None` I prefer not to run `sls wsgi serve`, because Django already has wonderful management CLI support. Instead, I like to run `manage.py runserver`. As part of its configuration, SSQL requires a bucket name. You can create it manually and set the name in `local_settings.py`, but note that under `serverless.yml` the Lambda function has `Get` and `Put` permissions on all S buckets. You should use your S bucket ARN instead. WhiteNoise configuration WhiteNoise allows our web app to serve its own static files, without relying on nginx, Amazon S or any other external service. Well use this library to serve our static admin files. Im not going to go over all the configuration details here, but you can feel free follow them on your own. Make sure the static files are part of the Lambda package. A tale of a missing SO While trying to make it work, I encountered a strange errorUnable to import module app: No module named _sqlite. After some digging, I found out that the Lambda environment does not contain the shared library which is required by SQLite. Luckily, Zappa has provided a compiled SO which is packaged as part of the deployment script. Deployment script Let's review the step: Collect all static files  Migrate our remote DB before code deployment  Create a default admin `root` user with password `MyPassword`  Add _sqlite.so to the mix  `sls deploy`  You have a deploy script located under `scripts` folder. So how do I prepare my environment locally? `npm install  save-dev serverless-wsgi serverless-python-requirements` Create a virtual env for your python project `pip install -r requirements.txt` Run DB migration: `./manage.py migrate` Create a super user for the management console: `./manage.py createsuperuser` Run the server locally: `./manage.py runserver` Go to http://...:/admin and log in onto the management console; add a configuration Try `curl -H Content-Type: application/json -X GET http://...:/configuration/` and see if you get the configuration back Fin We covered how to use Django with the Serverless Framework, using SQLite as our SQL database, which was served from a S bucket. I hope you enjoyed the journey! You are more than welcom to ask question below, and/or fork the repository.",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f3c",
      "title": "How to streamline your serverless workflow with WebStorm",
      "content": "If you build serverless applications, heres a serverless-specific IDE setup that will help you streamline your workflow. Over the past few years, Ive been fortunate enough to work exclusively on serverless tooling. During that time, I put together a certain workflow that integrates quite nicely with my IDE of choice: WebStorm. And I recently thought to myself, Oh, I bet lots of other serverless devs out there might find this useful! So here it is. In this post, Id like to share with you my serverless-specific IDE setup, and how it can massively accelerate your serverless workflow. We will cover: - creating a brand new serverless project - linters - Live Templates (to integrate keyboard shortcuts for the Serverless Framework) - ways to streamline testing and debugging Getting started If you havent already, install the Serverless Framework: `npm install serverless -g`, and type `serverless login` to create your Platform account. Then, create a new project using the `aws-nodejs` template: `serverless create -t aws-nodejs`. Youll also need to download and install WebStorm. Streamline your serverless workflow Now that the basics are out of the way, were going to talk about the four key WebStorm features I use to help me code (and debug) even faster. Step : set up a linter A linter is a must-have during any coding session, especially when its integrated to your IDE. It provides instant feedback on any typos that you might have missed, saving you tons of time correcting basic errors. Heres the kind of feedback WebStorm provides for you once youve set up your linter: Pretty great, right? You can use any linter you want, but my personal favorite is the Standard linter. Its the simplest of them all and provides a zero-config experience. To set it up on WebStorm, just add it to your `devDependencies`. WebStorm will detect it and ask you if youd like to use it as your project linter. To which you say: yes! Now youll need to disable WebStorm default inspections. Open WebStorm Preferences > Editor > Inspections > JavaScript and disable all rules except for the Standard Code Quality Tool. Thats it, youre done. Lets move on to one of my favorites: Live Templates. Step : Start using serverless Live Templates Live Templates is probably one of WebStorms best features. It allows for custom-made auto code completion when you type a trigger keyword. Heres an example live template that auto generates a simple function by typing `fn` + `tab`: Amazing. I love this feature. To set up your own live template, go to WebStorm Preferences > Editor > Live Templates > JavaScript. From there, you can view the built-in templates, see how they work, and create your own. Integrating with the Serverless Framework Wouldnt it be so awesome if you could hit `COMMAND + SHIFT + D` to instantly deploy your service? Thats what I thought too. Well, luckily WebStorm allows you to hook up external tools, have them available to the IDE UI, and even assign keyboard shortcuts to them! Heres what this looks like in practice: To add the Serverless Framework as an external tool, go to WebStorm Preferences > Tools > External Tools. Click on the + sign at the bottom left, and add the following settings: You can follow the same steps if youd like to add other tasks as well, such as `sls invoke`. Step : streamline your testing & debugging WebStorm comes with first class support for many testing frameworks, including Jest (my personal favorite). This makes it very easy to quickly run individual test cases/suites with a point-and-click interface. All you have to do is install your favorite testing framework, and WebStorm will detect it. When it comes to testing my serverless applications, I personally try to aim for around % code coverage via unit tests, plus a one or two handler integration tests. Running those tests is a breeze with WebStorm, and they become extra useful when you use them with WebStorms built-in debugger. Using the WebStorm debugger WebStorms debugger is powerful. You set it up once, define your break points, then run it. WebStorm will then show you tons of crucial information and data about your code and its context. Once you get used to this debugger, youll probably never use `console.log()` for debugging again! Note: I wont be going deep into how to set up the WebStorm debugger in this post, but you can check out this other walkthrough if youd like. Testing serverless REST APIs REST APIs are one of the most common use cases for serverless applications. Luckily for us, WebStorm comes in with a built-in REST client that you can use to test your deployed serverless endpoints. The WebStorm REST client is a lot easier to use than curl. It includes most of what you need right inside your editor, so that you dont need to use another REST client such as Insomnia or POSTMAN. Open it by Clicking on Tools > Test RESTful Web Service: Final Thoughts This covers the most essential serverless-specific setup for WebStorm. I hope its helpful for your workflow! If you want to get even more advanced or customized, check out the WebStorm docs. They have lots of powerful features you might like. Feel free to share your own setup with me in the comments or on Twitter; Im always down to learn about ways to make my workflow even more productive. Dont forget to set your perfect set of shortcuts, and happy hacking.",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f3e",
      "title": "Host your own CNCF CloudEvents compatible Event Gateway on Kubernetes, point to any FaaS",
      "content": "Last year, we launched the Event Gateway open source project: the event router for the serverless world. In a nutshell, the Event Gateway is the way you take your business events, and route them to serverless (FaaS) compute. You can use it to easily build APIs or react to custom business events. You can even connect events across different vendors to integrate your new applications with legacy infrastructure or other SaaS. While we do have a hosted (read: serverless) version of the Event Gateway contained inside our new Serverless Platform, the open source version is one that you have to host yourself. And today, doing that is about to get a lot easier. You can now deploy your own self-hosted Event Gateway instance to a Kubernetes cluster in just a few commands! Even better: the Event Gateway uses the CNCFs CloudEvents format. Meaning, its cloud-agnostic down to its core. Use it with any FaaS provider you want. Read on to learn how to get set up with the Event Gateway on your own Kubernetes cluster, or read the full release notes in GitHub. Self-hosting the Event Gateway The Event Gateway is a distributed system. Its designed to be highly-available and continue running even in the event that one of its component pieces fail. In order to self-host it, you would need to stand up all the underlying infrastructure for the Event Gateway. That means building out and managing the VM, setting up an etcd cluster, and handling ingress via a load balancer (nginx, proxy). This can be a complex undertaking. But now, if youre already Kubernetes, self-hosting your own Event Gateway instance is easy. _YES._ Why Kubernetes? Kubernetes abstracts away all the support logic required in running your own infrastructure. You can just let the system do it for you. _YES x._ How to run the Event Gateway right now, on Kubernetes Here are all the steps you need to get set up with a multi-node cluster. Getting Started You'll need to have an existing Kubernetes cluster that supports Ingress for Deployments. The instructions contained here outline how to get that configured with minikube's native nginx ingress. Install Helm Youll also need Helm installed. Run `helm init` on your cluster to generate the `helm` and `tiller` files. This will help us easily deploy our config files later. Install the Helm Components Youll need to grab the `etcd-operator` from Helm, and use `event-gateway` components from the serverless/event-gateway github repo. The `etcd-operator` is a community-added component that starts, stops and manages the `etcd` cluster youll be using. In this case, we're going to run distinct Event Gateway copies and they'll each hook into the central `etcd` cluster managed by the `etcd-operator`. To install the `etcd-operator` component, type:  Then install the `event-gateway` component:  These commands will install the components into the `default` namespace in Kubernetes. If youd like to install them in a specific namespace, then you will need to append a `--namespace ` on the end of your install command, like so:  Note: the `namespace` here is separate from the concept of a `space` within the Event Gateway. You can read more about Event Gateway spaces here, but its not necessary for installation. Accessing the Event Gateway If you have successfully installed the helm charts as listed above, you should now be able to access the `event-gateway` at your minikube IP. The helm chart explicitly assigns `eventgateway.minikube` to your Ingress ExternalIP. To enable your DNS resolution you can add this to your `/etc/hosts` file as follows:  The Ingress explicitly routes path access to the Event Gateway on your behalf, sending all configuration API calls over to the `config` service and all events API calls over to the `events` service. To test this out, you can pull the Event Gateway Prometheus metrics as follows:  Note: if you did not add the hostname to your `/etc/hosts` file as above, you can replace `eventgateway.minikube` with your Ingress IP to do the same. Congrats, youre done! Well, that was easy. Now you have your very own self-hosted Event Gateway up and running on Kubernetes! How (and why) youd use the Event Gateway The Event Gateway gives developers a really easy way to ingest all business events as data, which they can then route anywhere they wantother cloud providers, SaaS, legacy infrastructure or containers. Building REST APIs One of the most popular use cases is building REST APIs, which we have a walkthrough of here. Reacting to custom events You can also use the Event Gateway to react to a single business event in multiple ways. Lets say a new user gets created. This event would route to the Event Gateway, from where three different things can occur asynchronously: () a new user gets created in your user table; () the user gets a welcome message; () the sales team gets notified that a new user just signed up. All this can happen without updating any code on the original business event itself. (Read more on this here.) Using services from different clouds together Kelsey Hightower demoed a multi-cloud scenario earlier this year. He ran the Event Gateway on Kubernetes via his own integration, and deployed it to Google Cloud. An S event on AWS was sent through the Event Gateway and routed to a Google Cloud Function: Austen Collins from Serverless, Inc., used the Event Gateway to trigger different cloud providers: Additional Resources - See the full release notes - Check out the Event Gateway open source project - Learn more about the CNCFs CloudEvents specification - Read up on popular Event Gateway use cases - Try the fully-hosted (serverless) version in the Serverless Platform",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f40",
      "title": "Strategies for implementing user authentication in serverless applications",
      "content": "Searching for a way to do user authentication in your serverless project? Look no further. In this post Ill be covering robust approaches to storing authentication-related data in serverless applications! Well talk about storing user information with sessions and JWT, token validity with Lambda Custom Authorizers, user management from scratch vs hosted services, and so much more. (Spoiler alert: there is no one perfect solution.) Ill cover a few examples of implementing both authentication and user management, and give my thoughts on the future of authentication mechanisms for the Serverless architecture. Ill be mentioning the following examples in this post; feel free to check them out beforehand if youd like: API Gateway + Custom Authorizer + Auth Serverless Authentication + Authorization Where to store user information When implementing authentication in your Serverless project, there are two steps: () give your users the ability to identify themselves, () retrieve their identity in your Serverless functions. The most common ways to accomplish this are storing user sessions, and writing user information inside JSON Web Tokens. Sessions - standard approach Sessions are a standard for storing authentication-related information. Upon authentication, the user gets a token. The token is then sent to the server on every request, and used to look up user information in the databasethe status of the session, expiration time, and authentication scopes. Typically, you would store session data in either Redis or Memcached. But for Serverless projects, it makes sense to use hosted datastores insteadAmazon ElastiCache or DynamoDB, Google Cloud Datastore, etc. The down side is, hitting DynamoDB or another datastore to retrieve session information can be a challenge. With a high enough load on your application, retrieving sessions might add a significant amount to the datastore costs and increase page load times for users. Not so optimal. JWT - convenient for serverless Enter JSON Web Tokens (JWT), a growing favorite for serverless projects. The authentication mechanism here is similar to sessions, in that the user gets a token upon logging in, and then sends that token back to the endpoint on every request. But JWT has a key advantage; it makes it easy to store additional user information directly in the token, not just the access credentials. On every request, the user will send the whole token to the endpoint. If you store their username or access scopes in the JWT token, it will be very easy to access that information in every HTTP request you receive. The good This has a number of benefits for serverless projects compared to sessions: - you dont have to access the datastore for getting user information, which can decrease operational costs significantly - changing the shape of the data stored in JWT tokens during development is faster, and that enables easier experiments - implementing JWT can be just plain easier than reading and writing sessions The bad Unfortunately, JWT isnt a holy grail: - JWT tokens are larger than average session keys, so your clients may be sending more data to your endpoints overall - All issued tokens are encrypted with a single keypair. If a leak occurs, the keypair-affected applications would need to invalidate all existing JWT tokens. Clients are allowed to choose the encryption method used on the JWT token issued to them, which could potentially expose additional attack vectors. (This whitepaper on the topic is quite thought-provoking.) - Implementing authentication via JWT in a production app certainly requires spending extra time on ensuring that the tokens are used correctly, that you only store the most necessary information in the tokens, and that you are keeping your encryption keys safe. Where to check session or token validity? So when and where should you check the users credentials inside your app? One solution would be to check the JWT or session content on every call to any of your functions. This gives you the most control over the authentication flow, but it is complicated to implement; you have to do everything yourself. Its also not optimal from the database access standpoint. Another solution that improves on some of these issues is using Custom Authorizers supported by API Gateway. Lambda Custom Authorizers AWS Lambda offers a convenient way to perform authentication outside of your core functions. With API Gateways Custom Authorizers, you can specify a separate Lambda function that is _only_ going to take care of authenticating your users. In `serverless.yml`, you can specify custom authorizers as follows:  Custom Authorizers are currently only supported in joint use of Amazon API Gateway + Lambda. The Authorizer function has to return a policy of a specific shape. Its a little inconvenient at first, but gets you access to a lot of flexibility. Amazon provides a blueprint for implementing authorizer functions, which you can find right here. You can also find a working implementation of an Authorizer function here in the Serverless Examples repo. The best part: API Gateway will cache the resulting policy that gets returned by the Authorizer function for up to one hour. If the Custom Authorizer gets user information from, say, DynamoDB, this caching is going to reduce DynamoDB traffic significantly and improve the load times of your Serverless apps endpoints. Nice. Check out our documentation on using the Custom Authorizers with the Serverless Framework. User management from scratch vs hosted services To manage users, youll need to create and delete them, as well as log them in and out. So the the big question is: should you manage users entirely yourself, or use a hosted service? Implementing it yourself This basically requires a CRUD interface for your Users database, plus a `login` method to generate a new JWT token or to create a session. Those can be implemented as separate functions. I found this example to be very useful. The Register User function is simply:  Which then gets wrapped in a handler:  And specified in the `serverless.yml`:  You can find the full example in this GitHub repo. Using hosted services Services like Auth and Amazon Cognito handle creating users, logging them in, and storing sessions. If your goal is to allow users to log in with their social accounts or their corporate SAML identities, this is especially useful. With Auth, your app's frontend gets a JS element via the Auth SDK that displays a nice-looking login window, as in the example here:  And then your Authorizer function will check the user's token using the Auth public key:  All without a need for you to maintain the Users database. Pretty slick. Conclusion Were unfortunately still in the early stages of authentication for serverless. While API Gateway provides a convenient way to implement authorization for Lambda functions (with, logically, more Lambda functions), other serverless compute providers dont offer ways to conveniently authenticate users. And even authorizer functions in Lambda have their issues, with fairly complex policies and caching limitations. We are eager to see what solutions for authentication the serverless compute providers offer going forward, and are always happy to hear from you about how youre handling authentication. Feel free to drop a comment, post in our forum, or hit us up on Twitter.",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f42",
      "title": "Things to consider before building a serverless data warehouse",
      "content": "We all know about data warehousingthe way organizations store and analyze data at scale. And we all know that data warehouses can be challenging to implement. There is complexity, upfront costs and, of course, finding the expert to build and manage the infrastructure. These challenges have made data warehousing prohibitively expensive for all but large-scale organizations to implement. But with serverless technologies, are those days over? Is it time for the rise of the serverless data warehouse, for organizations large and small alike? In this post, we explore the benefits and downsides. Read on to see if serverless data warehouses are ready for prime time, and whether they would be a good fit for you. Some background Lets start by clarifying what serverless data warehousing means. A serverless data warehouse will get its functional building blocks via serverless third-party services, or a collection of services. These services are fully managed. They handle major complexities such as reliability, security, efficiency, and costs optimizations, and provide a consumption-based billing model for their usage. Note: in reality, the internal complexities handled by a serverless data warehousing solution will be vendor-dependent, but they should all provide these baseline abstractions from above. Serverless building blocks Although there are various types of data warehouses out there, they all rely on the same building blocks. Let's look at the common serverless tools and technologies youll need to use. A centralized repository The central repository is where the data is analyzed. Think Amazon Redshift, Azure SQL Data Warehouse, and Google BigQuery. Each of these solutions will support two important things: storage online analytical processing, where large aggregated queries can be run on the data Which repository you use is up to you, but the most important thing you should consider is how it integrates with the other building blocks in your solution. Simply put, vendors make it easier to interface with other tools from within their own ecosystem. If you decide to use AWS Redshift, for example, youll get lots of benefits if you stay within the AWS ecosystem for the rest of your warehousing solution: S supports event triggers to connect with Lambda Lambda loading data into Redshift is provided as open source code (See: AWS Lambda Redshift Loader) Amazon QuickSight (a business intelligence tool) directly integrates with Redshift Even when some of the integrations are not cloud-native (e.g, Lambda loading data into Redshift), the vendor provides additional code and support to make things simple to integrate. Data pipeline (Serverless ETL) Data will be coming from various sources, and youll have to preprocess it before moving the data to the central repository. This requires serverless tools for ETL/ ELT. There are a lot of tools to choose from here, specializing in things like data extraction, loading and transformation, or different scenarios for loading data into the central repository. On AWS for example, you can build your own solution using Lambda and its integrations (like our Redshift example above). Or, you can use specific solutions: AWS Glue, AWS Data Pipeline, AWS Data Migration Service, AWS Batch, AWS Kinesis Analytics, and so on. Here are questions you should ask yourself when selecting serverless ETL tools for your use case: . What are the sources and destinations of data in the data warehousing solution? Can the tool load data and process within the cloud, on-prem to cloud, on-prem to cloud and back to on-prem? . What is the frequency and size of the data being ingested? Youll obviously need to find tools that support the scale of data ingestion. But youll also need to provide integrations with middleware to hold data before transforming it. If youre handling a large amount of data, you can utilize scalable compute to process and transform it. . What is the tools level of extensibility, configurability, and convenience? Does the tool let you do data transformations? Is it configurable for your specific use case? Does it allow your developers to manage it in a way thats familiar to them (programming language, etc)? . What is the pricing model? You should find out whether its cheaper to keep the ETL tools provisioned or running on demand. On-demand tools have a consumption-based cost model for usage, and until a certain tipping point, they will be more cost-effective than up front provisioning. Benefits and drawbacks of a serverless approach When designing a data warehousing solution, youll need to optimize for these four key areas: security, reliability, performance, and efficiency. Lets see where serverless wins, and loses, in each of these areas. Where serverless wins Here are the upsides to a serverless approach. Simplicity in management In a traditional data warehousing solution, architects have to have in-depth knowledge at both the hardware and software level in order to optimize reliability, performance and efficiency. They have to understand how to handle growing data volume, and how to design the solution to work efficiently based on the frequency of data loads, complexity of workloads, and supporting the query performance. This will require a data warehouse that can easily scale out data storage, network, and processing capacity _without having any single points of failure_. With a traditional approach, your architect would have to implement each building block themselves. With a serverless approach, every building block in your data warehouse will be fully-managed by definition, and individual services are usually covered by SLAs under the provider. Another bonus is that implementing new building blocks becomes a matter of simply choosing which service to use. However, you'll still need expertise in a serverless solution to address how these individual serverless services are integrated, as well as non-functional requirements like reliability, performance and efficiency for the overall solution design. No upfront commitment The nature of serverless itself carries a cost advantageits provisioned mostly in the cloud and scales on demand. You dont have to provision a lot of resources up front, or pay for resources that go unused. In a traditional data warehousing solution, the costs of setting up the environment include things like: implementing redundant servers for durability, availability, and fault-tolerance, having excess capacity for scalability needs, and software licensing. Even if you deploy in the cloud (in a non-serverless way), your overall cost will be higher as you implement your individual building blocks. Reduced Operational Costs With a serverless approach, most of the individual building blocks provide their own support for monitoring and automation. And with proper planning, you can further reduce cost by utilizing the on-demand and pay-per-use nature of serverless. Challenges and shortcomings So does it means serverless data warehousing is a silver bullet to solve all the problems with traditional data warehousing? The answer is maybe, but were not all the way yet: - Though many serverless building blocks are fully managed, not _all_ of them are. If we consider Amazon Redshift, we need to choose the node type (e.g. compute optimized or storage optimized and selecting the node size from large and xlarge instance size. In addition, it will require to select the number of compute nodes for the cluster and manually do the sizing. - There can be challenges in integrating different serverless building blocks. Sometimes, youll need to use a lower level of serverless units (e.g. serverless compute like Lambda), or even non-serverless blocks, to connect the entire solution. - You lose the ability to have one big solution, and instead must integrate individual building blocks. While this makes your solution configurable, it also means added complexity. - Costs are not _always_ on-demand and variable. Some serverless solutions, like Amazon Redshift, Azure SQL Data Warehouse have both upfront and variable cost components where an upfront cost is applied when provisioning the baseline compute infrastructure. - They can require vendor-specific expertise for maximum efficiency while building the solution. Having a deeper understanding of whichever cloud provider you use will save a lot of time as you connect in other building blocks to create the entire solution. - Vendor lock-in is a risk. Although this is not seen as a major problem today and it is possible to work around, it does require additional forethought and using the right abstractions within the design. The verdict? Although challenges to serverless are there, we still see organizations increasingly adopting a serverless data warehousing approach, or using a partially-serverless approach. The main reasons are the advantages of serverless like auto-scaling and pay-per-execution pricing models. So, is this the right time to move for a serverless data warehousing solution? I would say, yes. It's the right time. Even if you are unable to build a fully-serverless solution, you can build the majority of the solution with serverless building blocks, and harness all the benefits discussed in this article. I hope this list of considerations was helpful throughout your decision-making process, and Im always available for questions or feedback below!",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f44",
      "title": "How to win followers & thought-leader people in  weeks",
      "content": "I work at Serverless, Inc, where one of our main goals is to eliminate overhead so people have more time to do engaging and creative work. This has become part of our company ethos as well. So much of our culture centers around empowering employees to do interesting things in and out of work. We have a distributed-first model where people can work remote, or work from home multiple days per week. We have a culture of freedom with accountability, which meansget your work done and make your meetings, but otherwise you run your own schedule. And now, we get to add another thing to this ever-growing list of initiatives: the thought leaders program. (Please forgive my exorbitant buzzword usage.) The program is designed to help anyone at the company build their own social media following, in any area they want. Maybe it isnt serverless. Maybe it isnt even developer tools. Maybe its art, eco fashion or gaming. Thats cool. The program went so well, I decided to open source the entire thing. Think of this like your very own how to social media like a pro guide. And the best part: it can be done in - minutes a day. Ready to start? Awesome. Read on. What is the program? There are weeks of assignments, meant to be done in - minutes a day. Each assignment is a microstep towards building an audience and establishing yourself in whatever space you choose: growing your social media presence, hashing out speaking topics, compiling bios and abstracts, getting better at writing and speaking. Tl;dr: thought leader-dom. Why run it? Because people at work wanted it. A lot of people were saying, I wish I had a voice in this area, or I wish I were more active on social media, but I dont know where to start. We decided to try this program as an experiment, and it really took off. It turns out, just having steps to follow eliminates the biggest barrier: peoples own uncertainty about what to do and when. The step by step: how to build a social media presence Here are the key things you need to do in order to run this program. Step : create a Slack channel. You know how youre way more likely to go to the gym when you have a buddy? Youre way more likely to do your weekly assignments that way too. Step : Every Monday morning, paste a weekly assignment in that channel. Step : Actually do the assignments. Or, you can go rogue and do the weekly activities all by yourself. Up to you! The assignments Now, Id like to offer one big word of caution here: I totally made these assignments up using my own best judgement and guesswork. You have been warned. That said, in my opinion, % of the job is showing up. This program was primarily designed to get you to show up. Youd be surprised how far just - minutes a day goes. Everybody, get ready to do some leading. Here are all the activity lists, straight from our Slack channel. Week : Choose social media platform. This is where you will be focusing. (The series will assume youre using Twitter; if youre not, tweak things until they fit your platform of choice.) Think about the type of thought leader you want to be (aka your goal with all this), and send it to the group. - E.g., I want to be an operations guru; or I want to be an all-around serverless thought leader; or or I want to be influential in the art world - Doesnt matter what you pick, this is just to help you keep centered on that goal. Update your bio, photo, theme colors, handle, etc. Make it awesome. Feel free to send it to the group for feedback! Find influencers in your space. FOLLOW THEM. At least -. If you already follow enough thought leaders, no you dont. Follow more. (PS, everybody should be following each other, too.) Week : Think of a daily convenient time for you to log in to your preferred social media platform for minutes. Is it as soon as you wake up in the morning? Over your lunch break? Right after dinner? Whatever time that is, set a M-F repeating reminder on your phone (or work calendar!) right now. Remember those influencers you followed last week? When that reminder goes off, open Twitter. Youll spend minutes retweeting & liking your favorite tweets from those people. At least a few a day. What hashtags are they using? What other people/orgs are they @-mentioning? Make a mental note. Or heck, make a physical note. Find - newsletters you can subscribe to that are relevant to your thought leader space. Subscribe to them. Make sure you have your email set up so that when the newsletters come, they wont go to spam or get filtered out of your inbox. (Subscribing to a set of updates on popular Medium blogs is also ok!) Week : Be glad you signed up for those newsletters, because now youre going to tweet your favorite articles x this week. Dont just tweet the link, give people a reason to read it. Loved the point here about X. Or you can directly quote a key line. - Use those hashtags you noted last week. - Directly @ mention anyone it makes sense to. You might get a retweet. Comment on a couple tweets from influencers youre following. Continue to retweet/like a couple times a day. You should still have your -m a day social media alarms set, and I bet you can do all this in that window! Week : What is your pinned tweet? Make sure its still current. Write your paragraph bio as though youre giving a meetup talk and they asked for one. - Bio should be something like: `Name` has ` years experience working for open source projects, both as an engineer and product leader`. They are currently `doing this cool professional thing`. In their free time, you can find them `doing this other cool fun thing`. > obviously your bio can be whatever you want & you dont have to use this template. - Save it somewhere. Now you never have to write it again! Especially not when we use it later (mwahahah). What are some opinions you have in your thought leader space? Send a couple to the group. - E.g., Python is THE BEST CODING LANGUAGE ever, and heres why; If youre not using an event-driven architecture, youre doing it wrong; Vendor choice is the most important thing IT leaders could be thinking about today; Diversity and inclusion are critical to the workplace; etcetcetc. JK, tweet one of those opinions. Use all the right s and @s. I realize that brazenly tweeting an opinion can be kind of scary. But you can do it. The internet is ephemeral, and people only remember things for minutes anyway. ;) Also, keep up what we had going last week. Tweet another - articles from your newsletter subs, links to cool github projects or other fun things. Week : Are there meetups relevant to your thought leader space in your area? Dont lie, yes there are. Find them and make a list. If, _hypothetically_, you were to give a talk at one of these things, what would the title be? Keep up the likes/tweets/retweets/comments/s/@s when your daily reminder goes off. Week : Make an outline for that talk title you came up with last week. You dont have to get it perfect, just make a bulleted list of some key points youd like to cover and go from there. It helps to think about what the point of your talk is, and write that on the top of the page. E.g., I want to convince people of X. Then think about the things you could say that would `convince people of X`. Those are your key points. Once you have your key points, take a few bullets to make notes about what youll say on those points. Thats it! Outline = achieved. Keep up the likes/tweets/retweets/comments/s/@s. BONUS: Try making a meme! Everybody likes memes. Tweet it. Week : Remember that list of meetups you made? Submit your talk to at least one of them. This will probably require writing a brief paragraph synopsis of your talk. When they ask for your bio, hey! You already have that. Keep up the likes/tweets/retweets/comments/s/@s. Week : Make a copy of your talk outline. Convert all the main points into full-fledged headers, in bold and everything. Take all of your supporting points and turn them into - full sentences. Keep up the likes/tweets/retweets/comments/s/@s. Is your pinned tweet still current? Week : Go back to your longform outline. Add a - sentence introduction. Make sure the first sentence is a clear statement about what the post will teach/talk about. E.g., In this post, we are going to learn how to draw cartoon kittens with the new Crayola glitter set. See, super clear. I know exactly what I get if I keep reading. Go through and flesh out some of the ideas. Take the - sentences you wrote before and turn them into a full paragraph where relevant. Do this with at least half of the post. Keep up the likes/tweets/retweets/comments/s/@s. Week : Out of curiosity, go check your Twitter engagement metrics. Notice a bump? Yesss. By the way, did you realize that you basically wrote a blog post last week? Huzzah! Feel free to send it to Andrea for feedback/edits. (note: sorry everyone, that last \"send it to Andrea\" step is for Serverless employees only ) AND THENpublish it somewhere! Keep up with the likes/tweets/retweets/comments/s/@s. Forever. You really do have to just do this forever. Sorry. Putting the program on maintenance mode By the end of all the assignments, everybody has had . months to build their social media following. The reality is, keeping that up is an ongoing process. I plan to ping the channel regularly with speaking opportunities I find, and or great blogs they should be reading about in their space. Anything to keep people excited. The results? Not everybody at the company chose to follow the program, which is cool. To each their own. Of the people who did, two have already booked speaking engagements at great conferences later this year, most are at least gotten halfway there to publishing their first blog post, and everyone has seen a huge bump in their twitter followers & metrics. For instance, I would say that, for someone who's just getting started on social media for the first time, this is a great -month growth trajectory: And some of our more established tweeters have grown their audience and engagement: Reviews of the program include quotes like: I gotta say, as someone who created this program having no idea what the outcome would be, it seems like the results were great! General tips for building an online presence The number one thing I say to people is: have a personality. The internet and text-based interactions abstract away so much of who we are already. Its easy to forget that there is a person behind the Twitter account. I bet if you think about your favorite social media presences, they will have one thing in common: they either have a lot of personality, or they have such great content that you dont care. The second one is, frankly, harder to do. People are too finicky; the content they think they want changes. But they will always be drawn to a person who they can relate to. Do you mostly tweet about Node.js? Cool. Should you worry about distracting your audience by tweeting a few times about your favorite movie? Absolutely not. (IMHO.) So be bold. Just tweet it. In sum We all really enjoyed this program, and hope you will too! See you on the social web. If you want to follow any of us on Twitter to see what were up to, heres a handy list! And if you'd like to work with us, we're hiring for lots of positions in growth, engineering and design, either remote or in San Francisco.",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f46",
      "title": "The Serveless design ethos: creating brand identity from a green field",
      "content": "Ive always been the kind of person who was obsessed with design. I was born in Porto Alegre, a bustling city in the extreme south of Brazil. From a young age, I explored the constraints of the places around me, studied the buildings, wrapped my mind around the implicit visual communication in the design of every space. I craved exploring new spaces. I traveled everywhere, and eventually moved to Buenos Aires. Professionally, I became a designer, a head of art and finally a design director. But yet again, I started to crave breaking out. I wanted to try something new. Design at Serverless So I ended up at this startup called Serverless, faced with a daunting task: create an entirely new brand identity for them, while paying homage to the identity that came before it. Their new identity had to be a perfect blend of the old and the new, and it had to factor in open source communities, technical docs, meetups presentations, video, intangible vibe, and so much more. And now, here I am on the other side. In this post, Im laying out my entire design process for anyone whos interested in learning more about how to create a solid brand identitywhether its from a green field, or youre working to evolve an existing identity. The growth design system The number one thing to remember with design, is that your design serves a purpose. It works to build a community, tell a story, grow product recognition and usage. And as such, its always in flux. Designers have to be good at instantly adapting to both the world outside the company, and the needs within it. Taking inspiration from Ryan Gums growth process, my design process respects some very familiar principles: - Ideate - Prioritize - Test - Analyze - Optimize - Rethink Identity, in other words, is a constructiona circle that evolves and constantly reinvents itself. As you traverse this circle, ask yourselves these two questions: . Whats next in the brand evolution? Design must constantly evolve, and you have to figure out how to do so without losing its essence and recognizability. Evolution doesnt mean that the past gets erased, it means the past is your foundation. Along the way, dont be afraid to experiment. Nothing is really _bad_, but everything can always be better. . Does this design have a voice? Your designs are more than just art; they move the needle for the company. And in order to do that, you as the designer need to understand where they want to be, and how you can help them get there. That is what great design does: it builds the visual voice that leads the company. The practicalities of good branding With that in mind, lets talk about the practicalities of implementing brand design across an entire organization, and what to watch for along the way. Check and recheck your presentation Technology changes. New platforms get developed, which make way for new types of behaviors and interactions. We find ourselves in a situation where evolution isnt a question of choice, but rather one of survival. This forces us to create new communication rules that follow the contextual logic of every platform. Dont blame the message; if something is failing, its the way we presented it that isnt working. For example, other day I was talking with the Serverless growth team about diversifying the way we share our content in Twitter. My first thought, as a very visual person was, lets use more images, more videos, more gifs!. This wasnt a bad idealike I said before, theres no such thing. But there was one thing I didnt consider: including images in tweets means the link shows up as blue text, instead of getting expanded into a visual card. The link blends in with the hashtags, and is more easily missed. The end result can mean less engagement and fewer clicks. Not good! Obviously, were still using a mix of content types on social media, but its important for designers to understand the main interaction patterns of each platform, and find ways in to make our communication as appealing as possible given their constraints. No matter how much prettier an image might be, a responsive piece of content is the most compelling message, and what ultimately creates value as a brand. Take risks, evolve, prove, analyse and rethink You have to be able to look at your own work with a critical eye. You want the kind of design that makes someone stop in their tracks, pull out their headphones, and look. If you passed by your own design on the street, would you stop? You have to be honest. If the answer is no, you have to be humble and go back to the drawing board. Part of doing show-stopping work is taking risks and experimenting, but also openly hearing feedback about whats working and what isnt. Once I began leading the growth teams design communications, I would ask the whole team for feedback on my designs. They always pointed out things I didnt see, and this meant we got the best possible result for every design that went public. One example: I designed a custom newsletter for our Serverless Platform announcement. I spent a _lot_ of time making custom illustrations to show its features, and I honestly really liked them. But some valid points got raised, too. The team thought we should use real product shots of the dashboard, to make it more tangible for people. I had to admit, I agreed. Sure, taking feedback is not always easy, but its important, and everybody needs it to do well: Farheen Gani, in her great article called Making feedback work for you As important as receiving feedback, is knowing how to ask for it. Make sure that youve designed so that people know where to focus. Tell them where you are in your process: just beginning, or doing the final adjustments? This will help you to get the help that you want with minimal overhead. Conclusion All these recommendations here are obviously based on my own experiences as a designer, but I think they can be useful for people in all different types of roles. So, take a risk, try something different. Search for ways to make something good even greater. UI/UX at Serverless At Serverless, we have a culture of recognition and evolution. We celebrate and improve what is working, and take the steps to change what is not. We value bravery in approach, which for me has been liberating and exciting. If you feel like you relate to the way we see design in Serverless, feel free to apply! Were now hiring UI/UX designers in San Francisco and remotely.",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f48",
      "title": "The Serverless Framework wins Best Microservices API at the API Awards!",
      "content": "We've been consistently honored and humbled by the incredible community support we receive for the Serverless Framework. In such a short time, it has become a lively open source project with over , Github stars, and the number one way developers build and deploy applications to any FaaS provider. And today, we can't say how proud we are to have received an API Award in the category: Best in Microservices APIs! What are the API Awards? API World is the largest API conference there is. And so, the API Awards truly represent technical innovation, adoption, and reception in the API & Microservices industry worldwide. The Serverless Framework was one chosen out of hundreds of nominations, based on the three criteria used by the judging panel: attention and awareness well-regardedness by the developer & engineering community leadership in its sector for innovation _-- Geoff Domoracki, Founder of DevNetwork, producer of API World & the API Awards_ See you at API World! Serverless will be given an award at the API Awards ceremony on Monday, September , . Come attend API World @ the San Jose Convention Center on Tuesday or Wednesday, Sep -, with a free EXPO Pass on us! Just follow the link.",
      "__v": 0
    },
    {
      "_id": "64e08923b72e199dda603f4a",
      "title": "Use Cloudflare Workers + Serverless Framework to add reliability and uptime to your FaaS",
      "content": "If you havent heard of Cloudflare Workers, serverless developers are already using them to cut costs, and add uptime and reliability to their Functions-as-a-Service. And now, you can deploy Cloudflare Workers from the Serverless Framework CLI as easily as you deploy to Lambda, Azure, or Google Cloud Functions! Read more on how Cloudflare Workers can add even more robustness to your existing FaaS applications, and how to integrate it with the Serverless Framework. What are Cloudflare Workers? Cloudflare offers a suite of products which add performance, security, and reliability for your website. They accelerate applications through their CDN, scan for malicious traffic patterns to proactively block DDoS attacks, provide DNS and free SSL, and load balance against origin servers to ensure application availability. But most importantly for the serverless world, the Cloudflare team recently announced Cloudflare Workersedge programmable bits of logic based on WC Service Workers spec. Cloudflare Workers ultimately function similarly to a FaaS provider (like Lambda or Azure Functions). Why use Cloudflare Workers with Serverless? FaaS providers like Lambda can provide your core business logic, doing things like connecting your application with existing infrastructure. Cloudflare Workers are there to _enhance_ your business logic: you can enforce geo-based access policies, for example, or perform A/B testing with a Cloudflare workers script (instead of integrating that into your core business logic). With the Cache API in Cloudflare Workers, you can also implement a custom caching logic to help reduce the operational and network costs. Adding reliability and uptime Using Cloudflare Workers can significantly enhance your existing FaaS implementations, adding reliability and uptime. Heres an example: Say you misconfigure your AWS API Gateway, and all the requests going to your domain are failing. If you have a worker in Cloudflare, it can check whether the endpoint is working correctly. If it isnt, Cloudflare can redirect them to a custom page, or send your users to a different region and issue an operational alert to notify the developer. Thats the beauty of using the Serverless Framework for your public cloud developmentdevelopers can deploy to multiple cloud providers from the same toolkit, easily taking advantage of the best features from each. Get started with Cloudflare using the Serverless Framework If you dont already have Serverless installed, youll need to do that: `npm install serverless@latest -g` Youll also need to create a Cloudflare account, and then grab your Cloudflare account and zone. Deploying your first Cloudflare Worker on the Serverless Framework First, lets create a template for Cloudflare workers: `serverless create --template cloudflare-workers --path new-project` The `--path` should be set to whatever you would like to call your project. `cd` into your new project folder and run `npm install`. In order to be able to deploy any Cloudflare Workers, You will need to set your Global API key from Cloudflare as an environmental variable named `CLOUDFLARE_AUTH_KEY`, and your Cloudflare account email as an environmental variable named `CLOUDFLARE_AUTH_EMAIL`. You can get your Global API key from your Cloudflare profile page. You will also need to set `accountId` and `zoneId` in `serverless.yml` under `service.config`. The first part of the path when you open Cloudflare dashboard as a logged in user is your `accountId` (e.g. `dash.cloudflare.com/{accountId}`). And the `zoneId` can be found from the overview tab after selecting the desired zone from the Cloudflare dashboard. Next, we can deploy a simple hello world: `serverless deploy` And test it to make sure it worked:  If we choose to, we can then remove the service with `serverless remove`. Check out the docs - See the full Quick Start guide here",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f4c",
      "title": "Keeping the culture in remote culture",
      "content": "Maintaining a company culture when everyone is co-located is hard. Doing it remotely is even harder. But it's far from impossible. As someone who's been working on remote teams for the past several years, first at Compose and now at Serverless, here are some things I've learned about how to keep remote culture great. Hint: it starts with leadership. Remote companies need the right employees Lets be honest, for a lot of people, remote work seems ideal. They'll have some autonomy not always afforded in an office environment, freedom to work hours that are more conducive to their lifestyle, and pants arent (always) required. But is it right for everyone? Probably not. It takes a ton of self-direction and self-motivation to do remote work well. And if this isn't something you're heavily screening for during the interview process, it could end up in unhappiness for both sides. Obvious previous remote work or independent work is ideal, but you won't find that in every candidate. So you have to make work expectations clear: yes, they'll have some work flexibility, but they can never miss meetings, and they'll still have to work a full day, they need to be in frequent contact with their team, etc. Speaking from personal experience, my previous company, Compose, took a chance on me. It totally worked out. When new people do come on board, you'll need a way to make work visible. Allowing people to work silently in the dark can be to the detriment of an employee and the company. For example, everyone who joins Serverless gets a buddy to help navigate working at Serverless and get used to how we work, including remote work. Companies also need to decide if remote is right for all positions. Even in a remote-first company, there may very well be a geographical need to mind. If you have a tech salesperson, its pretty important that salesperson live somewhere near a tech hub. Does your management team need regular meetings to hash things out? If so, they need to be in compatible time zones. These are important factors that need to be addressed to make sure your culture works. Communication is key In an office, employees gather around the water cooler, grab lunch together, go out for events or drinks, and have opportunities for less formal communication with one another. In a remote-first or remote-only environment, those water coolers dont exist. You have to create a way for your team to chat. When you work remotely, Slack becomes your virtual office. We have a team-chat channel for the team to talk, misc channel for random junk, music channel for sharing music, and standups for daily standup checkins. And we use them. Not only that, we heavily invest in emoji reactions so people feel heard.  We make video chats mandatory for meetings, and even use them just to talk casually now and then. Shortly after I joined Serverless, one of my teammates invited me to a -on- video chat just to get to know one another. We talked books, weight lifting, Christmas, the arts, friends, family, hometowns. In an hour of chatting, I felt like I made a friend on the team. On a more business level, we have team syncs every Monday, I have -on-s with everyone on the Growth team bi-weekly to catch up on projects and sync, and our team has an All Hands video call to go over company reports. During our All Hands, every team member builds a slide where we share some good things that have happened in our lives over the last two weeks and any challenges weve faced. All of these things are great when it comes to building a team, but nothing replaces team retreats. Every six months, the Serverless team comes together from around the world. We meet up in a central location, strategize for the next six months, prioritize projects, and present on how the company is growing. We also have team building activities like Battle of the Air Bands, and down time to sit and chat over a meal. We get the chance to visit and share with each team member, and develop real relationships that help us to better understand who were working with, whats important to them, and how to better communicate. Feedback is a must One last key element to building and maintaining a culture with a distributed team is providing a channel for feedback. Serverless practices Radical Candor, which encourages and empowers each person on our team to provide candid feedback in an open and honest environment. This candor is an open dialogue between all levels of the company. To make this work, you need buy-in, and wholehearted participation, from the leadership. Companies succeed when they take feedback from employees on what is working, what isnt, and what could be done better. Teams succeed when managers listen to what employees need. It helps everyone. And that feedback must be acted on. When leadership listens, everyone feels like they have a voice and a stake in the company, which pushes each person to do more to guarantee success. A remote-first culture isnt easy to define, is tough to build, and takes constant nurturing to maintain. But at the end of the day, you have the opportunity to work with people from around the world, with diverse backgrounds and viewpoints, all coming to together to make what youre building that much better. !Teamwork!",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f4e",
      "title": "Common Node mistakes in Lambda",
      "content": "Its been months since AWS Lambda added support Node.js .. Im super happy that I can finally use `async/await` to simplify my Lambda functions. In the meantime, I have helped a few clients with their Node serverless projects. In doing so I have seen some recurring mistakes around `async/await`. Still using callbacks Many people are still using the callbacks in their `async` handler functions:  instead of the simpler alternative:  Not using promisify Before Node, bluebird filled a massive gap. It provided the utility to convert callback-based functions to promise-based. But Node's built-in `util` module has filled that gap with the `promisify` function. For example, we can now transform the `readFile` function from the `fs` module like this:  No need to use bluebird anymore. That's one less dependency, which helps reduce the cold start time for our functions. Too sequential `async/await` lets you write asynchronous code as if they're synchronous, which is awesome. No more dealing with callback hell! On the flip side, we can also miss a trick and not perform tasks concurrently where appropriate. Take the following code as example:  This function is easy to follow, but it's hardly optimal. `teamModel.fetch` doesn't depend on the result of `fixtureModel.fetchAll`, so they should run concurrently. Here is how you can improve it:  In this version, both `fixtureModel.fetchAll` and `teamModel.fetch` are started concurrently. You also need to watch out when using `map` with `async/await`. The following will call `teamModel.fetch` one after another:  Instead, you should write it as the following:  In this version we map `teamIds` to an array of `Promise`. We can then use `Promise.all` to turn this array into a single `Promise` that returns an array of teams. In this case, `teamModel.fetch` is called concurrently and can significantly improve execution time. async/await inside forEach() This is a tricky one, and can sometimes catch out even experienced Node.js developers. The problem is that code like this doesn't behave the way you'd expect it to:  When you run this you'll get the following output:  See this post for a longer explanation about why this doesn't work. For now, just remember to avoid using `async/await` inside a `forEach`! Not using AWSSDKs .promise() Did you know that the AWS SDK clients support both callbacks and promises? To use `async/await` with the AWS SDK, add `.promise()` to client methods like this:  No more callback functions, yay! Wrap-up That's it, common mistakes to avoid when working with Node.js . in Lambda. For more tips on building production-ready serverless applications and operational best practices, check out my video course. ;-) Further reading: AWS Lambda Node support: what it changes for serverless developers",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f50",
      "title": "Using AWS CloudTrail to enhance your serverless application security",
      "content": "CloudTrail is one of those AWS services that folks usually take for granted. Its been there doing its thing for a while, but unless you really had a good reason to use it, you wouldnt. Tracking events in your serverless functions is a start on the path to rock solid security, but there are a wealth of activities in any serverless platform that can have an unexpected effect on your applications security. In this post, well talk about AWS CloudTrail audit trail logging. Specifically, how it can help enhance your AWS Lambda security, and in general increase your control over whats going on in your cloud environment. CloudTrail & AWS Lambda: brief overview CloudTrail is enabled by default on every AWS account once the account is created. When a supported event activity occurs in AWS Lambda, that activity is stored in a CloudTrail event, along with other AWS service events in the Event History console. Event History: misconceptions best practices Many people mistakenly get the impression the Event History is everything there is in CloudTrail, but theres much more you can actually do with it. Creating a trail In order to maintain an ongoing record of events in an AWS account, users must first create a trail. A trail enables CloudTrail to deliver log files to an Amazon S bucket. Once logs are stored in S, they can be queried using SQL queries on the trails through AWS Athena. This is by far more efficient than grepping through JSON log dumps. :  We can see the details of the user who performed the change, as well as the nature of the change. In this case, the user configured the Lambda function `test-s-change-dev-hello` to receive notifications from the s bucket named www.some-bucket.xyz'. CloudTrail can also track changes throughout your AWS account, allowing you to trace any infrastructure modification back to its source. This includes details on the login that initiated the configuration change, timestamps, and other associated data that will allow you to fully track your applications environment configuration. CloudTrail & Automation One of the most significant benefits of enabling CloudTrail for your AWS Lambda serverless functions comes from the built-in automation functionality. CloudTrail lets you set up notifications, messages, and alerts that trigger off of configuration events in your AWS ecosystem. This means you can react to configuration errors and potential security risks as they are introduced. For example, trigger a CloudWatch Alert when there is a specific type of activity being done on an S bucket. Automation is pretty critical in serverless development as a whole, and thats no less true for serverless security. With strict use of automated verification and validation, you can test and document your serverless execution environment, creating a robust and predictable application that has all the benefits of a serverless application while enjoying the security of a more traditional architecture. Summary Serverless development has a security challenge: how do developers meet existing regulatory schemes and requirements that were designed for a more traditional application architecture? The answer: with infrastructure and application audit logs, call paths, and code side effects. When your infrastructure is decentralized in a cloud-native environment, this becomes more difficult, yes. But it is far from impossible. You just need a little additional configuration and attention. By enabling CloudTrail for AWS Lambda, you can gain audit trail logging easily, including both application functionality logs as well as application environment configuration logs. If youve got any questions about serverless security, hit me up in the comments! Further reading - Fantastic Serverless security risks, and where to find them - How to monitor AWS account activity with CloudTrail, CloudWatch events and Serverless",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f52",
      "title": "No Server November: Join the noServerNovember challenge!",
      "content": "Ah, November. The month of re:Invent, pumpkin spice, and now: the noServerNovember challenge. Every week this month, we're releasing some Serverless Challenges that are designed to help experienced users level up, and brand new users get started. Do one, or three, for fun in your spare time! If you do any of the challenges and tweet a link to your GitHub repo with the hashtag noServerNovember, you just might win some official Serverless swag. Be sure to check the rules before submitting! As a side note, we also have a Serverless Examples Explorer on our website. So if you complete a challenge and feel good about it, submit it to our Examples Repo! It might get featured on serverless.com. The challenges There are currently challenges and hackathon to choose from! Nov : Special edition re:Invent hackathon! We've got even cooler prizes this week for our hackathon entrants. The re:Invent virtual hackathon can be done from anywhere, on a team or not on a team, however you want to enter. Check out the hackathon assignment and get building! Nov : Twilio reminder, Slack bot, stock ticker Make an SMS reminder bot with Twilio. Beginner track Create a serverless-backed Twilio reminder bot that sends you a text message. Have it tell you to take out the trash. Or move your car to avoid parking tickets. Or text your mom happy birthday. Here are some resources to get you started: How to avoid parking tickets with Serverless How to submit to the Reminder Bot challenge: Tweet the link to your GitHub repo with a screenshot of the sent text message. Include the hashtag noServerNovember. Make a Slack bot that suggests a random s action flick. Intermediate/Advanced track Create a serverless-backed Slack bot. Users should be able to type a slash command (such as `/action`), and receive the name of a random action flick. Bonus points if you include the cover art, a link to the IMDB / Rotten Tomatoes page, a quote, or really anything else that makes it more robust. Here are some resources to get you started: The `serverless-slackbot` example in GitHub The Movie Database API TMDB movie metadata Create a Slack bot with Serverless Make a serverless chatbot How to submit to the s Action Flick challenge: Tweet the link to your GitHub repo with a screenshot of the bot in action. Include the hashtag noServerNovember. Create a cron + ETL-backed stock ticker. Intermediate/Advanced track Create a serverless-backed cron job that runs an ETL script to pull data from one SaaS service into another. For example, a phone number you can text to receive stock (or for additional buzzword bonus points, crypto) price information. We recommend going Twitter -> serverless backend (AWS Lambda, Microsoft Azure, or Google Cloud Functions) -> Here are some resources to get you started: ETL job processing with Serverless and redshift How to submit to the Stock Ticker challenge: Tweet the link to your GitHub repo and a screenshot of your Stock Ticker in action. Include the hashtag noServerNovember. Nov : Cute Cats, Alexa skill, AnimalBot Make a website that serves visitors cute cat gifs. Beginner track Make a website. On that website, pull in a random cat gif. At its most basic, the gif should change every time the page is refreshed. Bonus points if you create a custom domain name. Here are some resources to get you started: Cute cat gifs on Giphy How to create a dynamic website with pre-built Serverless Components Check our Examples Explorer for a dynamic website example Create a custom domain name for Lambda and API Gateway How to submit to the Cute Cat challenge: Tweet the link to your GitHub repo and your Cute Cat webpage. Include the hashtag noServerNovember. Build an Alexa skill that tells you a random fact about One Direction. Intermediate/Advanced track (fun version) Make a serverless-backed Alexa skill. When you say, Alexa, tell me something about One Direction, or Alexa, hit me with some One Direction facts, Alexa should answer you and tell you a random fact about One Direction. Here are some resources to get you started: Here are some One Direction facts; youll need to convert this to a database Search Alexa examples in the Serverless Examples Explorer Building Alexa skills with the Serverless Bespoken plugin How to build a serverless Alexa skill How to submit to the One Direction challenge: Tweet the link to your GitHub repo, ideally also with a video (or sound clip) of the Alexa skill working, because that is way more fun. Include the hashtag noServerNovember. Create a Twitter bot that recognizes animals in images. Advanced track Make a serverless, image-recognition-backed Twitter bot. When a user tweets at the bot: @animalbot, whats in this image?, the bot should reply with the name of the animal, Its a panda! Here are some resources to get you started: Using Tensorflow with the Serverless Framework for image recognition Deploying bots on Azure using the Serverless Framework Making a Twitter AWS Lambda bot How to submit to the AnimalBot challenge: Tweet the link to your GitHub repo and AnimalBot account. Include the hashtag noServerNovember. Nov : Serverless Ipsum, DadJokeBot, GitHub Check We've got three challenges this week to suit all levels! The Serverless Ipsum challenge can be done even if you've never set up an AWS account before, and have never coded anything in your life. Every challenge you complete gets you one entry into the drawing. Build a Serverless Ipsum generator. Beginner track Build a simple serverless-backed web app that displays Serverless Ipsum when it is loaded. Or Tony Danza Ipsum. Or The Office Ipsum. Or Reasons-I-Cant-Take-Out-The-Trash Ipsum. As long as it looks like Lorem Ipsum, but uses different words, were good. The page doesnt have to look fancy, and you can do this even if youve never coded anything in your life! Heres a tutorial to get you started: I just deployed a Serverless appand I cant code Plus, some inspiration for all of you not-a-developer-yet-but-learning types: From chef to Serverless developer in years How to submit to the Serverless Ipsum challenge: Tweet the link to your GitHub repo and your Ipsum webpage. Include the hashtag noServerNovember. Make a Twitter bot that tweets dad jokes. Intermediate/Advanced track (fun version) Write a serverless-backed Twitter bot. Make it tweet dad jokes. Thats really all there is to it. Here are some helpful links to get you started: A dad jokes API Write a Twitter Wordpress AWS Lambda bot Write a Twitter bot using Microsoft Azure How to submit to the DadBot Twitter challenge: Tweet the link to your GitHub repo and your DadBot account. Include the hashtag noServerNovember. Automate a GitHub Check with Serverless. Intermediate/Advanced track (actually useful (!!!) version) This project is one of our favorites around the office, for its sheer usability. Automating anything to do with GitHub is just incredibly useful. For the easy version, set up a GitHub check that makes sure theres a reference to a GitHub / Jira / etc issue in the PR title. For the harder version, set up a GitHub check to automatically lint & reformat your code on a new commit. Or, do something else fun. Well leave this open-ended. Here are some resources to get you started: GitHub Checks documentation Using git on AWS Lambda Set up a serverless GitHub webhook See the GitHub webhook example in our Examples Explorer How to build a GitHub bot How to submit to the GitHub Check challenge: Tweet the link to your GitHub repo with a screenshot of the Check in action. Include the hashtag noServerNovember. Nov : Special Edition: re:Invent serverless hackathon How it works The first three Mondays in November, we'll release a new set of Serverless Challenges. Well also include some resources to get you started. The final week of November, we'll have a special edition re:Invent hackathon instead. Find a challenge you like! Or better yet, find multiple challenges you like! You can enter separately for every challenge you complete, and you can complete any challenge at any time. Do the challenge. Put it in GitHub. Tweet a link to your repo, plus any other relevant links or screenshots, with the hashtag NoServerNovember. Each entry qualifies you to win a prize. We will draw three winners every week, and we don't remove you from the pool until you win a prize. So if you complete a challenge during week , you could be in the drawing for weeks and as well. All the more reason to start early! Draw dates: - Nov. - Nov. - Nov. Rules To qualify, the entry must use the Serverless Framework and a serverless backend (such as AWS Lambda, Google Cloud Functions, or Microsoft Azure). You may only enter one time per daily challenge. You can, however, complete as many daily challenges as you want, and those will count as separate entries. You must follow any additional instructions contained within the challenge descriptions to have your entry counted. Only entries that use the hashtag noServerNovember will be qualified to win.",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f54",
      "title": "Jared Short: Why I joined Serverless",
      "content": "On Monday, I joined the Serverless team as the Head of Developer Relations and Experience. I can't tell you how excited I am about this opportunity to be working directly with you, the Serverless community. As some of you know, I'm not new to the Serverless space. I was a contributor to the Serverless Framework back when it was still called JAWS (and I have the OG t-shirt to prove it). I've been here since the infancy of FaaSbefore \"serverless\" was a buzzword, before the Serverless Framework even entered its alpha release! In my previous role as Director of Innovation at Trek (a Serverless partner!), I worked daily on architecting, building and operating serverless and event-driven architectures. I felt the pain of working on the cutting edge. But I also saw the tremendous value Serverless delivers. I relished the moments of seeing someone's eyes light up as serverless clicked for them. I built demos that changed how a company would do business in mere days. I am joining Serverless, Inc. because I believe in the future that we can build. \"We\" being Serverless, Inc. the company, and serverless the community. I believe in a future that has us building and delivering modern applications that are more resilient, powerful, and cost effective. I believe in a future that has us operating applications and services, not infrastructurea future that has radical focus on business value delivery. My day-to-day hustle will be making sure developers are served by the Serverless ecosystem. That means making sure we are innovating and accommodating in the name of progress. We at Serverless, Inc need to continue to be good stewards of the serverless and serviceful future. The future is always just over the horizon, but there is no other community, or company, I would rather be chasing it with. It's going to be a great and beyond.",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f56",
      "title": "Managing secrets, API keys and more with Serverless",
      "content": "Serverless applications are often _service-full_ applications. This means you use hosted services to augment your applicationsthink DynamoDB for data storage or Mailchimp for sending emails. When using other services in your Serverless applications, you often need configuration data to make your application work correctly. This includes things like API keys, resource identifiers, or other items. In this post, we'll talk about a few different ways to handle these configuration items. This post covers: - Using environment variables in your functions; - Handling secrets for small Serverless projects; and - Managing secrets for larger Serverless projects. Let's get started! Using Environment Variables with Lambda When building my first web applications, Heroku's Factor App was hugely influentiala set of twelve principles to deploy stateless, scalable web applications. I found many of them were directly applicable to Serverless applications. One of the twelve factors was to store config in your environment. It recommended using environment variables for config (e.g. credentials or hostnames) as these would be easy to change between deploys without changing code. Lambda and Serverless provide support for environment variables, and I would recommend using them _in certain situations_. Check out the last section on managing secrets with large projects for when you shouldn't use environment variables and how you should approach configuration in those situations. Let's do a quick example to see how environment variables work. Imagine you're making a Twitter bot that checks for tweets with certain hashtags and retweets them with slight changes, similar to the Serverless Superman bot. To post these retweets, you'll need to get an access token to sign your requests. Once you have your access token, you'll need to make it accessible to your function. Let's see how that's done. Create an empty directory, and add the following `serverless.yml`:  This is a simple service with a single function, `superman`, which will run the code for the Serverless Superman bot. Notice in the `provider` section that we have an `environment` block -- these are the environment variables that will be added to our Lambda environment. In Python, you'll be able to access these environment variables in the `os.environ` dictionary, where the key is the name of the environment variable. For our example above, this would look like:  In Javascript, you can access environment variables from the `process.env` object:  One final note: environment variables in the `provider` block are accessible to _all_ functions in the service. You can also scope environment variables on a per-function basis by adding environment variables under the `function` block. For example, imagine you ran both the Serverless Superman and Big Data Batman Twitter bots. Because these are separate accounts, they would each have their own Twitter access tokens. You would structure it as follows:  Now we have two functions`superman` and `batman`and each one has its unique access token for authenticating with Twitter. Success! Handling Secrets for Small Teams & Projects Now that we've got the basics down, let's dig a little deeper into handling secrets for your projects. In the example above, the big problem is that our access token is in plaintext directly in our `serverless.yml`. This is a sensitive secret that we don't want to commit to source control. A better option is to use the Serverless Framework Pro Dashboard Parameters feature. If you are already using Serverless Framework Pro for monitoring your Serverless Services, it makes sense to use the same tool to help you centrally maintain your secrets. And this can be very easily done across stages (or environments) as well. When you have logged into your `org` on the dashboard, click on 'profiles' in the top left, then either choose from an existing profile you already assign to a particular stage or create a new one. Once you have opened a profile, you will see a tab labelled parameters. It is here that you can then add whatever parameter you like for that deployment profiles stage. You can then repeat that for any other stage that your services may need and add the values specific to that stage. But how does this work? Well, within our `serverless.yml` we can reference those parameters using the `${param:keyname}` syntax. And then, on deployment time, the Serverless Framework will read the values from our Serverless Pro configuration and replace the variable syntax with the actual values.  !Video of process to add new param Managing Secrets for Larger Projects and Teams The methods mentioned above work well for certain types of projects. However, there are two different areas that may cause problems. First, environment variables are inserted into your Lambda function as plain-text. This may be unacceptable for security purposes. Second, environment variables are set at _deploy time_ rather than being evaluated at _run time_. This can be problematic for configuration items that change occasionally. This is even more painful if the same config item is used across multiple functions, such as a database connection string. If you need to change the configuration item for any reason, you'll need to redeploy all of the services that use that configuration item. This can be a nightmare. If this is the case, I would recommend using AWS Parameter Store to handle your secrets. It's very simple to use and allows for nice access controls on who and what is allowed to access certain secrets. However, you'll have to write code within your Lambda handler to interact with Parameter Storeyou can't use the easy shorthand from the Serverless Framework. Here's an example of how you would get a configuration value from SSM in your Lambda function in Python:  We create a simple helper utility that wraps a Boto call to the Parameter Store and returns the value for a requested secret. Then we can easily call that helper function by providing the name of the secret we want. Other considerations This is just scratching the surface of handling configuration in a larger Serverless project. Another issue you'll want to consider is refreshing your config within a particular Lambda container. Because a Lambda instance can be reused across many function invocations, you'll want to periodically refresh the configuration in case it changed since the instance was initially started. We have another blog post that goes into even more detail about secrets management and if you are looking for more information I would recommend reading that one as well.",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f58",
      "title": "Introducing FONK: a serverless LAMP stack for KS",
      "content": "There's a statement about serverless I've heard too many times: \"Serverless is a solution looking for a problem.\" It's the kind of statement somebody makes defensively, when a new technology comes along to disrupt the workflow they're used to. Its also the kind of statement that should reveal to all of us in the serverless community how steep the learning curve can be. If people understood serverless, they wouldnt say something so broadly negative. So I pose the question: How can we tame the Serverless learning curve? The answer I came up with? A web application. Let me explain. Guestbook: a serverless web app example Last spring, I heard that opening quote for the fourth time. So I asked myself, Whats the simplest thing you can build with Serverless that someone can relate to a thing they already know? I thought I could show the value of Serverless by demonstrating to someone how they can do something they're doing now, just more simply. So I built an example web app, and called it Guestbook. Using web apps to lower the serverless learning curve Just about everybody understands the LAMP stack. !LAMP Logo Its simple and provides choice among its components, which explains why it's been so popular over the last + years. We see its influence even in the Kubernetes (KS) learning curve: !Guestbook Traditional Architecture The Guestbook is among the first applications that most people deploy when first learning Kubernetes, and although it uses a NoSQL server in place of MySQL, the same basic LAMP structure exists. What does this mean for serverless? With that in mind, one way to lower the serverless learning curve is to show people how to build similarly complex web applications with far less code and configuration. Using AWS constructs, that web application would look like this: !Guestbook AWS Architecture And while that requires about half as much code and configuration, it also locks you into AWS. What if you could create a design pattern similar to LAMP but that used Serverless concepts on top of KS to insure portability? !LAMP Serverless KS Introducing FONK: a serverless LAMP stack for KS The components of that AWS architecture are a Functions-as-a-Service (FaaS) runtime, an object store, and a NoSQL server. If you used one of the five FaaS runtimes on any GitHub project that has more than , stars or other popular open source components, the entire stack is installable on top of KS as follows: !Guestbook FONK Architecture Put all that together, and you get a tidy acronym for a serverless design pattern: !FONK Logo (I owe the animal icon to my daughter who said, FONK sounds like something a goose would say.) Implementing Guestbook on FONK In September, some friends and I soft-launched fonk-apps.io, an open source project with the goal of lowering the Serverless learning curve for people. It does this by providing simple web app examples in every possible language. In an attempt to make the transition from native KS easier for people, the first of these web app examples is the Guestbook. Heres our early progress: !FONK Guestbook Progress While Guestbook, which has only Create and List functions, was a natural first choice, the plan was to get more sophisticated with the applications. We wanted to add things like ToDos (full CRUDL operations), a blog (authenticated CUD, public RL), and a forum (authenticated CRUDL). Longer term, it would be cool to build in some CI/CD with some test automation down the columns or performance benchmarking across the rows as well. FaaS runtimes compared In the process of building out this first set of examples, weve learned a great deal about comparing and contrasting the FaaS runtimes with one another. From a developer experience perspective, here are some early findings: !FaaS on KS Landscape The development experience on some of the FaaS on KS runtimes is closer to native KS development; it exposes some of the innards of the image upon which the function will run. Others are closer to the AWS Lambda model that obscures image details. The -pound gorilla in this space is Knative from Google, which hasnt yet reached the , GitHub star threshold that would warrant a Guestbook example. But were keeping our eye on its progress, as it will likely get there. Come join the fun! Once this idea got going, we thought it would be much better with a community of people around it. So, weve tried to make the whole thing an inviting place. While not all of the FaaS on KS runtimes support the Serverless Framework, Kubeless and OpenWhisk do very nicely. An easy way for you to get started would be for you walk through a completed example: FONK Guestbook/OpenWhisk/Node.js FONK Guestbook/Kubeless/Node.js Wed be appreciative of a GitHub star from you, but wed love a PR of a new FaaS runtime/language combination even more. Come check us out at the FONK project on GitHub, and try some of the examples yourself. Or feel free to call dibs on a FaaS/language combination as we build out all the Guestbook examples possible!",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f5a",
      "title": "The true cost of a new employee: compensation calculator for startups",
      "content": "If youre a founder, this shouldnt surprise you: for most companies, employee compensation is its largest expense. At Serverless, employee compensation makes up about % of our annual expenses. And we have to keep very accurate estimates of how much each employee costs so that we know what our actual burn rate is. But the calculation isnt as straightforward as the salary. For starters, you have to budget an additional % for things like benefits and taxes. Thats the rule of thumb at least; it, of course, has exceptions. In San Francisco, the % rule works well when applying it to employees who make less than $, per year. But it doesnt factor in that many payroll taxes max out after a certain amount, and benefits for a family cost a lot more than benefits for an individual. So to track this better, I made a total employee compensation calculator! Feel free to use it. The total employee compensation calculator At Serverless, the majority of our employees qualify as highly compensated individuals. (Such is the world of software.) When I applied the % rule of thumb, depending on their benefits selection, I found the estimate to be off by as much $,. Were a small and lean startup that has to make every dollar count. So, I decided to create my own compensation calculator for a more accurate estimate based on the costs that we have for each employee. The calculator is focused on San Francisco, but feel free to copy it and edit it with your own state and city numbers: Employee Cost Calculator Disclaimer - the purpose of this calculator is to share a tool that we use internally at Serverless to estimate the total cost of an employee. It is not intended for commercial use. We are not tax professionals and encourage you to consult with a financial professional before making decisions based on this calculator. All numbers are based on tax rates. Inputs into the calculator Salary This calculator assumes an annual salary and does not take into account any commissions or bonuses. Health Insurance We work with Sequoia One, a PEO that specializes in technology companies in the Bay Area and New York. In sum, this lets us benefit from average rates instead of age banded rates, so our insurance costs are the same for each employee depending on which tier they are in. We pay $, per month for a family, $ per month for individual & children, $ per month for individual & spouse, and $ per month for an individual. This makes it a lot easier to estimate the annual benefits cost for an employee, unlike with age banded rates that change depending on an employees age and location. Taxes Social Security - employers pay .% of the first $, of an employees payroll Medicare - employers pay .% of an employees payroll with no limit I was a little confused as to how these next three are calculated, so I looked up how much we paid for each employee in the last year and used the max amount as default. Federal Unemployment Tax Act (FUTA) - Assuming annual max of $. based on our payroll CA SUI - Assuming annual max of $. based on our payroll CA ETT - Assuming annual max of $. based on our payroll Human Resource Information System We originally used Gusto for our payroll and benefits provider, and really liked them. But then we started hiring all over the United States and found that the cost of benefits through them was too high for the quality of coverage we wanted to provide to our employees. We recently switched to Sequoia One, which costs us $/ month/ employee. K Administration We use Guideline to administer our K. Our team has been really happy with the low fee model they provide. We dont currently offer a match and it costs us $/ month/ employee to administer the plan. Lunch and Snacks We provide lunch and snacks to our San Francisco employees. This costs about $, per employee per year. Whats not included in this calculator: Recruiter costs - this is accounted for separately in our budget The cost of interviewing and onboarding a new employee (Ill cover this in a blog post at a later date) Equipment/ setup cost - this is usually a one time cost and its accounted for separately in our budget. Options/ Equity - If you are trying to calculate an employees total compensation check out my nifty Equity Calculator that I created to simplify writing up options agreements. Cost of office space - We currently view the cost of our office as a fixed and its accounted for separately in our budget Team retreats - We bring our entire team together twice a year. Past retreat locations include Austria, Northern California, and Morocco. Budgeting for retreats depend on where a team member is flying from and where the retreat is located, so we have a separate budget for that. Stuff I probably forgot - see anything that is missing? Shoot me a tweet @alaskacasey and let me know! How to use this calculator If you think that this calculator would be helpful to your team and youd like to use it, go for it! Make a copy To make a copy from the master Google Spreadsheet, go to `File` and select `Make a Copy`. This will prompt you to change the name of the file and select where you would like to save it in your Google Drive. You can also download the calculator in different formats like Excel by going to `File` > `Download as` and choose your preferred format. Setting up the calculator Everything highlighted in blue is where you can input your own costs for these items. Or, you can change the headings to completely different costs that you may have. Social Security and Medicare are federal tax rates, so you shouldnt need to edit those except when the feds change the rates. Using the calculator Once you have all the blue areas set up with your local numbers, you just update the cells highlighted in yellow with an individual employees numbers. Voila. Was this helpful? This calculator is the perfect example of a minimal viable product. Please let me know if you found this useful or feel free to comment on how it can be improved!",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f5c",
      "title": "Building a chat application using AWS AppSync and Serverless",
      "content": "GraphQL gets a lot of praise for its expressiveness, for the idea of batching requests for data, and for its great development tooling. But there is an additional benefit that mostly goes unnoticed. Namelymany frontend GraphQL frameworks make a distinction between the data in the app state and the data on a remote server. This is what allows React apps powered by GraphQL APIs to seem so fast, even if they are moving a lot of data: the moving of data happens in the background. Users get from more responsive frontend apps, while also saving bandwidth. Developers can now model the data better, and deliver a more pleasant experience to the end user. AppSync, AWSs managed GraphQL layer, builds on the benefits of GraphQL and adds a few more cool things in its mobile and web SDKs: subscriptions, convenient authentication via Cognito Pools, and the ability to plug in directly to a bunch of AWS services for data. AppSync can do a lot while still being a fully managed service, which works out great for Serverless applications. No more GraphQL resolvers in Lambda functions. No more hand-rolled authentication. Its the best of GraphQL with less complexity than before. In this article, we show how you can get started with AWS AppSync in a Serverless project, and talk about the benefits and drawbacks of using AppSync for your Serverless applications. Lets get to it! Building a chat app with AppSync We broadly divided the process of getting a chat app running on Serverless with AWS AppSync into two parts: setting up the backend part of the service to fetch the data and deliver it via the GraphQL API, and creating a simple frontend to consume the API. The backend We start by defining how we will be using AppSync in our Serverless project. We are using the Serverless AppSync plugin to simplify the configuration, and all we need to provide, in addition to the authentication config, is: - A set of mapping templates that will help AppSync understand how to resolve each GraphQL you send out - A GraphQL schema that describes our API - A data source, in our case a DynamoDB database. The AppSync section in our serverless.yml looks like this:  Our mapping templates for DynamoDB are almost an identical copy of the example from the AppSync docs, and allow us to get and create items in the Messages table. We place all mapping templates in the `mapping-templates` subdirectory. For our GraphQL schema, we are starting simple, with only a few actions that are strictly necessary for a useful chat app: - A way to create a message  in this case, the createMessage mutation. - A way to get all messages  the getMessages query. - A subscription for all incoming messages, addMessage. - A description of the fields of the Message object  in this case, we want a message ID, the text of the message, the date it was posted, and the handle of the person who posted it. With all those things our schema looks like this:  This is all we need on the backend side to get AppSync up and running. We can now deploy the service:  And then watch all resources get created. Frontend On the frontend, we use the GraphQL operations and the Authentication module from AWS Amplify. The core of the app is the `App.js` file where we configure Amplify with all our authentication settings and point it to our GraphQL endpoint. The whole user interface, in addition to the login / sign up screens provided by Amplify, consists of two components: `MessagesList` and `SendMessage`. We use `react-chat-ui` for the messages list:  We then create our own Send Message box that allows us to type in it and save the contents in the components state:  We then use the two components in `App.js`. We use the `Auth` info thats coming from Amplify to get the username that we need to associate each sent message with. The `getMessages` subscription that we defined before plugs into the `MessagesList` component neatly, and the `submit` action from the `SendMessage` component triggers a GraphQL mutation that sends the message to the backend:  This is all for our frontend! Once we install all the dependencies we can run it via:  We land on the authentication screen provided by AppSync, where we can pick a username and a password. We can then sign in and see the list of messages, send some messages, and get responses from other users: Ready for production? Getting started with AppSync takes very little time compared to creating and deploying your own GraphQL service, building authentication for it, and adding new API functionality. The simplicity of AppSync, as it is generally the case for managed services, comes with a few limitations. Data sources In the chat app, we are using the DynamoDB data source, which is one of the better-supported sources in AppSync. Another fully-managed data source thats available out of the box is the Amazon Elasticsearch Service. AWS Lambda is the third data source option supported by AppSync. You could create a service in AWS Lambda that would query an RDS database, or go to an HTTP service outside of AWS to get the data. While this allows for some extensibility, doing anything with Lambda would require more work than just using a fully managed service like in our DynamoDB example above. Finally, you can use Aurora Serverless as a data source for your resolvers as well. Aurora Serverless is a fully-managed relational database with on-demand scale-up and scale-down. Aurora Serverless has versions compatible with MySQL or PostgreSQL, so they work well with existing tooling. While it's still early for Serverless Aurora, I'm very bullish on its future in the serverless ecosystem. Authentication options In the chat app project, we used the Cognito User Pools authentication mechanism. If that doesnt work for you, there arent many other options that dont require managing the users yourself. You can use an OpenID provider (Google and Heroku are some of the providers), but otherwise, youll have to come up with a user management solution yourself. Metrics and logging AppSync currently only supports submitting metrics to CloudWatch, and the metrics it can submit are limited to `xx` responses, `xx` responses and the latency of AppSync operations. If AppSync becomes part of your production service, you dont have much granularity in the metrics or the logs if something goes wrong. Conclusion In this article, we went through creating a chat app with AWS AppSync and Serverless, and saw that it's pretty easy to get started. The service isn't necessarily ready for production, but allows for fast development and prototyping. The complete example is available here. Check out AWS AppSync, its developer guide, and the docs for Amplify. You can find the docs for the Serverless AppSync plugin here. The React Chat UI project is here. More on AppSync & GraphQL - Build a serverless GeoSearch GraphQL API using AWS AppSync & Elasticsearch - Running a scalable & reliable GraphQL endpoint with Serverless - How to make a serverless GraphQL API using Lambda and DynamoDB",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f5e",
      "title": "The re:Invent serverless virtual hackathon begins NOW",
      "content": "Were closing out NoServerNovember with a special-edition re:Invent virtual hackathon. Winners get fame, glory, and an Amazon Echo or other cool prizes! Want to participate? Awesome. Read on for the challenge description and details. The deadline to submit your project is Sunday, Dec at :pm PT! So you've got all week. (Note that this hackathon isn't in direct partnership with AWS, it's just a fun thing Serverless is doing alongside re:Invent.) The hackathon challenge It's GivingTuesday. As a community, let's help AWS's Andrew Jassy fulfill his newest mission: putting a cat in every home in America. Jassy's newest non-profit, Amazon Feline, is seeking to help match cats with new homes and offering free relocation for each pet adopted. This year, we're going to work together to ensure Jassy has everything he needs to build a successful non-profit just in time for AWS re:Invent. You're probably wondering, \"How can I help?\" With a Serverless app, of course. Or a new component, plugin, or example to help get the non-profit up and running. Important note: This scenario is all in good fun, and Amazon Feline is not an actual AWS-sponsored hackathon or non-profit initiative. However! We do plan to take your projects and create replicable applications and repos that non-profits around the world can use to help promote their own missions. Some ideas include: A payment gateway for adoption fees A text or email app to send pictures of cats to Amazon customers identified as currently catless A website for the organization A photo-editing app to help new pet families share their pics A chatbot that shares interesting cat facts Or an IoT project to automate pet-related chores The sky is truly the limit. Make whatever your heart desires, as long as it fits the theme. How to enter You can win an Amazon Echo, Serverless swag, or other goodies! Just make sure to follow the submission guidelines for your entry to be counted: Tweet the link to your GitHub repo and a screenshot of your project in action, if applicable. Include the hashtags reInvent & NoServerNovember. Mention @goserverless. Also, you should totally submit your finished project to our Examples Repo as well, where it will stand a pretty good chance of getting featured on our website in the Examples Explorer. Rules To qualify, the entry must use the Serverless Framework and AWS Lambda. These projects will all be open-sourced to help other charitable organizations clone, modify, or expand upon to grow their own reach. Winners Winners will be hand-picked by the Serverless team. We will judge entries based on their relevance to the theme, as well as the project's broader applications for non-profits. More NoServerNovember fun We kind of took over November this year with a month-long series of Serverless challenges! If youd rather try your hand at a smaller project, check out our weekly NoServerNovember challenges as well: Join the NoServerNovember Challenge",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f60",
      "title": "All the Serverless announcements at re:Invent",
      "content": "Last updated: Nov , : AM re:Invent has begun! But there is ever so much to track. If last year is any indication, we expect AWS to have a long list of serverless-centered announcements and launches. If you want to keep up, you've come to the right place. We're watching all the keynotes and announcements live as they happen, and compiling the \"what it is\" and the \"why it matters\" right here. Updating live all week! Oh, and while you're here, you should check out our re:Invent virtual hackathon. You can participate from anywhere, even if you're not at the conference, and win prizes while helping non-profits along the way. re:Invent announcements Latest: Websocket support for Lambda functions Bring your own runtime to AWS Lambda AWS Lambda Layers - ready to publish and use with the Serverless Framework right now AWS IDE Integrations Better Step Function Integrations ALB Support for Lambda AWS Lambda Ruby support Amazon Managed Streaming for Kafka Most Exciting: Websocket support for Lambda functions AWS Lambda Layers - ready to publish and use with the Serverless Framework right now Bring your own runtime to AWS Lambda Timestream timeseries database AWS open-sources Firecracker virtualization technology DynamoDB Transactions Lambda: Websocket support for Lambda functions Bring your own runtime to AWS Lambda ALB Support for Lambda AWS Lambda Ruby support AWS Lambda Layers Lambda + Kinesis Data Streams Upgrades Python . for Lambda Compute: AWS open-sources Firecracker virtualization technology Databases: Amazon Quantum Ledger Database Timestream timeseries database DynamoDB per-request billing DynamoDB Transactions Serverless Aurora Data API Preview of Aurora Serverless (PostgreSQL) Storage: S Batch Operations (preview) S Intelligent-Tiering Security: AWS Control Tower AWS Security Hub Operations & Observability: AWS CloudMap CloudWatch Logs Insights Machine Learning: Textract Amazon Personalize AWS Sagemaker Ground Truth AWS Inferentiacustom-built chip for faster ML inference Amazon Elastic Inference Blockchain, what: Amazon Quantum Ledger Database Amazon Managed Blockchain Pre-re:Invent announcements Amplify Console AppSync Pipeline Resolvers Lambda + Kinesis Data Streams Upgrades Python . for Lambda AWS Transfer for SFTP Announcements: Websocket support for AWS Lambda What it is: Use websockets with your Lambda functions Why it matters: This is awesome. Websockets enable bi-directional interaction between client and server, making it much easier to do real-time functionality like chat. Previously, you could use AWS IoT to get Websockets with Lambda, but this is much cleaner. This feature is not released yet but coming soon. For our full explainer on why WebSockets are cool and how they make real-time apps so much easier (with architecture diagrams!), see here. Bring your own runtime to AWS Lambda What it is: A way to bring your own runtime to AWS Lambda Why it matters: You don't need to wait for AWS to add your favorite language -- you can bring your own! This is a great add and a common ask from Serverless users. Fans of more obscure languages will be particularly happy. Taking it one step further: At Serverless, we saw BYOR and decided to take it one step further. Check out Serverless Open Runtime on GitHub. Build and share common solutions to complex problems before they even get to your business logic. AWS Lambda Layers What it is: AMIs for Lambdabuild base layers that can be used across multiple Lambda functions Why it matters: Layers allow you to pack code or data into a base layer which is then packaged into your function packages. This can be used to handle difficult dependencies or to package common code across all of your Lambda functions. The Serverless Framework has day-one support of Lambda Layers, so you can start using it today! Here's how to publish and use Lambda Layers with the Serverless Framework. AWS IDE integration What it is: Deep AWS integrations with your favorite IDEs Why it matters: If you're a PyCharm, IntelliJ, or VS Code user, this is for you. Handy shortcuts and step-through debugging of Lambda functions. Much easier to get your functions into production! Better Step-Function Integrations What it is: Use services like SNS, ECS, DynamoDB, SageMaker, and more in your Step Functions. Why it matters: Huge step up for multi-step workflows. Rather than writing your own custom logic in Lambdas, you can interact with AWS services directly. Remember, the best code is the code you don't have to write. Want to get started with Step Functions? Check out this post on managing your AWS Step Functions with Serverless. Lambda Ruby Support What it is: AWS Lambda now supports Ruby! Get started with Ruby: The Serverless Framework already supports the Ruby runtime. Here's our guide on deploying your first API with Ruby. Lambda ALB Support What it is: Invoke Lambdas directly from ALB, without using API Gateway. Amazon Managed Streaming for Kafka What it is: A managed Kafka service for streaming data. Nested Apps using SAR What it is: The Serverless Application Model now supports nested applications via the Serverless Application Repository. AWS CloudMap What it is: A hosted service discovery system from AWS. Why it matters: This looks pretty neat. Typically, service discovery systems are more server-full as you're trying to find the hosts where your services are moving around. CloudMap has support for IP address discovery, but it also allows you to register services generally within AWS. You can register services within CloudMap, and other services can reach out to grab the current configuration as needed. Previously, serverless developers would use things like AWS SSM to manage this service discovery, but CloudMap looks like an interesting solution. Textract What it is: OCR++ service to extract text and data from documents, no machine learning experience required Why it matters: It builds on the capabilities of previous text-recognition services, correctly parsing tables and other tricky text formats. Also, it can be used by anyone, even those with no previous machine learning experience. This shows AWS's commitment to widening developer accessibility to cutting-edge tech. Amazon Personalize What it is: Real-time personalization and recommendation service Why it matters: This is the same recommendation foundation Amazon.com uses for their own product recommendations, and now they're making it available to everyone. The best part: they claim no machine learning experience is required in order to use it. Amazon Forecast What it is: Time series forecasting Why it matters: This is based on the same technology they use at Amazon.com, and no machine learning experience is required to use it. AWS Sagemaker Ground Truth What it is: A way to label your data for machine learning training. Why it matters: Machine learning relies on properly-labelled data to train your models, and this can be a manual, time-consuming process. Sagemaker Ground Truth helps with this with both automatic and manual labelling for your existing data sets. AWS Inferentia What it is: A custom-built chip from AWS to improve machine-learning inference Why it matters: Machine learning is all the rage, and it takes large amounts of computational power to train and inferj with machine learning. AWS is pushing the envelope, just like Google is with its TPU chips. Look for these to help the serverless crowd down the road. AWS Elastic Inference What it is: Add elastic GPUs to your EC instance for faster machine learning training and inference. Why it matters: There's a huge hunger for GPUs for machine learning, and AWS is making it easier to attach to your EC instances. You can get serious performance -- up to TeraFLOPS of performance -- and you pay on a per-hour basis, just like EC. Quantum Ledger Database (QLDB) What it is: A fully-managed ledger database Why it matters: Track and verify history of data changes. QLDB has similar mechanics to a SQL database but without the ability to permanently overwrite or delete data. This can be very useful for times when you have strong audit requirements and need to ensure long-term integrity and completeness of your data. Also, obvious usage for blockchain technology, if that's your thing. Amazon Managed Blockchain What it is: Lets you create and manage blockchain networks. Essentially, blockchain-as-a-service. Why it matters: It makes it much easier to set up a blockchain network on Ethereum. We're admittedly more instantly excited about the Quantum Ledger Database, but there are some potentially interesting applications for companies to get up and running with smart contracts more easily with the Managed Blockchain service. Timestream timeseries database What it is: A fully-managed timeseries database Why it matters: AWS continues to innovate on purpose-built datastores and now adds a time series database. Time series databases have grown in popularity in recent years. Having a fully-managed solution is a great win for serverless fans! DynamoDB per-request billing What it is: You can know pay for DynamoDB on a per-request basis, rather than pre-provisioned read and write capacity. Why it matters: DynamoDB continues to make huge progress. One issue with DynamoDB with serverless is that you had to determine your capacity ahead of time. No more. Like AWS Lambda, you can now pay per-request. This is great for coupling the cost to the value you're provided your users. Should you use it right now? We wrote a full guide on when (and when not) to use DynamoDB on-demand, plus how to implement it in your existing serverless applications. See the full DynamoDB on-demand guide here. AWS Control Tower What it is: A centralized place to manage multiple accounts in AWS. Why it matters: This is a great addition for Serverless users. We're seeing a lot of teams that have separate accounts for each stage. Or, a team might give an isolated account for each developer for quickly testing changes before moving into the official CI/CD pipeline. This makes it a lot easier to give your developers flexibility without having an Excel sheet of AWS accounts. AWS Security Hub What it is: A tool to centrally manage security and compliance across many AWS accounts. Why it matters: Like the AWS Control Hub, this helps manage the growing number of AWS accounts under your purview. Security has long been a tough thing for fast-moving product teams, and a centrally managed tool like this will help you move fast and stay secure. DynamoDB Transactions What it is: DynamoDB now supports transactions. Why it matters: The best database for Serverless gets better and better. Now you can read and/or write multiple items on a single table or across multiple tables and get ACID transactions. This is a great addition and removes a lot of complicated logic from client libraries. CloudWatch Logs Insights What it is: A faster, better query language for CloudWatch logs. Why it matters: CloudWatch Logs has been the default logging solution for AWS Lambda and all container-based services from AWS. However, it hasn't kept up with third-party logging solutions out there. This is a step in the right direction to make it easier to see what's happening in your serverless applications. Amplify Console What it is: Deployment and hosting platform for web applications with serverless backends. Easily build and deploy your static site using Gatsby, Hugo, Jekyll, or other static site generators, as well as your backend APIs. Why it matters: JAMStack, here I come! This is a low-config way to manage your JAMStack. Think Netlify, but with backend functions as well. For many projects, this is a great way to get your code from dev to production quickly. AWS open-sources Firecracker virtualization technology What it is: Firecracker is a virtual machine manager built by AWS that hosts Lambda functions and Fargate containers. It's extremely lightweight, able to create a microVM in as little as milliseconds. Why it matters: We have a full explainer on Firecracker, and what it means for serverless developers. In sum, this isn't something most serverless users should care about. Yes, ServerlessHasServers, but you don't need to know about them! However, it's still really exciting to see the amazing tech that is underlying all of these services from AWS. Further, the fact that Firecracker is open source means that it could receive community contributions that continue to push the envelope on serverless performance. It's great to see AWS making core, original contributions to the open-source community. S Batch Operations (preview) What it is: Select batches of existing objects in S to run actions on -- add tags, copy to another bucket, or even send to Lambda functions. Why it matters: This eliminates a ton of toil around operating on existing objects in S. You would need to write a ton of custom logic to make sure you're hitting the right objects, handling errors, etc. Now you can easily manipulate a huge block of objects in a single go. Serverless Aurora Data API What it is: An HTTP endpoint for accessing your Serverless Aurora database. Why it matters: This is a big deal. In our post last year on why Serverless Aurora is the future of data, we noted that an HTTP-accessible relational database would be a huge step forward for the Serverless ecosystem. AWS is starting to deliver on this promise. Friend-of-the-Framework and all-around awesome guy Jeremy Daly has done a great review of the Serverless Aurora Data API. TL;DR: It's not quite ready for primetime. AWS often releases things early and rapidly improves them, so look for this to get a lot better in . Preview of Aurora Serverless (PostgreSQL) What it is: A PostgreSQL-compatible version of the Aurora Serverless database is now available in preview. Why it matters: We're very bullish on Serverless Aurora being an important tool in the Serverless ecosystem. The MySQL-compatible database was released earlier this year, and now the PostgreSQL version is getting closer. This is great news for Postgres fans. AppSync Pipeline Resolvers What it is: Break up GraphQL resolvers into multiple steps when using AppSync. Why it matters: AppSync is a great way to build Serverless GraphQL applications, and this service continues to get more and more powerful. The pipeline resolvers are great for adding authorization to the front of your GraphQL api or for more complex flows. AppSync is definitely a service to watch in . Lambda + Kinesis Data Streams Upgrades What it is: AWS Lambda can now use Kinesis Data Streams Enhanced Fan-Out, a faster implementation of consumers for Amazon Kinesis Why it matters: The Enhanced Fan-Out for Kinesis Data Streams greatly increases the performance of Kinesis Data Streams. You can read up to MB per second per shard on your Kinesis Data Stream. Further, you can have multiple, independent consumers with the Enhanced Fan-Out that helps you get around the limitations of previous Kinesis consumers. This is a huge step forward for fans of stream-based processing with AWS Lambda. Python . for Lambda What it is: AWS Lambda now supports the Python. runtime. Why it matters: You get all the latest Python features with your Lambdas! The most exciting addition to Python. is likely dataclassesa much simpler way to define classes. How do I use it: You can use Python. in the Serverless Framework by setting `runtime: python.`. The built-in `aws-python` template will use Python. in the next release of the Framework. AWS Transfer for SFTP What it is: A managed SFTP service for Amazon S Why it matters: Lock down your file transfers with SFTP, without modifications to your app, and without needing to manage any SFTP servers. S Intelligent Tiering What it is: A new storage class for S which intelligently moves your objects between Standard Storage and Infrequent Access based on the individual object's access patterns. Why it matters: This is a great addition from AWS to help you save money on your bills. Choosing the right storage class for your S objects can be a chore. Doing it manually often results in subpar pricing decisions. This is another example of AWS managing the boring stuff so you can focus on what matters to your users.",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f62",
      "title": "What Firecracker open-source means for the serverless community",
      "content": "At this year's re:Invent (see here for live recap), AWS announced that it was open sourcing Firecracker. If youve never heard of Firecracker, its the technology that powers innovative serverless compute from AWS, like Lambda and Fargate. In a nutshell, Firecracker is a virtual machine managerresponsible for launching, managing, and killing tons of tiny virtual machines on a server. Its ideally suited for serverless because it marries the capabilities of virtual machines (security, isolation) with the capabilities of small and agile functions (speed, resource efficiency). It is insanely performant. AWS did a live demo of Firecracker after announcing it, during which they spun up micro-VMs at once. The longest one took a mere ms; on average the VMs take ms to spin up. And now, this powerful technology is open-source. What does this mean for serverless at large? Firecracker open source: implications for Lambda or Fargate users If youre consuming serverless services like Lambda or Fargte right now, then honestly, you shouldnt care that much about Firecracker. This is the magic of serverlessyou get all of AWS improvements for free, without needing to migrate instances or run upgrade scripts. Firecracker being open-sourced does mean that there are more opportunities to improve upon Lambdas coreto make it more performant, etcbut overall, you probably wont (and shouldnt) personally use Firecracker. Who Firecracker does matter for Firecracker could be pretty useful to you if youre building container orchestration platforms or running loads of containers, and need to do so with sub-second latency. For instance, Kubernetes can use Firecracker to start micro-VMs. Firecracker could also be extremely useful to you if youre running on-premises at massive scale. However, wed be remiss not to mention that managing low-level infrastructure, especially all the way down to managing micro-VMs, is a bit against the serverless ethos. In sum Yes, Firecracker powers serverless compute like Lambda. But should you worry about it as a serverless developer? Probably not too much. Things you can possibly expect from Firecracker going open source are future improvements to Lambda functionality and performance, via contributions from the open source community. But it is kind of cool to see AWS initiating an open source project like this. A few years ago, AWS barely contributed to open source at all. Then, they started contributing to popular projects like Kubernetes in ways that made it easier to run on AWS. Now, were seeing them originate and open source foundational projects. More re:Invent news How to publish and use Lambda Layers with the Serverless Framework Join the Serverless virtual hackathon at re:Invent!(ends Sunday at : PM PT) All the Serverless announcements at re:Invent ",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f64",
      "title": "Real-time applications with API Gateway WebSockets and AWS Lambda",
      "content": "AWS just announced the launch of a widely-requested feature: WebSockets for Amazon API Gateway. This means Framework users around the world finally have a straightforward way to create client-driven, real-time applications via WebSockets. Read on for more info on how this changes the game for real-time development, and the Serverless Framework plans to support WebSockets in API Gateway! Life before WebSockets support Sure, there were hack-y ways to do real-time applications before. You could, for instance, use AWS IoT topics with MQTT over WebSockets, and I have even heard of folks running containers or cluster to broker WebSocket connections even though the rest of their systems are serverless. There is also fantastic WebSocket support in AWS AppSync, but this introduces some complexities with a GraphQL layer and you give up a bit of control in exchange for simplicity. Undoubtedly a great solution if it can meet your needs. WebSockets support makes real-time so much easier The WebSockets support that was announced today means developers have much more control over the WebSocket layer itself, delivering payloads directly to lambda functions and shuttling results back. WebSocket APIs enable you to support a WS connection to APIGateway, which can then invoke Lambda when a message is received (also on connect/disconnect). You can send a message by making a request to a callback URL with the connectionId.&mdash; Dougal Ballantyne @ re:Invent (@dsballantyne) November , There are other classes of services out there, like Ably or PubNub, which offer solutions in this space. But staying closely integrated to your platform provider for core services can make a lot of sense for many organizations. Security teams and billing departments appreciate working with a known vendor. A real-time WebSockets example To understand the power of this new feature, lets look for example architecture building the canonical chat example for real-time WebSocket driven applications. With these native WebSockets in API Gateway, you establish a single WebSocket connection to API Gateway from the device. AWS Lambda is notified of the connection in your normal event-driven compute method. You get some metadata, the payload and a connectionId that you use later. It's probably a good idea for you to store this connectionId and information the device sent you (perhaps topics or channels they are subscribed to) in a datastore such as DynamoDB, so you can reference it later when needed. Next, say someone wants to send a new message out to the channel. They would send it over their established WebSocket to API Gateway, to a waiting AWS Lambda function. On invoke, the business logic would check your datastore for the connections subscribed to that channel, and callback to API Gateway with the connectionId and your payload. API Gateway will take it from there and send your payload through on the established WebSocket connection. You Lambda function would be invoked on disconnects as well, allowing you to clean things up in your data store so you don't waste cycles trying to send messages to non-existent connections. In sum: this is simple, event-driven, and real time. This single feature makes a whole new class of applications first class citizens in the serverless ecosystem! Serverless Framework support WebSockets for API Gateway is unfortunately not GA yet. We are working to bring this new feature to the Serverless Framework ASAP so you can leverage it as it becomes available, so stay tuned!",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f66",
      "title": "How to publish and use AWS Lambda Layers with the Serverless Framework",
      "content": "AWS re:Invent is in full swing, with AWS announcing a slew of new features. Most notably, were pretty excited about AWS Lambda's support for Layers. Layers allows you to include additional files or data for your functions. This could be binaries such as FFmpeg or ImageMagick, or it could be difficult-to-package dependencies, such as NumPy for Python. These layers are added to your functions zip file when published. In a way, they are comparable to EC AMIs, but for functions. The killer feature of Lambda's Layers is that they can be shared between Lambda functions, accounts, and even publicly! There are two aspects to using Lambda Layers: Publishing a layer that can be used by other functions Using a layer in your function when you publish a new function version. Were excited to say that the Serverless Framework has day support for both publishing and using Lambda Layers with your functions with Version ..! See how you can publish and use Lambda Layers with the Serverless Framework below. Example use case: Creating GIFs with FFmpeg For a walkthrough, lets make a service that takes an uploaded video and converts it to a GIF. Well use FFmpeg, a open source tool for manipulating video and audio. FFmpeg is a binary program and a great example use case for a layer as managing the binary falls outside the responsibility of your runtimes packaging system. In this example, well build and publish a layer that contains FFmpeg. Then, well create a Lambda function that uses the FFmpeg layer to convert videos to GIFs. To get started, create a serverless project for your layer & service:  Then at the bottom of your `serverless.yml` add the following to define your layer that will contain FFmpeg. The `path` property is a path to a directory that will be zipped up and published as your layer:  Run the following commands to download the contents of your layer:  Youre ready to test deployment of your layer. Deploy and youll see the layers ARN in the output info:  Next, well add a `custom` section to `serverless.yml` to specify the S bucket name (choose your own unique bucket name):  Now rename your function from `hello` to `mkgif`, specify that your function uses the layer youre publishing, and add an S event configuration:  Youll also need to give your service permission to read & write your S bucket, add the following in the `provider` section of your `serverless.yml` file:  Your `serverless.yml` should now look like this. We need to make our handler. Replace the contents of `handler.js` with the following code, which gets the file from S, downloads it to disk, runs ffmpeg on it, reads the GIF, and finally puts it back to S: ```javascript const { spawnSync } = require(\"child_process\"); const { readFileSync, writeFileSync, unlinkSync } = require(\"fs\"); const AWS = require(\"aws-sdk\"); const s = new AWS.S(); module.exports.mkgif = async (event, context) => { if (!event.Records) { console.log(\"not an s invocation!\"); return; } for (const record of event.Records) { if (!record.s) { console.log(\"not an s invocation!\"); continue; } if (record.s.object.key.endsWith(\".gif\")) { console.log(\"already a gif\"); continue; } // get the file const sObject = await s .getObject({ Bucket: record.s.bucket.name, Key: record.s.object.key }) .promise(); // write file to disk writeFileSync(`/tmp/${record.s.object.key}`, sObject.Body); // convert to gif! spawnSync( \"/opt/ffmpeg/ffmpeg\", [ \"-i\", `/tmp/${record.s.object.key}`, \"-f\", \"gif\", `/tmp/${record.s.object.key}.gif` ], { stdio: \"inherit\" } ); // read gif from disk const gifFile = readFileSync(`/tmp/${record.s.object.key}.gif`); // delete the temp files unlinkSync(`/tmp/${record.s.object.key}.gif`); unlinkSync(`/tmp/${record.s.object.key}`); // upload gif to s await s .putObject({ Bucket: record.s.bucket.name, Key: `${record.s.object.key}.gif`, Body: gifFile }) .promise(); } }; ``` Now you can deploy both the layer & updated function with `sls deploy`. Lets test it out by uploading a video to our S bucket:  You now have a GIF copy of the mp you uploaded! For the full source of this example, check it out in our examples repo. Some tips on working with layers In the example above, instead of specifying an ARN for the layer that the function is using, we used `{Ref: FfmpegLambdaLayer}`. This is a CloudFormation Reference. The name is derived from your layer's name, e.g., `ffmpeg` becomes `FfmpegLambdaLayer`. If you're not sure what your layer's name will be, you can find it by running `sls package` then searching for `LambdaLayer` in `.serverless/cloudformation-template-update-stack.json`. You may have noticed that every time you deploy your stack, a new layer version is created. This is due to limitations with CloudFormation. The best way to deal with this is by keeping your layer and your function in separate stacks. Let's try that with the example we just made. First, create a new folder and move the layers directory into it:  Remove the top-level `layers` section in `gifmaker/serverless.yml`, then create a new `serverless.yml` in the `ffmpeg-layer` folder containing: ```yaml service: ffmpeg-layer frameworkVersion: \">=.. ",
      "__v": 0
    },
    {
      "_id": "64e08924b72e199dda603f68",
      "title": "Introducing: the Serverless Open Runtime",
      "content": "Its time to get excited: AWS has just announced Bring Your Own Runtime (BYOR) for AWS Lambda. While running your own languages has some pretty obvious benefits, we at Serverless, Inc want to take things to the next level. We see this as an opportunity to unlock more flexibility, organization and customization within your serverless runtimes and use casesnot just for AWS, but for any event-driven compute platform. That is why we are releasing the Serverless Open Runtime! Check it out now on GitHub, or keep reading for all the details. The Serverless Open Runtime The Serverless Open Runtime makes it simple to build and share common solutions to complex problems. Better yet, it makes it possible to solve these problems before they even get to your business logic. This could mean capabilities like: more graceful timeouts the ability to transform AWS (or other provider-specific) events to CloudEvent spec or HTTP requests more detailed tracing and debugging middleware implementations in any language, compatible with your service regardless of the language you chose to use prepackaging of common libraries better local development experience and emulation security implementations or perhaps even running a sidecar for service discovery, managed by the runtime What a list. And Im sure there are many interesting and powerful use cases yet to be thought of. How it works This is all possible with the concept of middlewares and pluggable architecture to the Serverless Open Runtime. With a pipelined approach to the event request/response lifecycle, its straightforward to build and integrate new capabilities. And if middleware is written as simple binaries, in many cases the middleware could be language-independent to increase reusability. Heres an example of a constructed runtime that leverages some request and response middleware examples (explanation below): The Open Runtime gets started when Lambda first receives a lambda execution request. It then actually fetches a request from the AWS Runtime API. This request is then processed by middlewares, by invoking them as executables and passing the event in via STDIN and reading the processed event back via STDOUT. It is then passed to the language specific runtime which invokes your handler code. The response is then processed by middlewares similarly to how they were invoked for the event. These are just examples, and the capabilities of middleware extend well beyond them. The important thing to note, is that the only thing you would have to worry about is Your Business Logic. The Serverless Open Runtime and its middlewares get out of you, the developers, way; you continue to focus on producing value for the business. Why Serverless Open Runtime? We see this initiative as joining the layerability and flexibility of containers, with the serverless promises of on-demand, event-driven compute. In sum: this is containers as they should be. Of course, your environment and requirements are going to have many similarities with others. The Serverless Open Runtime will enable you to customize your serverless experience within your organization, while still remaining standardized. Contribute to Serverless Open Runtime The opportunity to do something great is here, and we are excited about what will be built on the Open Runtime concepts! You can check out the github repository to learn more, and peep the initial drafts of source code. As always, the Serverless community is relentlessly innovative, and we are always open to your thoughts as we push forward on this new initiative. More re:Invent news All the Serverless announcements at re:Invent How to publish and use Lambda Layers with the Serverless Framework What Firecracker open-source means for the serverless community Join the Serverless virtual hackathon at re:Invent; participate from anywhere, win prizes!(ends Sunday at : PM PT)",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f6a",
      "title": "Building an API with Ruby and the Serverless Framework",
      "content": "On the heels of re:Invent, it's been a great week for the serverless community. And one of the most exciting things in AWS's re:Invent goodie basket? Ruby support for Lambda! Personally, I love Ruby, and was really excited to play around with a Ruby deployment on the Serverless Framework. So here you have it, Ruby fans. Your Ruby + Serverless Framework getting started template. I'm going to start by covering some Ruby + Serverless Framework basics and testing practices, and then we'll build a fully-fledged Ruby API. Let's get to it. Getting Started First up, we need to install the Serverless Framework (if you haven't already), and create a new Ruby service:  Navigate to your new service folder, and deploy the default `hello world`:  Testing Locally Right now the framework only supports NodeJS, Python and Java in the local testing. (We hope to get that updated soon!) In the meantime, because Ruby is awesome, we have some great testing tools and capabilities right at our fingertips with some of the built-in testing tooling. Let's say we want to run our local handler and make sure it returns the status code:  Open `test/handler_test.rb` in a and copy/paste in the following code:  As always, let's update our test to make sure we're still happy. Make `test/handler_test.rb` contain the following: ```ruby require_relative '../handler.rb' require 'test/unit' class TestHandler ",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f6c",
      "title": "A Serverless Twitter bot helps house Camp Fire victims",
      "content": "For those of you not living in Northern California, Im sure youve at least heard of our most devastating wildfire to date: Camp Fire, which spread out north of Sacramento, taking out entire towns like Paradise, Concow and Magalia. I live in Chico, a mere twenty minutes from Paradise, where thousands of people lost their homes in a span of hours. Especially because of my proximity, the devastation from the fire was really poignant for me. Many of these people werent strangers; they were my friends. I knew I wanted to do everything I could to help out, and housing was the biggest need. There was a website, campfirehousing.org (created by nvpoa.org), where individuals could post about temporary housing opportunities for Camp Fire victimsextra bedrooms in homes, and things like that. The idea behind that website was fantastic, but it was missing some critical features that I knew would help people find housing faster. For instance, there was no way to be notified when a new housing opportunity was posted, and it was hard to tell which postings were still open. People who were looking for places to stay still had a hard time seeing what was even truly available, and jumping on new housing as it got added to the site. So I thought, hm. How long would it take to throw some code together that would notify people of new housing opportunities as soon as they were posted? The Serverless Twitter bot I decided to create a Twitter bot that would tweet out housing opportunities as they were added to campfirehousing.org. Anyone who was interested could follow the bot on Twitter and stay on top of what was newly-added and available. I chose to power the app with a serverless backend for a couple reasons. For starters, it was going to be the quickest way to get the app off the ground and helping people. On top of that, due to Lambdas generous free tier, it was almost certainly going to cost me nothing. The whole thing was built with the Serverless Framework + AWS Lambda. I was able to get this entire project off the ground in only two hours, from initial research to first successful tweet. Building the bot: just one function Campfirehousing.org didnt have an RSS feed, so I had to set up a CRON (scheduled function) that would check the website for updates every five minutes, and read the values from a Google Sheet. When there were new postings, my Twitter bot would tweet them out. The tweets included important info like price, city, availability date, whether pets were allowed, and a short description. I scraped all this data directly from campfirehousing.org, and truncated the full text down to characters when applicable. Posted: Dec th @ ::AM bed/ bathPrice: $City: Valencia (Santa Clarita)Term: TemporaryAvailable: //Pets: NoDescription: Bedroom(s) Available&mdash; CampFireHousing (@CampFireHousing) December , Overall, the code is just a pretty simple and straightforward single function; check it out here and feel free to use it in your own projects! You can also find the full Twitter stream at @CampFireHousing. Challenges along the way The hardest part of this entire project was actually figuring out how to handle dates. I was working locally on my machine in PST, but when I ran code in AWS it defaulted to GMT. I ended up having to tell Lambda function to run as though it was in PST, so I could have the same experience locally as in the cloud. A special shout-out to the Serverless Framework and open source Working on side projects like these make me realize how amazing the open source community is. Im grateful for the open source projects that allow date manipulation and quick tweet integrations, and for how easy the Serverless Framework made it to ship my code into the world. The serverless-plugin-typescript plugin made writing TypeScript functions in Lambda super easy, for example. Its just awesome to know I can think of a project, and be able to lean on open source contributions like this to make it happen much quicker and easier than I could do it all alone. A note in closing from the Serverless team We absolutely love the way that serverless technologies are enabling people to build applications that serve their communities like this! If youve done something for a non-profit with Serverless, please dont hesitate to share it with us so we can help you spread the word. We want your code to help others do the same. opensourceforever",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f6e",
      "title": "DynamoDB On-Demand: When, why and how to use it in your serverless applications",
      "content": "At re:Invent , AWS announced DynamoDB On-Demand. This lets you pay for DynamoDB on a _per-request basis_ rather than planning capacity ahead of time. We at Serverless are really excited about this new pricing model and can't wait to use it in our applications. This post is your one-stop-shop on all things DynamoDB On-Demand + Serverless. In this post, we'll cover: What is DynamoDB On-Demand How do I use DynamoDB On-Demand in my Serverless applications When does it make sense to use DynamoDB On-Demand Other facts and questions about DynamoDB On-Demand - Can I use it with my existing tables? - How does this compare with reserved capacity? - What does this mean for hot partitions? - Are there any limits? Let's get started! What is DynamoDB On-Demand? DynamoDB On-Demand is a new pricing model for DynamoDB. Previously, you had to set read and write throughput capacity on your DynamoDB tables. This specified how many and how large of reads and writes you could make on your table in any given second. Read and write capacity units were charged by the hour, and your requests would be throttled if you exceeded your provisioned capacity in any given second. The throttles could be an annoyance, particularly for Serverless developers. The whole premise of Serverless is based on auto-scaling, pay-per-use so that I don't have to think or care about capacity planning ahead of time. Yet there I was, trying to predict how many kilobytes of reads per second I would need at peak to make sure I wouldn't be throttling my users. In , DynamoDB added Auto-Scaling which helped with this problem, but scaling was a delayed process and didn't address the core issues. With DynamoDB On-Demand, capacity planning is a thing of the past. You don't specify read and write capacity at allyou pay only for usage of your DynamoDB tables. This fits perfectly with the Lambda and Serverless modelI pay more when I have more usage, which means I'm delivering more value to my customers. Now that you know that DynamoDB On-Demand is a great fit with your Serverless applications, let's see how you can use it with the Serverless Framework. How do I use DynamoDB On-Demand in my Serverless applications (First of all, huge props to Doug Tangren for publishing a guide on how to do this with the Serverless Framework!) If you're using DynamoDB tables in your Serverless Framework applications, you're likely managing your tables using infrastructure-as-code in the `resources` block of your `serverless.yml` file. To move from provisioned capacity to on-demand pricing, you need to do two things: Remove the `ProvisionedThroughput` section of your DynamoDB table. If you're using any Global Secondary Indexes, you should also remove the `ProvisionedThroughput` section of your indexes. Add `BillingMode: PAY_PER_REQUEST` to your table. That's it! Below is an example of how your table will change: For a starter that you can copy and paste into your `serverless.yml`, use the block below:  When does it make sense to use DynamoDB On-Demand DynamoDB On-Demand is a great step forward, and I'm excited to not have to think about capacity-planning for my databases anymore. That said, there may be times when you want to use the traditional provisioned mode. In general, there are two reasons why you may want to use DynamoDB provisioned pricing rather than on-demand pricing: You can predict your traffic patterns pretty well. You are worried about a runaway bill. This is pretty rare, so see comments below. You have predictable traffic patterns If your applications have predictable traffic patterns and you don't mind spending the time to understand those patterns, using DynamoDB's provisioned throughput capacity can save you money. Let's walk through some basics on DynamoDB pricing and then do some simple math. DynamoDB charges in terms of read and write request units. A read request unit lets you read KB of data in a strongly-consistent way. A write request unit lets you write KB of data in a standard way. Note: There's additional nuance both in read requests units (strongly-consistent vs. eventually-consistent) and in write request units (standard vs. transactional). The math ends up similar, so we'll skip the complexity. With on-demand pricing, you pay directly based on the requests you use. In `us-east-`, on-demand pricing costs $. per million write request units and $. per million read requests units. With provisioned throughput, you pay based on having the capacity to handle a given amount of read and write throughput. You pay for _read and write capacity units_. Each read capacity unit allows you to handle one read request per second and each write capacity unit allows you to handle one write request per second. Read and write capacity units are both charged on an hourly basis. In us-east-, read capacity units cost $. per hour and write capacity units cost $. per hour. If we normalize the capacity units to a -day month, a read capacity unit costs $. per month and a write capacity unit costs $. per month. If you _fully utilized_ your capacity units, each unit would give you ,, requests in a -day month. With this full utilization, you would be paying $. per million read requests and $. per million write requests. Thus, DynamoDB On-Demand pricing is about .x the cost of provisioned capacity. However, it's highly unlikely you are fully-utilizing your provisioned capacity units. This difference in pricing is the maximum difference if you're working at peak capacity. It would be near impossible for you to have % utilization of your read and write capacity units for every second of a month. That being said, if you have predictable patterns and can stay at %+ utilization over a month with low maintenance around it, using provisioned throughput can be the right move for you. This is even more true if you utilize reserved capacity to knock down the provisioned throughput pricing even further. You have concerns about a runaway bill For the FUD crowd out there, it is possible that on-demand billing could result in an unexpected bill spike. If you have a huge spike in DynamoDB requests due to a spike in your application's popularity, it's possible that you will pay much more in DynamoDB costs than if you were using provisioned throughput. With provisioned throughput, you're essentially putting a cap on how much you will spend on your DynamoDB table. Your users will pay the price with throttling. With on-demand, your bill could be unexpectedly high but you won't be making your users pay the price with a poor experience. Ultimately, having happy users should be more important than short-term budget fluctuations. If you are worried about runaway costs, there are better ways to handle it than capping it via provisioned throughput. Other facts and questions about DynamoDB On-Demand Here are some other commonly-asked questions about DynamoDB On-Demand: Can I use it with my existing tables? Yep! You can switch your tables over to it right now. You can do this in your `serverless.yml` as shown above. You can also do it in the AWS console (but you really shouldn't do it in the consoleuse infrastructure-as-code!): How does this interact with reserved capacity? With DynamoDB's provisioned capacity, you can use reserved capacity. With reserved capacity, you pre-pay for a certain amount of provisioned capacity. In return, you get a lower price. It is similar to Reserved Instances with AWS EC. At this point, you cannot use reserved capacity with DynamoDB On-Demand. It is a feature of provisioned capacity only. With so many optionsreserved capacity, provisioned capacity, on-demandhow do you know which pricing option is right for you? Here's a quick guide: If you have _steady, predictable traffic_, choose reserved capacity. Since you know you need a certain amount of capacity at all times, you can save from reduced rates. If you have _variable, predictable traffic_, choose provisioned capacity. Imagine you have significant traffic during the day but no traffic overnight. Reserved capacity would be wasted overnight, but your patterns are predictable enough that you could scale up your provisioned capacity when you need it. If you have _variable, unpredictable traffic_, choose on-demand. If your application gets random spikes, it can be hard to provision capacity to match demand. Use the on-demand feature so you don't throttle your users. What if you don't know what bucket you fit into? Just choose on-demand and let AWS handle it for you. Get back to building! What does this mean for hot partitions? Over the years, there has been a lot of content around managing DynamoDB's partitions to avoid degraded performance. For example, you need to avoid hot partitions so that you get the most of your throughput. Or you need to worry about throughput dilution from excessive scaling. So how does DynamoDB On-Demand play with this? The first thing you need to know is that the partition problems have largely gone away. The DynamoDB team has done a bunch of work behind the scenes around adaptive capacity. This helps adjust to your usage patterns so that additional compute is shifted to hot partitions to avoid resource exhaustion. From some early tests, it seems like On-Demand has no issues with scaling up and hot partitions. In Danilo Poccia's example with On-Demand, he shows a table that scales from _zero to , write units per second_ without any throttling! The rapid, instant scaling of DynamoDB On-Demand is truly impressive and a major feat by the DynamoDB team. Jim Scharf, the former GM of DynamoDB had a great answer when our own Jared Short asked him how this affects partitions: Why does it matter? I know customers used to need to think about partitions, but with on demand and adaptive capacity, were really taking big steps towards this being an unnecessary detail for customers. There are good Reinvent talks on this. See DB blog for listing.&mdash; Jim Scharf (@jim_scharf) December , Exactly\"Why does it matter?\" Partition-planning is basically an unnecessary detail for users. Are there any limits with DynamoDB On-Demand? There are a few limits you should know about. First, there are some limits on how high DynamoDB On-Demand can scale up. By default, that limit is , read request units and , write request units per table in most regions. You can increase that if needed. Those numbers are _per second_, so we're talking some serious traffic. Second, you are limited in how often you can change between provisioned capacity and on-demand pricing. As of now, it looks like you can only switch once per day. Conclusion DynamoDB On-Demand pricing is a huge move forward for Serverless applications and will be the default on all of my tables moving forward. In this guide, we discussed how, when, and why to use it. Now, go build!",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f70",
      "title": "Serverless Framework v..: Local Invoke Ruby, CloudFormation variable syntax",
      "content": "Hot on the heels of our day support for Lambda Layers and the AWS Lambda Ruby Runtime, were announcing an even fresher Serverless Framework v.! This release is shipping with features, enhancement and bug fixes, as well as accompanying documentation updates. Features Local invoke Ruby support PR , thank you Dean Holdren - @dholdren! You can now `serverless invoke local -f {function}` for AWS Lambda Ruby runtimes. This makes it easier to pass in test payloads and quickly iterate locally. An important note, you will want to make sure you are running a Ruby version equal to AWS Lambdas runtime (as of this writing, it is `.`). macOS system Ruby likely will NOT work properly. AWS `${cf}` syntax now supports outputs from other regions PR , thank you TATSUNO Yasuhiro - @exoego Often, youll want to centralize configs in CloudFormation outputs, making them easy to access for use in other services. However, this becomes complex in the case that you want to have a multi-region service, or otherwise regionally distributed services. This feature addition makes it so you can optionally specify a region to look at for outputs, allowing cross-region resolution of the outputs. Usage is as simple as`${cf.REGION:stackName.outputKey}`. For example, `${cf.us-east-:my-service-dev.kinesisStreamArn}`. Check out the docs for more information on CloudFormation output usage in the variable resolution system. Enhancements Faster exclusion of files during packaging PR , thank you Jeff Soloshy - @MacMcirish. You may know that you can exclude various things from being packaged, including dev only packages in `package.json`. You can check out the docs for more info. This could be a slow process if you had a large project with many dependencies. According to Jeff, when running package on a project with a large file count with many dev dependencies. The difference in timing is ~s compared to ~ minutes. - github. WOW! Bug fixes - PR `logRententionInDays` regression fix, once again parses strings to integers. - PR Set reserved function concurrency even if it was set to . - PR Set env vars from --env last in invoke local, allowing for more intuitive overrides. - PR Preserve whitespace in variable fallback - PR Upgrade to aws-sdk v... Fixes SDK bugs aws-sdk package had with using profiles. Roadmap & focus We are focusing our internal efforts on eliminating regressions, and tackling the backlog of bugs. Other members of our fantastic community have been adding features and enhancements, and as always these contributions are welcomed and valued! Contributor thanks Each release, there are always many people involved. This release is no different, and we would like to thank everyone below for their contributions and participation in the community. We couldnt do it without you! Rupak Ganguly - @rupakg, Dean Holdren - @dholdren, TATSUNO Yasuhiro - @exoego, Jeff Soloshy - @MacMcirish, Federico - @asyba, Jaap Taal - @qjaap, Joshua Napoli - @joshuanapoli, Enrique Valenzuela - @enriquemanuel",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f72",
      "title": "SQQUID: a % serverless startup",
      "content": "My name is Ron Peled and Im the CEO and founder of SQQUIDa platform that automates merchandising and fulfillment for retailers. We help small- and medium-sized retailers automate their web order processing and shipping from brick-and-mortar stores, lowering their costs to enable them to compete against the likes of Walmart and Amazon Prime. Its a mission that we really stand behind, and it puts us on a path of disrupting a gigantic industry that has tons of old, legacy infrastructure supporting it. The only way to break new ground is to move fast and run lean. When we launched, our CTO and I made a bet on Serverless. We were early movers on AWS Lambda and the Serverless Framework. From the beginning, SQQUID has been a near % serverless shop. This was one of the best decisions weve made, and because of it the tech stack at SQQUID is a fun and exciting environment to build upon. If you want to learn all about what it means to go fully serverlesswhat our architecture affords us, what new design patterns weve built, and how this environment allows our small team to iterate fasterthen read on! Before Serverless: Docker SQQUID isnt my first company. I was previously CTO and cofounder of Educents, a YC-backed education startup. At Educents, we relied on Docker. It was super scalable, but from a CTO and budgeting perspective, it was a nightmare. This was before AWS supported Kubernetes, so our infrastructure costs were sky high and we were in constant maintenance mode to keep everything running smoothly. I knew when I built my next startup, on the top of my priority list would be finding a way to move off a fully-managed Docker environment. The Serverless way forward When I started SQQUID, FaaS systems were just starting to gain popularity. After reading up on serverless for a couple months, we decided to go all in on AWS Lambda, the larger AWS ecosystem, and the Serverless Framework. It was a risky, but ultimately fantastic, decision. Because were serverless, were fast and agile. We consistently launch robust features with a relatively small team. We have so much power at our fingertips, its unbelievable. Serial Governor: feature architecture At SQQUID, one of our main jobs is to manage a multitude of integrations across platforms and across customers. As any business working with many external APIs understands, handling integration differences, like throttling or error management, can be very different from API to API (such as Magento, to BigCommerce, or Shopify.) On top of that, within each specific integration, there are individual account challenges from specific throttling limitations to custom field management. To manage all this, we've developed a design pattern we call our serial governor. The serial governor works to watch over a suite of concurrent Lambda executions for each integration (e.g., product and order imports), but run independently from customer to customer. During function execution, the system needs to manage the amount of concurrency per account (not just per function), and deal with API limitations and errors on both the customer level and overall integration level. This is important since our system is connecting to multiple channels with multiple accounts /. But error management is half the battle. The serial governor's error management system and dead man switch helps us manage issues arising from specific accounts and overall integrations. What happens when an integration has a system-wide hiccup or a particular customer's account has been declined? With Lambdas, it's easy to to just keep hitting the servers over and over again, though it's not very helpful to strong arm the situation. Maybe a particular server is having issues with load or networking issues. This is why our system includes logic to automatically back off a customer's integration. The more errors we get, the longer we wait to try the API again. When a successful connection is established, we get back up to speed ASAP. What did we gain from this new architecture? The combination of serverless environment, our serial governor design pattern, plus our error handling algorithms affords us tremendous flexibility and capabilities. We are now able to maximize throughput for each integration and for each customer accountsomething that would have been too resource-heavy for a startup to focus on in older devops paradigms. This architecture allows us more time to focus on new features and business logic because our systems are rarely down. Since we have some level of automatic healing, theres very little we need to do when an API isnt responding as it should or is down. In many cases, our customers rely on us to understand which ones of their other systems were unavailable and for how long. What we learned along the way: the serverless learning curve I wont liethe serverless (or FaaS infrastructure) learning curve can be steep, especially when youre used to monolithic architectures. That said, the benefits of serverless far outweigh any drawbacks. My team and I would only recommend FaaS architectures for any new projects. If youre thinking of going serverless, here are some things to know in advance. Think FaaS, not Monolithic Due to the nature of FaaS and Serverless architecture, you should think in terms of this new paradigm. Don't try to make old techniques or paradigms fit. Once you understand what this means, development can move so much faster and you start seeing the benefits of using function-based architecture. Protect your downstream systems Lambda is auto-scaling. Most APIs out there are not. We were reminded of this the hard way when we accidentally brought down another companys API, one with substantial data center infrastructure, by hitting their APIs too hard. Out of the box, Lambda errors will automatically launch a series of retries. A tsunami effect can start from there if you are not careful. On the bright side, our Lambda cost for this incident was only $. Ultimately, as a former CTO myself, its a mind-altering change to know that from the first line of code, your product is scalable. The challenge then becomes interfacing with these outside systems all of which have different throttling and requirements. Observability isnt great, but its getting better We also ended up building our own custom tooling for error handling and alerts because what was out there didnt fit our business needs. That said, we have seen improvements in Cloudwatch over the past year, and we expect this will get a lot better. In sum: serverless, or not? At SQQUID, we pride ourselves in providing top tier channel management, merchandising tools, and order fulfillment automation for retailers. Choosing serverless as our architecture enables us to best serve our customers. We are launching large-scale, robust features at significantly shorter intervals than we were able to at my previous startup. And we are doing it using a tenth of the workforce with incredibly low infrastructure costs. As a startup, this is a critical advantage. We are incredibly happy with our decision to go serverless. Best of luck on your serverless journey, and feel free to drop a comment if you have any questions!",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f74",
      "title": "Using API Gateway WebSockets with the Serverless Framework",
      "content": "Update: As of v., the Serverless Framework supports WebSockets in core. No need for a plugin! Read the announcement and how-to here. As we approach the end of , Im incredibly excited to announce that we at Serverless have a small gift for you: You can work with Amazon API Gateway WebSockets in your Serverless Framework applications starting _right now_. But before we dive into the how-to, there are some interesting caveats that I want you to be aware of. First, this is _not_ supported in AWS CloudFormation just yet, though AWS has publicly stated it will be early next year! As such, we decided to implement our initial support as a plugin and keep it out of core until the official AWS CloudFormation support is added. Second, the configuration syntax should be pretty close, but we make no promises that anything implemented with this will carry forward after core support. And once core support is added with AWS CloudFormation, you will need to recreate your API Gateway resources managed by CloudFormation. This means that any clients using your WebSocket application would need to be repointed, or other DNS would have needed to be in place, to facilitate the cutover. I recommend you check out my original post for a basic understanding of how WebSockets works at a technical level via connections and callbacks to the Amazon API Gateway connections management API. With all that out of the way, play with our new presents! How it workswe kept it familiar Integrating a WebSocket API in your serverless app will feel like second nature if youre already using our `http` events. A simple application might look something like the following `serverless.yml`:  To get started from scratch, you'll need to create your serverless project: `sls create --template aws-nodejs`. Go ahead and `npm install --save serverless-websockets-plugin`, and then add the plugin to your `serverless.yml` plugins listing:  Check the plugin docs for more about configuration of the plugin and related events. The hello world of WebSocket apps No release of anything using WebSockets would be complete without an example app, so we put one together. And it just so happens to be a massively scalable, serverless chat app. Were leveraging the usual suspects here: API Gateway WebSockets (of course), AWS Lambda, DynamoDB, andperhaps the most interesting piece of this entire thingwe'll talk about DynamoDB streams. Chat app architecture As users connect and disconnect, we store their connection Id in the DynamoDB table, as well as register them into the \"General\" chat channel. Users can then: - Subscribe to a channel (the first subscription creates the channel) - Unsubscribe from a channel - Send a message to all users in a channel Each time any of these things occurs, we send out a broadcast to all subscribers of a channel what has happened. If someone joined the channel, left (or disconnected and left all channels), or a message was sent. When a user disconnects, we use the \"disconnect\" message from API Gateway to delete all the connection subscriptions so we don't waste cycles trying to send messages to dead connections. When a user sends a message via the WebSockets, we look up all the subscriptions and their connection Id's from the DynamoDB table, and send them a message over their corresponding WebSocket with the content and other informationstraightforward behavior, and similar to what you would expect for WebSockets. Why DynamoDB streams? So, what are we leveraging DynamoDB streams for you ask? We decided to think about things a bit differently to demonstrate the power of this architecture. When a user unsubscribes or subscribes to a channel, we don't _immediately_ notify everyone in the same Lambda invocation. Rather, we have AWS Lambda receive that stream and process it asynchronously. It still happens extremely fast, and to all WebSocket clients, it appears no different. The real power of this approach is: say you have sub-services or systems running that want to send messages, or ban users. Those subsystems don't need to care about the implementation of the WebSocket system; they simply work with the DynamoDB table and can create, update and delete subscriptions, send bot messages, etc. Those changes flow through the exact same pattern as if they were issued via WebSocket clients themselves. I think this is a pretty neat concept, and I am curious to see what folks build with it! A couple notes about WebSockets and the ApiGatewayManagementApi You cannot send messages back the typical way as an HTTP response payload you may be used to with API Gateway HTTP. Just return back a statusCode (ex: ) property in your payload to tell API Gateway everything is good, but it will not send that to the client. If there are errors like a , those _will_ go to the client. You cannot send a WebSocket message via the Management API in the `$connect` route, that needs to succeed before the socket connection will allow messages to flow. You will get a `` code meaning the connection is \"Gone\" (or doesn't exist yet.). For some psuedo-code, it would look something like this:  `` error codes mean the connection is gone (or isn't established yet). Depending on your use case, you may want to clean those up in your data store so you don't keep trying to send messages! You can close connections from the \"server\" side via the ApiGatewayManagementApi. In addition, the `$disconnect` route invoke is a best attempt, and not a guarantee. (That said, I haven't seen it fail yet, so it seems like a small edge case.) If you are using the AWS CLI to send messages, be sure to use the `--endpoint` parameter to override the default api used to your actual `wss` api endpoint. The docs mention this in the top level description of the command, but not in the `post-to-connection` description. Check out the docs for the ApiGatewayManagementApi for AWS NodeJS SDK and Boto to learn more. Now, try it out! You can find all the code on GitHub, and run this sample chat app in your own AWS account with a single `serverless deploy`. Now get out there and build something great! Resources - Full chat app example on GitHub - Explainer: Real-time applications with API Gateway WebSockets",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f76",
      "title": "Serverless Framework v.: Bug fixes and quality of life improvements for all!",
      "content": "Let's bring in right. New year, new Framework release! Framework v. focuses heavily on bug fixes and Framework enhancements and documentation. Check below for the full list, or jump to the bottom for our Framework roadmap. Enhancements - Handle scoped npm packages (specifically the `@`) in variables+/- Limess - Added currently supported regions for GCP functions+/- welkie - feat(log): Log AWS SDK calls in debug mode+/- jlamande - Issue Support for native async/await in AWS Lambda for aws-nodejs-typescript template +/- janvanzoggel - Issue - Update Cloudflare Templates+/- benwillkommen - change behaviour on initial stack create failed+/- Imran - AWS: Validate rate/cron syntax before Deploy+/- exoego - Add warning for multiple functions having same handler+/- exoego - AWS: Add API Gateway stage name validation.+/- exoego Bug fixes - Fix error log output+/- medikoo - aws-csharp create template uses handler-specific artifact+/- odedniv - Fix ResourceLimitExceeded for cloudwatchLog event+/- rdsedmundo - AWS: Fix ${cf.REGION} syntax causes deployment in wrong region+/- exoego - Cloudflare config should be under provider property+/- webmasterkai Documentation - Update http.md+/- devWebNuts - AWS Docs: Fixing link for HTTP Endpoints with AWS_IAM..+/- Murraymint - updating with more detail about service tracking+/- thomcrowe - Update credentials.md+/- rosner - Add step to populate ~/.wskprops file to OpenWhisk docs.+/- welkie - update layers doc to reflect max of +/- dschep - Add Ruby to supported languages in README.md+/- dbw - direct link to video guide+/- dschep Roadmap and focus We will continue to focus on tackling the backlog, bug fixes and quality of life improvements for the next release. A common request has been Amazon API Gateway v and its WebSockets, as well as Application Load Balancer AWS Lambda targets. While we have implemented WebSocket support via a plugin, the core Serverless Framework support for both these requests will be introduced when CloudFormation officially supports them. Contributor thanks As always, we appreciate each and every one of you that use and contribute to the Framework and Serverless ecosystem!",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f78",
      "title": "The definitive guide to using Terraform with the Serverless Framework",
      "content": "If your organisation uses automation to manage cloud infrastructure, youve almost certainly heard about Terraform. And if youve built anything serverless, you might have noticed that deploying with the Serverless Framework is a lot like running Terraform. To which we say: youre absolutely right. Many companies using Serverless already use Terraform, and some Serverless Framework functionality is similar to what Terraform can do, especially when it comes to provisioning cloud resources. So if both Terraform and Serverless can solve your infrastructure automation needs, which one should you use? And should you use just one for all purposes? Weve got answers. In this article, well talk about the right way to manage infrastructure when using both Terraform and Serverless, and check out a real-world example of integrating Terraform and Serverless in a project. Why automate infrastructure management Infrastructure as Code (IaC) becomes really important once developers need a way to organize their growing cloud infrastructure and collaborate across teams. Most importantly, IaC tools make it necessary to have process and discipline; theres a smaller chance of accidental or unexpected changes, and its easier to share configuration between different parts of your infrastructure. Managing shared vs. app-specific infrastructure While we believe that all infrastructure should be managed with IaC automation, we like to distinguish between the infrastructure thats specific to one application and the infrastructure thats shared between multiple applications in your stack. Those might need to be managed in different ways. Application-specific infrastructure gets created and torn down as the app gets deployed. You rarely change a piece of application-specific infrastructure; youll just tear everything down and re-create it from scratch. As the app is developed, the infrastructure that supports it also needs to change, sometimes significantly from one deploy to another. The shared infrastructure, on the other end, rarely gets re-created from scratch and is more stateful. The core set of infrastructure (such as the set of security groups and your VPC ID), wont change between the deploys of your application, as theyre probably referenced by many applications in your stack. Those more persistent pieces of infrastructure will generally be managed outside of your deploy pipeline. So, application-specific and shared infrastructure are different enough that they should be managed with different tools. Serverless vs Terraform: when to use which For an organization using both Terraform and Serverless, here are the benefits of each, and when you should choose one over the other. Serverless for app-specific infrastructure For application-specific infrastructure, we suggest managing all the pieces with the Serverless Framework, for a few reasons. First, you couple this infrastructure to the application itself. Second, we like to think that the application \"owns\" things, like the tables in the Postgres database. There is little value in managing the table names outside of the application context (e.g. in Terraform). Third, you can iterate your application release without touching your shared infrastructure. Software releases decouple from shared infrastructure, allowing you to focus on the application itself without having to worry about infrastructure changes. Terraform for shared infrastructure However, coupling shared infrastructure to a specific application isnt correct. Shared infrastructure will usually get updated instead of re-created from scratch. This makes Terraform a nice way to manage that shared infrastructure; it can be a central source of truth for the persistent cloud infrastructure and it manages updates to the existing infrastructure very well. For example If you have a shared database and two Serverless applications that create tables in it, the database should be managed by Terraform. The specific tables should be created and destroyed by the Serverless Framework during the app deployment and teardown process. Where to draw the line With a database and its tables, the distinction between app-specific and shared infrastructure is clear. But what happens if the entire database is only being used by one app? What about the queues and queue subscriptions? What if there is a contract between two Serverless microservices and they use a queue as an interface? All these items fall somewhere between the app-specific and the shared. For cases like those, we believe either option is fine. Its more important to avoid confusion by keeping the decision consistent across your infrastructure. Sharing data between Terraform and Serverless with SSM If you use Terraform and Serverless to manage different pieces of your infrastructure, youll eventually need to share data between Terraform and Serverless projects. Think VPC IDs, security group IDs, database names for RDS instanceseverything that gets created via Terraform and consumed in Serverless. The SSM parameter store is a great way to share the values between the two systems. Terraform provides an SSM parameter resource that you can use to write arbitrary SSM keys. You can then consume those keys in your `serverless.yml` via the `${ssm:...}` reference. An example of using SSM with Terraform and Serverless To illustrate the passing of parameters via SSM, weve created an example! Infrastructure is managed by Terraform, and there is a Serverless app that uses the results of Terraform operations to connect to a database. The application can use that database connection to create the database tables or anything else required for the application itself to work. Lets walk through both the Terraform and the Serverless configuration files to see how this looks in a simple project. Terraform In the Terraform project, we create a resource that we need, in this case its a MySQL RDS instance:  We use the `aws_db_instance` data source (you can find full documentation for it here). Instead of specifying the database name directly, we reference the variables called `name` and `user`, and generate a random string to act as a password. We then create a `variables.tf` file with the content of `name` and `user`, and set a few parameters on the random string:  When we run `terraform apply` the following happens: `${var.name}` gets replaced by the name value that we define in the `variables.tf`. Once the database we specify in `main.tf` is created, the `${aws_db_instance.default.address}` value is replaced with the IP address of the database instance. An SSM parameter is created with the name `/database/testdb/endpoint` and contains the IP address of our database instance. Our Terraform configuration stores not only the database endpoint, but also the database user, the password, and the name of the database we are accessing. Serverless In our Serverless config file, we define a function that needs to connect to the database that we manage with Terraform. In the definition of the function, we create the environment variables to contain all the database connection parameters. In each variable, we reference an SSM parameter (you can find docs on this here). Note that we save the parameters to SSM as SecureStrings in the Terraform files above, so we need to use the special `~true` syntax to get those values inside `serverless.yml`:  The variables we specify in the environment section get populated with the correct values from SSM during the deployment process, and those values become available in the functions runtime environment. In the body of the Serverless function we can then configure a MySQL connection with these values:  After that, were able to access the MySQL database managed via Terraform in our Serverless application! Changing the database connection data If you need to change the configuration of the database in Terraform, upon running `terraform apply` the SSM parameters the Serverless app references get updated. But you need to redeploy the Serverless application to get those updated in the running app. The limitations of SSM SSM provides a convenient way to reference parameters from Terraform in your Serverless projects. Its important to note, however, that SSM is only available in Amazon Web Services. Its also not the most secure solution, as the values from SSM might end up in the build logs or CloudFormation templates. (See the disclaimer in this doc section). Despite these limitations, the option of using SSM to pass data from Terraform to Serverless works for most cases of managing shared and app-specific infrastructure. Conclusion Terraform is best suited for managing more persistent shared infrastructure, while Serverless is a good fit to manage the application-specific infrastructure. Check above for the example of sharing information between Terraform and Serverless, and you can find the full example here in the GitHub repo. Do you currently use Terraform together with Serverless? Share your approach in the comments below or in our forum!",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f7a",
      "title": "Serverless Framework v..",
      "content": "The regular cadence of releases is full steam ahead as we release another version of the framework, v.., bringing bug fixes and enhancements. Bug Fixes - AWS: Fix stage name validation timing and allow hyphen +/- exoego - Fix awsProvider.js : \"Cannot use 'in' operator to search for '' +/- exoego - AWS: Request cache should add region as key to prevent cross-region cache collision +/- exoego - Fix array notation in stream ARN +/- ctindel - Fix sls plugin install -n @scoped/package +/- dschep Enhancements - AWS: Consolidates Lambda::Permission objects for cloudwatchLog events +/- exoego - Provide AWS_PROFILE from configuration for invoke local +/- revmischa - Increase @types/aws-lambda version in aws-nodejs-typescript template +/- gabrielkaputa - Enable download template from a private github repo using personal access token +/- StevenACoffman - Suppress confusing warning \"A valid undefined...\" +/- exoego - Update aws-scala-sbt template +/- NomadBlacky - Add google go template +/- toshi - Test that CLI does not convert numeric option to number +/- exoego - Remove duplicate-handler warnings based on community feedback. +/- exoego Documentation - docs: Kubeless secrets +/- alexander-alvarez - Clarify docs for the http key for GCF +/- sparkertime - Fix layer doc reference to functions (should be layers) +/- et - Fixed a link +/- venkatramachandran - docs menu sidebar - added [Getting Started] above [Providers] +/- pdaryani Roadmap and focus The next release plans to maintain our cadence and keep tackling our issue and PR backlog. Contributor thanks We value all your input and contributions, thanks to all of you for your continued usage and feedback on the Serverless Framework!",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f7c",
      "title": "Register for the Serverless workshop on March  in San Francisco!",
      "content": "One question that comes up time and time again about Serverless is how to run it at scale across a team. There are a lot of things to plan for and address: operations, automation, security, team collaboration, architecture design, and process. These are tough questions, which is why we're bringing Jared Short, our Head of Developer Relations and Experience, to San Francisco for a one-day Serverless Workshop! The workshop is on March st in San Francisco, California. Read below for more details, or go ahead and Register here. Who is this workshop for? Short answer, you. Whether you're new to serverless and this is your first time deploying an API, or if you've been running serverless for a few years, you'll take a lot of great things away from this workshop. We'll start with serverless APIs and carry it all the way through serverless DevOps and how you should think about organizing your code and teams. What you'll learn: - Deploying functions and APIs from start to finish - Managing event-driven computing and deploying event-driven services - Organizing code and teams, devops, CI/CD, and culture and ownership This workshop is designed for you (and your whole team). By the end, we want to ensure you're confident in following best practices, and you know how to build out and run serverless tech within your team. How to register The workshop is one full day. Registration is $, with limited early-bird tickets available for $. Plus, if you use the promocode `SLSBlog`, you can save an additional % off of the cost of registration! Register for the workshop today! If you have any questions at all, reach out to us on Twitter or drop a comment below.",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f7e",
      "title": "Serverless Framework v.",
      "content": "We have a lot of bug fixes and enhancements on the menu for the v. release. bug fixes and enhancements to be exact. Plus, a few updates to our docs! Bug Fixes - Fix broken Azure Hello World Example documentation +/- eeg - Fix - Rollback fails due to a timestamp parsing error +/- luanmuniz - AWS: Tell S bucket name and how to recover if deployment bucket does not exist +/- exoego - Do not print logs if print command is used. +/- exoego - Fix assuming a role with an AWS profile +/- piohhmy - Resolve profile before performing aws-sdk dependent actions +/- dschep - don't check call tty on macs +/- dschep - Require provider.credentials vars to be resolved before s/ssm/cf vars +/- dschep - Preserve whitespaces in single-quote literal fallback +/- exoego - Fixes for AWS cors config issues +/- pchynoweth Enhancements - Enable tab completion for slss shortcut +/- drexler - Default to error code if message is non-existent +/- drexler - Add resource count and warning to info display +/- alexdebrie - Allows Fn::GetAtt with Lambda DLQ-onError +/- martinjlowm - Updated aws provider to invoke .promise on methods that support it. Otherwise falls back to .send with a callback +/- exocom - Upgrade google-cloudfunctions to v and set defaults to node etc +/- bodaz - Add uploaded file name to log while AWS deploy +/- Enase - Add template for provided runtime with the bash sample from AWS +/- dschep - Throw an error if plugin is executed outside of a serverless directory +/- shanehandley - handle layers paths with trailing slash and leading ./ or just . +/- dschep - Convert reservedConcurrency to integer to allow use env var +/- snurmine - Provide multi origin cors values +/- richarddd - Add Hello World Ruby Example +/- yuki - AWS: Add fallback support in ${cf} and ${s} +/- exoego Documentation - Fix link +/- kazufumi-nishida-www - Fix typo in Multiple Configuration Files example +/- paflopes - Document how to use Secrets Manager +/- dschep Roadmap and focus The next release plans to maintain our cadence and keep tackling our issue and PR backlog. Contributor thanks We had more than contributors have their work go into this release and we can't thank each of them enough. You all make the community special. Want to have your github avatar and name in the next release post? Check out these issues we are looking for help on!",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f80",
      "title": "Serverless named EMA Top  for serverless technologies!",
      "content": "We're are incredibly proud to announcethe Serverless Framework has been chosen as an EMA top for serverless technologies! EMA chose Serverless for its top because of its simple deployments, rapid community adoption, and seamless developer experience. They specifically highlight the ways the Serverless Framework addresses common pain points in serverless development, via: - Continuous compliance and security - Consistency between dev, test, and production - Significant reduction in complexity - Ease of onboarding new developers into serverless development EMA believes that the Serverless Framework is a great fit for enterprises who want to move forward quickly, easily, and safely with serverless. If you have more serverless research to do, EMA also just launched a website, The EMA Top for Serverless Technologies, which contains adoption and job metrics, as well as aggregated Serverless news and other data. Its well worth checking out. We are humbled by our community and the support you provide; we wouldnt be here without you. Heres to a great start in ! The simplest way to build applications on the cloud If youre interested in using Serverless at your company, or have questions about how to expand usage across the team, wed love to chat! Get in touch to learn more about Serverless Framework Enterprise. Similar posts - The Serverless Framework wins Best Microservices API at the API Awards!",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f82",
      "title": "Serverless Framework v. - Introducing Websockets Support",
      "content": "As many of you likely remember, AWS finally added support for WebSockets in API Gateway & Lambda at last year's re:Invent. While we were all waiting for CloudFormation support, we created an official Serverless Framework plugin to support WebSockets in the Framework right away. This was only a temporary measure until CloudFormation support was added. And now, it has been! So we are incredibly excited to announce that the Serverless Framework v. now has support for WebSockets integrated into Framework core. No plugin required. Note: This means that the serverless-websockets-plugin is officially deprecated. We will no longer maintain it. Get Started with WebSockets on the Serverless Framework To start using WebSockets in your serverless projects, subscribe your functions to the new `websocket` event by specifying the desired routes that would invoke your function:  This will create a `$default` route that will forward all WebSocket events (including `$connect` and `$disconnect`) to your `default` function. You can also specify your event as an object and add more routes:  The object notation will be useful when you want to add more configuration to your WebSocket events. For example, when we add support for authorizers in an upcoming release. Once you deploy this service, you'll see the endpoint of your WebSocket backend in your terminal. Using this endpoint, you can connect to your WebSocket backend using any WebSocket client. You could also have another function with `http` event, so your service would expose two endpoints: one for websockets, the other for http. Specifying Route Selection Expression By default, the route selection expression is set to `$request.body.action`. This property tells API Gateway how to parse the data coming into your WebSocket endpoint. So with this default behavior and using the service above, you can invoke the `echo` function by using the following JSON object as your websocket event body:  You can overwrite the route expression by specifying the `websocketsApiRouteSelectionExpression` key in the `provider` object:  In that case, your WebSocket body should be:  Remember that any other body/data coming to your WebSocket backend would invoke the default function. Communicating with clients The Framework also takes care of setting the permissions required for your Lambda function to communicate to the connected clients. This means you'll be able to send data to any client right away by using the new `ApiGatewayManagementApi` Service, without having to worry about IAM policies: ```js const client = new AWS.ApiGatewayManagementApi({ apiVersion: '--', endpoint: `https://${event.requestContext.domainName}/${event.requestContext.stage}` }); await client .postToConnection({ ConnectionId: event.requestContext.connectionId, Data: `echo route received: ${event.body}` }) .promise(); ``` Note: At the time of writing, the Lambda runtime does not include the latest version of the AWS SDK that contains this new `ApiGatewayManagementApi` service. So you'll have to deploy your own by adding it to your `package.json`. And that's pretty much all you need to do to get started with WebSockets events! For more information, please check out our docs. Changelog Other than WebSockets support, we added a lot of enhancements and bug fixes in this release. Here's our changelog, with some links to the corresponding PRs. - Set timeout & others on context in python invoke local - Append in Custom Syntax - Don't load config for `config` - Replace blocking fs.readFileSync with non blocking fs.readFile in checkForChanges.js - Added layer option for deploy function update-config - fix makeDeepVariable replacement - Make local ruby pry work - Replace \\ with / in paths on windows before passing to nanomatch - Support deploying GoLang to AWS from Windows! - Fix windows go rework - Make use of join operator first argument in sns docs - add support for command type='container' - Add Google Python function template - Update config-credentials.md - Update bucket conf to default AES encryption. - Fix: override wildcard glob pattern () in resolveFilePathsFromPatterns - Indicate unused context in aws-nodejs-typescipt - Add stack trace to aws/invokeLocal errors - Missing underscore - Updating cloudformation resource reference url - Docs: Replacing \"runtimes\" with \"templates\" - Add support for websockets event - AWS: ${ssm} resolve vairbale as JSON if it is stored as JSON in Secrets Manager - Fix service name in template install message Roadmap and focus Over the next few releases, we'll be enhancing WebSockets support with more features, like authorizers and request responses. We're also focusing on improving the local development experience. Keep an eye on the upcoming milestones to stay up to date with what's coming: - v.. Milestone - v.. Milestone Contributor thanks We had more than contributors have their work go into this release and we can't thank each of them enough. You all make the community special. Want to have your github avatar and name in the next release post? Check out these issues we are looking for help on!",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f84",
      "title": "How Shamrock transacts billions of dollars with Serverless Framework Enterprise",
      "content": "See how Shamrocks serverless invoicing system handles billions of dollars of transactions with no active scaling required. Plus: their multi-cloud approach with AWS and Google. Shamrock Trading Corporation began in as a small freight brokerage that served agricultural and commodity shippers. But as their industry changed, they got bullish on new technologies and were unafraid to keep innovating. Today, they are a high-tech shop with over employees that manages software, financial services, and large-scale transportation logistics. They create the software that handles trucking fleets as well as the mobile applications that drivers use on the go to find nearby gas stations. They have a check depositing image upload/recognition app that handles billions of dollars in transactions a year. And all those services are serverless. Read on to see how Shamrock uses serverless to create massively scalable and performant softwareat a tenth of their former Docker cost. The invoicing app that handles billions Shamrock has been running their invoice management software for four years, and it was originally built as a Docker container running in GKC. But as their usership increased, it got increasingly painful to manage, and would frequently go down during peak traffic on Fridays. Two years ago, CTO Tim Bachta had been looking into serverless, and decided to give it a try. The team moved their Docker app over to a serverless workload using the Serverless Framework and AWS Lambda. A website running in S hits AWS API Gateway, all powered by a Lambda backend and deployed with the Serverless Framework. The software does image recognition (to e.g. detect duplicate checks, or adjust for size, shape, and color), scales them, bunches them into a PDF, and sends them to a commercial system for processing. Their new architecture was recently put to the test when they got a % usage spike in a single day after Thanksgiving, setting an all-time record for traffic. The engineering team didnt have to touch anything: no adding memory or provisioning more resources. The app just auto-scaled. The engineering team is relieved to have a system that they dont have to babysit every day. Their business team has been ecstatic about the new system, and how much happier customers are with its performance. _Tim Bachta, CTO_ Their cost for all this? \\$, a month, x less than their previous Docker cost with less to manage and no active scaling required. Because of its success, the Shamrock engineering team is actively converting more legacy applications over to serverless and building new internal tools to automate more of the business teams workflow: an internal staff tool to audit paperwork, for example. The migration process from Docker to Serverless They took their old Node.js code from Docker and moved it over to the Serverless Framework over the course of six months. The migration was quick and straightforward, which CTO Tim Bachta attributes largely to the Serverless Framework. _Tim Bachta, CTO_ Their serverless invoicing application currently has about services total, and is able to be managed by a -person engineering team. For their migration, they leveraged Serverless Framework Enterprise to help streamline their serverless operations and scale development through training, additional tooling, our dashboard, and enterprise support. Multi-cloud: AWS cloud + Google services The Shamrock engineering team is doing something many engineering teams are beginning to doleveraging the best-in-class services from all over the cloud, combining services from different providers together in a single application. In order to build their invoicing application, they relied on AWS for infrastructure (Lambda, API Gateway), but wanted to use Googles AI and image recognition services. But thats not the only advantage they see to a serverless multi-cloud approach. They have multi-region set up in AWS, but being on multiple providers gives them even more robust failover. Plus, with the Serverless Framework, they can easily deploy to other clouds without having to learn new tooling. This function goes to AWS, this one goes to Google; it doesnt matter because all the infrastructure is code. DevOps, and the impact on engineering culture CTO Tim Bachta admits that adopting a serverless and event-driven mindset can be a culture shift, but ultimately one that comes with a lot of upsides. His teams have autonomy to work on things they feel will drive productivity or business value, and at the end of the day, whoever built it is responsible for it. _Tim Bachta, CTO_ CI/CD with Serverless The Shamrock team has serverless functions in production, with a lot of shared components. Everything is run in a CD environment, meaning each piece of code, regardless of which team built it, is everybodys code. Can this introduce complications when someone makes a code change? Sure. So the engineering team places a lot of emphasis on code reviews across all teams. When new functionality gets built, teams do active demos to talk about how it works. They run their CI with Jenkins (with plans to move over to GitHub actions soon), and ship everything as manually-managed blue/green deployments (with plans for canary deployments once theres more control over them). Overall, the engineering culture at Shamrock is one with a high level of ownership. _Tim Bachta, CTO_ Challenges along the way Thinking small, not monolithic The Shamrock team, like many newly-serverless teams, took a bit to get used to thinking of everything in terms of events. In a serverless paradigm, developers have to learn to focus on building a function for each piece of functionality, instead of building out a monolithic piece of code. Its essentially a shift toward thinking in terms of specific, single units: microservices. Data manipulation Much of the data that the system needed to interact with was in databases tucked away in VPCs. So Shamrocks developers had to run Lambda functions inside VPCs and against traditional databases, which came with some obstacles. For example: dealing with VPC based cold-starts (an issue soon to improve according to AWS!), and understanding how to create and maintain database connections across invocations, as well as deal with traditional database connection limits without connection pooling type tools at their disposal like a traditional server-based environment. Shamrock engineering: the future is serverless Overall, CTO Tim Bachta is incredibly impressed with the way serverless, and the Serverless Framework, has empowered his team to take more ownership over their own projects, and ship new features at record speed and cost. Theyve managed to migrate a legacy Docker application over to AWS Lambda using the Serverless Framework quickly and painlessly, and now have an active piece of infrastructure that handles billions of dollars in a transactions a year with no active management or scaling required. Theyre in the process of migrating more and more legacy infrastructure over to serverless, while simultaneously feeling empowered to create tooling used across the organization that can increase everyones productivity, from the engineering teams to the business department. Most of all, Bachta is floored with how easy Serverless makes it to ship software and keep innovating. _Tim Bachta, CTO_ Try Serverless Framework Enterprise If you are thinking of moving onto the cloud, we're here to help you with serverless-specific tooling and dedicated support. Talk to us about Serverless Framework Enterprise to learn more!",
      "__v": 0
    },
    {
      "_id": "64e08925b72e199dda603f86",
      "title": "Dynamic image resizing with Node.js and the Serverless Framework",
      "content": "Does your website or app contain images? Then you've probably had to deal with the problem of resizing those images for different page layouts and devices of all shapes and sizes, not all of them predictable. But the lower-effort alternative, serving the full-size original image, doesn't scale. Above all, large images dramatically increase page load times, impacting the user experience and driving up bounce rates. Then there's the fact that mobile visitors may well have a less-than-reliable network connection. If the image download gets interrupted, they have to reload the entire page, eating into their data plan and causing frustration to boot. And finally, most visitors can't tell the difference between an original image and one with a data footprint times smaller. With those downsides in mind, it's in your interest to generate and serve a compressed image of lower but still good quality. But what's the ideal image size? Given the huge variety of potential screen sizes, that question is nearly impossible to answer. And pre-scaling the original image to all imaginable sizes will result in an unsustainable hit to your cloud storage capacity and therefore to your monthly bill. So what's the solution? Serverless Framework, which makes it very easy to build applications using AWS Lambda and other Serverless compute providers, is a great solution for this use case. Serverless' auto-scaling, pay-per-execution functions not only let you avoid that massive collection of pre-scaled cloud data, but also significantly cut down on your compute costs, since you won't need to maintain a fleet of image scaling servers. In this article, we'll take an in-depth look at the benefits of dynamic image resizing and walk you through using the Serverless Framework to resize your images dynamically with AWS Lambda. So, let's get to it! A solution using S In this example we'll be using Node.js along with the Serverless framework to build our app. Well also be using S, the AWS cloud storage service, but the Serverless Framework works with Azure, GCP and Kubernetes as well, among others. Here's the core logic of our process: If we already have the properly sized image in our S storage, calling the corresponding S URI will serve us the previously stored image directly. But if we don't have the image in that size yet, following the S link will first generate the image in that size and then serve it to us. And, of course, the newly resized image gets stored in S. To implement this process we'll first pick a few ranges of possible image sizes (for example, we'd serve one size image to screens -px wide, and a slightly larger image to those -px wide). Then, well build a system from event-driven functions that will generate the specified image sizes from the original photo. By only generating images sized for the devices that actually request your site, we make the system smart. When a particular article has only been viewed by px-wide screens, the Serverless app won't generate the rest of the sizes... not until someone with a different screen size comes along. The first reader with a px screen to load the article must wait for a split second while the system generates the new image size, but the user experience impact is minimal. Once the new size has been generated, we save it for future use. And so this Serverless approach represents an easy, low-cost and scalable solution. Should no one happen to visit your site or use your app, no code will run, costing you nothing. But when new visitors begin needing different image sizes, the system will scale, doing all the necessary work and no more. As soon there are no more new images to generate, the system stops running. Another plus: it's an optimal experience for users, where their devices get served the right size image in a timely manner. Creating a Serverless API for image resizing Lets look at how to implement the Serverless image-resizing API in detail. In this section, well cover the following steps: - Writing a `serverless.yml` config file that contains all the specifics for the resizing service - Implementing the resizing logic in a handler function - Setting up an S bucket to work with the resizing endpoint Writing the `serverless.yml` file Lets start with the `serverless.yml` config file. First, we define the name of our service:  We then specify our cloud provider (AWS) and a Node.js version that works for us:  Now, lets add the definition for our resizing function:  We define the location of our future image-resizing handler and specify the path our handler will accept. We only need two components in the path: the size of the image and the image name. In the `environment` section, we define the S `BUCKET` where our images will be stored, and the `REGION` where the S bucket will live (in our example, its `us-east-`). We also specify the IAM roles that we want to grant to the resizing function. Well make it broad and let the function read from and write to all paths in the S bucket. The handler function Now well switch to the `src/handlers/resizer/index.js` file that we mentioned previously as the location of the image-resizing handler. Our handler looks quite simple: ```javascript import { resizeHandler } from \"./resizeHandler\"; export const handler = async event => { try { const imagePath = await resizeHandler.process(event); const URL = `http://${process.env.BUCKET}.s-website.${ process.env.REGION }.amazonaws.com`; return { headers: { location: `${URL}/${imagePath}` }, statusCode: , body: \"\" }; } catch (error) { console.log(error); return new Error(error); } }; ``` The handler accepts an HTTP request, calls `resizeHandler._process` on it, and returns an HTTP redirect to the location of the new image once its been successfully generated. Well put the `resizeHandler` code in a separate file: `src/handlers/resizer/resizeHandler.js`. We start by importing the S supporting functions and our image processing library, `sharp`:  We then build a handler and add our `process` function:  Now that our `process` function is receiving a raw HTTP event from our HTTP handler, we can derive the size and the name of the image from the parameters. Then we use those values to call the `resize` function. In `resize`, we convert the parameters to integers and construct the path where we can find the resized image after conversion:  We then call `sharp` to create a resizing stream for the image with the corresponding width and height, specifying `png` format as the output. Finally, we create S read and write streams, allowing us to string together our input stream, `sharp` stream and output stream. After all that, we need only wait for the upload to finish, at which point we can return the new image path:  The `sHandler.js` file contains the `sHandler` convenience functions, which wrap the `S.getObject` and `S.upload` functions from the AWS SDK for Node.js:  We use both `readStream` and `writeStream` to simplify the streaming in the `resizeHandler` functions. Deploying the image resizing API Now that weve set up our code, were ready for deployment. In the Serverless framework, we can deploy the change (or changes, as the case may be) to our `serverless.yml` file by running: `serverless deploy` This translates the syntax of our `serverless.yml` file into an AWS CloudFormation template and sends that change to AWS. For more on the deployment process, check out the Serverless AWS documentation. Setting up the S bucket When a user requests a file from an S bucket that doesnt exist, S conveniently lets us call a function to create or get that file. This allows us to implement the following logic: - If the properly sized image exists in the S bucket, return it. - If the image does not yet exist in the requested size, call our resizing function and then return the newly available image. Once weve deployed our new API, we need to configure our S bucket to work together with our Serverless function as follows: Configure our S bucket for website hosting as shown in the S documentation. In the Advanced Conditional Redirects section of the Website Hosting settings for the S bucket, set up the following redirect rule:  In place of `YOUR-API-ENDPOINT` we will add the Lambda endpoint of our Serverless function. We can get that by running: serverless info Keep in mind that if you specify a custom stage during deployment you also need to specify it in the `info` command to get the right endpoint address. See the docs for the `info` command for more details. The API in action Lets take a look at the API in action. First well request a size we know exists:  It works! The next time we request this size, the image will be served directly from S. Wrapping up This article walked you through the process of creating a Serverless app that dynamically resizes images. In the process, we saw that image resizing using Serverless keeps costs low, gives users a good experience and scales perfectly with your needs. If you've never used Serverless before, building an app that resizes images is a great introduction. But Serverless also offers significant advantages in use cases much more complex than this common one, such as workflow automation and task scheduling. To get going with Serverless, start with their own documentation, or check out their AWS-based introduction to the Serverless framework. For other cloud providers, Serverless can help you there too. You can find the full example project from this article in this GitHub repo.",
      "__v": 0
    },
    {
      "_id": "64e08926b72e199dda603f88",
      "title": "Announcing Serverless Framework Enterprise: The Total Serverless Solution",
      "content": "Last year, we announced our Serverless Platform Beta. Today, were bringing it out of Beta, renaming it to Serverless Framework Enterprise and introducing a handful of new features that empower developer teams to build amazing serverless applications. Check it out here to get access. Sorry, your browser doesn't support embedded videos. The serverless movement was never supposed to be about needing multiple tools to build and operate your applications. Serverless is about doing more with less. What Serverless Framework users appreciate most is that it provides everything a developer needs to build and deploy their serverless application, in one simple, rich experience. Now, Serverless Framework Enterprise adds to that. It focuses on solving serverless operations problems for developer teams, with a richer experience, all while keeping it simple, so you can focus on results, not more tech complexity. Once you turn on Serverless Framework Enterprise, heres what you get out-of-the-box... Serverless Insights Serverless applications are comprised of many functions and cloud services, which means they must be monitored differently than traditional applications. Serverless Framework Enterprise offers a solution for this by including a complete monitoring and alerting feature-set called Serverless Insights. Automagically upon deployment, your Functions will be instrumented to generate performance information and alerts, which can be viewed in the Serverless Framework Enterprise console. !Serverless Framework Enterprise Dashboard The Enterprise console includes simple charts that you can click for low-level details. It also includes an Activity feed that will report alerts and events about your serverless application, like Deployments, Unusual Invocation Rates, and New Error Detected. When a new error is detected in your code, well tell you about it immediately. Well even tell you where the error is. !Serverless Framework Enterprise Dashboard Error Alerts Serverless Secrets As our users ship increasing amounts of functions and applications, it becomes more difficult for them to manage their sensitive information, like credentials. Serverless Framework Enterprise ships with a solution for secrets management called Serverless Secrets, so your team can easily manage and delegate access to sensitive information your serverless applications require (e.g. access keys). You can create secrets within the Enterprise console and easily reference them via the Serverless Frameworks variable system. You can also create a special type of secret that gives the Serverless Framework temporary access to deploy to your Amazon Web Services account. Using this, the developers on your team and your CI/CD system do not need access credentials to your cloud account. All thats needed is to add the AWS Secret to the credentials property in serverless.yml. !Serverless Framework Enterprise Secrets Serverless Safeguards Many organizations are seeking to standardize serverless development across their engineering teams. However, they need to ensure their developers are following best practices as well as organizational policies. Serverless Framework Enterprise includes its own policy engine called Serverless Safeguards. Safeguards enable managers and operations teams to configure policies that must be complied with, like restricted-deploy-times, required-stack-tags, or no-overly-generous-iam-role-statements, in order for a Serverless Framework deployment to succeed. Safeguards can be configured in the Enterprise console. There are over a dozen Safeguards that come out-of-the-box with Serverless Framework Enterprise, that will warn you if your application contains any well known architectural anti-patterns. Think of it as a linter for serverless applications. !Serverless Framework Enterprise Safeguards Available Now Serverless Framework Enterprise is available today. Click here to get access",
      "__v": 0
    },
    {
      "_id": "64e08926b72e199dda603f8a",
      "title": "Serverless Framework v.",
      "content": "With the v. release we added support for Docker-based local invocation, which makes it possible to support any runtime and AWS Lambda Layer combination with the `invoke local` command, enhanced the Frameworks AWS websockets support to include websockets authorizers, added support for AWS X-Ray tracing, and addressed a number of additional bug fixes and enhancements. With the v. release we addressed additional bug fixes and enhancements. bug fixes and enhancements to be exact, across both v. and v.. Improved `invoke local` support The v.. release of the Serverless Framework expanded `invoke local` support to include ALL supported AWS runtimes and layers. Previously we provided support for local invocation on a per-runtime basis. We started with Node.js and extended this further to add Python and Java. In doing so we studied and implemented the AWS Lambda specific behavior for every runtime so that `invoke local` could be run from the Serverless CLI and return the exact same result as `invoke`. While that works great for Node.js, Python and Java, AWS Lambda now supports any runtime, and AWS Lambda Layers makes it possible to further customize the Lambda experience. With v.. we now support Docker-based local invocation, which makes it possible for the Serverless Framework to support any runtime and AWS Lambda Layer combination. The implementation itself leverages the `lambci/lambda` Docker images. Docker-based local invocations are enabled by default for runtimes that weren't previously supported. It can also be enabled with the `--docker` flag for already supported Node.js / Python / Ruby and Java runtimes. Docker-based invoke local also includes support for Lambda Layers. Both local, as well as external Layers referenced via an ARN, are supported. You can learn more about using Docker-based local invocation with the Serverless Framework in our documentation. Authorizers Support for Websockets In v.. we completed the websockets story by adding support for websockets authorizers. It works like http Authorizers. The only key difference is that AWS only supports websockets authorizers for the `$connect` route. Here's an example yaml configuration that uses authorizers to protect connection requests:  With this configuration, any connection request to the websockets URL must include the `Auth` header by default, otherwise connections will be rejected automatically. If it does include the header, your `auth` function will be invoked first. If this invocation succeeds by returning a valid policy statement, your `connect` function will be invoked, otherwise, the connection will be rejected. When using the `wscat` client, you can connect with the following command:  You can change this header to any other value, or to be a query string by specifying the `identitySource` property:  With this configuration, you need to specify a querystring instead of a header:  For more information on websocket support please check the docs, and for a simple yet complete example using websockets authorizers, check our websockets-authorizers example in the examples repo. AWS X-Ray Tracing for Lambda Mature serverless applications tend to utilize a large number of internal and external cloud services. The larger the application, the harder it becomes to get useful insights into an applications overall performance. One way to get better end-to-end visibility into the performance of a serverless application, running on AWS, is to instrument it to use AWS X-Ray, which will trace requests as they flow through your serverless application and generate a Service Map. In Serverless Framework v.. we added AWS X-Ray tracing support for AWS Lambda. X-Ray tracing can be enabled service-wide or on a per-function level. To enable X-Ray tracing for all your Services Lambda functions you just need to set the corresponding tracing configuration on the `provider` level:  If you want to setup tracing on a per-function level you can use the `tracing` config in your function definition:  Setting `tracing` to `true` translates to the `Active` tracing configuration. You can overwrite this behavior by providing the desired configuration as a string:  Also note that you can mix the `provider`- and `function`-level configurations. All functions will inherit the `provider`-level configuration which can then be overwritten on an individual function basis:  It's recommended to setup X-Ray tracing for Lambda with the aforementioned `tracing` configuration since doing so will ensure that the X-Ray setup is managed by the Serverless Framework core via CloudFormation. You can learn more about X-Ray Tracing for AWS Lambda in our documentation. Bug Fixes - Fix bug when using websocket events with functions with custom roles +/- eahefnawy - Print customized function names correctly in sls info output +/- dschep - [SLS-] fix regression with golang check on windows +/- dschep - Align error logging +/- dnicolson Enhancements - Allow Fn::Join in SQS arn builder +/- alexdebrie - Support API Gateway stage deployment description +/- vkkis - Allow individual packaging with TypeScript source maps +/- therockstorm - Packaging exclude only config file being used +/- danielcondemarin - Add AWS x-ray support for Lambda +/- pmuens - Put `Custom Response Headers` into `[Responses]` +/- etc-tiago - Invoke local docker +/- dschep - Websockets: Support more route characters +/- eahefnawy - Added websockets authorizer support +/- eahefnawy - Enchancement/kotlin jvm maven updates +/- paul-nelson-baker - Fix CloudFormation template normalization +/- bokan - Support for asynchronous lambda invocation with integration type AWS +/- snurmine - Support for Cloudwatch Event InputTransformer +/- fivepapertigers - Add Serverless instanceId concept +/- pmuens Documentation - Add links to the respective core concepts +/- matheussilvasantos - Fixing minor typo +/- trevorallred - Documentation tweak around shared authorizers +/- stuartsan - Document changes from +/- luclement - Doc: Include that APIGateway status code of async events +/- sime Contributor thanks We had more than contributors have their work go into this release and we can't thank each of them enough. You all make the community special. Want to have your github avatar and name in the next release post? Check out these issues we are looking for help on!",
      "__v": 0
    },
    {
      "_id": "64e08926b72e199dda603f8c",
      "title": "Serverless Framework v. - X-Ray for API Gateway, Invoke Local with Docker Improvements & More",
      "content": "With the Serverless Framework v.. release, were adding AWS X-Ray Tracing support for API Gateway, which complements the AWS tracing story and makes it possible to trace incoming events from API Gateway all the way through your Lambda functions. Our new version also adds support for multiple API Gateway usage plan and key definitions as well as lots of enhancements for local function invocations via Docker. In addition to that, we also addressed a couple of bug fixes and enhancements. bug fix and enhancements were merged and are now available in our v.. release. X-Ray support for AWS API Gateway AWS API Gateway is one of the central services used in many serverless applications. Interactions with an API Gateway-driven serverless backend start with an event which is triggered via an HTTP request and then re-routed to the corresponding AWS Lambda function. It would be great to monitor and trace requests through the service-stack to better understand how requests are processed and where they spend most of their lifetime. In one of our previous Serverless Framework releases we introduced AWS X-Ray Tracing for AWS Lambda. With this post, we now complete the picture by adding AWS X-Ray Tracing support for API Gateway. Enabling tracing for API Gateway is as easy as enabling the corresponding config on the `provider` property:  X-Ray tracing works best when its used across multiple AWS services. If youre using X-Ray Tracing for API Gateway you might want to enable it for your Lambda functions as well:  This way you can get more insights into your API Gateway  Lambda setup when using the X-Ray Tracing Service Map IMPORTANT: Due to CloudFormation limitations it's not possible to enable AWS X-Ray Tracing on existing deployments which dont use tracing right now. Please remove the old API Gateway and re-deploy it with tracing enabled if you want to use AWS X-Ray Tracing for API Gateway. Support for multiple usage plans Sometimes its useful to limit access to your API Gateway when exposing it to the public. In previous versions of the Serverless Framework this could be easily done via API Keys and usage plans:  The initial implementation which supported one usage plan and multiple API Keys was usually enough for simple API Gateway setups. However in production setups one usually needs more flexibility. Its very common to have different types of usage plans for different user types, such as free plan users and paid plan users. The Serverless Framework v.. adds support for multiple usage plans. Multiple API Keys can be assigned to each usage plan:  Docker Invoke Local improvements Serverless Framework recently added support for local function invocation via Docker, meaning that every AWS Lambda runtime can now be invoked locally in a Docker container. Serverless Framework v.. adds support for: function environment variables; access to function dependencies; lambda layer download caching; and Docker argument passing. Bug Fixes - : Invoke local docker to pass env vars to lambda container +/- endeepak Enhancements - Add error message when provider does not exist +/- Xenonym - The code for removing comments is easy to read +/- xichengliudui - : Ensure invoke local docker runs lambda with the dependencies +/- endeepak - Add additional Capability when Transform is detected +/- pofallon - Allow specifying a retention policy for lambda layers +/- dschep - Updating Node.js runtime version +/- ffxsam - Make it easier on the eyes of serverless newcomers +/- guerrerocarlos Documentation - Update quick-start.md +/- allanchua - Update newsletter + enterprise link in readme +/- pdaryani Features - Add AWS x-ray support for API Gateway +/- softprops - Java invoke local support for handlers that implement RequestStreamHandler +/- XaeroDegreaz - Add support for multiple usage plans +/- pmuens - Added rust template for Cloudflare WASM +/- jspies - : Ability to pass args for docker run command during invoke local docker +/- endeepak Contributor thanks As always, we appreciate each and every one of you that use and contribute to the Framework and Serverless ecosystem!",
      "__v": 0
    },
    {
      "_id": "64e08926b72e199dda603f8e",
      "title": "AO.com: the path to Serverless First",
      "content": "AO.com is one of the UKs leading online electrical retailers, that is dedicated to giving its customers an exceptional experience, throughout the purchasing journey. From choosing the right item for their needs, ordering on its website to delivery of the item at a time that suits them, AO.com is passionate about creating happy customers.. One of their ways of achieving this was to spawn their Single Customer View (SCV) team. They play a key role in helping the company stay compliant with user privacy and GDPR legislation. Until recently, their tech stack was made up of everything youd expect: containers, EC, Kafka, SQL Server. Upcoming GDPR legislation gave them the opportunity to try something new. GDPR compliance pushes them toward serverless At pace, the SCV team needed to build an entirely new feature to help make sure AO.com were compliant with new GDPR legislation due to come into force. The SCV team thought about what this would mean for them. Theyd have to set up the containers, provision the server resources, they needed to consider load balancing and security concerns (like patching), and ship a fail-proof feature in less than two weeks, with a relatively small team. The team knew they could have a far bigger impact by trying something new. So they started looking at more time-efficient options. Some people on the team had already played around with AWS Lambda and the Serverless Framework, and it seemed promising. So the AO.com team committed. They decided to build a new GDPR feature in a serverless way. It was the teams first production serverless applicationand such a resounding success, the team has never looked back. Theyve gone all-in on serverless for every new feature since, truly adopting the Serverless First mindset. The process: building the first serverless feature Lambda + the Serverless Framework empowered the AO.com team to complete their new feature in only three days. After the feature was launched, they did not have to think about constantly monitoring the feature to make sure it wouldnt go down, since Lambda scales automatically with demand. --Jon Vines, AO.com Software Development Team Lead They kept building new API-based services with Serverless. They built a subject access request feature (another aspect of GDPR compliance), then right to be forgotten requests, and a whole suite of other user-facing features. The architecture for these APIs is pretty straightforward: the Serverless Framework defines their Lambda functions, which then interfaces with SNS and S buckets. They were able to ship each successive feature in only a few days as well. And their Lambda cost for all this? Less than $ a month. From APIs to full serverless data pipelines One of the AO.com teams core competencies was building data pipelines and getting that data to the right teams. Their initial architecture used Kafka on EC instances, with services deployed in containers in ECS. The team were happy with the robustness of this solution and they now had the opportunity to make it more efficient. The nature of the architecture meant that any time they wanted to deploy changes, they had to redeploy the entire functionality for the whole pipeline. It also meant scaling of functionality was across the whole service, and not the individual features. Moving their data pipelines to Lambda with the Serverless Framework The AO team decided to integrate Lambda into their data pipeline. This would give them the ability to easily make incremental deployments, and the pipeline would scale automatically as needed. They kept Kafka in place, but now interface with Kafka using Lambda and S buckets. So their current serverless data pipeline architecture looks like this: _-Jon Vines, AO.com Software Development Team Lead_ Challenges along the way The AO.com team was most familiar with .NET, and decided to continue developing in .NET even after moving to serverless and Lambda. The serverless ecosystem is primarily focused on languages such as JavaScript, and Python. This meant it was more challenging to find .NET examples, however, the team contributed back with posts on their learnings. In other areas, the tooling for .NET in observability and monitoring werent as advanced, but this is improving all the time. Most impactfully, when dealing with Lambda functions, developers will have to work around cold starts. Cold starts are fairly minor in languages like JavaScript or Python, but in .NET could be as much as - seconds. The AO.com team is working around this by tweaking their Lambda provisioning. Overall, Jon Vines, Team Lead at AO.com, said hed still choose .NET again. Were more familiar with it, and its just easier for us. The gains we see from Serverless are worth the trade-off. Company-wide change The SCV team at AO.com is just one of many. After seeing the huge impact serverless has made in the SCV team, other teams at AO.com are beginning to experiment with and adopt serverless as well. _-Jon Vines, AO.com Software Development Team Lead_",
      "__v": 0
    },
    {
      "_id": "64e08926b72e199dda603f90",
      "title": "Serverless Framework Enterprise .. - Error Insights",
      "content": "With todays Serverless Framework Enterprise release we are extending the capabilities of Serverless Error Insights to support invocation logs. Developers can now easily access invocation logs along with stack traces from new error type alerts and error metrics to help developers discover, troubleshoot and easily resolve errors. New error type alert Errors happen, and the sooner you know about them after they are introduced the better equipped you are to proactively mitigate their impact. Serverless Framework Enterprise will track all the unhandled exceptions in your application and notify you when a new error type is identified. In the Serverless Framework Enterprise dashboard you can see the new error type identified alert on the activity & insights list. Error metrics We wish the world was bug free but we cant fix every single one of them. To manage the errors over time we need to view the trends. The errors metrics chart shows error count trends for a service over time. Click into a point on the chart to see error counts and error types by function. Click on an error type to see an occurrence of an error. Error stack traces Serverless Framework Enterprise shows your code stack trace right in the dashboard. You can navigate from a new error alert or the error metrics chart to view the stack trace of your service when the error occurred. If you use tools like Webpack or Typescript which generate the package code, you can also upload a source map to properly generate the stack trace. CloudWatch logs with stack traces \\[new] AWS Lambda automatically monitors and reports metrics on your lambda functions through AWS CloudWatch. AWS Lambda automatically tracks the requests, execution duration per request and a number of other metrics. While CloudWatch captures a lot of critical information to help you identify and troubleshoot errors with your functions, it is also difficult to identify an individual invocation and log stream related to an error or unhandled exception. To help navigate CloudWatch logs weve added the CloudWatch logs directly into the Serverless Framework Enterprise dashboard. When you receive a New Error Type alert, the CloudWatch logs for that error are made available with the alert and stack trace. Similarly, when you select an individual error from the errors chart, you will be presented with the stack trace and the AWS CloudWatch logs in the same dialog. You no longer need to fire up the AWS console to get the relevant CloudWatch logs. Getting Started with the new error insights If youre already developing your service using the Serverless Framework then integrating these new features is easy peasy. First, register for a Serverless Framework Enterprise account, then just update your existing service to use the enterprise plugin.",
      "__v": 0
    },
    {
      "_id": "64e08926b72e199dda603f92",
      "title": "Serverless Framework v.. -  API Gateway Logs, Binary Media Type Responses, Request Body Validations & More",
      "content": "It is now easier to build even more robust APIs using the Serverless Framework. The Serverless Framework v.. release adds support for REST API access logs and API Gateway binary media type responses. Furthermore its now possible to set API Gateway request body validations and API key values. In addition to that we also addressed bug fixes and enhancements. bug fixes and enhancements were merged and are now available in our v.. release. API Gateway REST API logs Operating a serverless REST service at scale requires access to logs in order to gain insights into the API usage and potential issues the current setup might run into. With Serverless Framework v.. its easy to enable API access logs. Just set the corresponding value on the `provider` config level like so:  After a redeploy you should see a dedicated log group where all your services API requests will be logged. Note that were planning to roll out some more fine grained configurability for API Gateway access logs. Feel free to join our discussion about potential enhancements in this issue. Binary Media Type responses Sometimes its a product requirement to not just support text-based REST APIs. What if a customer should be able to download .pdf invoices, .xlsx spreadsheets or you want to be able to return images based on API requests. With Serverless Framework v.. its now possible to support a range of different Binary Media Types. Enabling support for API Gateway binary responses is as easy as configuring the corresponding property on the `provider` level:  You could use the wildcard setup (as shown above) to allow all binary media types. Additionally you can specify which files youll return by using the following config:  Note that you might also want to make sure to return the correct `Content-Type` header and (e.g. base) encoded body in your Lambda response. Request body validation Validations are useful to stop processing malformed requests early on. Having support for such checks on the API level is beneficial because it makes it possible to reject invalid requests at an early stage without the need to go all the way through until the request hits the Lamdba function which will reject it anyway. Using the `http` event one can now configure request body validations which are JSON documents used by API Gateway to filter incoming API requests. Setting request body validations is best done by creating a `.json` file with the definition of the validation. Currently, API Gateway only supports draft- compliant schemas. Here is an example:  After that you just need to point to that file in your `http` request schema configuration:  Note that you can also inline your JSON validation definition, however its often easier to just reference a file on your filesystem. API Key values Controlling access to your API Gateway is best done by leveraging usage plans and API Keys. The Serverless Framework already supports both via the `apiKeys` and `usagePlan` configs. When using such configurations API Gateway took care of the API Key value generation. The Serverless Framework v.. adds support to control such values, making it easier and more deterministic to generate and hand out API keys to users. Heres an example that shows how API keys and usage plans can be used with the new Serverless Framework version: API key value definitions without usage plans  API key value definitions with usage plans  Bug Fixes - Support setting both proxy and ca file for awsprovider AWS config agent +/- snurmine - Remove safeguards when using API Gateway Stage resource settings +/- pmuens - Merging v.. changes back into master +/- pmuens Enhancements - Use region pseudo parameter +/- daaru - Add more specific sub command error handling +/- TylerSustare - Support wildcard in API Gateway cors domains +/- tdmartino - Allow Fn::Join in stream event arns +/- Tybot - Highlight skipping of deployments +/- pmuens - Improve integration test of aws-scala-sbt +/- NomadBlacky - SDK based API Gateway Stage updates +/- pmuens Documentation - Update cors.md +/- fabiorogeriosj - Fix doc: How to update serverless +/- maplain - Update event.md +/- PatNeedham - Fix markup error with Authentication value +/- rakeshyoga - Drop duplicate paragraph in aws/guide/credentials +/- bfred-it - Update serverless.yml.md +/- marcinhou - Fixed three small typos in doc +/- xflotus - fixed small errors in spotinst docs +/- xflotus Features - AWS API Gateway request body validation +/- dschep - Enable Setting Amazon API Gateway API Key Value +/- laardee - Add authorization scopes support for cognito user pool integration +/- herebebogans - Add support for API Gateway REST API Logs +/- pmuens - Add support for API Gateway Binary Media Types +/- pmuens - Implement logging with Logj for aws-scala-sbt +/- NomadBlacky Contributor thanks Once again, big thanks to all involved who contribute to the Framework to make these releases a success.",
      "__v": 0
    },
    {
      "_id": "64e08926b72e199dda603f94",
      "title": "Uploading objects to S using one-time pre signed URLs",
      "content": "AWS provides the means to upload files to an S bucket using a pre signed URL. The URL is generated using IAM credentials or a role which has permissions to write to the bucket. A pre signed URL has an expiration time which defines the time when the upload has to be started, after which access is denied. The problem is that the URL can be used multiple times until it expires, and if a malevolent hacker gets their hands on the URL, your bucket may contain some unwanted data. How then do we prevent the usage of the presigned URL after the initial upload? The following example will leverage CloudFront and Lambda@Edge functions to expire the presigned URL when the initial upload starts, preventing the use of the URL Lambda@Edge functions are similar to AWS Lambda functions, but with a few limitations. The allowed execution time and memory size are smaller than in regular Lambda functions, and no environmental variables can be used. The example project is made with the Serverless Framework. Lets go through the basic concept and components. The Concept The objective is to ensure that every pre signed URL is only ever used once, and becomes unavailable after the first use. I had a few different ideas for the implementation until I settled on one that seemed to be the most efficient at achieving our objective. !s-signed-url-at-edge-get-signed-url Figure . Presigned URL creation First, the user makes a request to the /url endpoint (step , Figure ). This in turn triggers a lambda function (step , Figure ) which creates a presigned URL using the S API (step , Figure ). A hash is then created from the URL and saved to the bucket (step , Figure ) as a valid signature. The Lambda function creates a response which contains the URL (step , Figure ) and returns it to the user (step , Figure ). !s-signed-url-at-edge-validate Figure . Verification of the presigned URL The user then uses that URL to upload the file (step , Figure ). A Cloudfront viewer request triggers a Lambda function(step , Figure ) which verifies that the hashed URL is indexed as a valid token and is not indexed as an expired token (step , Figure ). If we have a match from both conditions, the current hash is written to the expired signatures index (step , Figure ). In addition to that, the version of the expired signature object is checked. If this is the first version of this particular expired hash everything is ok (step , Figure ). This check is meant to prevent someone intercepting the original response with a signed URL and using it before the legitimate client has had a chance to. After all the verifications have successfully passed, the original request is returned to Cloudfront (step , Figure ) and to the bucket (step , Figure ), which then decides if the presigned URL is valid for PUTting the object. AWS Resources The S bucket will contain the uploaded files and an index of used signatures. There no need for bucket policy, ACLs, or anything else; the bucket is private and cannot be accessed from outside without a pre signed URL. The generation and invalidation of the signed URLs will happen on the Lambda@Edge functions, which are triggered in the CloudFronts viewer request stage. Functions have a role which allows them to generate the presigned URL, check if the URL hash is in the valid index and add it if not. The bucket and CloudFront distribution are defined in the resources block of the `serverless.yml` file. Since we cannot pass configuration values via environment variables (since Lambda@Edge functions cannot access environment variables), the bucket name is stored and fetched from an external json file.  The Cloudfront distribution has its origin set to our S bucket, and it has two behaviours; the default is to perform the upload with a pre signed URL, the second supports a URL pattern of /url which will respond with the presigned URL that is used for the upload. The default behaviour in the distribution configuration allows all the HTTP methods so that `PUT` can be used to upload files. S allows files up to gigabytes to be uploaded with that method, although it is better to use multipart upload for files bigger than megabytes. For simplicity, this example uses only `PUT`. Cloudfront should also forward the query string which contains the signature and token for the upload.  The behaviour for the `/url` pattern only allows GET and HEAD methods and it doesn't have to forward anything; the response will be created by the Lambda function. The origin contains only the domain name, which is the bucket name, and id. The `SOriginConfig` is an empty object because the bucket will be private. If you want to allow users to view files which are saved to the bucket, the origin access identity can be set.  Lambda Functions Both of the functions are triggered in the viewer request stage, which is when CloudFront receives the request from the end user (browser, mobile app, and such). The function which creates the presigned URL is straightforward; it uses the AWS SDK to create the URL, stores a hash of the URL to the bucket and returns the URL. I'm using node UUID module to generate a random object key for each upload. ```js const signedUrl = s.getSignedUrl('putObject', { Bucket: bucket, Key: `uploads/${uuidv()}`, }); js const { path } = url.parse(signedUrl); const host = headers.host[].value; const response = { status: '', statusDescription: 'OK', headers: { 'content-type': [ { key: 'Content-Type', value: 'text/plain', }, ], 'content-encoding': [ { key: 'Content-Encoding', value: 'UTF-', }, ], }, body: `https://${host}${path}`, }; ``` The function that checks whether the current upload is the first one uses the indices of signatures written into that same bucket. The first check is to confirm an entry in the \"valid index\" and that the \"expired index\" doesn't contain the hash. Then the function will continue executing the code. Otherwise, it will return a ` Forbidden` response.  If the entry doesn't exist, then it will write current filename and signature to index. ```js const { VersionId: version } = await s.putObject({ Bucket: bucket, Key: `signatures/expired/${hash}`, Body: JSON.stringify({ created: Date.now() }), ContentType: 'application/json', ContentEncoding: 'gzip', }).promise(); ``` Lastly, there is an extra check that fetches the versions of the index key. If it is not the first version, the response is again ` Forbidden`. ```js const { Versions: versions } = await s.listObjectVersions({ Bucket: bucket, Prefix: `signatures/expired/${hash}`, }).promise(); const sortedVersions = versions.concat().sort((a, b) => { return a.LastModified > b.LastModified; }); if (sortedVersions.length > && sortedVersions[].VersionId !== version) { return forbiddenResponse; } ``` If the version id matches the initial version id, Lambda will pass the request on as it is to the origin. Permissions The function that creates the presigned URL needs to have s:putObject permissions. The function that checks if the current upload is the initial upload requires permissions for s:getObject, s:putObject, s:listBucket, and s:listBucketVersions. Development and Deployment Deploying the stack with the Serverless Framework is easy; `sls deploy` and then wait. And wait. Everything related to Cloudfront takes time. At least minutes. And removal of the replicated functions can take up to minutes. That is a good driver for test driven development. The example project uses jest with a mocked AWS SDK; that way local development is fast and if you make small logic errors, they are caught before deployment. Bear in mind that Lambda@Edge functions are always deployed to the North Virginia (us-east-) region. From there they are replicated to edge locations and called from the CDN closest to the client. Time for a Test Run! First, determine the domain name of the created distribution, either by logging in to the AWS web console or with the AWS CLI. The following snippet lists all the deployed distributions and shows domain names and comments. The comment field is the same one that is defined as a comment in the Cloudfront resource in `serverless.yml`. In the example, it is the service name, e.g. `dev-presigned-upload`.  Pick the domain name from the list and run `curl https://DOMAIN_NAME/url`. Copy the response and then run following snippet.  You should get something like this as a response. ```shell ",
      "__v": 0
    },
    {
      "_id": "64e08926b72e199dda603f96",
      "title": "Using the Serverless framework to deploy hybrid serverless/cluster workflows",
      "content": "Serverless infrastructure is becoming ever more popular and a lot of organisations want to benefit from the advantages it provides, such as on demand pricing and scalability. Having said that, it may be hard for an organisation to switch completely from VM and container based workflows to serverless based workflows. That is why, from my perspective, orchestrators allow us to take the best from both and benefit from the advantages serverless and cluster workflows. In this post, Ill cover a method to build a serverless workflow using AWS Lambda as a Serverless processing node, AWS Fargate and AWS Batch as cluster processing nodes and AWS Step Functions as the orchestrator between them. We will cover the following: - AWS Batch and AWS Fargate: why they are beneficial and what their differences are. - AWS Step Functions: how it is different from other ways of connecting services and what the advantages are. - What are some specific cases where hybrid infrastructure could be beneficial - Example code which allows us to deploy an AWS Lambda and AWS Fargate solution. - Example code which allows us to deploy an AWS Lambda and AWS Batch solution. AWS Batch and AWS Fargate AWS Batch and AWS Fargate implement a Container-as-a-Service approach: you just need to define a docker image, some CPU/memory resources and you are good to go. AWS Batch provides a way to have an on demand ECS cluster which scales according to what you are trying to process. You can use any instance (including GPU) as well as Spot instances, which can save you up to % of the cost of on demand instances. The process works in the following way: you send jobs to the jobs queue of AWS Batch and based on the number of jobs it scales the number of instances in the cluster. If the queue is empty it will eventually clear the cluster. AWS Fargate, on the other hand, is closer to AWS Lambda in terms of organisation. Every job is executed on a single instance which is created just for this job. So while on one hand Fargate scales a lot faster than Batch (s of seconds vs minutes), it is limited in terms of types of instances. You can only use CPU instances, but you can customize the amount of memory and vCPU available which can help to reduce the cost. So while both Fargate and Batch provide an on-demand cluster experience, they are very different in terms of how it is organised. If you need specific instances (let's say with GPU), then you will need to use AWS Batch. If you need to have low latency and better scaling, then you will be better off with AWS Fargate. While both AWS Batch and AWS Fargate are very convenient services for on demand processing, the real game changer came during Re:Invent when AWS announced native integration to AWS Step Functions. AWS Step Functions One of the challenges of a microservices architecture is communication between different services. There are three broad methods by which services can communicate: - through synchronous API requests (for example API Gateway) - through asynchronous messaging between services (for example SQS and SNS) - through a state machine orchestrator API requests are great for requests that finish quickly with a limited need for parallelism and asynchronous messaging excels in dealing with longer running processes and a large amount of parallelism. But the biggest advantage of an orchestrator is that it enables the complete specification of every step of a workflow, how it is processed, the state of data between each step (making it a state machine), custom error handling and monitoring jobs processing at scale. The biggest disadvantage of an orchestrator is usually either price or the need to deploy it separately as another piece of infrastructure. Which is where Step Functions comes in. AWS Step Functions enables us to construct a state machine graph with custom logic, where each processing node can be either AWS Lambda, AWS Batch or AWS Fargate. The Step Function service tracks the completion of the task as well as if an exception occurred. It enables us to branch out logic in case of error (with the ability to customize the handling of an error), execute jobs in parallel and implement retry logic. In summary, AWS Step Functions enables us to combine Serverless processing with container based cluster processing via Batch and Fargate, expanding our capabilities and the options available to us. Use cases Machine learning training pipeline For a machine learning pipeline, we can benefit from the large amount of parallelization AWS Batch or Fargate gives us on our various hyperparameters while still benefitting from storing and comparing results via Serverless Lambda functions !Machine learning training pipeline Machine learning deployment pipeline A hybrid infrastructure enables to solve a number of issues which occur during the implementation of a machine learning deployment pipeline: - A modular approach which enables to combine multiple models and frameworks into one pipeline. - A/B testing which allows us to compare model performance, to ensure the best model goes into production. - Scalable inference allows us to run batches in parallel which increases the speed of processing. !Machine learning deployment pipeline Data pipeline A data pipeline can utilize hybrid infrastructure to modularize the processing parts into several types of modules. Modules which can be easily parallelized can be processed through AWS Lambdas, modules which need to be processed through GPU instances can use AWS Batch and modules which require long processing times can utilize AWS Fargate. !Data pipeline Pushing Docker container image to ECR Let's start with downloading code from the repo. We will create an example docker image and publish it to ECR. Prerequisites: You will need to have docker and AWS CLI installed  Example code for AWS Fargate line example Lets get started with an AWS Fargate example. Well use the following stack: - AWS Lambda and AWS Fargate for processing - Serverless framework for handling deployment and configuration To get started, youll need to have the Serverless Framework installed. Run the following commands from the root folder of the cloned repository.  Youll receive the following response:  Visit the console to confirm your new Step Functions state machine was created (https://console.aws.amazon.com/states/home) and you can invoke it using output endpoint. Code decomposition The configuration file consists of the following parts: - `functions` which contain information about the Lambdas which are used - `stepFunctions` which contains description of the state machine graph - `resources` where AWS Fargate is defined. You can adjust the parameters section to adapt the config to your needs. Example code for AWS Batch line example Lets get started with AWS Batch example. Well use the following stack: - AWS Lambda and AWS Batch for processing - Serverless framework for handling deployment and configuration To get started, youll need to have the Serverless Framework installed. Run the following commands from the root folder of the cloned repository.  Youll receive the following response:  Visit the console to confirm your new Step Functions state machine was created (https://console.aws.amazon.com/states/home) and you can invoke it using output endpoint. Code decomposition Configuration file consists of the following parts: - `functions` which contains information about Lambdas which are used - `stepFunctions` which contains description of the execution graph - `resources` where AWS Batch is defined. You can adjust the parameters section to adapt the config to your needs. Conclusion Weve created two processing workflows with AWS Step functions, AWS Batch, AWS Fargate and AWS Lambda via the Serverless Framework. Setting everything up was extremely easy, and saved us a lot of time. By modifying the serverless YAML file, you can change configuration of state machine graph to accomplish whichever task you need. Feel free to check the project repository at https://github.com/ryfeus/stepfunctionsprocessing. Im excited to see how others are using Serverless to empower their development. Feel free to drop me a line in the comments, and happy developing!",
      "__v": 0
    },
    {
      "_id": "64e08926b72e199dda603f98",
      "title": "Serverless Framework Enterprise .. - Deployment Profiles",
      "content": "With todays Serverless Framework Enterprise release we are adding support for Deployment Profiles, enabling serverless development teams to move fast and scale while enforcing operational and security best practices. One of the greatest challenges of a scaling engineering team is educating new team members about organizational security and operational practices. This is especially challenging when those team members are also new to serverless. For example, when they deploy a service you want to make sure they deploy to the right regions and use the AWS Account designated for that stage. Or maybe you want to make sure that they do not deploy to production on Fridays. Engineering Managers and Operators, with Serverless Framework Enterprise Deployment Profiles, can designate AWS Accounts for each of the stages in their applications, and enforce security and operational policies using Serverless Safeguards. AWS Accounts for each stage AWS Credentials Access Role helps you secure your service deployments by enabling Serverless Framework Enterprise to issue temporary AWS Access Keys to deploy your services to AWS. The AWS Access Keys are generated by Serverless Enterprise on every command and the credentials expire after one hour. The AWS Credentials Access Roles are defined in the Deployment Profiles enabling your team to automatically use the right AWS Account for each stage. Safeguard Policies for each stage Safeguards enable managers and operations teams to configure policies that must be complied with, like restricted-deploy-times, required-stack-tags, or no-overly-generous-iam-role-statements, in order for a Serverless Framework deployment to succeed. It comes with safeguard policies available out of the box, with pre-configure for you. If those policies arent enough, you can implement your own too. Safeguards are configured in the Deployment Profiles enabling your team to comply with the requirements unique to each stage. Safely deploy By simply using Serverless Framework Enterprise, when you run `serverless deploy`, a short-lived AWS credentials will be generated and the Safeguard Policies will evaluated for that service before any resources are provisioned. With Deployment Profiles you can now automatically use the right AWS accounts and safeguard policies for each stage. ",
      "__v": 0
    },
    {
      "_id": "64e08926b72e199dda603f9a",
      "title": "Lessons Learned From Sending Millions of Serverless Webhooks",
      "content": "In March, Dwollas engineering team launched an updated webhook notifications architecture, cutting delivery times during peak load from minutes to seconds. At the same time, we increased reliability, reduced costs and enabled configuration on a per-customer basis. Is your company just starting out and cant handle much traffic? We can send webhooks one-by-one. Do you have an auto-scaling API that can handle hundreds of parallel requests? Well send them as fast as you can receive them. Webhooks are HTTP calls to our customers APIs providing real-time updates for specific events, eliminating the need for long polling. Our old architecture was simple by design. We used a RabbitMQ queue serviced by a pool of Elastic Container Service (ECS) handlers. As events occurred in the system, we sent them to the queue. The handlers received them, called the appropriate customer's API and sent the result to another queue for storage. After years of serving us well, we needed improvements. The shared queue meant high-volume customers doing large payouts and APIs with high response times delayed webhooks for everyone. Scaling the handlers to drain the queue caused all customer APIs to receive webhooks in parallel, even those that couldn't handle them. To further scale, we moved to a multi-queue, serverless architecture. When a customer subscribes to webhooks via our API, we dynamically provision a Simple Queue Service (SQS) queue and Lambda handler just for them. Now as events happen, we look up the appropriate customers queue, send them there for handling and send the result to a separate queue for storage. This allows us to configure the send rate of each individually and ensures high-volume customers or those with high response times arent impacting others. After sending millions of webhooks on the new architecture, we've learned valuable lessons. Not of fan of lists? Head straight to the open-sourced code! TypeScript is great. Our old handlers were written in Scala and while you can run Scala on Lambda via the Java runtime, cold start times are high. Since our handlers get invoked a lot and scale up and down with load, we run into cold starts often. TypeScript is a superset of and compiles to JavaScript, the runtime of which performs much better in this regard. Weve appreciated the type safety and improved IDE experience TypeScript provides. Serverless Framework and AWS Cloud Development Kit (CDK) work well together. The Serverless Framework allows you to configure Lambda functions and event source triggers (SQS, in our case) with a few lines in a `serverless.yml` file. The file also allows custom CloudFormation YAML for resources it doesn't support. That's where AWS CDK comes in. With it, you can configure AWS resources with all the power of TypeScript and then run `cdk synth` to produce a CloudFormation template YAML file. You can then import this file into your `serverless.yml` file and deploy the whole thing with one `serverless deploy` command! Audit dependencies to keep bundle sizes small. Frontend JavaScript developers are familiar with this concept, but it's less of a concern for backend developers. Since Lambda cold start times are impacted by bundle sizes, though, it's important to keep your eye on them. Serverless Webpack is a great plugin to minify your code and Bundlephobia allows you to compare popular libraries and their sizes. CloudWatch's default log retention period is forever. This can get expensive with high-volume Lambda functions. Either ship your logs to your preferred aggregator or set the retention to a finite value. With Serverless Framework, this is as easy as adding `logRetentionInDays: ` to `serverless.yml`. Follow AWS Best Practices. The Lambda and Using Lambda with SQS Best Practices helps avoid Lambda throttles, understand SQS message batches (they succeed or fail together) and configure redrive policies high enough to prevent prematurely sending messages to dead-letter queues. Structure your logs to ease alert creation and debugging. As an example, we preface errors with [error] allowing us to create a Log Metric Filter and get alerts anytime they occur. Consistently including high cardinality values in log messages (think account or transaction ID) is another good habit, allowing you to more easily track specific requests through the system. Lambda errors can be elusive, but CloudWatch Insights helps. When a Lambda Error alert triggers, its not immediately clear what happened, especially if there are lots of logs to search through. Only through experience do you find timeouts log timed out and out-of-memory errors log process exited. CloudWatch Insights provides query capabilities to easily search Log Groups for these messages:  Understand AWS account limits. Each service has its own limits and while some can be increased, others cannot. By default, Lambda has a limit of , concurrent executions, for example, and CloudFormation has a limit of stacks. Before getting too far along with a solution, understand your limits. AWS Trusted Advisor can help keep tabs on them and trigger alerts if you cross certain thresholds. Think twice before dynamically provisioning AWS resources. In our initial testing, we created the SQS queue and Lambda handler when a new customer subscribed to webhooks via the API and deleted them on unsubscribe. These are time intensive, however, and we quickly ran into race conditions during functional testing. Instead, we provision disabled resources on customer creation and only enable/disable the Lambda Event Source Mapping on subscribe/unsubscribe. This is much faster and still ensures we pay nothing for resources not in use. Utilize tagging to manage lots of resources. Each SQS queue, Lambda handler, and CloudWatch Log Group have Project, Version, and Environment tags. This allows us to easily search for, update, and monitor costs across thousands of AWS resources. Moving to a serverless architecture improved the timeliness, configurability, cost and reliability of our webhooks. Taken together, they make the Dwolla Platform even more valuable to our customers. We hope these lessons ease adoption of Serverless on your projects. For more details, check out the open-sourced code detailed below and the slides presented at Des Moiness JavaScript meetup. - webhook-provisioner - Create, update and delete customer-specific AWS resources - webhook-handler - POST webhooks to APIs - webhook-receiver - Sample application to receive and verify Dwollas webhooks - cloudwatch-alarm-to-slack - Map and forward CloudWatch Alarms to Slack - sqs-mv - Move SQS messages from one queue to another. In the event of errors, used to move messages from the dead-letter-queue back to the appropriate customer queue - generator-serverless - Yeoman generator for TypeScript and JavaScript serverless functions",
      "__v": 0
    },
    {
      "_id": "64e08927b72e199dda603f9c",
      "title": "Deploy a scalable API and Backend with Serverless, Express, and Node.js",
      "content": "As more people begin deploying production web applications with Serverless it comes into question how exactly to structure an application repository that has multiple components with Serverless. Imagine you're building an e-commerce website where you expect users to register, and those users can create an order. These front end transactions can be handled in this example. Lets say as part of that order, you have a backend system for fulfillment of the order such as an API request to another system. In this example, when an order is created it will trigger a message sent to a backend function to process the order. In this post, I have come armed with a repository help get you started with your Serverless application development. With this example you get two DynamoDB tables representing a place to store your application's data, an API Gateway that you can hook into a front end, or back end process to interface with your application and separate endpoint Lambda functions, a SQS queue to simulate a message being sent to the acting backend, and the backend which is one Lambda function that is triggered from the previously mentioned SQS queue. As well as demonstrating a functioning example, this will also go over some __best practices__ for using multiple environments, custom variables per environment, how to package functions individually, how to use IAM roles per function individually, import yaml template fragments, and exporting API Gateway RestApiId to use in other `serverless.yml` files. As well as getting you started with an application template, this design will also help you circumvent around a the well known resource limit with AWS Cloudformation. Another benefit of this design is you are able to deploy your API endpoints separately from your backend infrastructure, while still having them loosely dependent on each other. Not only can you deploy the infrastructure together, but you can decouple the infrastructure and deploy individual API endpoints for development and deployment agility. Getting Started To get started you'll need your environment configured with AWS credentials. Next, lets pull down the example repository and jump into it:  How to use `make buildAll` and `make deployAll` Getting started is simple, after cloning the repository all you need to do is build the project with `make buildAll` and then deploy the project with `make deployAll`. `make buildAll` will run `yarn install` in the `./backend` directory, then it will look at each directory in the `./backend/src` directory and run `yarn install` for each, then it will run `make buildAll` from the `./api` directory. This will look at each directory in the `./api/src` directory and run `yarn install` for each. `make deployAll` will run `serverless deploy --stage dev` in the `./backend` directory and then it will run `make deployAll --STAGE='dev'` from the `./api` directory. This will look at each directory in the `./api/src` directory and run `serverless deploy --stage dev` for each. What does this build This will define an example Serverless infrastructure stack containing: an API Gateway The API has endpoints. One to create a user, one to get a user information, one to get a list of users, one to get a list of orders for a user, one to get order information for that user, and one to create an order. two DynamoDB tables One is the user table and one is the order table. a SQS queue An SQS queue that looks out for orders and moves them to fulfillment. one backend lambda function, and three api lambda functions The backend function will look for messages in the Order queue, then move them to fulfillment. The API functions are split into endpoints where you can implement different packages scope. Dev Environment One benefit that Serverless has out of the box is support for multiple stages. This example takes advantage of using multiple stages in order to define __custom stage variables__. In the `./backend/serverless.yml` you can see the following:  This allows us to define specific parameters for different environments of our site. In our example, we are setting a specific logging level, as well as specifically defining our table capacity. To utilize this feature, lets take a look at the `provider` section:  Here we are taking in the option `stage` from the command line, and setting the default to `dev` with the `stage` parameter. In the _environment.logLevel_ parameter, we are importing a __custom stage variable__ from our defined variables previously. Package functions individually and reducing dependencies Dependencies are something that might be challenging to manage. I like to keep my functions below _ MB_ whenever possible to preserve console editing ability. Some tools I use to accomplish this include packaging my functions individually, and using `serverless-plugin-reducer` to ensure I'm not uploading unnecessary dependencies. This plugin will look at your `requires` from your code, and resolve dependencies from that and ensure only _lambda_ dependencies get packaged. Using this plugin is simple, first include it in your plugin, and ensure package individually is set to true:  Use IAM Roles individually w/ serverless-iam-roles-per-function Serverless by default allows you to specify an iam role per `serverless.yml`, we are using the `serverless-iam-roles-per-function` plugin to add the _functions.iamRoleStatements_ parameter. With this we are able to define specific iam statements per function giving you precise security controls.  Export API Gateway RestApi In Serverless, when you define a function with a http event this will create an API Gateway. In this example our `./backend/serverless.yml` defines a `root` endpoint, this endpoint is a simple health check. We then export this API Gateway's _RestApiId_ and _RootResourceId_ in order to reuse in our separate API Gateway endpoints defined in `./api/src`. To demonstrate, in our `./backend/serverless.yml` we export _RestApiId_ and _RootResourceId_.  We then import these __export variables__ into for example `./api/src/order/serverless.yml` into _provider.apiGateway_.  This will tell the API Gateway what _RestApiId_ to use, and what the _RootResourceId_ is so that it can build child endpoints from the Root Resource. When you are nesting a route within a route, you will need to export the parent route's _ResourceId_. For example, in `./api/src/user/serverless.yml` we export the parent _ResourceId_ that our nested endpoint of `/user/{userid}/order` will use, which is `/user/{userid}`.  And import the _ResourceId_ into the child route defined in `./api/src/user/order/serverless.yml` as a _provider.apiGateway.restApiResources_ parameter for the route.  Reference pseudo parameters w/ serverless-pseudo-parameters When implementing iam best practices, you need to specify specific __Resource__ statements and this often requires utilizing Cloudformation Psuedo Parameters to reference _AccountId_ or _Region_. Using the plugin `serverless-pseudo-parameters` you are able to use Cloudformation pseudo parameters nearly the same as you would in Cloudformation. Consider the following example:  Here we are defining a DynamoDB table and specifying the _Region_ and _AccountId_ using `{AWS::Region}` and `{AWS::AccountId}` respectively. What to do from here? In more complicated examples you would be able to use AWS Cognito in the `/user` endpoint to set up authentication. This endpoint would be scoped for user functions around Cognito and will likely have similar imports. You could also import Stripe in a `/billing` endpoint to facilitate collection of payment information. Within the `/orders` endpoint, you can set up your DynamoDB queries for managing your order collection. You could set up CI/CD simply by adding a CodePipeline resource, and utilizing CodeBuild to pull down this repository, and run the make files. Use the `serverless-domain-manager` plug-in to enable domain functionality. Most of this structure is laid out, you just need to provide a valid `ApiHostedZone`, `ApiSite`, and `ApiCert`. This can be created in the AWS Console for Route and ACM and provided here as variables.",
      "__v": 0
    },
    {
      "_id": "64e08927b72e199dda603f9e",
      "title": "Serverless Local Development",
      "content": "Developing Serverless applications is a very different way of building applications we've been building for decades now. We've gotten used to having awesome local tooling for us to essentially run our entire application on our local machine. Boy does this make development easier if you can just play around with the app as you build it without even requiring access to the Internet. Then microservices came along and things have dramatically changed. Serverless arrived to help ease that infrastructure burden that building an application consisting of many small services adds. But it still means we need to figure out what the new dev workflow looks like in a Serverless microservice world. So lets set some goals. By the end of this post we should understand how we can reclaim to a large degree the ability we have always had to develop our applications on our local machines but still set us up for integrating our services into a larger team. Oh, and one small caveat. With Serverless as a development methodology still in its infancy, the ideas expressed here are my own opinion born out of experience building multiple Serverless projects solo and with teams. There will be others with other ideas about how to accomplish the same thing. It doesn't make any one right or wrong; we are all still working out the best ways to accomplish our goals. Goals What is it we want to accomplish in this article? Lets start with a list of items that we should hopefully have some answers for. The ability to execute and debug code locally in a repeatable way. Handling API calls to cloud vendor services locally. Unit testing. With that basic outline out of the way, lets get stuck in... Local development Because we are building microservices, we need to get used to the idea that we should not expect to run the entire application on our development machines. Having every service running constantly on your machine just so you can open up a web browser to \"play\" with the application offline doesn't make much sense; especially if your application is going to consist of 's or even 's or 's of seperate services. What we need to focus on then is getting each service running in some fashion locally so that we can easily execute the code we write; our handlers that will eventually be our Lambda functions that execute our business logic. To that end, I have, put together a Serverless bootstrap as I like to call it. It is publicly available as a Gitlab Project and is incredibly easy to use to get started. So lets get it cloned and go through some of the details. `git clone https://gitlab.com/garethm/serverless-nodejs-template.git` Getting started Once you have cloned the template, run `rm -rf .git` to get rid of the current .git folder so that it is ready for you to use for your own project. You will also notice some folders: src: This is where we will store our handler function code as well as our unit tests and any entities, classes or any other code we write. templates: This contains the base templates used by one of our plugins to generate new functions and tests. The `src/functions` and `src/test/functions` both already contain some example files for us to look at. Lets take a quick look at the `src/functions/exampleFunction.js` file:  As you can see, this is a very simple function. Its only purpose is as a way for us to see that our local development environment is configured correctly. The other part of that is the test file; `src/test/functions/exampleFunctionTest.js`  It is this test file that really contains the brunt of our local development functionality. What do I mean? Well, if we want to actually execute the code in our function, we run this test. On the line `return wrapped.run({}).then((response) => {`, the run function contains whatever event object we want, in this case nothing. But it could be an API Gateway event object, SNS, S, SQS, or any other possible event object a Lambda function could receive. Running code locally By default, the example function and test are linked together, so lets just execute the code in our function to see how it all works. Make sure you have the `mocha` plugin installed globally (`npm install -g mocha`) and then run the test from the root of the service: `mocha src/test/functions/exampleFunctionTest.js` What you should see are some successful results:  Lets make a small edit. On the line `return wrapped.run({}).then((response) => {` change it to:  Then lets edit the function itself at `src/functions/exampleFunction.js` to look like:  When you run `mocha src/test/functions/exampleFunctionTest.js` again you should now see our `console.log` included in the response.  Thats it. We now have local execution working. You could use this very easily to setup mocha integration with your IDE of choice and even setup debugging so you can step through your code, line by line, inspecting variables and everything else that comes with a local debugger. !VSCode Mocha Plugin One step in the right direction! But we don't want to have to copy and paste the contents of this test file and its handler function everytime we want to create a new function! Well, we don't have to. Because of the awesome `serverless-mocha-plugin` that has been responsible for making our lives easier so far, we can also use a CLI command that will create a function, a linked test file as well as an eventless entry into our serverless.yml for us. The command looks something like this: `sls create function -f functioname --handler src/functions/fileName.handlerName --path src/test/functions/ --stage local` For more details about this awesome plugin, check out the Github page. AWS Services locally One of the benefits of Serverless (especially in the AWS ecosystem) is that we have access to an incredible array of managed services that take away a large amount of the drudgery from building complex web applications. However, when you need to test code locally that tries to communicate to services such as S, DynamoDB, SNS, SQS amongst others, it becomes tricky. There are many different ways people try to solve this problem but my favourite is a technique called mocking. By making use of another NPM module, `aws-sdk-mock`, we can capture requests that would normally go to an AWS service and then return ... whatever we want. We can return a successful response or we can even simulate an AWS service error if we wish to test how we manage failures. To try this out, lets go back to our `src/test/functions/exampleFunctionTest.js` file and edit the test section to look like this:  All that we have done here is add a mock in to catch any call to S's `putObject` API call when we run our tests. In this case we are just responding with a success and an empty object. If we had returned with a `reject(new Error('Some error here!'))`, our function would need to catch that error and deal with it; just like in the real world. The `aws-sdk-mock` module can do with this any SDK and API in the `aws-sdk` module and you should go check out more details here. With the combination of those two plugins alone, we now have ways to run our functions locally, step through them with a debugger and even simulate success and failure response from the AWS services we will more than likely be using. The next step from here is to look at how we can incorporate this service we have been building in isolation into the rest of our application as a whole. If you have any comments to add, join us on the Serverless Forum or feel free to fork the project on Gitlab and make any merge requests to improve the bootstrap template.",
      "__v": 0
    },
    {
      "_id": "64e08927b72e199dda603fa0",
      "title": "Serverless Framework v.. -  ALB event source, API Gateway Websocket logs, S hosted deployment packages, Custom configuration file names & More",
      "content": "The Serverless Framework v.. release adds the new ALB event source, support for API Gateway Websocket Logs, deployment packages hosted on S and custom configuration files. In addition to those features weve addressed a number of bug fixes and other enhancements. bug fixes and enhancements were merged and are now available through our v.. release. Important: Due to the Node.js release cycle weve dropped support for Node.js version which is no longer in LTS. Going forward well slowly phase out non LTS Node.js version in the future to ensure that we keep on innovating while keeping our users secure. Please ensure that youre using an up to date Node version when working with the Serverless Framework. ALB event source API backends are one of the top use cases for serverless architectures. In fact, serverless computing really took off once AWS introduced the integration between AWS Lambda and API Gateway, making it possible to create whole web applications which auto-scale, are easy to maintain and only generate expense when actually used. Nowadays, API Gateway usually plays an integral part in such API backends. However, due to API Gateway being such a configurable and feature-rich service it can be costly if its only used to dispatch incoming data to Lambda functions. At the last re:Invent Amazon announced the integration between Application Load Balancers and Lambda functions. Rather than using ALBs to dispatch traffic between different compute instances (usually EC) its now possible to use an ALB as a gateway for incoming HTTP traffic which can then be forwarded to Lambda functions. The great news is that its usually an order of magnitude cheaper to use ALBs as API Gateways compared to the AWS API Gateway service. The Serverless Framework v.. introduces the new `alb` event source which makes it easy to hook up a Lambda function with an ALB listener. The following example shows how the `alb` event source is used to create a GET event at an ALB listener:  Note that you have to update your Lambda handler code slightly to adhere to the format the ALB expects:  If you want to learn more about ALBs and their usage in a serverless architecture feel free to read through the excellent writeups by Jeremy Thomerson. API Gateway Websocket logs Back in v.. weve introduced support for API Gateway REST API logs. Setting them up was as easy as adding the following lines of code to the `provider` property:  With Serverless Framework v.. we complement this feature with support for API Gateway Websocket logs. Enabling logging for Websockets follows the same pattern as the REST API logs:  Note that its also possible to combine a REST API with a Websocket API in one service and enable logging for both:  Do you have any wishes for further configurability or extra features? Wed love to hear your thoughts about Websocket logs in our feedback issue. S hosted deployment packages The Serverless Framework makes it possible to separate the package and deployment process with the help of the `serverless package` and `serverless deploy --package` commands. This feature is often used in CI / CD setups where one service creates the deployment artifact which is then consumed by another service and carried out via a deployment. When using this kind of configuration, the inevitable question of where to store the deployment artifact comes up. With Serverless Framework v.. we introduce the possibility to pull down deployment artifacts which are stored in an S bucket. Using this feature only requires you to set the `package` path to the corresponding `.zip` object in a S bucket:  Note that the packaging itself wasnt changed. This means that its still possible to package functions individually when using S hosted deployment artifacts:  We kept the implementation agnostic, meaning that we can extend it to pull `zip` files from any remote location. Let us know if thats something youre interested in. Custom configuration files It has been frequently requested to make possible to configure multiple services within the scope of a single project. With a single serverless configuration (aimed to cover single service) it couldnt be done easily, therefore weve enriched Serveless CLI with a `--config` option, that allows you yp pass a custom configuration filename to be used for given deploy or package command. This results in the ability to pass individually crafted and specialised configuration files under different circumstances for the same service:  Bug Fixes - Adding a validation to validation.js script +/- camilosampedro - Use common prefix for log groups permissions at Lambdas' execution roles +/- rdsedmundo - Update Scala version to .. for aws-scala-sbt template +/- NomadBlacky Enhancements - Add support for S hosted package artifacts +/- pmuens - `--config` option +/- dschep Documentation - Remove root README generator +/- pmuens Features - Add Application Load Balancer event source +/- pmuens - Add support for Websocket Logs +/- pmuens Contributor thanks We had some community contributions included in this release and want to say thanks once again to those of you that help make the framework better. Its truly appreciated.",
      "__v": 0
    },
    {
      "_id": "64e08927b72e199dda603fa2",
      "title": "Choosing a Database for Serverless Applications",
      "content": "When designing and building an application, one of the key considerations is which database to use. A poor decision here can cost you severely, either by requiring costly ongoing maintenance of a database or by forcing a sensitive data migration to another solution. In this post, we'll cover how to choose a database for your serverless application. With serverless applications, there are new elements to consider as compared to traditional, instance-based architectures. This post will start with some key high-level factors that impact your choice of a serverless database. Then, we'll look at a few key categories of databases to see how well they fit with the key factors we've identified. The table of contents of this post is as follows: - Key Factors for Serverless Databases - Data model needs - Connection model - Infrastructure-as-code - Fully managed - Pricing model - Serverless Database Categories - Server-based, relational databases - Server-based, NoSQL databases - DynamoDB - Aurora Serverless Key Factors for Serverless Databases Before we get too far, let's consider the different factors you should consider when evaluating databases for a serverless application. I've listed five factors that I consider most important for choosing a database in a serverless world. The first factor, centered on your data model needs, applies to serverless and non-serverless applications alike. The remaining four factors are more focused on particular attributes of serverless architectures. After we review the five factors generally, we'll look at a few classes of databases to see how they rate on the five factors. Data model needs The first factor you should consider is the data model needs of your application and how well a database fits those needs. Amazon has been pushing the notion of purpose-built databases for a bit now. The idea here is that in the past, most applications were forced to use a relational database. Now, there are a variety of database types to choose from, and you should pick the one that fits your application best. I'll oversimplify a bit, but I see database options as three main classes: - Relational / SQL / normalized: Traditional RDBMS systems that allow for significant query flexibility at the cost of top-end performance. Examples include MySQL, PostgreSQL, - NoSQL / denormalized: More recent database options that optimize for read-time queries at the expense of denormalized data sets. Lots of variety here but include MongoDB, Cassandra, and DynamoDB. - Hyper-specific use cases: Databases that are specialized for a specific purpose. This may include Elasticsearch for full-text search, NeoJ for modeling graphs, or Redis for in-memory operations. I don't see this grouping a lot, but I think it's a fruitful way to think about it. For some very specialized use cases, your choice is basically made for you. This is for anything in the third bucket -- you need graph traversal queries or full-text search and thus need to use a database specifically suited for that access pattern. Most applications can model their data in either of the first two buckets and the choice is more about flexibility vs. performance. If your data access patterns may change and you need the flexiblity, then go with a relational database. If you need hyper-scale and high performance, then go with a NoSQL database. Connection model The second factor to consider is the connection model of the database. This factor is a bit different than traditional, non-serverless applications. Most databases were built for a pre-serverless world. In this world, database clients were long-running applications on servers. Most databases want you to set up a persistent TCP connection to the database server and reuse this connection across multiple requests. There are some downsides to this persistent connection. First, setting up and tearing down the connection takes time. When you're using a long-running application, this doesn't matter as much. You can pay that upfront cost once and then get the benefits of a persistent connection across all of your subsequent requests. A second issue with the persistent connection is that each connection uses up resources on the client. Having too many open connections can hurt your database performance. Again, in the old world this was acceptable. You generally had a small, static number of application instances that were connecting to your database. In the serverless world, this has been turned upside down. We're in a world of hyper-ephemeral compute, where your compute instance can be created, used, and destroyed within moments. This makes it inefficient to create a persistent database connection with every request, as you're paying the connection cost for something that may not be used again. Further, the autoscaling attributes of serverless compute means that your application can scale up to thousands of compute instances within seconds. With certain databases, this can be a problem as you quickly reach the database connection limits. There are ways of working around these issues, but there are serious downsides. A more serverless-friendly connection model is a better option when available. Infrastructure-as-code A third factor to consider is how well your database can be managed via infrastructure-as-code. Infrastructure as code is becoming more and more of a best practice for applications. With infrastructure-as-code, you have fully defined your infrastructure in a way that it can be updated in a consistent, repeatable way. These practices are particularly useful in serverless applications where your application and infrastructure are so intertwined. A serverless application contains not just compute but queues, streams, blob storage, and event triggers to wire them all together. If you're not using infrastructure as code in a serverless application, you're going to end up with a confusing, unmanageable mess. Fully managed The fourth factor to consider with a serverless database is whether you can use it as a fully-managed service. Serverless is about offloading the undifferentiated heavy-lifting that doesn't matter to your users. Nowhere is this more true than in low-level infrastructure management. Just like serverless compute has freed up developers to do more work without managing servers, you should aim to use fully-managed databases to avoid the maintenance associated with patching, upgrading, and scaling a database. Pricing model The final factor to consider when choosing a serverless database is the pricing model. Many serverless applications utilize pay-per-use components. Rather than paying hourly for a server, no matter how much traffic you get, you can pay for only the compute you use with AWS Lambda. Similarly, services like Amazon SQS, SNS, and API Gateway use a pay-per-use pricing model. Pay-per-use in the database world is a little different, as you need to pay for storage in addition to the compute necessary to access the stored data. However, remember that _storage is usage_, and that paying a storage price per GB is still pay-per-use pricing and much better than paying for the full EBS volume attached to your instance, regardless of the amount of data you have stored. Serverless Database Categories Now that we've reviewed some key factors to consider when evaluating databases for your serverless applications, let's look at a few different options and see how they compare on the listed factors. Server-based, relational databases The first big category that developers reach for is the traditional RDBMS. And for good reason! Relational data modeling is well-known, SQL is ubiquitous, and most applications can model their data in a relational way. Relational databases are the top four databases on the DB-Engines rankings, and they represent a huge portion of databases in use today. So how do relational databases rank with our five factors? Honestly, not great. That said, they may still be the right choice in certain situations. Let's start with the positive. Relational databases probably fit your data model needs pretty well. Relational databases have supremely flexible query patterns, allowing you to iterate on your application without slowing you down much. It's true that there is a tradeoff between flexibility and query performance, but it's at a level that won't matter to most people. You can scale relational databases up quite a ways before you'll really hit performance issues. Relational databases also do pretty well on the fully-managed factor. There are a number of services that will run a relational database for you without requiring you to spin up an EC instance and `apt-get install` your way to success. If you're using AWS, Amazon RDS is a clear option here but there are a number of other services out there. Whatever you do, don't run your own RDBMS instances unless you definitely know what you're doing. The bigger problems with relational databases are with the other factors. And these downsides are pretty nasty. First, the connection model is all wrong for ephemeral compute. A RDBMS wants you to spin up a persistent TCP connection, but this is all wrong for AWS Lambda and other serverless offerings. The overhead of creating these connections and making sure you don't trigger connection limits will add complexity to your serverless applications. Further, the relational database model fits awkwardly in an infrastructure-as-code paradigm. It's true that you can create an RDS Database in CloudFormation. However, attempting to use another RDBMS provider will require you to write a CloudFormation custom resource to bring it into your CloudFormation stack. Even if you do provision your RDBMS via CloudFormation, you still need to figure out a way to create your tables and run migrations as part of your deploy step. It feels like a kludge to fit this into your CI/CD system or add a Lambda that's triggered after a deploy to run these administrative tasks. It's not impossible, but it doesn't fit cleanly. Finally, the billing model for relational databases is based on old-school hourly billing based on instance size. No pay-per-use here. All in all, RDBMS is an OK choice for serverless application in certain situations. If you like the relational data model and don't want to step out of your comfort zone, it can work for you. However, there are a number of factors that make it a less-than-ideal fit for serverless applications. Server-based, NoSQL databases The second category of databases is server-based, NoSQL databases. In this category, you have options like MongoDB and Cassandra. I'm pretty bearish about these databases in serverless applications. These databases bring a lot of the baggage of server-based relational databases with less of the upside. First off, all the issues about the connection model, infrastructure-as-code, and pricing model with relational databases also apply here. You're paying for instances, running one-off scripts during deploys, and trying to reuse connection pools with these instances. However, you don't really get the benefits of a serverless database either. The fully-managed options for these databases are improving, but they're still a bit sparse. Further, you often need to go outside the AWS ecosystem to use these, which adds additional overhead in orchestration. Finally, these NoSQL solutions do offer better scalabilty than their SQL brethren. However, that scalability comes at a premium. You'll either need to run a giant cluster of your own instances (and have the team to maintain it!) or pay for the fully-managed options mentioned above. Ultimately, I wouldn't recommend using server-based NoSQL databases in a serverless architecture unless you have strong experience with the data model and prefer it to a relational model. If you do use it, be sure to use a managed service so you're not dealing with failed upgrades or missing backups at the wrong time. DynamoDB While the previous two sections were broad categories of databases, the next two are specific database technologies. First up is Amazon DynamoDB. DynamoDB is a NoSQL database, like Mongo and Cassandra mentioned previously. There's a big difference between DynamoDB and the others. For lack of a better term, I'll say that DynamoDB is not 'server-based', while the others are. What does this mean? When you're using MongoDB, Cassandra, or other NoSQL databases, even if in a managed capacity, you're still working within a server-focused paradigm. You specify the exact number and size of nodes that you want in your cluster. You connect to a certain IP address or hostname that goes directly to your cluster. You probably partition your cluster in a private network so that it's not exposed to the public internet. With DynamoDB, none of these things are true. You have no idea how many servers AWS is using behind the scenes to service your table. You don't connect to a unique host; you make direct HTTP requests to the general DynamoDB endpoints. There's no fumbling around with security groups to make sure your applications have network access to your database. All you need is the proper IAM credentials to access your table. Given the above, DynamoDB stands head and shoulders above other options in terms of the connection model for use in a serverless application. But how does it compare on the other factors? DynamoDB shines in many other aspects of the serverless paradigm as well. It works well with infrastructure-as-code -- there's full CloudFormation and Terraform support. Further, there's no separate administrative tasks -- like creating database users or performing table migrations -- that happen outside the infrastructure-as-code process. Everything just works. Further, DynamoDB is fully-managed. In fact, you don't have an option to run DynamoDB yourself (unless you want to run DynamoDB Local on an EC instance, in which case you are beyond saving). For a NoSQL database that scales to terabytes of data, this is exactly what you want. Don't spend your precious resources babysitting your database. DynamoDB also has a great pricing model for serverless. You can do pay-per-request pricing using the on-demand billing model, which gives you pay-per-use pricing just like Lambda, SNS, SQS, API Gateway, and more. If you do have a better sense of your traffic patterns, you can use the standard provisioned throughput billing model. The last factor is the data model needs, and this is where it gets iffy with DynamoDB. There are two main issues with DynamoDB data modeling: It's a significant learning curve and shift for those coming from a RDBMS background. It's much less flexible if your access patterns change over time. These are completely different types of problems. The first one is a temporary problem -- you and your fellow engineers will need to spend some time learning how to model data with DynamoDB. There's a cost here, but it's easily paid if you put in the time. The second problem is more difficult. Many users that are building serverless applications are in the early stage of application development. They are expecting significant changes to their application over time as they iterate based on customer feedback. With an RDBMS, it's easy to change your access patterns. With DynamoDB, it's not -- you may find you need to perform a data migration to accommodate new use cases. This is my only hesitation with recommending DynamoDB whole-heartedly. If you know your application access patterns and know they won't change, you should absolutely use DynamoDB. If you are expecting them to change over time, you need to make some harder choices. Aurora Serverless The last category of database is Aurora Serverless. Aurora is a cloud-native implementation of RDBMS that AWS created. Aurora Serverless is a \"serverless\" implementation of Aurora. There are two aspects to Aurora Serverless that are different than traditional RDBMS options: There's a pay-per-use billing model. There's a Data API which allows you to make database requests via HTTP. Remember our initial qualms with using server-based, relational databases in serverless applications: the connection model isn't a fit; the billing model is not based on usage, and it's a bit of an awkward fit with infrastructure-as-code. The improvements in Aurora Serverless address two of these three issues. With a pay-per-use billing model, you get something that's more in line with the rest of your serverless architecture. And while this billing model update is interesting, it's the Data API that is the real game changer. The Data API for Aurora Serverless allows you to make HTTP requests to your RDBMS database. No need for persistent TCP connections. Further, it will handle connection pooling for you, so you don't need to think about connection limits in your Lambda function. Jeremy Daly has done an awesome deep dive on the Aurora Serverless Data API with great thoughts on the mechanics around using it and the performance characteristics. Currently, the Data API is not as performant as a persistent TCP connection or as a DynamoDB request. However, the performance is getting better. I doubt we'll get full parity with a persistent connection, but something in the ballpark would be a gamechanger. I've long been a fan of the potential of Aurora Serverless, and I'm bullish as ever on its future. Conclusion There is no easy answer for which database you should choose in a serverless application. DynamoDB checks a lot of the boxes, but its steep learning curve and lack of flexibility have burned more than a few people. I still think it's the right choice in most situations, but you have to make a call based on your team and application needs. In this post, we looked at the different factors you should consider in choosing a serverless database. Then we looked at a few categories of database you may consider in your application. The serverless compute revolution is still new, and it is taking some time for databases to catch up. We'll see new, cloud-native database options that fit well within the serverless ecosystem. The future is bright, and we just need to make do until it arrives.",
      "__v": 0
    },
    {
      "_id": "64e08927b72e199dda603fa4",
      "title": "Serverless Microservices in a Team",
      "content": "In a previous blog post, we looked at how we would structure our new services to allow for developers to write and debug their functions locally as easily as possible. This included the setup of some unit testing frameworks and mocking tools so we didn't have to call out to AWS services over the Internet. But if you are a developer in a team, you probably need to find ways to work with your team when developing your Serverless microservices. Goals To get started, just like the previous blog post, lets set some goals to meet for this blog post: Using VCS for our services Easy CI/CD setup Integration testing Going live Using Version Control Obviously, as with any software project, we need to use some form of version control and predominantly that's git. And while there are divided opinions on the matter, I prefer the idea of keeping each service in its own git repo. A lot of proponents prefer the monrepo approach, but let me provide reasons why I prefer to keep each service entirely seperate: Promotes re-usability of a service if you can include it into any serverless application (that being a collection of microservices). Since we should be building our services to be as decoupled as possible, we should not need to include other services just to have ours working as intended. Forces atomicity of our services, meaning that each service is built to contain everything it needs to function. All DynamoDB tables, all S buckets. And they are not shared across services. Since we are forced to keep services decoupled and atomic, synchronous communications are discouraged (since they add hard dependancies) and asynchronous communications are encouraged to pass data between services via PubSub or message queues. Each service can be super specialised in its architecture based on the need it fulfils. Even a different language runtime. Local development is lighter since you will only need to pull from the repo and run locally whichever microservice you are currently working on and not have to have a local version of all or more services your team has developed. If each service is a git repo on its own that means that managing the collection of services is relatively easy when it comes to setting up deployment. No complicated setup to deploy each individual service that has been combined under a single repo. Each service is a mini application all on its own. If we assume that your services are built as per the previous blog post about setting up a local development environment, then we also have a very easy method to have other developers get involved with maintaining any of our services. The process looks like so for a developer looking to work on an existing service: Fork the repo for the service to maintain. Clone the forked repo to your local Run `npm install` to setup all local dependancies. Create branch off of develop to do work in. Develop locally as normal using the unit testing and mocking environment to execute and debug. Deploy to personal AWS account for some final integration testing with AWS services. Push branch with new feature/bug fix to forked repo. Create pull request to the original repo. Now its up to the team around the service to decide whether to accept the pull request after a code review. And the best part is that you can also do your own CI/CD setup at that point to run unit and integration tests as you wish. Since a developer will be creating a branch off of develop, their PR is back onto develop. Continuous Integration and Deployment I really love simple continuous integration setups. And I love portable setups. You don't get much more simple and portable than a single yml file in the service root. I'm a bit of a fan of Gitlab, and one of the biggest reasons is because of their integration of CI/CD by default even into private and free projects. Of course, you can use whatever flavour of VCS and CI tools you wish, but this example leverages off of Gitlab and their runners system. Similar types of configurations exist for a lot of the CI/CD tools that exist. Gitlab allows you to create a `.gitlab-ci.yml` file that describes the CI/CD process for even multiple branches of your repository. Here is an example:  Here's the quick rundown of the configuration above. It starts by you selecting which image from Dockerhub you wish your container to run on. You could also build your own image from scratch and store it on Gitlab itself and use that too so you have ultimate flexibility. Then `before_script` is executed before the rest of the file which just does an `npm install` to ensure all our dependancies are loaded. After that we have two sections; our `staging_deploy` and `production_deploy`. Each one `only` executes on specific branches; develop for `staging_deploy` and master for `production_deploy`. Its at this point that configurations can diverge. The staging configuration installs and run's mocha to ensure that all unit tests pass before getting to deployment. We then have to setup our access details into the AWS account we want our staging environment to deploy to. This is accomplished by setting `AWS_ACCESS_KEY` and `AWS_ACCESS_SECRET` values in Gitlab manually with the access details from AWS as environment variables made available to the script in our container on every CI run. Finally, we do a deploy to the staging stage. As noted by the comment, after this you can then kick off any integration test you wish or even look into Gitlab's sophisticated CI environment for more options; there's a lot more to it than just this example. Regardless of which VCS and CI tool you use, you should be able to achieve similar results; Bitbucket has Pipelines that works similarly, Travis is popular with Github, Circle CI is a popular bet as well. Choose whatever floats your boat, the principles stay the same. Integration testing Integration testing is a funny beast, and can be tricky to master in ANY environment, not just a Serverless application environment. In the book \"Building Microservices\" by Sam Newman, the go to resource about building microservices, he describes how difficult it can be to accomplish a good integration testing strategy. Problems exist along the lines of keeping the correct versions of services in sync when running the test, resetting the test environment and for large applications, just the sheer amount of time it can take to get an environment setup and configured for each test before you even start running it, then waiting around for the results, spending all that time writing tests and tests becoming stale... Running a good integration testing strategy can be difficult and very time consuming. However, it doesn't mean you shouldn't try and wouldn't be useful in your situation. Integration testing includes a LOT of possibilities. From simple API endpoint testing by just calling HTTP endpoints in sequence and assessing the results to full blown browser simulation by \"clicking\" on links and filling in forms. One thing all of these have in common is the need to do the testing in the cloud. And thankfully this gets easier to do when using stages with the Serverless Framework. Here is a sequence of steps I have seen used when attempting to provide integration tests to a code base on being taken live: A pull request submitted by a developer on the team gets merged into a services develop branch. A merge (potentially after peer review) triggers the start of the CI process (whether thats Gitlab CI/CD, Bitbucket Pipelines, CircleCI, Travis or any other CI/CD tool). Unit tests are executed on the service if they exist and if any fail, deployment fails. Altered service has a deployment occur by default to the `test` stage using `serverless deploy --stage test`. CI system waits for human intervention to decide if an integration test is required. If yes, an endpoint is called to a service dedicated for the purpose of resetting data on integration test stage to bootstrap for a new integration test run. Integration test (API endpoint or browser emulation) is executed. If successful (or if an integration was bypassed) develop gets merged into master and deployment occurs. This is naturally very simplified, and is only one of many possible configurations. As for the actual integration testing tool? There are many options here, everything from rolling your own with `mocha` or `jest` and the `supertest` module, to tools such as Cypress and Puppeteer for that emulated browser testing. Production Going into production can be a little scary. However, even here we can help ameliorate the concern somewhat. Canary deployments is a feature added by AWS not so long ago and shortly thereafter, David Garca authored a great Serverless plugin, the serverless-plugin-canary-deployments plugin. For anyone who is not aware what canary deployment means, its a means by which you can have only some of your traffic shift over to a new version of your Lambda function after deployment. The function is monitored for errors, and if none are found, then more traffic is moved over and monitoring continues. This continues until all traffic is switched over to the new version. However, if an error is detected, then the deployment is immediately rolled back to the known working previous version. There are a ton of options with this plugin, everything from setting up additional Lambda functions to execute prior to or after traffic shifting, custom alarms to trigger rollbacks besides just errors, etc. I highly recommend taking a good look at that plugin if you want to ensure that deployments into production go smoothly. Conclusion Serverless can and does work well in a team setting with features and options available to work together as a team (including Serverless Framework Enterprise). If you have any extra advice or tips to offer, please do so in the comments below or drop by our forums if you have any questions so the rest of the community can get involved.",
      "__v": 0
    },
    {
      "_id": "64e08927b72e199dda603fa6",
      "title": "Dynamic image resizing with Python and Serverless framework",
      "content": "Images are essential to creating an engaging user experience, but more isnt always better. Large, high-resolution images may require little effort to integrate into the user interface, but they can drag down download speeds for the whole webpage - and the impact of all that additional points of resolution on the user experience is often minimal. Imagine that you run a news site. A large percentage of your readers are viewing your site on their phones and dont need the high-resolution pictures. Some of your users, though, are reading on a desktop computer where they have a better connection and a better screen, and so will appreciate the higher-quality images. The right option from the user experience standpoint is to provide all your images in different sizes based on the device that you are visiting from. However, new devices with unexpected screen proportions come online unpredictably, and resizing an image in advance to fit any conceivable screen size is virtually impossible. Should we pay a huge storage bill and pre-generate all images in all possible sizes? Should we generate each image on the fly for every request? Neither option sounds like a good idea. However, what if we created an image in each size when its first requested, and then saved it for later? This way each device would get the right image size for it, and we would save significantly on the storage costs and the compute costs. This also is a use case that Serverless is a great fit for. With Serverless, you only use the compute you need at each moment, and you only pay for the compute you use. The Serverless applications are already designed to auto-scale to suit the user demands, so you dont need to pre-scale any servers and can, therefore, reduce your costs even further. Even where the resizing of images in our use case is necessary, the compute costs are much lower when the resizing is done by a Serverless function. Using S and Python to scale images with Serverless In this article, we use Python within the Serverless framework to build a system for automated image resizing. We use S, the AWS cloud storage service, as an example, but Serverless also works well with other cloud services including GCP and Azure. Storing images in S is an easy, scalable way to avoid the high compute costs of hosting a vast library of pre-scaled images without sacrificing the versatility of a dynamic image interface. Essentially, what well do in this example is let each request generate the image of the size it needs. We then store the result in our S bucket. Next time someone requests the same image, one of two things will happen: if the image already exists in that size, the corresponding S URI will serve us the previously stored image directly. But if we don't have the image in that size yet, following the S link will first generate the image in that size and then get it served to us, as well as saving that newly resized image in the cloud for future use. This is what we mean by a smart resizing system: instead of preparing for every possible outcome of an image request, we let the users request the images in the sizes they actually need. Creating a Serverless API for image resizing How exactly do we go about implementing the Serverless image-resizing API? In this section, well cover the following steps: - Writing a serverless.yml config file that contains all the specifics for the resizing service - Implementing the resizing logic in a resizing function - Setting up an S bucket to work with the resizing endpoint Our plan is to have our S bucket do most of the work: if the image is present in the size we want in the S bucket, we just need to serve it to the customer. If the requested size of the image is not available in S yet, however, we will have our S bucket call our image resizing function which will create the image in the size that we need and respond to the end user with the resized image. Lets start by looking at the `serverless.yml` that sets up everything that will be needed for our function. First, we specify the name of our service, its runtime and location, and grant permissions to our future function to access S:  Note that the `Resource` declaration in the `iamRoleStatements` policy includes my bucket name. You'll need to change it for your bucket name. Next comes the definition of the function that we are going to expose and its parameters:  We also need a Resource declaration for the S bucket where we will store all the images:  Thats it for our `serverless.yml` file. You can view the full version of it here. Now, lets look at the implementation of the image resizing function in Python. We start by importing a number of modules that we will need in the function:  The `json` and `datetime` modules are self-explanatory. `boto` is the Python wrapper for the Amazon Web Services API which we will need to download and upload images from and to S. We use Pillow for image resizing  thats what `PIL` is. We also include `BytesIO` as our function will work with file streams, and the `os` module so that we get access to the environment variables via `os.environ`. Lets now take a look at the very outer function in our file:  This is the function that will get called when there is a new incoming request for an image to be resized. We parse out the `key` and the `size` properties from the named path elements that we put down in the `serverless.yml` file previously. We then call `resize_image` with the key to the image and the new size that we need. Finally, we return the redirect to the `result_url` location where our new resized image is. Lets see what the `resize_image` function does under the hood. First, it gets the image from S and reads it into a variable:  Second, it resizes the image it just read to the new size:  Third, it uploads the newly resized image back to S:  And the final step, it returns the resized image URL so that we can put it into the redirect in the outer function:  The resized image URL is built in a separate function as follows:  At this point, we have the outer function `call` that resizes an image and performs the redirect to the new location, which is the business logic we wanted for the function. You can see the entire `handler.py` file right here on GitHub. Our `requirements.txt` file for our function is just one line, as we only need two dependencies:  We now have everything we need for our image resizing function and can proceed to deploy it. Deploying the Serverless API for image resizing In order to deploy our function, we need the API credentials to our AWS account with permissions to access AWS Lambda, S, IAM, and API Gateway. To configure these credentials, we use a `.env` file. You can find an example `.env` file in the example repo on GitHub here. Please make sure that you never check the `.env` file into Git, as that might leak your AWS credentials. To make sure that our Python dependencies compile correctly both in development and in production, we use the `serverless-python-requirements` plugin. It will make sure that, independently of the operating system we are developing on, the Python dependencies will get packaged correctly for the Lambda environment (Amazon Linux). If developing on a non-Linux system, deploying our function will require Docker to be installed and running. So in order to deploy our function we need to: - Fill out the credentials in the `.env` file. - Run `sls deploy`. There is one last step before everything is functional. In the output of the deploy we get the URL of the function on the AWS API Gateway, it looks like this:  We now need to configure our S bucket to work together with our Serverless function as follows: - Configure our S bucket for website hosting as shown in the S documentation. - In the Advanced Conditional Redirects section of the Website Hosting settings for the S bucket, set up the following redirect rule:  In place of XXXXX we add the Lambda endpoint of our Serverless function from the deployment step output. Seeing the API in action Once we configure the redirect rule on S, we have a fully working solution. Once we upload the image to the S bucket, we can get the function to resize the image to the size we need on the fly. Wrapping Up In this article, we walked you through setting up a dynamic image resizing API with Python and the Serverless framework. Image resizing is a great use case for Serverless. When implemented with Serverless, the resizing of the images scales effectively with the load. The function will only use the compute it needs to quickly resize the images  you wont waste compute time if there are no resizing requests. The solution with S and a Serverless function also provides for a very simple architecture and minimizes the number of moving parts, therefore ensuring the stability of the system. There are many other use cases that can benefit from Serverless, from workflow automation and event streaming to backends for mobile apps and log processing. If youd like to explore Serverless, start with the Serverless documentation and check out the Introduction to AWS with the Serverless framework. If you are not using AWS you can find the documentation for your provider on the Providers page. You can find the full example project in this GitHub repository.",
      "__v": 0
    },
    {
      "_id": "64e08927b72e199dda603fa8",
      "title": "Managing Secrets and Output Variables With Serverless Framework Enterprise",
      "content": "With the Serverless Framework Enterprise v.. release, we are adding support for output variables and secrets management to make it easier for developers to separate secrets and shared components from their services. Output Variables As developers developing a Serverless Framework service we typically include two distinct types of resources in a single `serverless.yml` file - a few _shared resources_ (e.g. auth function, SNS Topic) and numerous _individual resources_ (like lambda functions) which depend on the shared resources. All of the shared and individual resources are bundled together into a single `serverless.yml` to accommodate the interdependency. Output Variables make it easier to share resources across services. Serverless Framework Enterprise Output Variables enable you to detangle the dependencies by separating the shared resources and the individual service resources into independent `serverless.yml` files all while sharing variables with each other. The shared resources publish output variables while each of the individual resources consume them. By refactoring into smaller services, as a bonus, we get smaller and easier to manage `serverless.yml` files, the ability to independently deploy each service, and shorter deploy times. There are two parts to sharing state across services - publishing the output variables and consuming them in other services. For example, if you have a service called products and you want to share the SNS Topic name with other services you can just add the `name` value to the new `outputs` section of the `serverless.yml` file. products/serverless.yml !products Now any other service (e.g. inventory) can use that output variable simply be referencing it using the new `${state}` variable. inventory/serverless.yml !inventory You can find more details on defining output variables, using them in dependent services and viewing them in the dashboard in the docs. Secrets When using third party services in your Serverless applications, you often need configuration data for things like API keys, resource identifiers, or other items. This configuration data often includes sensitive secrets which must not be stored in source control. Properly securing the sensitive secrets requires the use of a secrets management solution like SSM, KMS, Vault, or Secrets Manager, to store and encrypt the variables. Each of these introduce significant new infrastructure with additional operational overhead and cost. This touches every part of the services lifecycle: - Developers must implement a way to retrieve secrets and load them into their service. - If developers deploy from their local environment then you need to securely distribute the secrets to the developers machine. - When running tests in a CI service, the secrets must also be configured in the CI service, or retrieved on each run. - Once tests are complete, the CI/CD pipeline will need the secrets again to perform the deployment to the appropriate stage. Serverless Framework Enterprise Secrets alleviates all of these challenges by providing a way to centrally secure and store sensitive secrets and use them in your `serverless.yml` file. With Serverless Framework Enterprise Secrets you can create a secret in the dashboard as a part of a deployment profile. !SFE Screenshot To use the secret you just add the `${secrets}` variable to your `serverless.yml` file. When you deploy itll decrypt and resolve the value.  Since the secrets are defined in a profile, you can set different values for each stage. For example, if you have two Stripe accounts, one for prod and one for qa, then you can create a secret `stripeApiKey` to store the API Key for each stage. When you deploy, itll automatically use the right stripeApiKey value. You can find more information on creating new secrets and using them in the docs.",
      "__v": 0
    },
    {
      "_id": "64e08928b72e199dda603faa",
      "title": "Serverless Framework v.. - Extended ALB configurability, Support for external Websocket APIs, Local plugins via relative paths & More",
      "content": "The Serverless Framework v.. release adds new ways to configure conditions for ALB events, support for externally managed Websocket APIs and local plugins which can be referenced via relative file paths. Weve also addressed a number of enhancements and bug fixes. In total bugs were fixed and enhancements were merged and are now available through our v.. release. Please ensure that youre using an up to date Node version when working with the Serverless Framework. Support for new ALB conditions In our last Serverless Framework v.. release we introduced support for the ALB event source which is a compelling replacement for the sophisticated, but costly AWS API Gateway service. While API Gateway is still superior for complex API setups one can achieve quite a lot with the much cheaper ALB service offering. This release extends the ALB event source capabilities by adding support for different conditions which need to be met in order for the ALB to route incoming requests to the connected Lambda function. ALB event sources can now be configured to accept different headers, IP addresses, methods, query strings and multiple paths. The following shows a complex ALB setup in which we leverage the new config options:  Do you want to learn more about ALB and how it can save you money if you use it as an API Gateway replacement? You can read more about the ALB event source and its capabilities in our v.. release blog post. External Websocket APIs Most Serverless Framework applications start with one `serverless.yml` file in which the whole application with all its infrastructure components is described. While this is sufficient in the beginning its recommended to split the whole application up into different services and use a separate `serverless.yml` file for every service. Splitting up an application into different services makes it a requirement for certain resources to be shared between such services. One very common resource type which needs to be shared across services are APIs. The Serverless Framework already supports an easy way to introduce an external REST API to a service, making it possible to re-use and extend that API within the service. In our v.. release were extending the support for external APIs to include Websocket APIs. Introducing an existing Websocket API into an existing service is as easy as using the `websocketApiId` config parameter under the `provider.apiGateway` property.  Do you want to learn more about best practices on how to split up your API-driven application into different services? Our API Gateway documentation provides some more insights into this. Local plugins via relative paths Our Serverless Framework plugin architecture provides an easy way to extend Serverless in various different ways to meet specific business needs. The community has been working hard on hundreds of plugins to help other Serverless developers achieve certain goals and make Serverless development easier than ever. While one can easily distribute and consume plugins via `npm` its sometimes necessary to work with plugins which are project specific or maybe not yet distributed via `npm`. Perhaps you wish to maintain your own plugins itnernally? Up until now there was a clear distinction between `npm` hosted plugins and local plugins. The only way to work with local plugins was to leverage the `plugin.localPath` configuration. Using that meant that only local plugins were supported for the whole service and `npm` hosted plugins were no longer an option.. Our v.. release finally makes it possible to mix `npm` hosted and local plugins in an easy way. Heres an example where we use the infamous `serverless-offline` plugin alongside a plugin which is project specific and stored in a separate directory of our service.  Bug Fixes - Split IAM Policy from IAM Role & Improve DependsOn for Streams +/- alexcasalboni - Fix duplicate packaging issue +/- alexdebrie - Fix lambda integration timeout response template +/- medikoo - Fix +/- JonathanWilbur - Do not set tty on stdin if no tty available +/- jpetitcolas Enhancements - Remove default stage value in provider object +/- mydiemho - Fix: Update azure template +/- tbarlow - Remove package-lock.json and shrinkwrap scripts +/- medikoo - Use naming to get stackName +/- joetravis - Add ip, method, header and query conditions to ALB events +/- cbm-egoubely Documentation - Update docs | dont use provider.tags with shared API Gateway +/- OskarKaminski - Fix formatting issue with Markdown link +/- awayken - fixed a typo +/- floydnoel - Update variables.md +/- ElinksFr - Added correction based on community feedback +/- garethmcc - Remove README redundant link +/- Hazlank - Fix typo in link to ALB docs +/- schellack Features - Allow to load plugin from path +/- mnapoli - Feature/support external websocket api +/- christophgysin Contributor thanks With different contributors, thanks again to all community members that got involved with this release to make it a success.",
      "__v": 0
    },
    {
      "_id": "64e08928b72e199dda603fac",
      "title": "How to Create a Serverless GraphQL API for MySQL, Postgres and Aurora",
      "content": "Exposing a simple database via a GraphQL API requires a lot of custom code and infrastructure: true or false? For those who answered true, were here to show you that building GraphQL APIs is actually fairly easy, with some concrete examples to illustrate why and how. (If you already know how easy it is to build GraphQL APIs with Serverless, theres plenty for you in this article as well.) GraphQL is a query language for web APIs. Theres a key difference between a conventional REST API and APIs based on GraphQL: with GraphQL, you can use a single request to fetch multiple entities at once. This results in faster page loads and enables a simpler structure for your frontend apps, resulting in a better web experience for everyone. If youve never used GraphQL before, we suggest you check out this GraphQL tutorial for a quick intro. The Serverless framework is a great fit for GraphQL APIs: with Serverless, you dont need to worry about running, managing, and scaling your own API servers in the cloud, and you wont need to write any infrastructure automation scripts. Learn more about Serverless here. In addition, Serverless provides an excellent vendor-agnostic developer experience and a robust community to help you in building your GraphQL applications. Many applications in our everyday experience contain social networking features, and that kind of functionality can really benefit from implementing GraphQL instead of the REST model, where its hard to expose structures with nested entities, like users and their Twitter posts. With GraphQL, you can build a unified API endpoint that allows you to query, write, and edit all the entities you need using a single API request. In this article, we look at how to build a simple GraphQL API with the help of the Serverless framework, Node.js, and any of several hosted database solutions available through Amazon RDS: MySQL, PostgreSQL, and the MySQL workalike Amazon Aurora. Follow along in this example repository on GitHub, and lets dive in! Building a GraphQL API with a relational DB backend In our example project, we decided to use all three databases (MySQL, PostgreSQL, and Aurora) in the same codebase. We know, thats overkill even for a production app, but we wanted to blow you away with how web-scale we build. But seriously, we overstuffed the project just to make sure youd find a relevant example that applies to your favorite database. If youd like to see examples with other databases, please let us know in the comments. Defining the GraphQL schema Lets start by defining the schema of the GraphQL API that we want to create, which we do in the `schema.gql` file at the root of our project using the GraphQL syntax. If youre unfamiliar with this syntax, take a look at the examples on this GraphQL documentation page. For starters, we add the first two items to the schema: a `User` entity and a `Post` entity, defining them as follows so that each User can have multiple Post entities associated with it:  We can now see what the User and Post entities look like. Later, well make sure these fields can be stored directly in our databases. Next, lets define how users of the API will query these entities. While we could use the two GraphQL types `User` and `Post` directly in our GraphQL queries, its best practice to create input types instead to keep the schema simple. So we go ahead and add two of these input types, one for the posts and one for the users:  Now lets define the mutationsthe operations that modify the data stored in our databases via our GraphQL API. For this we create a `Mutation` type. The only mutation well use for now is `createUser`. Since were using three different databases, we add a mutation for each database type. Each of the mutations accepts the input `UserInput` and returns a `User` entity:  We also want to provide a way to query the users, so we create a `Query` type with one query per database type. Each query accepts a `String` thats the users UUID, returning the `User` entity that contains its name, UUID, and a collection of every associated `Pos``t`:  Finally, we define the schema and point to the `Query` and `Mutation` types:  We now have a full description for our new GraphQL API! You can see the whole file here. Defining handlers for the GraphQL API Now that we have a description of our GraphQL API, we can write the code we need for each query and mutation. We start by creating a `handler.js` file in the root of the project, right next to the `schema.gql` file we created previously. `handler.js`'s first job is to read the schema:  The `typeDefs` constant now holds the definitions for our GraphQL entities. Next, we specify where the code for our functions is going to live. To keep things clear, well create a separate file for each query and mutation:  The `resolvers` constant now holds the definitions for all our APIs functions. Our next step is to create the GraphQL server. Remember the `graphql-yoga` library we required above? Well use that library here to create a working GraphQL server easily and quickly:  Finally, we export the GraphQL handler along with the GraphQL Playground handler (which will allow us to try out our GraphQL API in a web browser):  Okay, were done with the `handler.js` file for now. Next up: writing code for all functions that access the databases. Writing code for the queries and the mutations We now need code to access the databases and to power our GraphQL API. In the root of our project, we create the following structure for our MySQL resolver functions, with the other databases to follow:  Common queries In the `Common` folder, we populate the `mysql.js` file with what well need for the `createUser` mutation and the `getUser` query: an init query, to create tables for Users and Posts if they dont exist yet; and a user query, to return a users data when creating and querying for a user. Well use this in both the mutation and the query. The init query creates both the Users and the Posts tables as follows: ```bash exports.init = async (client) => { await client.query(` CREATE TABLE IF NOT EXISTS users ( id MEDIUMINT UNSIGNED not null AUTO_INCREMENT, created TIMESTAMP DEFAULT CURRENT_TIMESTAMP, uuid char() not null, name varchar() not null, PRIMARY KEY (id) ); `) await client.query(` CREATE TABLE IF NOT EXISTS posts ( id MEDIUMINT UNSIGNED not null AUTO_INCREMENT, created TIMESTAMP DEFAULT CURRENT_TIMESTAMP, uuid char() not null, text varchar() not null, user_id MEDIUMINT UNSIGNED not null, PRIMARY KEY (id) ); `) } ``` The `getUser` query returns the user and their posts: ```javascript exports.getUser = async (client, uuid) => { var user = {}; var userFromDb = await client.query(`select id, uuid, name from users where uuid = ?`, [uuid]) if (userFromDb.length == ) { return null; } var postsFromDb = await client.query(`select uuid, text from posts where user_id = ?`, [userFromDb[].id]) user.UUID = userFromDb[].uuid; user.Name = userFromDb[].name; if (postsFromDb.length > ) { user.Posts = postsFromDb.map(function (x) { return { UUID: x.uuid, Text: x.text } }); } return user; } ``` Both of these functions are exported; we can then access them in the `handler.js` file. Writing the mutation Time to write the code for the `createUser` mutation, which needs to accept the name of the new user, as well as a list of all posts that belong to them. To do this we create the `resolver/Mutation/mysql_createUser.js` file with a single exported `func` function for the mutation:  The mutation function needs to do the following things, in order: Connect to the database using the credentials in the applications environment variables. Insert the user into the database using the username, provided as input to the mutation. Also insert any posts associated with the user, provided as input to the mutation. Return the created user data. Here's how we accomplish that in code:  You can see the full query in this file. Bringing everything together in the serverless.yml file Lets take a step back. We currently have the following: - A GraphQL API schema. - A `handler.js` file. - A file for common database queries. - A file for each mutation and query. The last step is to connect all this together via the `serverless.yml` file. We create an empty `serverless.yml` at the root of the project and start by defining the provider, the region and the runtime. We also apply the `LambdaRole` IAM role (which we define later here) to our project:  We then define the environment variables for the database credentials:  Notice that all the variables reference the `custom` section, which comes next and holds the actual values for the variables. Note that `password` is a terrible password for your database and should be changed to something more secure (perhaps `p@sswrd` ):  What are those references after `Fn::GettAtt`, you ask? Those refer to database resources:  The `resource/MySqlRDSInstance.yml` file defines all the attributes of the MySQL instance. You can find its full content here. Finally, in the `serverless.yml` file we define two functions, `graphql` and `playground`. The `graphql` function is going to handle all the API requests, and the `playground` endpoint will create an instance of GraphQL Playground for us, which is a great way to try out our GraphQL API in a web browser:  Now MySQL support for our application is complete! You can find the full contents of the `serverless.yml` file here. Adding Aurora and PostgreSQL support Weve already created all the structure we need to support other databases in this project. To add support for Aurora and Postgres, we need only define the code for their mutations and queries, which we do as follows: Add a Common queries file for Aurora and for Postgres. Add the `createUser` mutation for both databases. Add the `getUser` query for both databases. Add configuration in the `serverless.yml` file for all the environment variables and resources needed for both databases. At this point, we have everything we need to deploy our GraphQL API, powered by MySQL, Aurora, and PostgreSQL. Deploying and testing the GraphQL API Deployment of our GraphQL API is simple. - First we run `npm install` to put our dependencies in place. - Then we run `npm run deploy`, which sets up all our environment variables and performs the deployment. - Under the hood, this command runs `serverless deploy` using the right environment. Thats it! In the output of the `deploy` step well see the URL endpoint for our deployed application. We can issue `POST` requests to our GraphQL API using this URL, and our Playground (which well play with in a second) is available using `GET` against the same URL. Trying out the API in the GraphQL Playground The GraphQL Playground, which is what you see when visiting that URL in the browser, is a great way to try out our API.  Comparing the MySQL, PostgreSQL, and Aurora implementations To begin with, mutations and queries look exactly the same on Aurora and MySQL, since Aurora is MySQL-compatible. And there are only minimal code differences between those two and the Postgres implementation. In fact, for simple use cases, the biggest difference between our three databases is that Aurora is available only as a cluster. The smallest available Aurora configuration still includes one read-only and one write replica, so we need a clustered configuration even for this basic Aurora deployment. Aurora offers faster performance than MySQL and PostgreSQL, due mainly to the SSD optimizations Amazon made to the database engine. As your project grows, youll likely find that Aurora offers improved database scalability, easier maintenance, and better reliability compared to the default MySQL and PostgreSQL configurations. But you can make some of these improvements on MySQL and PostgreSQL as well if you tune your databases and add replication. For test projects and playgrounds we recommend MySQL or PostgreSQL. These can run on `db.t.micro` RDS instances, which are part of the AWS free tier. Aurora doesnt currently offer `db.t.micro` instances, so youll pay a bit more to use Aurora for this test project. A final important note Remember to remove your Serverless deployment once youve finished trying out the GraphQL API so that you dont keep paying for database resources youre no longer using. You can remove the stack created in this example by running `npm run remove` in the root of the project. Happy experimenting! Summary In this article we walked you through creating a simple GraphQL API, using three different databases at once; though this isnt something youd ever do in reality, it allowed us to compare simple implementations of the Aurora, MySQL, and PostgreSQL databases. We saw that the implementation for all three databases is roughly the same in our simple case, barring minor differences in the syntax and the deployment configurations. You can find the full example project that weve been using in this GitHub repo. The easiest way to experiment with the project is to clone the repo and deploy it from your machine using `npm run deploy`. For more GraphQL API examples using Serverless, check out the serverless-graphql repo. If youd like to learn more about running Serverless GraphQL APIs at scale, you might enjoy our article series \"Running a scalable & reliable GraphQL endpoint with Serverless\" Maybe GraphQL just isnt your jam, and youd rather deploy a REST API? Weve got you covered: check out this blog post for some examples. Questions? Comment on this post, or create a discussion in our forum.",
      "__v": 0
    },
    {
      "_id": "64e08928b72e199dda603fae",
      "title": "Dynamic image resizing with Ruby and Serverless framework",
      "content": "If youve grappled with resizing images for various page layouts and devices, you know the pain of accounting for all different types of screens, the scaling problem that serving the full-size original image presents, and the fact that large images increase page load times and increase bounce rates. And the difference between an original image and a compressed one is essentially unnoticeable to the user. And so it follows that youll want to generate and serve an image of lower but still good quality. However, its not possible to know in advance which image sizes to serve. And pre-scaling the original image to all imaginable sizes will result in an unsustainable hit to your cloud storage capacity and therefore to your monthly bill. What if there were a way to easily build applications using functionality available from cloud providers, for example AWS Lambda, that could scale your images on the fly? The Serverless framework provides such a solution with its auto-scaling, pay-per-execution functions, which circumvents keeping a massive collection of pre-scaled cloud data. As an added bonus, maintaining multiple, often-idle servers is unnecessary when using the Serverless framework, so your app will save significantly on compute costs. In this article, we'll take an in-depth look at the benefits of dynamic image resizing and walk you through using the Serverless framework to resize your images dynamically with AWS Lambda. A solution using S In this example, we'll be using Ruby in conjunction with the Serverless framework to build our app. Well also use the AWS cloud storage service, S, but the Serverless framework also supports Azure, GCP and Kubernetes, among other cloud providers. First, well pick a few ranges of possible image sizes (for example, we'd serve one size image to screens -px wide, and a slightly larger image to those -px wide). Then, well build a system from event-driven functions that will generate the specified image sizes from the original photo. When a device requests our site, if we don't have the correctly sized image yet, following the S link will first generate the image in that size and then serve it to us. This newly resized image is then stored in S for future use. If the correctly sized image in our S storage does already exist, calling the corresponding S URI will directly serve us the previously stored image. We make the system smart by only generating image sizes requested by specific devices. If a particular article has only been viewed by px-wide screens, the Serverless app won't generate the rest of the sizes until someone with a different screen size comes along. The first reader with a px screen to load the article must wait for a split second while the system generates the new image size, but the user experience impact is minimal. Once the new size has been generated, we save it for future use. Since you are not maintaining idle servers, if no users visit your site, no code will run and you wont pay for anything. When new visitors do begin needing different image sizes, the system will scale, doing all the necessary work. But this approach is efficient: the system stops running as soon there are no more new images to generate. The Serverless framework also provides an optimal experience for users, as their devices get served the right size image in a timely manner. This approach thus offers an easy, low-cost and scalable solution. Creating a Serverless API for image resizing How exactly do we go about implementing the Serverless image-resizing API? In this section, well cover the following steps: - Writing a serverless.yml config file that contains all the specifics for the resizing service - Generating and redirecting to a new image URL in a handler function - Implementing the resizing logic in a resizing function - Setting up an S bucket to work with the resizing endpoint Writing the serverless.yml file Our serverless.yml file defines the Ruby environment for our resizing function defined above. We first specify our cloud provider (AWS) and a Ruby version that works for us. We also give it the right IAM roles so that it can interact with S.  Next, we define the function that well expose (`resize`), along with the `BUCKET` and `REGION` environment variables. Lets also add an event to our function, which in this case is an HTTP path that our function will work with. As you might imagine, `{size}` will become the `size` parameter here, and `{image}` will become the `image` parameter here.  The handler function Our image-resizing handler function itself will live in handler.rb. First, it will accept an HTTP request. After receiving this path, lets extract the size and image from it:  By calling `resizeImage.new` on the size and image, we convert the parameters to integers and construct the new image URL where we can find the resized image after conversion:  The handler then returns an HTTP redirect to the location of the new image once its been successfully generated.  Aside from these elements, the only other code in `handler.rb` is our standard error handler. As you can see, in this file we simply call `ResizeImage.new(image, size)`, but to keep things tidy, lets put the resizing logic somewhere else. Well now switch to `resize_image.rb` to look at that underlying logic. Image Resizing The resizing functionality will live in the resize_image.rb file. First, we need to download the file that needs to be resized.  We then define the function which actually does the resizing work.  After that, we upload the resized object to S.  Lastly, we wire the preceding three functions together.  We need the rest of the definitions in `resize_image.rb` (after private) to make sure we have a clean way to do all the resizing work. These definitions hold temporary variables needed to name, resize, and upload the new image, but are not relevant in the final product, so keeping these private ensures the code stays clean. Deploying the image resizing API Our API is all set up, and were ready for deployment. One last step before running the deploy command is to set up the AWS credentials in the `.env` file in the root of the project. Take a look at the example `.env` file in the GitHub repo here. Please make sure that you dont check the `.env` file into Git, as that might leak your AWS credentials. We have added this file to the list of ignored files in .gitignore to prevent you from accidentally publishing your AWS credentials. Once the AWS credentials are set up, we can deploy the change(s) to our `serverless.yml` file by running  This translates the syntax of our `serverless.yml` file into an AWS CloudFormation template and sends that change to AWS. For more on the deployment process, take a look at the Serverless AWS documentation. Setting up the S bucket As mentioned above, our Serverless function works with AWS S, so we need to set up our S bucket that will store our already requested image sizes. The core functionality we want is as follows: If the image exists in the right size in the S bucket, return it to the requester. If the image does not yet exist in the requested size, download the original size, resize the image, return the resized image to the requester and save it in our S bucket. To get this working, we will need to make the following changes in our S bucket configuration: Ensure our S bucket is configured for website hosting as explained in the S documentation. In the Advanced Conditional Redirects stanza of the Website Hosting settings for the S bucket, set up the following redirect rule:  In place of `YOUR-API-ENDPOINT`, we will add the Lambda endpoint of our Serverless function. We can get that by running:  Keep in mind that if you specify a custom stage during deployment, you also need to specify it in the `info` command, the output of which is information about the deployed service. We are specifically looking for the right endpoint address in this output. See the docs for the info command for more details. The API in action Weve set up our code, so lets now take a look at our work in action. Well first request a size we know exists: Now well request the image in a size that doesnt exist yet: It works! The next time we request this size, the image will be served directly from S. Wrapping up In this article, we saw that image resizing using Serverless keeps costs low, provides users with a good experience and scales images perfectly with our needs. We did this by walking through the process of creating a Serverless app that resizes images dynamically. New users to Serverless will find that building an app for resizing images is a great introduction to the framework. But Serverless is also applicable to much more complex cases, such as event streaming and processing, building multilingual apps and workflow automation. The Serverless Framework documentation is a great place to begin. You can also read the AWS-based introduction to the Serverless framework. If youll be using another cloud provider, Serverless can help you there too. This Github repo contains the full example project.",
      "__v": 0
    },
    {
      "_id": "64e08928b72e199dda603fb0",
      "title": "How BuildCenter and Serverless Guru Streamlined Their Serverless Development Cycle",
      "content": "BuildCenter makes smart and easy to use digital tools for builders to streamline their operations. Why Serverless? BuildCenter chose to use serverless over more traditional options because they wanted the ability to scale easily as well as not pay for resources that are not being used. They are also a small team and wanted something that would be easy to maintain in a central location as the demand scaled. Where did Serverless Guru come in? After the decision was made to move towards serverless, BuildCenter reached out to Serverless Guru, to discuss the best path forward. Over the following days, the Serverless Guru team began evaluating the BuildCenters requirements and what the scope of the project would entail. BuildCenter evaluated Serverless Gurus proposal and decided that this team was the best fit for the project. Serverless Guru brought in a wealth of experience around serverless, cloud development, automation, and application development which allowed BuildCenter to augment their entire backend development and DevOps needs to the Serverless Guru team. Green lights Serverless Guru began transitioning the BuildCenters existing Terraform infrastructure to use the Serverless Framework. The project was at such an early stage that the transition away from Terraform only required a simple re-write. Even this quick change achieved a large reduction in infrastructure size and complexity by allowing Serverless Framework to automatically generate much of the code in the background, meaning once again less time was being devoted to anything other than the product. When Serverless Guru finished transitioning the existing Terraform pieces into Serverless Framework the team began extending the BuildCenter backend and frontend infrastructure. The frontend was a SPA (Single page application) written using AngularJS. The supporting frontend infrastructure that served the frontend application was a combination of the following services: AWS S for static hosting AWS Cloudfront for caching AWS ACM for HTTPS AWS Route for DNS Each one of these services, above, was automated via the Serverless Framework and all of it was able to be deployed and connected with a single terminal command. The backend was written using NodeJS and was broken apart into a microservice architecture where each microservice was a single AWS Lambda function. The supporting backend infrastructure required the following services: AWS Lambda for the business logic Amazon Aurora Serverless for the MySQL database AWS Cognito for adding an authentication layer to the REST API Amazon API Gateway for hosting the REST API AWS SES for sending emails to users The beauty of automation While building the frontend and backend infrastructure, Serverless Guru was able to hit automation levels as high as %. At some points, the Serverless Framework or the underlying AWS Cloudformation did not support the functionality required for these high levels of automation which led Serverless Guru to seek out third-party Serverless Framework plugins and leverage the AWS CLI. Whenever the AWS Cloudformation functionality simply didnt exist yet Serverless Guru wrapped the edge case into deployment scripts which would kick off a series of events. The chain of events would pull in values, write out files, pass those files into the Serverless Framework, and finally make a deployment to AWS. The result of that work gave BuildCenter confidence that if anything ever goes down or is accidentally deleted it could easily be brought back up in less than a couple of minutes without any manual intervention. Multi-stage deployments The Serverless Guru team spent a lot of time ensuring that every piece of infrastructure BuildCenter relies on for production could be redeployed in an exact mirror environment. This was achieved by using the built-in Serverless Framework flags provided. Lets take a look: This command would deploy to a stage called test. By replacing that single word we can easily spin up every detail that is being used in production, including the database, authentication, etc. to a whole new environment. In the background the Serverless Framework takes that stage name and sets it as a variable. Then to reference that variable Serverless Guru would do this: Serverless Guru writes about topics like this on their training site and their blog. If youre interested in diving deeper than this basic example you should give them a look. Separating Serverless stacks: stateless vs. stateful When it comes to organizing projects using the Serverless Framework Serverless Guru has found a lot of success by defining Serverless stacks around the question, Are these AWS resources stateless or stateful?. How to identify if a component is stateful or stateless Stateless resources can easily be torn down (e.g. an AWS Lambda function) and recreated. As between AWS Lambda invocations, there is no shared data. Stateful resources, if torn down, could result in a serious impact on your business (e.g. databases, AWS S buckets, AWS Cognito User Pools) and require more careful attention. If youre not careful the automation youve built to streamline deployments can just as easily be flipped to cause a huge outage and data loss. This is due to the fact that when a Serverless stack is deleted all the resources contained in that stack are deleted as well. To ensure BuildCenter didnt have that problem, Serverless Guru implemented safeguards to avoid juggling knives. The first way Serverless Guru did this was through resource isolation. They separated BuildCenters Serverless stacks based on how critical the contents were to help prevent developer mistakes from bleeding over to core components of the application. For example, one Serverless stack could be a few AWS Lambda functions and an API definition: this stack would be considered stateless and could be destroyed and redeployed without impact. Another stack could hold our database or user file storage: this stack would be considered stateful and any tampering would cause a major outage. The second way to protect stateful resources is by using Termination Protection to block any developers from accidentally running `serverless remove --stage prod` and tearing down the production stack. Since this is a common fear, Serverless Guru created a Serverless Framework plugin to help companies enable termination protection on Serverless stacks based on specific stages. If youre interested in learning more about their plugin you can check it out here. The termination protection plugin was recently released on npm and its being used in the BuildCenter project! The third safeguard is the use of resource level protection via an AWS Cloudformation property called DeletionPolicy. It allows Serverless Guru to tell AWS Cloudformation that if the BuildCenter stack is deleted, dont delete this specific resource. Here is a snippet of what this looks like, `DeletionPolicy\" : \"Retain`. These safeguards add an important layer of development protection but what happens when someone deletes the AWS resource itself? For these instances Serverless Guru makes sure they have backup strategies for all stateful resources. For some scenarios, AWS Cloudformation has built-in solutions for this which can easily be used while for others Serverless Guru rolls out custom solutions to achieve the level of reliability that BuildCenter requires. Important takeaways When it comes to edge cases where AWS Cloudformation doesnt support certain functionality its critical to take the additional time to find a way to work these gaps into your automation. Serverless Guru will typically leverage a Serverless Framework plugin or the AWS CLI to handle unsupported functionality that AWS Cloudformation is missing. This ensures that if anything ever happens where you need to completely recreate your production application, you can do so through a single terminal command. This proactive approach also simplifies training new team members as you keep everything in source control and avoid black box development. Another key area is having a solid local testing workflow. When working with AWS Lambda functions and Amazon API Gateway its important to find and fix bugs locally before ever deploying to the cloud. BuildCenter uses a lightweight ExpressJS server with hot reloading which will point to the different AWS Lambda function files based on the path in the URL. For example, when a developer sends a POST request to `localhost:/register` the ExpressJS server would take the body of the request and pass it into an event object, then point the request to the register.js file, and everything would function the same as if it was deployed on AWS. By focusing energy on solid local testing you are increasing developer velocity and when your developers are moving faster and debugging more efficiently you save time and money. What has your experience been working with Serverless Guru? Everything has been great. Having Serverless Guru develop the back end and automate the deployment of both the back and front ends has really helped streamline our development cycles. As we build more applications and get into more automation with things like testing and CI/CD, we are excited by the potential we can achieve with our current team, as well as how easy it would be to grow the team with our current tool set. - Jason Alcaraz, Project Manager at BuildCenter.",
      "__v": 0
    },
    {
      "_id": "64e08928b72e199dda603fb2",
      "title": "Basic Integration Testing with Serverless Framework",
      "content": "With the latest Serverless Framework release, we made it easier to test APIs built with the Serverless Framework. Testing is important! We do it all the time when getting started developing a new service with the Serverless Framework. We might not have unit tests or a CI/CD in place yet, but at a minimum we will run `serverless invoke` or `curl` a few times with various inputs to make sure our APIs dont return s. After hitting `[up]` & `[enter]` in the terminal once too many times we begin to wonder if there is a better way. Yes, you should probably write some unit tests. When you are ready for that, here is an in depth guide on writing unit tests for Node.js Serverless projects with Jest. Once you have the coverage and maturity you need, youll also need some integration tests. However, integration tests can be complicated and time consuming to implement. So if you need some basic integration tests in the meantime but you are not ready to commit to writing integration tests, then let me introduce you to a new testing tool in Serverless Framework to add to your testing arsenal. Serverless Framework provides a new way to define basic integration tests for functions with HTTP endpoints. Its goal is to enable you to test your serverless applications without having to manually write a lot of code to do so. Tests are defined in a new file, `serverless.test.yml` and they are tested using the `serverless test` command. The testing documentation goes into detail about writing tests but here is a quick preview. As an example let's look at this `handler.js` and `serverless.yml` file. handler.js ```bash module.exports.hello = async (event, context) => { let body = {}; if (event.body) { body = JSON.parse(event.body) } const name = body.name || 'world' return { statusCode: , body: `Hello, ${name}`, }; }; ``` This function will return `hello, ` if a name was provided in the JSON request body, and `hello, world` if the name wasnt set. Here is the `serverless.yml` for this function with an HTTP POST endpoint. serverless.yml  Go ahead and deploy the service using `sls deploy`. Now let's write some basic integration tests in a new `serverless.test.yml` file. serverless.test.yml  Once the service is deployed run `sls test` and you can expect output like this:  As you can see, this light-weight test framework enables us to write some basic integration tests and run them on the live service. The docs show you how to send HTTP headers, JSON bodies, string bodies, and submit forms. You can also test the responses for HTTP response codes, string bodies, or JSON content.",
      "__v": 0
    },
    {
      "_id": "64e08928b72e199dda603fb4",
      "title": "Plugins Support Program",
      "content": "Since the early days of the Serverless Framework, plugins have been a useful tool for our community. It has been a way for anyone to contribute to the effectiveness of the framework by solving problems that just dont fit within the design of the core framework or are super specialised for important niche use cases. Plugins have become a vital aspect to many projects and yet, there has been a problem. If you are an organisation building Serverless solutions, Serverless, Inc is here to offer you the support you need so that you can be assured that if any issues arise, you can get a solution as fast as possible. However, if you are that same organisation using plugins, risk rears its head once again. Will the plugins you use have the required level of support you need? We at Serverless Inc. decided that we needed to find a way to show our appreciation to our community for the amount of work and effort put into developing so many plugin solutions, and at the same time reduce the risk for our enterprise users who need the safety and security of a consistent entity that can be there should the need arise. This is why were proud to introduce the Serverless Plugin Support program, an effort designed to allow Serverless Inc. the ability and capacity to assist in supporting the ongoing maintenance of key plugins developed by our community, so that those organisations who rely on them as a critical part of their Serverless ecosystem can rest assured they will receive the same level of support as the core framework. The key aspect of this program is the allocation of a support status based on criteria that a plugin must reach in order to qualify. Each level of support also provides different benefits to the authors, from being able to proudly display a support badge on their Github pages, to being able to attend the weekly open source sync meeting where new features for the framework are planned. Each support status also has different SLAs for existing and future enterprise support customers depending on the package chosen. For all details about plugin criteria, rewards and enterprise support, we have created a [page specifically for that] (https://serverless.com/plugins/criteria/). While we have a [form available] (https://docs.google.com/forms/d/e/FAIpQLSfliYMIgUvmHzzPcsJqZaGZeyODa_hYdXHCruOgQ/viewform) for any plugin that meets the criteria above to apply for support status, we have already pre-allocated some plugins with support status levels. We will be giving the authors of these plugins until the end of August to make the last few tweaks their projects need to meet any outstanding criteria. The program is entirely opt-in and while we look forward to as many plugins as possible joining the program, if any authors that have been pre-allocated a certain status do not wish to be a part of the program, they can just contact us at hello@serverless.com and we will remove that status. If, however, you are a plugin author and would like your work to be recognised and supported by Serverless, please apply with the form above. The three levels of support are: Community: The plugin meets some necessary basic documentation and licence requirements. At this level support is provided by the plugin author and Serverless community. Approved: The plugin has more stringent documentation as well as automated testing requirements and an author that shows responsiveness to pull requests from Serverless. An SLA to enterprise customers using Approved plugins is in effect. Certified: Additional testing requirements as well as providing read and write access to the plugin repo to Serverless. This status has an SLA applied for enterprise support customers The SLA available on each status depends partly on the support package chosen but can be broken down as follows: Community: Receives no additional support beyond what the community can provide, but has been recognised as a potentially supported plugin in the future. Approved: Approved plugins will receive the contracted SLA time or hours, whichever is longer. Certified: Certified plugins will receive the contracted SLA time or hours, whichever is longer. We look forward to continue the growth of our community and we foresee plugins remaining a very powerful way for anyone to contribute to the growth of Serverless development and the Serverless Framework. Our thanks goes out to all the members of the community that have put so much time and effort into improving Serverless for everyone, and we look forward to feedback regarding the Plugin Support program.",
      "__v": 0
    },
    {
      "_id": "64e08928b72e199dda603fb6",
      "title": "Serverless Guru Joins Serverless as a Development Partner",
      "content": "Serverless is proud to announce that Serverless Guru has joined its Partner Program. As one of the first serverless consulting companies, Serverless Guru brings in a level of experience that is hard to match. Their Serverless experts are driven to deliver business solutions to their clients with as little overhead as possible. They break tasks into small chunks and through writing infrastructure as code, theyre giving clients self-documenting value that they can then take and develop on their own as they choose fit. With this partnership in place, Serverless Guru will be able to work more closely with Serverless Inc. on delivering solutions for its customers. The two companies will collaborate on training and supporting customers as well as building out further functionality into Serverless Framework. Learn more about how Serverless Guru and Serverless work together to deliver on customer needs in this case study.",
      "__v": 0
    },
    {
      "_id": "64e08928b72e199dda603fb8",
      "title": "Serverless Framework – Now, Full Lifecycle",
      "content": "If we end up in a situation where we need to cobble together several tools and services to build and manage serverless applications, something has gone wrong. Serverless is about simplifying software, by radically reducing its complexity. We're not here to recreate that complexity. We're here to challenge it. Today, we're doing just that. We're expanding the Serverless Framework to be more than a tool for development and deployment. Now, the Serverless Framework includes real-time monitoring, testing, secrets management and security features, in a single, unified experience. All of which are now available to every developer, for free. Click here to try the new bigger, bolder Serverless Framework. In fairness, it's been hard to avoid using multiple tools to build and manage serverless applications. Serverless apps are x more efficient, but there are real differences in how you make and maintain them, which no single solution has accommodated well. Our new goal is to give developer teams everything they need to build incredible amounts of amazing work, without having to ask, \"Now, how do I test, monitor and secure this?\". Were taking on the entire serverless application lifecycle, giving you convenience and a streamlined workflow. !Serverless Framework - Develop Deploy Test Monitor Secure Further, given were your deployment framework and we know how your app is modeled, were able to instrument it for metrics, error and performance alerts, deployment notifications, automated tests and more, without you having to do it yourself. We're also now offering simple, transparent pricing. Every feature the Serverless Framework offers  across development, deployment, testing, monitoring and security  can be used by you and your team within a free tier of up to million transactions a month. After that, we charge for monitoring additional invocations, as well as by team seat via an Unlimited pricing tier that grows with you. Lastly, we offer plans for large purchases, as well as support, services, training and a self-hosted version of Serverless Framework. !Pricing If you want to just use the open source Serverless Framework CLI to develop and deploy your serverless applications, you still can. It is and will always be open source. The only difference now is you have powerful monitoring, testing and security features available to you, right after first deploy. Were fighting in the name of simplicity with the Serverless Framework. As it evolves into a more complete solution, we hope it enables everyone to build more and manage less.  The Team @ Serverless Inc.",
      "__v": 0
    },
    {
      "_id": "64e08928b72e199dda603fba",
      "title": "Zero Configuration AWS Lambda Notifications & Alerts with Serverless Framework",
      "content": "Zero configuration, anomaly detection The Serverless Framework makes it super easy to identify problems with your deployed serverless applications before they impact the quality of your service. When you deploy a service using the Serverless Framework your functions are automatically instrumented to detect anomalies and generate alerts. __Screen shot: alerts feed & details tab - new error identified__ !Screen shot: alerts feed & details tab - new error identified One of the greatest benefits of serverless architecture is that many of the things that could go wrong with our apps are no longer applicable. We generally no longer have to maintain VMs, storage, or any persisted resources. There is a lot less to monitor! However, there are still things that can go wrong and we still need to keep an eye on them. Weve identified the most important things to monitor in your serverless applications and included, out-of-the-box, the charts, alerts and notifications youll need to efficiently and effectively develop and operate your serverless applications Memory usage & duration There are two configurable settings for each function in a serverless application, memory and timeout, that can directly impact quality of service. The memory setting controls how much memory is allocated for each invocation. The timeout setting controls the amount of time given to the invocation. If a functions memory usage or duration exceeds those limits, the function invocation is terminated before it completes its job. As such, we want to make sure we keep an eye on memory usage and durations, and the Serverless Framework will alert you if your functions are approaching either of these limits. Errors Bugs happen! Our code has the most direct impact on the quality of service. We do our best to catch bugs with automated tests and pre-production releases, but it is never foolproof. But, now, the Serverless Framework has got your back. It watches for both new errors and unexpected error rates, alerts you when they occur, and provides you with the stack traces and logs youll need to figure out what happened. Invocations Lastly, we want to keep an eye on the incoming traffic. In general traffic spikes should have minimal impact on the quality of service of the functions as they will automatically scale. However, there are a few cases to consider. A spike could be the result of recursive loop or a DDoS attack. Even legitimate spikes need to be introspected. A spike in traffic can lead to more cold starts, so a lot of API requests may see much slower than usual responses. We also have to consider the down-stream impact of a spike. If our services have dependencies on third party APIs or statically provisioned resources (e.g. RDS) the dependent services may not be able to handle the request load. The Serverless Framework keeps an eye on memory usage, durations, errors and invocations for you. When you deploy your service, the Serverless Framework will automatically instrument your service and start monitoring it right away. Enabling these alerts couldnt be easier. Just run `serverless` to start a new project or to update an existing project to work with the new alerts feature. You dont have to instrument your code as it is done automatically when you deploy. You can learn more about installing & configuring alerts in the docs. The full list and details of each of the alerts is available in the docs; however, here is a summary of all of the alerts available to you out of the box: __Duration:__ Approaching timeout, Timeout, Unusual function duration __Memory:__ Approaching out of memory, Out of memory, Unused memory __Invocations:__ Escalated invocations __Errors:__ New error type identified, Unusual error rate All of these alerts are listed in the alerts tab of the service instance view in your dashboard. Each alert details page is customized for each type of alert so you get all the necessary details specific to that alert. For example, the approaching timeout alert shows you a graph of the durations over the past hour, stats about the duration, and suggested steps to resolve the issue. __Screen shot: alerts feed & details tab - approaching timeout alert__ !Screen shot: alerts feed & details tab - approaching timeout alert Of course, you arent going to be sitting around waiting for a notification in the dashboard. So we also have support for notifications via Slack and email. If you want to send it to something other than Slack or email, it also supports SNS Topics and Webhooks, so you can add your own custom integrations. You can add as many notifications as youd like to your application. Each notification can be scoped by application, service, stage or alert type and each alert is crafted by us to minimize noise. This way youll get alerted on only what is relevant to you. __Screen shot: notification settings__ !Screen shot: notification settings As an example here is what a Slack notification looks like for a new error type identified alert. It gives you just enough information to be dangerous, and you can follow the link to get more details in the dashboard. __Screen shot: Slack notification__ !Screen shot: Slack notification The Serverless Framework provides a powerful, unified experience to develop, deploy, test, secure, and monitor your Serverless applications. Learn more or just get started for free.",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fbc",
      "title": "EventBridge Use Cases and Examples",
      "content": "In serverless applications, events are the main communication mechanism. Sources such as API Gateways, databases or data streams emit events while other infrastructure components such as functions react to them in order to fulfill a given business need. Every major cloud provider has rolled out their own internal event system to reliably dispatch events from component to component within their ecosystem. Given that events are such an integral piece for serverless architectures, the question arises whether theres a way to generate and inject custom events into the system. To address this need one pattern which arose throughout the years was the usage of Webhooks to connect external event sources with the cloud. Webhooks are HTTP endpoints which consume HTTP requests, translating the incoming request data into an event which is then forwarded to the respective cloud service. While this pattern served us well there are a couple of downsides associated with it. Webhooks require additional infrastructure such as API Gateways to be created and maintained. The API endpoint should be scalable and always available to ensure successful event delivery. The act of marshalling and unmarshalling of requests and responses introduces overhead and can be tedious to handle. Scaling the event delivery gets tricky if theres a need to dispatch events to multiple recipients based on rules. This isnt the Serverless way. Theres a clear stance that events in serverless applications should be treated as first-class citizens. It should be easy to create, ingest and react to events. No matter the origin. Luckily AWS recently announced the EventBridge offering which helps developers build universal, reliable and fully event-driven applications. EventBridge is a serverless pub / sub service which makes it possible to seamlessly connect different event sources with AWS cloud services via event buses. Event publishers can be any (e.g. legacy) application, SaaS providers or internal AWS services. Event consumers can range from EC instances, to other event buses to Lambda functions. Given this simplicity, yet flexibility its easy to see why other developers call this announcement one of the most important services for serverless application development. Are you curious how the EventBridge can be used in your serverless applications? The Serverless Framework v.. has got you covered as it includes native support for the `eventBridge` event source! Lets take a look into EventBridge and the Serverless Framework `eventBridge` event implementation to see how easy it is to build truly event-driven applications without the need for other infrastructure components or Webhook workarounds. Use Case : AWS events The AWS EventBridge service is internally built on top of CloudWatch Events which makes it possible to easily react to events generated by other AWS services. Generally speaking there are different types an EventBridge event can be operated with. The \"Schedule\" type schedules event emission while the \"Pattern\" type matches incoming event data based on a configured schema to only invoke the Lambda function when the pattern matches. Lets start by looking into the \"Schedule\" type and define an `eventBridge` event configuration which invokes our Lambda function every minutes. In the following `serverless.yml` file we declare our `reminder` function which has an `eventBridge` event source configured to invoke the Lambda function every minutes:  Scheduling the event delivery in that way makes it possible to create % serverless cron services (One can even use cron expressions to define when a function should be invoked). The second, \"Pattern\" event type gives us great flexibility to react to different events emitted by AWS services such as EC, Batch, EBS and more. A pattern is basically a schema definition AWS uses to filter out relevant events which are then forwarded to your function. Lets imagine that we want to listen to state changes of AWS Key Management Service (KMS). To do that we need to tell AWS that were only interested in the KMS service and its event types. The following is a `serverless.yml` file set up with such configuration:  More CloudWatch event types and their pattern definitions can be found in the AWS documentation. If youre familiar with the Serverless Framework and its different event types you might think that this looks like another way to write `schedule` and `cloudwatchEvent` event definitions and youre absolutely right about that. You might remember that we just learned above that the EventBridge service is built on top of CloudWatch Events. Use Case : Custom events Using internal AWS event sources is nice since it gives us a great flexibility to react to different changes in our infrastructure. However weve learned that this capability was already introduced via CloudWatch Events a while back. What makes EventBridge even more useful is the support for custom events. But how would that work? Every AWS account is preconfigured with an \"AWS internal\" event bus called `default`. This is the bus weve just used above when working with internal AWS event sources. However with the introduction of EventBridge were now able to create and configure our own event buses. We can create many different buses for different use cases and / or applications. One could, for example, configure a \"marketing\" event bus which is solely used for marketing-related event sources and sinks. Lets use EventBridge to create our own `marketing` event bus. Furthermore lets see how an external Newsletter application can emit events to that bus to invoke a Lambda function responsible for team-wide notifications. The following `serverless.yml` definition describes a service which manages the `notify` function. This function uses the `eventBridge` event to create a `marketing` event bus and listens to event sources we define as `acme.newsletter.campaign` on that event bus:  Next up we need to add some logic to our Newsletter application so that events are emitted to the `marketing` event bus everytime something noteworthy happens: ```javascript // In our marketing application code const AWS = require('aws-sdk'); function notifyMarketingTeam(email) { const eventBridge = new AWS.EventBridge({ region: 'us-east-' }); return eventBridge.putEvents({ Entries: [ { EventBusName: 'marketing', Source: 'acme.newsletter.campaign', DetailType: 'UserSignUp', Detail: `{ \"E-Mail\": \"${email}\" }`, }, ] }).promise() } if (userHasSubscribed) { return notifyMarketingTeam(user.email); } ``` Once the code is deployed well reliably receive `acme.newsletter.campaign` events on our `marketing` event bus which will be forwarded to our `notify` Lambda function whenever a new user has subscribed to our newsletter. This is just one example where custom events and dedicated event buses can be useful. As you can see its pretty simple to configure EventBridge and hook it up with existing applications. Following this pattern we can replace pretty much every Webhook based implementation out there. Using EventBridge for such scenarios is more reliable, way easier to setup and scale and cheaper to operate since AWS manages everything for us behind the scenes. Conclusion The recent announcement for AWS EventBridge support is a true game changer for serverless application development. By nature, serverless is all about functions and events. EventBridge provides us with a managed, centralized event bus system we can use to react to any AWS infrastructure event, external SaaS provider events or even our own, custom defined events. Using the EventBridge as a central component to receive and forward events helps us build reliable, more robust and decoupled event-driven systems at scale. The Serverless Framework v.. release ships with first-class citizen support for the `evenBridge` event source, making it easy for us to benefit from a serverless event bus system. Feel free to check our EventBridge docs for more information. Are you excited about Serverless Framework v.. and its support for EventBridge? Let us know what youre planning to build with the Serverless Framework and the `eventBridge` event source in the comments below or via Twitter @goserverless.",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fbe",
      "title": "Serverless Components Beta",
      "content": " Forget infrastructure  Today, were giving you a new option to deploy serverless use-cases  without managing complex infrastructure configuration files. Its called Serverless Components and you can now use them with the Serverless Framework to deploy software on serverless cloud services, more easily than ever. Deploy Serverless Components right now, or keep reading to learn why we believe they are a game-changer for the serverless era. Your browser does not support HTML video. Deploy Serverless Use-Cases More Easily Serverless, as a paradigm, is about focusing less on technology, and more on outcomes. But, the software development tools we use for serverless applications are still very focused on the underlying infrastructure and suffer from the bloated configuration files and complexity that comes with that mind-set. The Serverless Framework helped by offering a single, simpler abstraction to deploy multiple use-cases. Serverless Components takes this a step further by offering infinite abstractions, for all use-cases. For example, here is a Serverless Component that deploys a React Application on AWS S, AWS Cloudfront with an AWS ACM Certificate and a custom domain.  The full codebase for this can be found here. Even better, here is a Fullstack Serverless Application using multiple Serverless Components, which you can deploy in seconds:  This fullstack app includes an AWS Lambda-based API, a react-based front-end, an on-demand DynamoDB database table, a custom domain, an SSL certificate and more, all on auto-scaling, pay-per-execution, serverless cloud infrastructure. This software stack has extremely low total overhead and cost. The full codebase for this can be found here. Serverless Components Are Reusable There hasnt been an easy way to create reusable outcomes built on serverless cloud infrastructure, like AWS Lambda, AWS S, Azure Functions, Twilio, Stripe, Cloudflare Workers, Google Big Query  especially across all cloud vendors. Serverless Components are designed for reuse and composition. They can be composed together in YAML (like the examples above) so you and your team can build software faster than ever. If you want to create a reusable Serverless Component, you can do it programmatically via javascript using `serverless.js`, like this:  The programmatic experience takes a lot of inspiration from Component-based front-end frameworks, like React. The APIs are simple and familiar. Lastly, Serverless Components are written in vanilla Javascript, making them as approachable as possible. Vendor Choice At Serverless Inc., we believe in order to deliver the best product, you must be free to use the best services. Serverless Components are being designed to be entirely vendor agnostic, enabling you to easily use services from different vendors, together. Whether it's big public cloud, like AWS, Azure, Google, Alibaba, Tencent, or services from smaller vendors like Stripe, Algolia, Twilio and others. Wrapping Up We have big plans for Serverless Components. Our belief is we will see an increasing number of serverless cloud infrastructure services in the future, and we aim for Serverless Components to help everyone create simpler abstractions on top of them, focused on outcomes. If you are a vendor of cloud infrastructure services lookng for a compelling developer experience and community, please reach out to us to learn more about our partner program with Serverless Components. Please Note While in Beta, Serverless Components does not yet work with existing `serverless.yml` files. You will have to create a new `serverless.yml` file. Additionally, Serverless Components are not yet integrated with the Serverless Framework Dashboard. We have some exciting announcements for this soon. Stay tuned. Go make amazing things with Serverless Components and let us know what you think . Cheers, Austen & the team @ Serverless Inc. P.S. Eslam is the lead architect of Serverless Components. He's stellar. Reach out to him to chat about anything related to Serverless Components!",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fc0",
      "title": "Secrets Management for AWS Powered Serverless Applications",
      "content": "Question: Whats the right way to manage secrets in serverless applications? The crowd answers: Secrets belong in environment variables! Secrets dont belong in environment variables! Secrets belong in parameter stores! Thats not what parameter stores are for! Vault! AWS KMS! SSM! Storing application secrets in serverless applications is a hot topic that provokes many (often contradictory) opinions on how to manage them right. By secrets management we mean the entire secrets lifecycle: from configuring, storing and accessing them to rotating them and enforcing secrets policies. Typical ways to configure secrets include hard-coding them in your application (not recommended!), using dedicated secrets files, storing them in environment variables, and using secrets stores like HashiCorps Vault. If youre running Serverless applications, most likely you are already using secrets to store data like database connection strings and API tokens for third party services, or you will start needing to use them soon. We want to help you make an informed choice about how to store and access your secrets with the Serverless Framework. In this article we explore three approaches to secrets management for Serverless applications: using environment variables, using the AWS SSM parameter store, and using the Serverless Frameworks secrets management features, and we discuss the benefits and drawbacks of each option. Using code, we show you in detail what each approach looks like, allowing you to choose your favourite way to manage Serverless secrets. Lets dive right in. Three ways to manage secrets for Serverless Framework applications To illustrate each approach to secrets management in Serverless applications, were using this sample weather forecast API on GitHub. Its a simple Serverless API that gets a weather forecast for a given location from three different weather service providers:  We used Dark [](https://darksky.net/dev)Sky, OpenWeatherMap, and the HERE Destination Weather API. For each provider weve chosen a different way to store API secrets. Of course, you would rarely need to do anything like this in a real-life project, but this is a convenient way to illustrate the differences between the secrets management approaches. To set the stage, lets take a look at the overall structure of the project, and then well dive into the implementation for each provider. Overall structure of the project We begin our weather API example with a service definition in the serverless.yml file. In the `provider` section, we specify that we want to use AWS in the `us-east-` region, that our environment is Node.js, and that we require the Serverless Framework version to be newer than `..` (we cover the version part later). We used the `serverless-offline` plugin for local testing, but this is optional.  The most interesting part of `serverless.yml` is the functions section where we define our API handlers. We define one handler per provider, define the HTTP route for each handler, and add any secrets needed to get that provider working.  We go into more detail on each specific provider later in this article. For more info on the `serverless.yml` format, please see the relevant Serverless documentation. Our `handler.js` file is quite simple, making reference to individual provider files:  The individual provider code is in the external-api subdirectory. Now that the structure is covered, lets take a look at how we can implement secrets access for each of the weather API providers. Approach : AWS SSM parameters AWS Systems Manager is a simple configuration management solution that integrates with many AWS services. Parameter Store is the part of this solution most relevant here. It allows us to store plain-text and encrypted string parameters that can be accessed easily during run time. Serverless Framework provides easy-to-use integration with AWS SSM Parameter Store. We used this approach with the Dark Sky weather API. To add a new secret in the AWS Systems Manager user interface, we specify the Secure String type and use the default KMS key to encrypt it.  if you have any suggestions on how to make the example better. You can find the docs for the solutions we used here: - AWS SSM Parameter Store - AWS docs, Serverless docs. - AWS Secrets Manager - AWS docs, Serverless docs. - Serverless Framework secrets - Serverless docs. If youd like to give Serverless Framework a try, have a look at the getting started guide.",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fc2",
      "title": "Leveraging existing event sources (S and CognitoUserPools)",
      "content": "Cloud computing introduced a drastic shift when it was first introduced in the early s. Providers such as Amazon Web Services (AWS) finally made it possible to rent compute, storage and other web related services on a monthly or yearly subscription basis. Getting projects into production was a matter of a credit card swipe and a deployment. Contrasting this with the usual, months long capacity planning and procurement processes shows why cloud computing was such a game changer in the tech industry. As a result more and more developers and companies adopted cloud computing as their go-to solution to host their applications in an easy, future proof and cost effective way. Given that AWS started in the early s its easy to imagine how applications grew within the AWS ecosystem and how much data such applications generated, processed and accumulated. The big data trend turned the attention to the value in the data. One of AWS reactions to this trend was the introduction of the Lambda compute service back in . The very first event sources AWS introduced for Lambda was support for events generated by S buckets. To this day the infamous Image resizer demo where an image is uploaded to an S bucket and automatically resized by a Lambda function is still the most prominent Hello World application to showcase the power of serverless architectures. Existing event sources Our dive into the AWS history and Serverless computing above showed us that AWS Lambda was initially built around the idea of working with data from existing infrastructure components such as storage buckets. Once API Gateway support as a Lambda event source was announced it was clear that the serverless paradigm will be huge. The JAWS CLI tool (The Serverless Framework precursor) was born in out of the necessity to provide an easy way to leverage the power serverless unleashes by making it as simple as possible to author serverless applications. In , the Serverless team sat down and worked together with the community to define the integral parts for our upcoming Serverless Framework v release. One main takeaway during this exercise was that we should support CloudFormation which is the de-facto standard to define and deploy infrastructure on AWS in a declarative way. While CloudFormation gives us a lot of upsides, mainly in the form of a battle-tested and well-known platform to perform resilient infrastructure changes, it not short of downsides as well. One such downside is the focus on infrastructure creation and teardown which means that CloudFormation was never designed to deal with resources which were not part of the initial creation phase. Its clear that this restriction prevents CloudFormation users from introducing existing infrastructure components such as S buckets or Cognito User Pools into their current setup. This problem trickles down to the Serverless Framework which utilizes CloudFormation under the hood. Introducing event support for existing S buckets or Cognito User Pools turned into a problem only Serverless Plugins were able to solve. Such plugins relied on raw AWS SDK calls, and their usage introduced a second method to allow the management of application infrastructure. Within recent versions of the Serverless Framework, we finally found a way to support existing resources via CloudFormation which means that we still get all the benefits CloudFormation has to offer while making it possible to introduce existing, potentially legacy infrastructure components, into the application stack. S We already read above that S is one of the first and most widely used AWS services. Given that S has been around for such a long time, its easy to imagine that a lot of data was accumulated in S buckets throughout the years. S buckets are often used as the central object store to save and retrieve files such as images, personal documents or log files. S buckets can emit a variety of different event notifications such as `s:ObjectCreated:` which can be used to get notifications when a new object is uploaded to the bucket or `s:ObjectRemoved:` which will send a notification when an object was deleted from the bucket. Different `prefix` and `suffix` configurations make it possible to further filter down when such events should be emitted. The Serverless Framework comes with native support for the `s` event source. Lets take a look at an example where we want our `notify` function to be called whenever a new `.pdf` file is created in the `documents` directory of our `acme-cloud-storage` bucket:  Deploying this service with the Serverless Framework will create the `acme-cloud-storage` bucket and sets up the bucket configuration to call the `notify` function accordingly. What if our `acme-cloud-storage` bucket is already in production and we want to setup our `notify` Lambda function after the fact? Thanks to our recent changes this can be achieved via the `existing: true` flag:  The Serverless Framework will automatically detect that the `acme-cloud-storage` bucket was already created previously and will just setup the configuration on the bucket. Once deployed our `notify` function will be called whenever files that match our rules are uploaded to the existing bucket. Cognito User Pools Sometimes its useful to react to specific user behavior within your application. A typical user-related use case is the sending of Welcome or Getting started emails whenever a user decides to sign up for your service. Cognito and especially Cognito User Pools are a widely used service to safely integrate and manage user accounts within web-facing applications. Cognito User Pools offer an easy way to create customized workflows via Triggers which can be setup on User Pools. Triggers include `PreSignUp`, `PostAuthentication`, `CustomMessage` and more. The Serverless Framework supports Cognito User Pools natively. Lets create a service in which a `greeter` Lambda function is invoked whenever a new user signup is registered in our `AcmeUsers` User Pool:  Deploying this service will create the `AcmeUsers` Cognito User Pool and sets up the configuration so that the `greeter` function is called whenever a new user signs up for our application. Given that the service definition above will always attempt to create a new Cognito User Pool it seems to be hard to just setup the trigger configuration without interfering the production system. Thanks to the recent changes we made its dead simple to setup trigger configurations on existing Cognito User Pools. The only configuration which needs to be added is the `existing: true` flag. This flag tells the Serverless Framework to skip the Cognito User Pool creation phase and instead configure the triggers on the existing one:  Using this service setup makes it possible to introduce a previously created Cognito User Pool (possibly running in production for a while) into our current serverless setup. Conclusion Using event-driven application patterns with existing infrastructure components such as S buckets or Cognito Users Pools is not a rare circumstance. Given that the Serverless Framework is built on top of CloudFormation to provide production-grade reliability and resilience it was only possible to introduce external event sources via Serverless Plugins. We thought hard about potential solutions to mitigate this shortcoming. While working on the solution one of our main goals was to not give up the benefits CloudFormation provides us. Recently we finally found a way to integrate support for existing infrastructure components into core. The first event sources for which we added this support include S buckets and Cognito User Pools. Setting them up requires just one line of code:  You can read more about the `existing: true` flag in the S and Cognito User Pools docs. How are you using this new feature with the Serverless Framework? Share your thoughts and ideas in the comments below or tweet us @goserverless.",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fc4",
      "title": "Serverless: The Ideal Choice For Startups? (CloudForecast Case Study)",
      "content": "When we decided to launch and bootstrap CloudForecast in , we faced questions that every startup face including \"Whats the right way to build this product with our resources without creating technical debts for the future?\" The \"$ Startup\" concept Around the time we started CloudForecast, I was reading \"$ Startup\" by Chris Guillebeau. In his book, Chris advises his readers to get their product out fast while being cost-effective to limit the damage if your idea fails (definitely possible, unfortunately!). We try to apply this advice on every side of our business including our technical decisions. Our goals/requirements With the \"$ Startup\" concept in mind, we started by listing the requirements that were important to us while building CloudForecast: Ship it out fast: We were excited to build CloudForecast to help companies save $$$ on AWS. We wanted to put it in front of our clients ASAP to keep our excitement going. Spending time debugging config, deploy process, etc. will not make the end product better. We were looking for an out of the box solution that we can grow with and allowed us to focus on our MVP the right away Cost-effective: As an early startup, we wanted to avoid fixed costs by building a system that will scale seamlessly based on our client base. AWS Lambda + Serverless = Easy + Focus + Cost-Effective We brainstormed on how we could achieve our goals and requirements. We discussed managing our own instances, using containers and half a dozen other ideas but we wanted something simple so we decided to use a Serverless solution (or FaaS). While there are a few downsides (e.g cold starts, ), we believed going serverless better suited our use case (nearly zero administration, pay-per-execution with no idle cost and auto-scaling). Cold starts is a known downside for Lambda but as we are mostly transforming and loading data from S in an offline fashion we decided that cold starts weren't a major concern for us. For the dev setup and deploy process, we decided to use Serverless Framework with AWS Lambdas for the following reasons: We can focus on writing our product and let Serverless manage the rest (permission, event management) Easy to configure and easy to deploy Serverless supports multiple platforms (GCP, AWS,  ) which could prevent headaches in the future. We picked AWS Lambdas to start with since our clients' data will be stored in S. It was a logical decision in order to reduce the network cost The Serverless framework offers a long list of plugins (see https://serverless.com/plugins/) Last but not least: Great documentation and community (Github, Gitter, Slack and Forums) Our original requirements were fairly simple: We needed AWS lambda functions each on their own cron job, and each lambda would need to talk to various AWS products (RDS, DynamoDb, SQS et al.). All that with an easy way to manage multiple environments (dev vs prod) and an easy/effective way to manage resource permissions. Here is how we do it: We used functions: !Functions Used the iamRoleStatements to configure all the permissions: !IAM With these two code snippets, we were able to set up most of our architecture. functions that will interact with each other through SNS. Two functions will run check on a cron schedule (via the schedule.rate params) to check a file need to be reprocessed and trigger another function via an SNS. This configuration will be able to scale effortlessly while keeping our costs under control. We are able to fully silo our environments using iamRoleStatements to configure the permissions. We originally used a simple YAML file to control our environment variables but we quickly switched to a `DotEnv` file using the DotEnv plugin. We considered running a couple of small instances to do the job which would cost us at least $k for the year. However, the AWS Lambda cost is effectively $ since we are only running a couple of functions a day which could easily be covered by the AWS free tiers. Looking back! Like every new startup, we made (and are probably still making) some mistakes along the way but picking AWS Lambda and the serverless framework wasnt one of them. Here are a few reasons why it was the right choice for us: CloudForecast was able to grow effortlessly with AWS Lambda: As we onboard clients, we will automatically run more functions and the cost will grow linearly with revenue. The Serverless framework is always improving and its keeping up with AWS Lambda. The recent Full Lifecycle feature announcement is a perfect example of how the Serverless framework is always evolving. Over time, our product evolved and so did our functions but Serverless and Lambda were always able to deliver. If you have any questions related to this post or what we do at CloudForecast.io, feel free to reach out to me at francois@cloudforecast.io. We would love to hear from you!",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fc6",
      "title": "Tips for Creating Robust Serverless Components",
      "content": " Serverless Components allow anyone to design and build specific use cases and share them with the world, without having to be tied to a single pattern or a single opinion. We then released + components to bootstrap these uses cases, and were constantly building more. One of our core design goals is simplicity, so we produced an incredibly simple core and components API that makes creating components a breeze, and weve covered them all in a simple one-page documentation. In this article, we will give you practical advice (not docs) on creating robust Serverless Components based on our experience building all these components over the past year, to help as you start creating components for your own unique use cases. So lets dig in.. Start with the Outcome Serverless Components is an outcomes framework. Every single component produces a specific outcome. Those outcomes could be low-level (like an S bucket), or high level (like an image processing feature). Before you even start building your component, you need to decide on what that outcome is and how much abstraction you want to provide for the best UX. Once you know what outcome youre after, youll need to think about your components inputs and outputs, as well as what the defaults are. We recommend that you minimize the inputs at first, then you can iteratively add more config if at all needed, simply because removing inputs later is a breaking change, but adding inputs is not. If at all possible, aim for a zero config component that would use sane defaults to deploy even a demo of that outcome. One example for a zero-config component is the aws-s component. If you dont specify any inputs to that component, itll just create an accelerated bucket for you with a randomly generated name. See Real-World Examples While the components core API is simple, the cloud provider youre working with (ie. AWS) might not be super straightforward. Fortunately there are some repeated patterns that we have seen after creating + serverless components. We recommend that you browse through the available component repos on Github and check which one is close to what youre building. Just open up the repo and check the code and see how everything fits together. All component code is open source, and we are striving to keep it clean, simple and easy to follow. After you look around youll be able to understand how our core API works, how we interact with external APIs, and how we are reusing other components. Which brings us to the next point Reuse Existing Components Serverless Components were designed from the ground up to be sharable and reusable. You can compose components together to deliver higher level component with minimum code. For example, the website component uses the aws-s component to deploy and upload to the S bucket, and it also uses the domains component to setup the custom domain. Likewise, you can reuse the website component, add to it your own frontend code (ie. an entire React app) to create a configurable/reusable chat app for example, and thats exactly what the chat-app component does! Here's a real example that illustrates how the backend component is reusing different components in a couple of lines of code:  We believe that the real value of Serverless Components is in the components themselves, not the core. As a matter of fact, the core is extremely thin and lightweight. Serverless Components is an ecosystem, and the more components there are, the easier it is to write more components. Keep Most of the State in the Provider The Serverless Components core has a simple built-in state storage system. However its very easy for your component state to diverge from the actual state in the cloud provider. For example, lets say you used the aws-lambda component to deploy a lambda function. When you do this, we save the lambda name locally in the .serverless directory. This is where the local state is saved. However, if you delete the lambda function from the AWS console, the aws-lambda component is smart enough to check with AWS what the state of your lambda function is before deploying. Itll detect that it no longer exists and will initiate a create operation again instead of the expected update, even though the local state indicate that it already exists. This also applies to all component inputs (in the case of aws-lambda, thats memory, timeout...etc). Thats why we recommend that you make a get request to the resource youre provisioning before deploying just to make sure it exists, and check whether any of its inputs has changed. This also helps the component pick up where it left from later on in case of any errors. In many cases, you may not need to store state locally, but if you do, only store what you actually need and try to verify with the provider on each deployment. Here's a simplified example on how we're doing it with the aws-lambda component:  Handle Name Collisions & Changes In many cases, especially if youre creating a low level component, youll have to provision resources in some cloud provider (ie. AWS). In that case, you will likely need to choose a name or some sort of an identifier to your resource (ie. bucket name). If the resource name is an input, you will need to be aware that a resource of that name might already exist and handle that edge case. You will also need to handle the case of the user changing the name input, in which case you should delete the old resource with the old name (that was saved in state in a previous deployment), and create a new one with the new name. We recommend that you create the new resource before deleting the old one, in case something went wrong during creation. Here's how this might look like: ```js // we already created a new lambda with the new name // now let's make sure we delete the old one... if (this.state.name && this.state.name !== inputs.name) { this.context.status(`Replacing Lambda`); await deleteLambda({ lambda, name: this.state.name }); } ``` However if your use cases allows it, we recommend that you use random names for better UX and to avoid collisions completely, and then save that in the local state. We have a helper function that makes it easy to create random resource names that share a global `contextId` as a form of tagging. Using this function could look something like this:  Detect Changes in Inputs On every deployment, you may be able to just update your resources with the inputs that the user provided. However, some resources could take a long time to update (ie. CloudFront takes around minutes). So we recommend that instead, you could just fetch the resource properties from the provider before deployment, and compare that to the inputs the user provided. If no changes were detected, the you can skip deployments completely, which could dramatically increase the deployment speed of your component, and any other component that depend on it. This also applies to code input, and a very good example of this is how we are deploying lambda functions. Before each deployment, we package your dependencies as a lambda layer and save its SHA string to the local state. Then on every deployment, we compare that SHA string to the one that AWS provides. If they match, then we skip the dependency upload step completely, which saves a couple of minutes of deploy time in practical scenarios. Here's how the `configChanged()` function looks like for the aws-lambda component  So we only deploy when we have to, and its the main reason why we can provide super fast deployment speeds for all our components. Use Provider Errors to Your Favor Handling provider errors can be a drag. But we recommend that you experiment with the types of errors your API calls might throw, and take advantage of that to detect the best course of action. A very common pattern that weve been using is to try to update a resource with a specific name, and if it throws a `NotFound` error, we can catch that and run a create operation instead of an update. This is another example of keeping state in the provider as we discussed above. Here's how the aws-lambda component is taking advantage of that:  Removal Should Use State, Not Inputs In case of removal, we recommend that you use state data that was previously saved by the previous deployment instead of using the inputs object. The reasoning behind that is that inputs do not represent the actual state on the provider, but rather the state that you want it to be. They are also not guaranteed to be always available (for example, if inputs are passed from other component's outputs, then running `sls remove`), but the state is accessed programmatically and will always be there. The state is always kept up to date with every deployment and it represents the last known state of the component and its resources. here's how the aws-lambda component removes the lambda using the name stored in state, instead of the name passed as an input (which the user might change):  Keep in mind however what we mentioned earlier about keeping most of the state in the provider. You cant completely trust the local state, so you better verify the state of the resources on the provider, either by making a get request, or as mentioned in the previous section, by using a try/catch strategy of the removal (if it doesnt exist, its already removed) Save Time by Using Core Utils When you first create your component, youll need to extend the Component class from the `@serverless/core` npm package. In that package, there are plenty of useful utilities that you can use to make your life easier. For example, we have a `Utils.sleep()` function that pauses runtime for a few seconds before moving on. This is extremely helpful when dealing with cloud infrastructure as it might take a while to create some resources before we make further operations on it. There are also file system, parsing and packaging utilities that you can use when dealing with files that you might need to upload to the cloud. Here's a common use-case:  We recommend that you check out the existing components and see how they use the core utilities to minimize technical debt. We also recommend that you use the `this.context.debug()` function to let users who enable debug mode know whats going on during deployment. Its very helpful to you as well during development, and youll find it all over our components code base. Publish Your Component Once your component is ready for prime time (and after a lot of testing ;), don't forget to publish your component! Components are published to npm just like any other npm package. Just make sure you point the `main` property of `package.json` to the `serverless.js` file and use semantic versioning just like any other npm package. Once its published, anyone could just use your new component in YAML or JS files. For extra exposure, we would be happy to host your component in our Serverless Components Github Organization, which acts as our official registry for the time being. Just ping me on twitter and Ill add you to the org and create a repo for you. Creating Serverless Components is one of the biggest contributions to the Serverless Framework and we highly appreciate it. We will provide all the help you need to get you up and running and make your component more discoverable. We are also building a Serverless Components registry that would make publishing a lot easier. Wrapping up We hope this article made you feel more confident and comfortable creating Serverless Components. We cant wait to see what you come up with, and remember, if you have any questions, feel free to reach out to me on twitter, or anywhere you could find me. Now go build something great!",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fc8",
      "title": "Tracking API Gateway metrics in Serverless applications",
      "content": "Thousands of developers are using serverless technologies to build web APIs. Tools like the Serverless Framework make it easy to build with foundational infrastructure like AWS Lambda, API Gateway and DynamoDB to build REST APIs and GraphQL APIs. But monitoring these APIs is still a bit of a black box. The out-of-the-box monitoring systems provided by AWS don't provide the granularity you need for inspecting your APIs, particularly when the failure could span multiple systems. In this post, I'll show you an easier way to monitor your Serverless web APIs using the new full lifecycle capabilities of the Serverless Framework. First, we'll cover three reasons why understanding your API performance is so hard using the native AWS tooling: - Lambda errors don't map to HTTP errors - There are many places your request can fail outside of Lambda - API Gateway metrics don't let you drill to the root issue After we review the problems, I'll show you how we're solving this problem with the Serverless Framework and some additional features we have planned going forward. Let's get started! Problem : Lambda Errors don't map to HTTP Errors If you have experience building services with AWS Lambda, you're probably familiar with CloudWatch Logs and CloudWatch Metrics. The great thing about CloudWatch is that it's integrated automatically with your Lambda functions. You don't have to do any instrumentation to get the logs flowing. That said, CloudWatch is a bit behind other monitoring tools in the space. And even if you are a CloudWatch expert, there are some gaps between Lambda and API Gateway that make it difficult to understand your application behavior. The first problem with traditional Lambda monitoring is that Lambda errors don't always map to HTTP errors. To see what I mean, look at the example code below. !Lambda handler This is a `getUser` endpoint in my application, where I use the `id` parameter in the HTTP path to fetch a User object and return it to the client. Note that on lines - I have an `catch` block that captures any errors and returns an error with a `` status code. By catching this error, I'm making it easier for the client of my API as I return a status code and a potentially meaningful error message. However, I've also lost some visibility into my application health. If my DynamoDB table went down and I couldn't read any User records, every single user would be receiving an error and seeing a `` status code. But from the perspective of my Lambda application, it would appear that everything is fine -- no errors! As a developer, I'm interested in more than just whether my Lambda function successfully handled all errors before returning to the client. I also care about the user-facing result of the invocation -- was the client able to perform the action it wanted? Problem : There are many places your request can fail outside of Lambda The second area where traditional Lambda monitoring falls down is that there are many areas where API Gateway can fail aside from your Lambda function. Your users can be experiencing errors before a request makes it to your Lambda function or even after your function completes successfully. API Gateway has a ton of features for super-powering your API processing. If you're taking advantage of these features, you may not see the errors your users are seeing. For example, you can add request schema validation on your API endpoints to reject any requests that don't match the required schema. Similarly, you can add custom authorizers to your endpoints to require authorization before hitting your Lambda function. In each of these cases, users may be seeing errors without you even seeing a Lambda invocation in your dashboard. In the former, they'll receive a ` Bad Request` error while in the latter they'll receive a ` Unauthorized` error. Further, even if your request makes it to your Lambda function and completes successfully, you may mess up the shape of the response format. As a result, your users will see a broken app due to ` Internal Server Error` responses. In each of these situations, the basic Lambda metrics and logs won't help you realize that your users are facing errors. Problem : API Gateway metrics don't let you drill to the root issue Let's switch over to the API Gateway side of the house. You can look at API Gateway-level metrics using CloudWatch Metrics. These will show you the number of XX, XX, XX, and XX status codes by resource and method in your application. The difficulty lies in debugging your requests to find a root cause. When you deploy some bad configuration which results in a spike in errors for an overly-restrictive request schema, how do you drill into the applicable requests to find the problem? When your database goes down and results in a number of errors, how do you find the stacktrace that indicates the issue? In the first case, you'll likely need API Gateway access logs enabled and some sort of system to process these logs as they come in. In the second case, you'll either need to instrument your code with some kind of application error platform, or you'll need to process your Lambda logs into an external system. How we're solving this problem In the sections above, we noted a few problems with the current, separated approach of CloudWatch Metrics. To solve this, you need a more holistic view of your application. While you may think of your Serverless application on a function-by-function basis, there are more pieces involved than just the function. You need a way to look at your endpoints on a results-basis -- what is the end result to my users? -- while still being able to drill in to specific invocations to see the error. If my `createUser` endpoint has a rash of errors, what's the reason? Is it because I couldn't access a critical service inside my Lambda function, or is it because I formatted my response incorrectly? If my `getOrder` endpoint has a ` Unauthorized` error, is it because the request has no authorization header and thus fails the custom authorizer, or is it because the authenticated user does not have access to the specific Order being requested? The Serverless Framework provides the solution to this, letting you see your endpoint at a , foot view and drill into the details. First, you can see endpoint errors right next to your function invocation errors. Remember what we discussed above -- it's possible for your Lambda function to return successfully but your endpoint to show an error to the user. This view allows you to see both sources of errors. !API Gateway and function errors If you want to dig deeper on a particular error, click in to see the full story. You get API Gateway metrics, function metrics, logs, and even a stacktrace. In one place, you can see everything that happened with a single request, making it much easier to debug. We've got a lot planned for this functionality moving forward. We'll be adding a chart explorer where you can build graphs for all requests that meet certain parameters. Want to see the status code distribution of all requests to your `getUser` endpoint between noon and PM today? No problem! Once you've used the filters to discover the problem with graphs, use those same filters in our invocation explorer to find problematic invocations. Find the exact invocations that are causing you problems and look to the logs and API Gateway metrics to debug the root cause. This is what's needed -- a unified way to identify and diagnose issues in your application, rather than cobbling together a bunch of disparate resources and copy-pasting IDs through sub-par search interfaces.",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fca",
      "title": "Zero instrumentation observability for AWS Lambda",
      "content": "We recently launched a few features which make troubleshooting Serverless Framework services much easier. Before diving into the details, Id like to share a personal story. About three years ago I was vacationing with my family in Michigan. At the time I was acting as interim backend engineer for a startup. We were out on the lake with friends enjoying the afternoon sun when PagerDuty started going off. Each component started falling over one by one and the angry customer emails about the thousands of dollars they were losing each minute started piling up. After getting off the boat, getting online and pulling my hair out for three hours, we finally identified the root cause - Facebooks APIs were responding about % slower than usual causing cascading slower response times in upstream dependencies and ultimately timeouts. This was an experience I would never wish upon anyone and I deeply sympathize with anyone who experiences such outages as a developer. Such issues likely would have been avoided with a serverless architecture, but a few lessons are still applicable. Amongst them is the importance of monitoring the performance of each dependent service. And secondly, that its hard to do. Given this experience, I am especially proud to show you how you can use the new invocation explorer and functions spans feature to troubleshoot issues similar to the one I experienced a few years ago. To begin our troubleshooting journey, lets assume we have a service with a few functions handling tens of thousands of invocations each hour. We are informed of degraded performance, but we have little to go on, other than knowing that there is a slow response time on a particular endpoint. Invocation Explorer First we need to identify the needle in the haystack. Luckily, we at least know what to look for. We know that the durations are probably taking longer than expected, we know the most likely functions causing the issue, and a time period during which the issue was reported. Using the invocation explorer we can quickly and easily identify the individual function invocations that match these characteristics by filtering on the function, time range, error state, cold start state, memory usage, and durations. Once you apply the filter youll be able to see the timestamp, duration, memory utilization, cold start state, and the error if one occurred for every individual invocation which meets the filter criteria. !Invocation Explorer Invocation Details While it is an improvement to be able to identify the invocations by a variety of filters and some basic stats about an invocation, we really need to dive into the details of each invocation to troubleshoot a problem. When you click into an invocation using the invocation explorer youll see all of those details. The first section includes basic stats, including duration, memory usage, whether it had an error, or if it was a cold start. !Invocation Details The invocation details also include CloudWatch Logs, stack traces and functions spans, so lets look at those in detail. Logs Long gone are the days of having to login to the AWS Console and sift through countless CloudWatch log streams to find the invocation logs you need. The logs section of the invocation details view loads the Cloud Watch logs for that particular invocation so any debugging logs your function generated are now available at your fingertips. !Logs Stack trace The invocation details view also includes the stack trace for your selected invocation. Perhaps the most popular and valuable tool for debugging your code, the stack trace will show you the exact line of code which caused the error and youll be able to navigate through the stack as well. Even if your code is obfuscated by typescript or minification youll be able to get this nice view with support for Source Maps. !Stack trace Function spans With function spans you can quickly pinpoint the cause of a slow response time. Every call to AWS services like S, DynamoDB, SES, and more is automatically instrumented and visualized so you can see when the call was made and how much time your function spent waiting for a response. Soon well also be adding support for HTTP(S) calls too. This would have been the killer feature for debugging the issue I described earlier. Without adding any manual instrumentation, I would have been able to see exactly how much time my function spent on dependent service calls and notice the performance degradation. !Function Spans Alerts & Charts While the troubleshooting journey may begin with a user reported issue; it may also begin with a notification from Serverless Framework. Serverless Framework provides notifications for a variety of alerts like approaching out of memory, new error type identified, or numerous others. Each of these alerts is now instrumented with a link to View Invocations. !Alerts and Charts When you follow this link, you will jump right into the invocation explorer with the filter pre-set to view the invocations which contributed to the particular alert. For example, if you get a New Error Type identified alert, youll be able to navigate to the invocation explorer to see the exact invocations which had that error and see the stack trace. Similarly, you can view the function invocations & errors for a service instance, or the invocation, errors, cold starts or timeouts per function. When you click on any one of the data points in these charts you will land on the invocation explorer with the list filtered for those particular invocations. !Graphs Automatic instrumentation As I mentioned in my own troubleshooting journey a few years ago, we hadnt instrumented all of our code to track the performance of all the countless dependent services. With Serverless Framework you DO NOT need to instrument your code to get the invocation explorer or function spans. Upon deployment, the Serverless Framework will automatically instrument all calls to AWS services and (coming soon) HTTP services. Try it out for yourself If you have an existing service, first make sure you are using the latest release of the Serverless Frameworking by running `npm i serverless -g`. Then run `serverless` in your working directory. If you have an existing Serverless account, itll walk you through updating your `serverless.yml` to work with the dashboard features. If you dont have an existing account youll be prompted to create one. Its that easy.",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fcc",
      "title": "Easily Deploy A Serverless CDN With Serverless Components",
      "content": " Two weeks ago we released a complete solution for deploying serverless websites that supports custom domain, SSL & CDN in a single website component. Since then we've received great feedback about related use cases of the underlying infrastructure and we realized there is a need for a standalone content delivery network solution to serve your static assets, even if they're not directly related to your website. Today we're releasing the zero-config Serverless CDN component to serve that exact use case. Just like the website component, it supports secure SSL-enabled custom domains and is powered by AWS S, AWS CloudFront, AWS Route & AWS Certificate Manager. All can be deployed with only lines of YAML. Deploying the Serverless CDN Component To deploy the Serverless CDN component, you'll need to first install the latest version of the Serverless Framework if you haven't done that already.  Once installed, make sure you have set your AWS keys in your machine. For more info regarding setting AWS keys, checkout this guide. Once you're done with that, you'll have over serverless components that you can instantly deploy with a single YAML file. In this article, we will focus on the Serverless CDN component. To deploy the Serverless CDN component, just create a `serverless.yml` template file in the current working directory. This YAML template file should have the following content:  That's literally all the YAML you need to deploy a complete serverless content delivery network. All you need to do now is just run the `serverless` command in the current working directory:  There you have it! Your Serverless CDN has been deployed. It may take a few minutes for AWS CloudFront to propagate across edge locations and be completely ready. While this is happening, you can upload your first file to your CDN. Just visit the AWS S console, find the bucket that is shown in the CLI outputs (in this case it's `chh-fswqc`) and upload your first file to it. You may of course do that programatically from your application. Also keep in mind that visiting the root URL shown in the CLI will likely show an error because you don't have any content in your CDN. After you upload your first file, you can request & view this file via your CDN by prefixing the root URL you see in the CLI. In this case that would be:  Adding a Custom Domain to Your Serverless CDN Just like the website component, you can add your own custom domain to your content delivery network with a single input.  Please note that your domain (`example.com` in this example) must have been purchased via AWS Route and available in your AWS account. For advanced users, you may also purchase it elsewhere, then configure the name servers to point to an AWS Route hosted zone. How you do that depends on your registrar. To deploy your custom domain, just run `serverless` again:  You'll now notice there's a new `domain` output in the CLI and that it's already SSL-enabled and completely secure by default, as the Serverless CDN component creates a free certificate for you automatically via AWS Certificate Manager. If this is the first time you use your domain with AWS, deployment may take a while during certificate creation and validation. You may now use this domain instead of the root CloudFront URL we used earlier to access our file. Again, keep in mind that domain propagation may take a few minutes:  Composing Your Serverless CDN with Other Components As mentioned before, the Serverless CDN is just one of + components we already have available for you. You will likely need to use this serverless content delivery network with other components of your application (for example the backend component) to be able to dynamically upload files from your backend to the bucket that was created for you. For a full stack application, your YAML file may look something like this:  Checkout the docs for the backend component and the website component for more information. Using the Serverless CDN Component in Your Own Custom Component If you're building your own custom component, we've also created a custom `upload` function that you can use to directly upload files to your bucket, without even knowing the bucket name. Here's an example on how this might look like:  Needless to say, you must deploy your Serverless CDN component before you're able to use the `upload` function as it stores the required CDN data in the state. Wrapping up In this article, we've seen how you can deploy a zero-config Serverless CDN, and adding your own custom domain to it using the Serverless CDN component. We've also seen how you could use this CDN component in your existing application by composing it with other serverless components, and finally, we demonstrated how you could use this Serverless CDN component as a child component in your own custom component and how you could utilize the custom `upload` function we're exposing to your custom component to easily upload files and directories to your new serverless content delivery network. We hope you will find this Serverless CDN component useful for your application, and we can't wait to see what you'll do with it. If you have any questions, feedback or showoffs (we love those!), feel free to contact me directly on Twitter. Go Serverless!",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fce",
      "title": "Setup monitoring for existing Serverless projects in  minutes",
      "content": "One of the challenges when working with the Serverless Framework and lambda is how to monitor your software. There exist various solutions for this, but nearly all of them require time and work to instrument your codebase. Today I will show you how in minutes, with zero instrumentation, you can add monitoring to your existing Serverless project. First, make sure you have your terminal open in the directory containing your Serverless project. Then run the `serverless` command without any arguments. You will then be presented with an interactive set of prompts to configure your service for monitoring! Create an account and it will handle the rest for you. Then run `serverless deploy` to redeploy your project with monitoring enabled. After that, you can open the monitoring dashboard for your service by running `serverless dashboard`.",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fd0",
      "title": "How to use Serverless and Twilio to automate your communication channels",
      "content": "The way people communicate has changed over the last few years. When was the last time you called a service-number because you had a problem? For me, this is already years ago, and I most often use chat interfaces when theyre available. These new interfaces are usually automated to a certain extent  its hard to tell if a human or a machine is replying to your question. Developers have the power to build interfaces that go beyond what were used to. Alexa, Whatsapp, emails, SMS  you can automate all these channels. Twilio is a communications API that enables you to tailor the experience to your needs. Want to do an SMS poll? No problem! Need a custom chatbot on your landing pages? Sure thing! Want to bring all your friends into a group phone call? Easy peasy! Webhooks  the foundation of future interfaces As a developer, you probably wont build the infrastructure to send SMS or make phone calls. Youll use SDKs and APIs for that. The way it works is that you can use Twilios RESTful API to initiate outbound communications. Phone calls, SMS, WhatsApp messages are only one HTTP call away. The other way around is a little bit trickier. How do you react to incoming messages or phone calls when youre not controlling the infrastructure receiving these events? Webhooks build the foundation for that. If someone sends a message, makes a phone call, or uses any other channel a webhook is sent to a URL you define. The response of the webhook controls what happens next. !SMS, Twilio, App In the above example, you see the flow for an incoming SMS. A user sends an SMS, Twilio handles this event and makes an HTTP request to your app to find out what to do next. But do you want to build a whole app to respond to an HTTP request? Or could a serverless function do the job? The Serverless Framework now supports deploying Twilio Functions Serverless functions are a perfect fit for responding to HTTP calls. The Twilio Runtime gives you a way to write serverless functions today. Were happy to announce that you can now deploy Twilio Functions using the Serverless Framework. !SMS, Twilio, Serverless If youre used to working with the Serverless Framework, there is no need to learn a new API. You can continue using the Serverless Framework to control your Twilio communications! Deploy Twilio serverless functions in two minutes Starting with the Serverless CLI `v..`, you can bootstrap a Twilio Runtime project with a single command. In this post youll learn how to do that. Make sure you have the Serverless CLI installed globally and run `serverless create` with the Twilio Node.js template.  Navigate into the new directory `my-twilio-project` and run `npm install`. Before you deploy a new serverless service, you need to authenticate. Head over to your Twilio Console and copy your account credentials (the Account SID and Auth token). Define the two authentication values as environment variables and deploy the new project using `serverless deploy`.  The command output should look as follows.  Congratulations! You just deployed your first Twilio Runtime service that is able to serve functions and assets using the Serverless CLI. The URLs displayed in the deploy log are now publicly available. !Twilio runtime page How does that work? And how can you tweak Twilio functions to your needs? Lets find out! The file structure of a Serverless Twilio project The `create` command created all the files you need  ready-to-deploy. It includes the following:  You see the familiar `serverless.yml` configuration file, a `package.json` and `package-lock.json`, and a function (`handler.js`) and asset file (`example.jpg`). The project has only one npm dependency defined inside of the `package.json`: @twilio-labs/serverless-twilio-runtime. You installed this dependency already, and if you now have a look at the `serverless.yml`, youll see that it defines `@twilio-labs/serverless-twilio-runtime` as a plugin.  This plugin definition makes it possible to deploy to the Twilio Runtime. The `serverless.yml` holds more configuration than the plugin definition, though. It also configures the runtime and provides a quick way to define and deploy serverless functions and assets. General configuration The main configuration for your serverless service happens inside of the `provider` property. Twilio is the provider in this case. You have the following configuration options: Authentication (required) Dependencies Name of the deployed environment Environment variables accessible inside of your functions Lets go over these options one-by-one. Authenticate your serverless services with Twilio To deploy functions to the Twilio Runtime, you have to define your `accountSid` and an `authToken`. You could potentially hardcode these values in your `serverless.yml` but it is recommended to pass these values via environment variables (`${env:TWILIO_ACCOUNT_SID}` and `${env:TWILIO_AUTH_TOKEN}`). This way, you dont risk to push your sensible credentials to GitHub.  The above configuration is the reason why the command `TWILIO_ACCOUNT_SID=AC TWILIO_AUTH_TOKEN=a serverless deploy` works. The `serverless` command picks up the environment variables and passes them to the Twilio Runtime plugin. Define needed npm dependencies If you have experience using the Serverless Framework, you may be used to the process of packaging dependencies into a single bundle. The Twilio Runtime handles dependencies differently. The `deploy` command does not need to pack all your local dependencies into a single package to upload. The Twilio Runtime allows you to define your dependencies, and you only need to upload your function and asset files. The rest just works! Having a look at the `serverless.yml`, youll see that the bootstrap project has one dependency  `asciiart-logo`. The definition of dependencies is similar to `package.json` dependency definitions.  You can now include `const logo = require('asciiart-logo');` in your function files and the dependency will be available. Define your environment If youre heavily relying on serverless functions, youll find yourself facing increased complexity quickly. To tackle this complexity, you can safely deploy QA and staging environments to test your functions before they go into production. The `environment` property lets you deploy different environments to the Twilio Runtime.  The `environment` property is also defaulting to `dev` and you can change it via the environment variable `TWILIO_RUNTIME_ENV`. A deployed Twilio function URL consists of the service name, a random hash, and the defined environment. The endpoint URL `my-twilio-project--dev.twil.io/hello/world` tells you that youre looking at a function in the `dev` environment included in the `my-twilio-project` service. Define accessible environment variables When deploying functions to the Twilio Runtime, you may also need a way to define variables that will be available in the function context. These environment variables can be handy to store authentication tokens for other services, to define used endpoints or any dynamic values. The `environmentVars` property lets you define these values. They will be available in the `context` property when your functions are executed.  The above `provider` properties are all the configurations you need to tailor your functions deployment to your use case! But where are the function definitions and how come there was an asset deployed? Define your functions You can define and configure functions by editing the `functions` property in your `serverless.yml`. When you look at it, there is already one function defined. Every function definition has to export a `handler` as follows:  The `function` property tells Serverless to use `handlers.js`, make it available at `/hello/world` and make it publicly accessible.  You can read more about Twilio functions in the docs. Define your assets The Twilio Runtime allows you to upload assets via the `assets` property. You can access these assets then inside of your deployed functions. The combination of assets and functions can become handy if you want to play an audio file or want to respond to a message with a specific image.  To learn more about how to use assets in your functions have a look at the docs. With these function and asset definitions, you are able to deploy a new service in just a few minutes. Other included Serverless commands The Serverless Twilio integration supports two other commands  `invoke` and `info`. Invoke `invoke` is a command that you can use to call a deployed function to see if the response is what you expect. ``` TWILIO_ACCOUNT_SID=AC... TWILIO_AUTH_TOKEN=a... serverless invoke -f hello-world ,--. | | | _____ _ _ _ ____ _ _ | | |_ _|_ _(_) (_) ___ | _ \\ _ _ _ __ | |_(_)_ __ ___ ___ | | | | \\ \\ /\\ / / | | |/ _ \\ | |_) | | | | '_ \\| __| | '_ ` _ \\ / _ \\ | | | | \\ V V /| | | | (_) | | _ ",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fd2",
      "title": "Monitor and debug all serverless errors",
      "content": "One of the most popular features of the Serverless Framework is its ability to provide monitoring with automatic instrumentation. By signing up for a free Serverless Framework account and deploying your service, it is automatically instrumented to capture all of the data needed to provide metrics, alerts, notifications, stacktraces and more. This is especially powerful when it comes to monitoring and debugging errors. When your code throws an error, then the Serverless Framework provides a few ways to monitor and debug those errors: You will get a New Error Type alert for your service instance, notifying you in Slack or Email that a new error was identified. The stack trace is captured and in the Serverless Dashboard you can see the stack trace highlighting the exact line which threw the error. The invocations & errors chart will show you the number of times errors have occurred over a span of time. Using the invocation explorer you can search and identify the individual invocations which got the error and dig into all the details. No wonder this is such a popular feature. However, until today, capturing the errors only worked for cases where the error was not caught by the code and resulted in a fatal crash of the Lambda invocation. But of course we do not want our services to return in such cases, so more often than not, the errors are caught and the Lambda function returns a nicer XX error. Today we are launching a new addition to the Serverless Framework to help capture errors even when they are caught by your code. So lets look at this code first.  In the example above, our lambda function handler throws an error; however, it is also caught. It calls the `captureError` function provided by the Serverless Framework SDK in the `context` object. The function was able to proceed and return a nice friendly error to the API while still capturing the error. The documentation provides more details on using the `captureError` method. Now that the error is captured by the Serverless Framework, you can use the powerful dashboard features to help monitor and debug these errors. Here are a few ways you can interact with these newly captured errors in the dashboard. When a new error is captured which hasnt been captured before, you will get a New Error Type alert. You can also setup notifications to get notified in Slack or email, or custom SNS Topics or API endpoints. !New error type alert All errors, including fatal errors and captured errors, are available in the Invocation Explorer so you can filter for invocations containing errors. !Invocation explorer with filter Also in the invocation explorer you can dive into the details of the individual invocation to get the details about the error, including the stack trace which was captured by the `captureError` method in the code. !Invocation explorer details with stack trace Lastly, in the service instance overview page you can view invocation metrics and filter the results to identify the captured errors. !Invocations and errors chart If you want to improve monitoring and debugging for your Serverless Framework application, getting started with the automatic instrumentation is incredibly easy. Sign up in the dashboard and follow the instructions to start a new Serverless Framework project or incorporate the dashboard features into existing services.",
      "__v": 0
    },
    {
      "_id": "64e08929b72e199dda603fd4",
      "title": "Lambda@Edge support now available",
      "content": "Functions-as-a-Service (FaaS) offerings such as AWS Lambda are key components in modern Serverless application stacks as they receive and process the event data generated by other cloud services such as storage buckets or databases. AWS announced AWS Lambda, their FaaS service, back in late alongside an S integration which made it possible to react on changes happening in S buckets. The primary use cases for AWS Lambda back then was to streamline the data processing pipelines many companies operate to crunch through the data stored in such buckets. The announcement of AWS API Gateway support for AWS Lambda finally made it possible to build % serverless web applications, removing the need to run and maintain a dedicated web server which processes incoming requests and serves responses. Since then web applications are the top use-case for serverless applications. Due to the growing interest in serverless web applications AWS introduced another type of Lambda function which is called Lambda@Edge. With Serverless Framework v.. were adding native support for Lambda@Edge functions. In this post well take a deeper dive into serverless web applications to see how they can benefit from Lambda@Edge functions. Before we get ahead of ourselves and talk about Lambda@Edge lets recap what a typical serverless web application consists of. Building serverless web applications Roughly speaking web applications can be broken down into parts: The frontend and the backend. Although serverless web applications are composed of several different cloud services one can still separate them into those parts. Lets take a look at the different pieces a modern, serverless web application is made of. The frontend The frontends main purpose is to serve the `.html`, `.css` and `.js` files alongside other static assets such as images which will be rendered by the consumer (usually the web browser) to display a UI our users can interact with. While our browser renders the static content it will also use our front ends JavaScript code to interact with our serverless backend (more on that in a minute). When using AWS to serve our frontend to our users we could use AWS S which is a storage solution to host our static assets and AWS CloudFront, a CDN service, to automatically distribute such assets in data centers all over the world. One might be asking, why CloudFront is necessary in this setup. Isnt it enough to upload all the files to the S bucket and call it a day? Usually when a user would access our website, his request would be routed to our S bucket which in turn serves our public assets (such as `.html`, `.css` and `.js` files). While this initial setup works just fine, its often a better idea to cache content which is infrequently changed. Thats where AWS CloudFront comes in. With CloudFront we can set up a caching layer in front of our S bucket. This cache is distributed around the globe. Users wont download our assets from S but rather pull them from a CloudFront location nearby, which is way faster compared to an S-only solution. The backend Having the frontend up and running, its time to take a quick look at a potential serverless web application backend. Depending on the use cases we usually need an API Gateway, data processing capabilities and a data storage solution. To satisfy such requirements, serverless web applications are usually built with AWS API Gateway which serves as the API Gateway, AWS Lambda to process the incoming event data and AWS DynamoDB which is used as the persistence layer (other DB solutions are available as well). When the user interacts with our web application frontend and wants to update his password, for example, a request will be sent from the frontend to API Gateway which is the entrypoint of our backend. API Gateway translates this request into an event which triggers our Lambda function. Such Lambda function will then process the event data and store the updated password in the DynamoDB table. Once done, a response is generated and sent back to the user via the API Gateway. As previously stated this setup is a very typical one for serverless web applications. Looking at the full stack consisting of frontend and backend one might ask if its truly necessary to perform a full roundtrip from the frontend to the backend and back again for every single user request. Imagine youre operating a social networking site and you want to redirect mobile users to a dedicated lite version of your site which serves fewer content and hence loads faster. Wouldnt it be a waste of time and resources to let the request travel all the way to your backend which detects that your user is using a mobile device and issues a redirect to your optimized mobile web app. Wouldnt it be better to process that request at the edge without hitting the backend at all? Thats where Lambda@Edge comes in! Lambda@Edge Lambda@Edge is a compute offering by AWS which makes it possible to deploy AWS Lambda functions to edge locations which are served by CloudFront. Lambda@Edge functions are triggered by CloudFront events such as incoming requests or outgoing responses and can use this event information to rewrite such requests and responses (in our case inspecting the device type to issue a redirect when its a mobile device). Lambda@Edge functions are quite similar to regular AWS Lambda functions. In fact they are normal AWS Lambda functions which need to adhere to a few limitations: Only versioned Lambda functions can be turned into Lambda@Edge functions Lambda@Edge functions can have a maximum memory size of MB Lambda@Edge functions can have a maximum timeout of seconds The Lambda@Edge handler response is different compared to normal Lambda functions That being said theres nothing new which needs to be learned in order to leverage Lambda@Edge functions. In fact the Serverless Framework v.. takes care of the heavy lifting for you. The only thing you need to do is to provide the function code and setup the corresponding `cloudFront` event in your `serverless.yml` file. Curious how this looks like? Lets take a look at an example. Example Lets implement our example of redirecting mobile users to a light web app URL without doing a full round trip from the frontend to the backend and back again. Lets start with our `serverless.yml` file and turn our regular Lambda function into a Lambda@Edge function. Doing this is as easy as adding the `cloudFront` event to the `events` definition and setting the correct configuration parameters:  Here you can see that weve added the `cloudFront` event with an `eventType` configuration of `origin-request` and an`origin` configuration of `https://app.acme.com`. This configuration tells the Serverless Framework that we want to invoke our `cfLambda` function every time a request is sent to the `https://app.acme.com` origin. Next up we need to write the function logic to redirect mobile users to our lite version of our web application.  This functionality is triggered every time a request is routed through our CloudFront distribution. Once this happens we take a look into the request headers to see if its a mobile user. If thats the case we prepend the `/lite` string to our uri and return the request. Thats it. Were now ready to deploy our Lambda@Edge function. As per usual, deploying this service is a simple as running `serverless deploy`. Note that the deployment might take a while since AWS will setup a CloudFront distribution for you behind the scenes and replicates your Lambda function across the globe. Once done you should see the CloudFront endpoint in your deployment summary. If you visit this endpoint with a mobile device youll be redirected to the lite version of your webapp! A note about removals When youre done with testing you might want to remove the service via `serverless remove`. Note that the removal also takes a little bit longer and wont remove your Lambda@Edge functions automatically. The reason is that AWS has to cleanup your functions replicas which can take a couple of hours. Removing the Lambda functions too early would result in an error. The solution for this problem right now is to manually remove the Lambda@Edge functions via the AWS console after a couple of hours. You might want to automate this process with a script which issues AWS SDK calls to streamline this cleanup process. Conclusion AWS Lambda@Edge is a great way to run function code in edge locations which are near to the user and therefore typically offer lower latency. Lambda@Edge functions are regular, trimmed down Lambda functions which can read and modify the request and response data. This makes it possible to e.g. intercept incoming requests and return a response immediately without passing the data through to the backend services. Typical Lambda@Edge uses cases include rewriting of response URLs based on device types or IP addresses, identifying crawlers and serving static, pre rendered assets, on-the-fly content compression or authentication header manipulations. With Serverless Framework v.. were adding native support for Lambda@Edge functions via the `cloudFront` event. Every Lambda function can easily be turned into a Lambda@Edge function and deployed via the familiar `serverless deploy` command. You can read more about the `cloudFront` event type in our `cloudFront` event documentation. We hope that you enjoy this new functionality. What do you think about Lambda@Edge? How are you planning to use it? Let us know via @goserverless on Twitter or leave a comment below!",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fd6",
      "title": "How to Create a REST API with Azure Functions and the Serverless Framework - Part",
      "content": "Overview With the recent updates to the `serverless-azure-functions` plugin, it is now easier than ever to create, deploy and maintain a real-world REST API running on Azure Functions. This post will walk you through the first few steps of doing that. To see the full end-to-end example used to create this demo, check out my GitHub repo. I structured each commit to follow the steps described in this post. Any steps named `Step X.X` are steps that involve no code or configuration changes (and thus not tracked by source control), but actions that could/should be taken at that point in the process. This is done to preserve the \"commit-per-step\" structure of the example repo. This post will only cover the basics of creating and deploying a REST API with Azure Functions, which includes step and step from the example repo. Stay tuned for posts on the additional steps in the future. I will make the assumption that you have the Serverless Framework installed globally. If you do not (or have not updated in a while), run:  Also, the `serverless` CLI can be referenced by either `serverless` or `sls`. I will use `sls` in this post just because it's shorter, but `serverless` would work just the same. Step : Create your local Azure Function project Let's begin by creating our Azure Function project with a template from serverless.  The resulting project will be in the directory `sls-az-func-rest-api`. `cd` into that directory and run `npm install`. To make sure you have the latest version of the Azure Functions plugin, run:  Its important to note that the generated `serverless.yml` file will contain a lot of commented lines, which start with ``. Those are purely for your benefit in exploring features of the Azure Functions plugin, and can be safely removed. Step : Add your own handlers For the sake of this demo, were going to create a basic wrapper of the GitHub API for issues and pull requests. As youve probably already noticed, the `azure-nodejs` template comes preloaded with two functions: `hello` and `goodbye`. Lets remove those before we start adding our own code. To do this, remove both the `hello.js` and `goodbye.js` files. Also, remove their configuration definitions from `serverless.yml`. Right now your file structure should look something like:  and your `serverless.yml` should look like (not including any comments):  Add Code Lets add in our own code. Well start by creating the directory `src/handlers`. This, perhaps to your great surprise, will be where our handlers will live. Inside that directory, we will put our two handlers: issues.js and pulls.js. ```javascript // src/handlers/issues.js const utils = require(\"../utils\"); const axios = require(\"axios\"); module.exports.handler = async (context, req) => { context.log(\"Issue Handler hit\"); const owner = utils.getQueryOrBodyParam(req, \"owner\"); const repo = utils.getQueryOrBodyParam(req, \"repo\"); if (owner && repo) { const response = await axios({ url: `https://api.github.com/repos/${owner}/${repo}/issues`, method: \"get\" }); context.res = { status: , body: response.data }; } else { context.res = { status: , body: \"Please pass the name of an owner and a repo in the request\" }; } }; javascript // src/handlers/pulls.js const utils = require(\"../utils\"); const axios = require(\"axios\"); module.exports.handler = async (context, req) => { context.log(\"Pull Request Handler hit\"); const owner = utils.getQueryOrBodyParam(req, \"owner\"); const repo = utils.getQueryOrBodyParam(req, \"repo\"); if (owner && repo) { const response = await axios({ url: `https://api.github.com/repos/${owner}/${repo}/pulls`, method: \"get\" }); context.res = { status: , body: response.data }; } else { context.res = { status: , body: \"Please pass the name of an owner and a repo in the request\" }; } }; ``` Just for fun, well also add a utils.js file for shared utility functions across handlers, and well put that just inside the `src` directory.  Youll also note that the handlers are using a popular NPM package for HTTP requests, `axios`. Run `npm install axios --save` in your service root directory. Current Folder structure  Now we need to add our new handlers to the serverless configuration, which will now look like:  Step .: Test your API Locally Run the following command in your project directory to test your local service.  This will generate a directory for each of your functions with the file `function.json` in each of those directories. This file contains metadata for the bindings of the Azure function, and will be cleaned up when you stop the process. You shouldnt try to change the bindings files yourself, as they will be cleaned up and regenerated from `serverless.yml`. If you make changes to your `serverless.yml` file, youll need to exit the process and restart. Changes to code, however, will trigger a hot reload and wont require a restart. Here is what you can expect as output when you run `sls offline`: !alt text When you see the Http Functions in the log, you are good to invoke your local service. !alt text One easy way to test your functions is to start up the offline process in one terminal, and then in another terminal, run:  Lets create a file with some sample data at the root of our project, and well just call it `data.json`:  Luckily, `owner` and `repo` are the same parameters expected by both the `issues` and `pulls` handlers, so we can use this file to test both. Well keep our `offline` process running in one terminal. Ill open up another (pro tip: use the Split Terminal in the VS Code integrated terminal), and run:  Heres my output: !alt text You can see that it made a `GET` request to the locally hosted API and added the info from `data.json` as query parameters. There are no restrictions on HTTP methods, you would just need to specify in the CLI if its not a `GET`. (Example: `sls invoke local -f pulls -p data.json -m POST`) You could also run a simple `curl` command that would accomplish the same thing: !alt text And here is the output in the terminal running the API. You can see our `console.log` statement from the handler output here: !alt text When Im done running the service locally, Ill hit `Ctrl/Cmd + C` in the API terminal to stop the process. You can see that it cleans up those metadata files we discussed earlier: !alt text Step .: Deploy Authentication Thats all the configuration we need, so were ready to deploy this Function App. In order to deploy, well need to authenticate with Azure. There are two options for authentication: interactive login and a service principal (which, if you are unfamiliar, is essentially a service account). At first, when you run a command that requires authentication, the Interactive Login will open up a webpage for you to enter a code. Youll only need to do this once. The authentication results are cached to your local machine. If you have a service principal, youll set the appropriate environment variables on your machine, and the plugin will skip the interactive login process. Unfortunately, if youre using a free trial account, your only option is a service principal. The process for creating one and setting up your environment variables is detailed in the Azure plugin README. Deploy Command With configuration and authentication in place, lets ship this thing. From the root of your project directory, run: `sls deploy` and watch the magic happen. Your app will be packaged up into a `.zip` file, which will be located in the `.serverless` directory at the root of your project. From there, an Azure resource group will be created for your application, containing things like your storage account, Function App, and more. After the resource group is created, the zipped code will be deployed to your newly created function app and the URLs for your functions will be logged to the console. !alt text.png) Step . Invoke Deployed Function We can invoke a deployed function in the same way we invoked our local function, just without the `local` command:  !alt text (Optional) Step .: Cleanup If you have been following this tutorial and would like to clean up the resources you deployed, you can simply run:  BE CAREFUL when running this command. This will delete your entire resource group. Additional Steps Stay tuned for future posts walking you through other steps of setting up your service, including adding API Management configuration, quality gates like linting and unit tests, adding Webpack support, CI/CD and more. Also, if you're going to be at ServerlessConf in NYC, the Microsoft team is putting on a Azure Serverless Hands-on Workshop on October th from : am to : pm. Contributing Were eager to get your feedback on the `serverless-azure-functions` plugin. Please log issues on the GitHub repo with any bug reports or feature requests. Or better yet, fork the repo and open up a pull request! Part Two of this tutorial can now be found here.",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fd8",
      "title": "Serverless Next.js At The Edge",
      "content": "Today, Im excited to announce a project Ive been working on over the past few months. It provides a simple, fast and efficient way to deploy your Next.js applications to AWS using CloudFront, Lambda@Edge and S. The project is the Serverless Next.js Component which you can use with the Serverless Framework to deploy Next.js apps to AWS Lambda@Edge functions in every CloudFront edge location across the globe. These Lambda@Edge functions do server-side rendering of your website pages, as close as possible to your end users, providing very low latency. Principles The project was developed with a few design principles in mind. . Zero configuration by default Thats right, you can get up and running in under a minute with no configuration required. . Feature parity with next All features of next are supported: Server-side rendered pages. No surprise here, this is what attracts most folks to use next. serverless-next.js deploys your pages to Lambda@Edge. Server side rendering happens right at the edge, close to your users. API Routes. Like the pages, your api backend is also deployed to Lambda@Edge. When fetching data client side this ensures very low response latency. Dynamic Pages / Route segments. Next recently introduced support in their built-in routing system for parameterised routes. serveless-next.js implements a lightweight router which is compatible with dynamic routes. Automatic pre-rendering. Some of your pages might not be SSR. These type of pages are compiled by next to HTML at build time. serverless-next.js takes advantage of this and deploys them to S. Lambda@Edge in CloudFront then takes care of forwarding requests to static pages (e.g. /terms, /about, /contact etc.) to S. These can be heavily cached. Client assets. Build files generated by next such as webpack chunks, css files, etc. are uploaded to S. In this case Lambda@Edge does not need invoking as any requests to _next/ is served by CloudFront from S. User static / public folders. User assets like images in the static folder or root level resources in your public folder are uploaded to S. Like build assets, CloudFront serves these assets from S. . Fast deployments The Serverless Next.js Component is fast. Deploying your application, typically takes less than a minute. `next build` is used behind the scenes, no magic there. A CloudFront distribution is provisioned for you with best practices in place. The pages compiled are zipped up and deployed to Lambda@Edge which is then associated to your distribution. An S bucket is also deployed for the static assets which are uploaded using S accelerated transfers. The only caveat is that the first deployment you have to wait a few minutes for the CloudFront distribution to be available. However, subsequent deployments dont have this problem. Once the distribution is up, deploying updates is fast. CloudFormation is not used for provisioning resources. This is partly why deployments are quick. It also means the project is not bound by CloudFormation limits, which is an issue on the predecessor of this project, serverless-nextjs-plugin. !serverless nextjs graphics aws lambda edge Architecture Lets look in more detail at the architecture deployed to AWS. !serverless nextjs aws lambda edge architecture Three Cache Behaviours are created in CloudFront. The first . `_next/` and `static/` forward the requests to S. The rd. is associated to a lambda function which is responsible for handling three types of requests. Server side rendered page. Any page that defines `getInitialProps` method will be rendered at this level and the response is returned immediately to the user. Statically optimised page. Requests to pages that were pre-compiled by next to HTML are forwarded to S where the HTML is stored. Public resources. These are requests to root level resources like `/robots.txt`, `/favicon.ico`, `/manifest.json` etc. These are also forwarded to S where these resources can be found. The reason why . and . have to go through Lambda@Edge first is because these routes dont conform to a pattern like `_next/` or `static/`. Also, one cache behaviour per route is a bad idea because CloudFront only allows per distribution. Getting Started Using the Serverless Next.js Component is easy, just add it to your `serverless.yml` like this:  Sane defaults are baked in, so no additional configuration is needed. Use the Serverless Framework to deploy via the `serverless` command, like this:  Remove it with the `remove` command:  Custom Domains You can set a custom domain for your application. serverless-next.js takes care of associating the domain with your CloudFront distribution, creates the sub domain in Route and even sets up the SSL Certificate using AWS ACM. It is optional and looks like this:  Behind the Scenes The project is powered by the amazing serverless-components. At its core it uses components: @serverless/aws-s @serverless/aws-cloudfront @serverless/aws-lambda @serverless/domain Most of the heavy lifting is done by the components themselves, serverless-next.js simply orchestrates. What's Next? Build time efficiencies, configurable caching options for users and potentially adding a separate `/api` cache behaviour for API Routes. I will also be working on more complete examples that integrate with other AWS Services. Hope you find the project useful. If you find any issues or would like to see a new feature please raise an issue. Also, contributions are welcome :)",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fda",
      "title": "Zero Configuration Monitoring and Alerts now available for serverless Python apps",
      "content": " The Serverless Framework has amazing monitoring and alerting; however, until today, this has only been available for the Node runtime. Now, we are adding support for Python to the Serverless Framework Dashboard to enable monitoring, alerts, and much more for your Python serverless applications. Python is the second most popular runtime for building serverless architectures, making up over % of applications built using the Serverless Framework. Considering that AWS Lambda started as Javascript-only, it is impressive that Python has been able to get so much traction amongst Serverless Framework developers. If you want to configure free monitoring and alerts for your Serverless Python project you can get started now here. Lets have a look at some of the great things you get as a Python developer using the Serverless Dashboard. Serverless Monitoring The minute you deploy your Python service with the Serverless Framework Dashboard, metrics will be collected and displayed in your dashboard. Without any additional configuration youll get access to invocations, errors, and cold starts across API Gateway and Lambda functions. !Monitoring Invocation Explorer The invocation explorer is one of the most powerful troubleshooting tools in the Serverless Framework Dashboard. Its powerful, yet simple, querying enables you to find the needle in the haystack. Filter by function, memory usage, duration, error status, and more, to find the invocation you need. No more sifting through CloudWatch! The invocation details will show you all of the details you need to identify the root cause of a problem. Youll get the CloudWatch logs, stack trace, functions spans, and more. !Invocations Explorer Stack Traces If the function invocation crashes, the stack trace is recorded and you can find the root cause in no time. The new error type alert will notify you of new errors and youll be able to see the errors in the metrics and invocation explorer. You can also use the integrated Python SDK to record handled exceptions even if your function doesnt crash. !Stack Traces CloudWatch Logs With the powerful Invocation Explorer you no longer need to sift through CloudWatch to find an invocation, and it provides you with CloudWatch logs for all invocations. The Serverless Framework Dashboard pulls in the CloudWatch logs you need so you have everything all in one place. !Logs Function Spans Deploy your application with the Serverless Framework Dashboard enabled and it will automatically instrument calls made by your Lambda functions to any AWS service or HTTP endpoint. Youll be able to see what AWS services were called, what methods they called, and their time span. Youll also see what HTTP services were called, including the host name, HTTP code response, method, host, path, and the time span. All of this instrumentation is % automatic, no need to add any plugins, wrappers, or instrumentation. !Function Spans Alerts Serverless Framework Dashboard comes with nine alerts available out of the box, no configuration or setup needed. Your serverless functions will be monitored for memory usage, durations, and errors the minute you deploy them. Youll be notified of these alerts in Slack, email, or you can easily set up custom notifications via point and click integrations with SNS and webhooks. !Approaching timeout And more... While I have shown you the monitoring features that the Serverless Dashboard now provides Python users, there are many other powerful features in the Serverless Framework. This includes features like security and operational safeguard policies, deployment profiles, output variables, and more, which together provide Python developers a full-lifecycle solution for their serverless application. Get started with zero configuration You can enable all of these powerful features without having to instrument your code or configure anything. This all works out of the box when you enable the Serverless Dashboard. Here is how you get started... If you are new to Serverless Framework install it with `npm install serverless --global`. While the framework uses Node & NPM to install, it still supports Python as a runtime. If you dont have a Serverless Dashboard account, register at https://app.serverless.com/ and then login in the CLI with `serverless login`. If you are new to the Serverless Framework, run `serverless` and select `AWS Python`. If you have an existing Python Serverless Framework project, run `serverless` in the directory containing your `serverless.yml` file. This will make sure all the requirements are met, and then update your `serverless.yml` to include `org` and `app` from your Dashboard. When you are done setting up the Dashboard, dont forget to redeploy your service with `serverless deploy`. And just like that, youll have access to metrics and alerts in the Dashboard for your Python Serverless Framework service.",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fdc",
      "title": "How to Create a REST API with Azure Functions and the Serverless Framework - Part",
      "content": "Overview Now that you've created and deployed a basic API from Part , let's take a few more steps towards making that API more resilient and secure. This post will still be based on the example repo, and will follow the same \"commit-per-step\" format as Part , which contains Steps and . To pick up where we left off in the example repo (after having completed Step ), run:  Step : Add unit testing and linting - (commit ecfe) Because this isn't a blog post on unit tests, linting or quality gates in general, I'll just share the tools that I'm using and the quality gates that I added to the repository. Feel free to use them as stubs for your own future tests or lint rules. For unit tests, I'm using the Jest test runner from Facebook. I've used it for several projects in the past and have never had any issues. Jest tests typically sit alongside the file they are testing, and end with `.test.js`. This is configurable within `jest.config.js`, which is found at the root of the project. Because my code makes REST calls via `axios`, I'm using the `axios-mock-adapter` to mock the request & response. The tests that I wrote (issues.test.js and pulls.test.js) run some simple checks to make sure the correct URLs are hit and return the expected responses. For linting, I'm using ESLint with a very basic configuration, found in `.eslintrc.json`. To run a lint check, you can run:  Many errors can be fixed automatically with:  Run your tests with:  For more details, take a look at the commit in the example repo or check out the commit locally  Step : Add basic API Management Configuration - (commit c) This was one of the first features we implemented into the `v` of the `serverless-azure-functions` plugin. because most Azure Function Apps are REST APIs, and it's hard to have a real-world API in Azure without API Management. If you have no special requirements for API Management, the plugin will actually generate the default configuration for you if you just include:  That's exactly what I did for Step . Also, because we want API Management to be the only entry point for our API endpoints, I also changed each function's `authLevel` to `function`. This requires a function-specific API key for authentication. You can see in the screenshot what happens in the first command, when I try to `curl` the original function URL. I get a `` response code. But when I hit the URL provided by API Management, I get the response I expect: !alt text For more details on `authLevel`, check out the trigger configuration docs. Consumption SKU One important thing to note is that the API Management configuration will default to the `consumption` SKU, which recently went GA. For now, the only regions where `Consumption` API Management is allowed are: - North Central US - West US - West Europe - North Europe - Southeast Asia - Australia East If you are deploying to a region outside of that list, you will need to specify a different SKU (`Developer`, `Basic`, `Standard` or `Premium`) within the `apim` configuration, which will be demonstrated in the next section. Deploy your updates:  Step : Add more advanced API Management Configuration - (commit a) If you need a few more knobs to turn when configuring your API Management instance, you can provide a more verbose configuration. Here is the verbose config I added to the sample repo (the `...` means the rest of the config for that section stayed the same):  If you did not want the `Consumption` SKU of API Management, you would need to have a verbose configuration and specify the `sku` as:  The example just uses the default and deploys to region(s) where Consumption API Management is currently available. Deploy your updates:  (Optional) Step .: Revert back to basic API Management configuration - (commit cf) To make the demo simple and easy to follow, I'm going to revert my `apim` configuration back to the defaults:  You might be able to do the same, depending on your requirements. Step : Add Webpack configuration - (commit aefac) Webpack dramatically reduces the packaging time as well as the size of your deployed package. After making these changes, your packaged Function App will be optimized with Webpack (You can run `sls package` to package it up or just run `sls deploy` which will include packaging as part of the lifecycle). Just as an example, even for this very small application, my package size went from KB to KB. To accomplish this, we'll use another awesome Serverless plugin, `serverless-webpack` to make Webpacking our Azure Function app really easy. First thing you'll want to do, assuming you're working through this tutorial in your own git repository, is add the generated Webpack folder to your `.gitignore`  Next, we'll need to install packages from npm:  Then we'll add the plugin to our `serverless.yml`:  And then copy this exact code into `webpack.config.js` in the root of your service directory:  And just like that, your deployed Azure Function apps will be webpacked and ready to go. !alt text Step : Enable Serverless CLI configuration - (commit cbfd) If you're running a real-life production service, you will most likely be deploying to multiple regions and multiple stages. Maybe merges to your `dev` branch will trigger deployments to your `dev` environment, `master` into `prod`, etc. I'll show you an example of that in Step . To accomplish CLI-level configurability, we need to make a few changes `serverless.yml`.  As you might have guessed, the values `West US`, `dev` and `demo` are my default values. If I wanted to deploy my service to `North Central US` and `West Europe`, but keep everything else the same, I would run:  We could do similar operations with `--prefix` and `--stage`. Now let's create a pipeline that actually does this. Step : Add CI/CD (with Azure DevOps) - (commit afabf) For the CI/CD on my sample repo, I'm using [Azure DevOps](), but it would work the same on any other service you want to use. If you want to use Azure DevOps for an open-source project, here are a few steps to get started No matter the CI/CD environment, here is what we are looking to accomplish: Install dependencies Validate the changes (run quality gates) Deploy the service These steps can all be accomplished in just a few CLI commands. At bare minimum, we'll want to run something like:  There are a lot more bells and whistles we could add, but that's essentially what it boils down to. Of course, we'll need authentication in whatever system we're deploying from, and that's where the service principal will come in. I'll show you how to use the service principal in the `deploy.yml` pipeline below. For my pipelines, I'm actually going to split up my CI and CD into `unit-tests.yml` and `deploy.yml`. Unit tests will be run on PRs into `master` or `dev` (this is assuming there are branch policies in place to prevent devs from pushing straight to either branch). Deployment will be run on commits (merges) to `master`. Unit Tests ```yaml pipelines/unit-tests.yml Only run on Pull Requests into `master` or `dev` pr: branches: include: - master - dev Run pipeline on node and on Linux, Mac and Windows strategy: matrix: Linux_Node: imageName: 'ubuntu-.' node_version: .x Linux_Node: imageName: 'ubuntu-.' node_version: .x Mac_Node: imageName: 'macos-.' node_version: .x Mac_Node: imageName: 'macos-.' node_version: .x Windows_Node: imageName: 'win' node_version: .x Windows_Node: imageName: 'win' node_version: .x https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devopsuse-a-microsoft-hosted-agent pool: vmImage: $(imageName) steps: - task: NodeTool@ inputs: versionSpec: $(node_version) displayName: 'Install Node.js' Make pipeline fail if tests or linting fail, linting occurs in `pretest` script - bash: | set -euo pipefail npm ci npm test displayName: 'Run tests' yaml pipelines/deploy.yml trigger: branches: include: - master https://docs.microsoft.com/en-us/azure/devops/pipelines/library/variable-groups?view=azure-devops&tabs=yaml variables: - group: sls-deploy-creds jobs: - job: \"Deploy_Azure_Function_App\" timeoutInMinutes: cancelTimeoutInMinutes: pool: vmImage: 'ubuntu-.' steps: - task: NodeTool@ inputs: versionSpec: .x displayName: 'Install Node.js' - bash: | npm install -g serverless displayName: 'Install Serverless' Deploy service with prefix `gh`, stage `prod` and to region `West Europe` - bash: | npm ci sls deploy --prefix gh --stage prod --region \"West Europe\" env: Azure Service Principal. Secrets need to be mapped here USE THIS EXACT TEXT, DON'T COPY/PASTE YOUR CREDENTIALS HERE. Azure DevOps will use the variables within the variable group `sls-deploy-creds` to replace all the $() values AZURE_SUBSCRIPTION_ID: $(AZURE_SUBSCRIPTION_ID) AZURE_TENANT_ID: $(AZURE_TENANT_ID) AZURE_CLIENT_ID: $(AZURE_CLIENT_ID) AZURE_CLIENT_SECRET: $(AZURE_CLIENT_SECRET) displayName: 'Deploy Azure Function App' ``` Notice this line in the deployment pipeline that leverages our setup from Step . You might have multiple pipelines for the different stages, you might dynamically infer these values from the branch name or you might just provide the values as environment variables. The point of the setup in Step was to provide you the flexibility to deploy your service to wherever you see fit at the time, without needing to change your `serverless.yml` file. Concluding Thoughts A big part of our reason for investing time and effort into the `serverless-azure-functions` plugin was so that developers could easily deploy Azure Functions to solve more real-world, business-level scenarios. We hope that as you use the tool and discover areas for improvement that you'll file issues on the repo or even open up a pull request.",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fde",
      "title": "Serverless Deployment Best Practices",
      "content": "Overview As you continue to develop serverless applications, their complexity and scope can start to grow. That growth brings a need to follow structured practices to deploy your applications in a way that minimizes bugs, maintains application security, and allows you to develop more rapidly. This post will review a variety of serverless deployment best practices listed below. As we review them, I'll also show you how you can use the new Serverless Dashboard Safeguards to help you easily implement these practices in your own Serverless Framework applications. If you're not yet familiar with the Serverless Dashboard take a look at the documentation to get started. So let's take a look at a few deployment best practices that you can implement in your own serverless applications! Security Properly Handling Secrets API Keys, database credentials, or other secrets need to be securely stored and accessed by your applications. There are a few different parts to this, but some of the most critical include: Keeping your secrets out of your source control Limiting access to secrets (the Principle of Least Privilege) Using separate secrets for different application stages when appropriate We've previously discussed several methods for handling secrets when using the Serverless Framework that might be good options for you. More recently, we've also added Parameters to allow you to configure your secrets across different services, AWS accounts, and application stages. You can also use Safeguard polices to block service deployments whenever there are plaintext secrets set as environment variables in your `serverless.yml`. Limiting Permissive IAM Policies Another important best practice is to try to limit the scope of permissions that you grant to your applications. In the case of AWS, whenever you create IAM policies for your services you should limit those roles to the minimum permissions required to operate. As part of this, you should try to reduce the use of wildcards (the `` character) in your policy definitions. And guess what? You can also use a Safeguard policy to block deployments that contain wildcards in IAM permissions: !Displays the configuration for the no wildcards safeguard You can use this policy to either block a deployment entirely or to warn developers to take another look at the IAM polices they're using. Restricting Deploy Times Imagine you're an ecommerce team going into your annual Black Friday rush. You're confident in the state of your code as it is but you'd like to limit even the hint of a possibility that a new bug is introduced during the busy season. One common way to do this is to lock down your deployments during that period. Something similar happens at organizations that really don't want to get on-call notifications on weekends so they may lock down deployments between Friday and Monday morning. !Displays the configuration for the deploy times restriction policy While these situations might not apply to your organization, if they do (you guessed it!) there is a Safeguard that will allow you to apply this policy to your environment. Consistent Conventions Stages Conventions rock. They help developers learn a set of standards and then intuitively understand other parts of a system. One of the most ubiquitous development conventions is having a separate place for code customers see (production) and one or more places for code that developers are working on that isn't quite ready (development/testing etc.). These different places are usually called `stages` and they allow you to set up a consistent path for your code to take as it moves towards customers. With the Serverless Framework, by default, your applications are pushed out in a `dev` stage as you work on them. Then, when ready for production you can deploy them to a stage like `prod` by updating your `serverless.yml` or running a deploy command with the `--stage prod` option. For each of these stages, you might want to use a very different set of configuration. Fortunately, there's a lot of new granularity to what you can do with the Serverless Dashboard when it comes to interacting with stages. Per-stage configuration can include things like: - What AWS account or region stages are deployed to - What Safeguards are evaluated against the deployment - What parameters and secrets are used This allows you to use Safeguards to do things ranging from blocking `dev` stage deployments to the production AWS account or making sure that your production API keys are always bundled in with production deployments. These options become very flexible to help you support the needs and workflows of your organization. Allowed Regions When working on a geographically distributed team, the default AWS region for each developer may not be the same. A developer in Seattle might default to `us-west-` and one in Philadelphia might use `us-east-`. As you start to deploy and develop independently, these differences can lead to inadvertent issues in your code. One service may reference one region, but actually need to be deployed in another. Or, different regions may have different supported features or limitations. To avoid issues like this, you can require developers to use a single region or a subset of regions that suit your needs. And of course... There's a Safeguard to do just that. At deployment time it ensures that your services are only deployed to a particular region or list of regions you specify: !Displays the configuration for the deploy times restriction policy Function Naming and Descriptions In combination with stage and region controls, maintaining consistent naming and descriptions in your infrastructure can help new developers quickly see what services do, how they are connected to different application stages and allow them to more easily build and debug them. One common pattern would be to require that your Lambda functions all consistently have the service name, stage, and function inside of each function name. This allows you to more easily find the relevant functions if they are in the same AWS account, and to more quickly tie multiple functions with a particular service. Let's imagine a service that processes content submissions from users, records them in a database, and indexes them for search. It might have one Lambda function to accept/reject the submission and store it in a DynamoDB table and another to index the new data in ElasticSearch. If you take this simple architecture and spread it across a prod or dev environment you've already got four Lambda functions to keep track of. Doing this becomes easier when we follow a Lambda function naming convention like this: `serviceName-stage-functionName`. Then, the function names become something like this: - `newSubmissions-prod-submissionGrader` - `newSubmissions-prod-elasticsearchIndexer` - `newSubmissions-dev-submissionGrader` - `newSubmissions-dev-elasticsearchIndexer` This way, you know exactly what the function you need is called and can find it when you need it. Now, if you don't want to worry about a new developer deciding to deploy an opaquely-named service, you can also enforce this naming convention using yet another Safeguard in the Serverless Dashboard. Takeaways This is just a subset of best practices that we want to enable in the Serverless Dashboard. There are also many other Safeguards to enable more application-specific practices like enforcing the creation of Dead Letter Queues or requiring services be within a VPC. Keep in mind that these best practice aren't only applicable to the Serverless Framework. However you decide to build your applications, many of these practices can help you do so more effectively and securely. Are there other development best practices you think we missed? Let us know in the comments! We're constantly looking for ways to improve the development experience for our users.",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fe0",
      "title": "Creating, monitoring, and testing cron jobs on AWS",
      "content": "Cron jobs are everywherefrom scripts that run your data pipelines to automated cleanup of your development machine, from cleaning up unused resources in the cloud to sending email notifications. These tasks tend to happen unnoticed in the background. And in any business, there are bound to be many tasks that could be cron jobs but are instead processes run manually or as part of an unrelated application. Many companies want to take control of their cron jobs: to manage costs, to make sure the jobs are maintainable and the infrastructure running them is up to date, and to share the knowledge about how the jobs run. For those already bringing the rest of their infrastructure to Amazons public cloud, running cron jobs in AWS is an obvious choice. If you are a developer using AWS, and youd like to bring your cron jobs over to AWS, there are two main options: use an EC machinespin up a VM and configure cron jobs to run on it; or use AWS Lambdaa serverless computing service that abstracts away machine management and provides a simple interface for task automation. On the face of it, EC might seem like the right choice to run cron jobs, but over time youll find yourself starting to run into the following issues: Most cron jobs dont need to be run every second, nor even every hour. This means that the EC machine reserved for the cron jobs is idle at least % of the time, not to mention that its resources arent being used efficiently. The machine running the cron jobs will of course require regular updates, and there must be a mechanism in place to handle that, whether its a Terraform description of the instance or a Chef cookbook. Did the cron job run last night? Did the average run time change in the last few weeks? Answering these and other questions will require adding more code to your cron job, which can be hard to do if your cron job is a simple Bash script. AWS Lambda addresses all of these issues. With its pay-per-use model, you only pay for the compute time used by your Lambda applications. For short-lived tasks, this can generate significant savings. When deploying Lambda with Serverless Framework, the description of all the infrastructure to which the function connects resides in the same repository as the application code. In addition, you get metrics, anomaly detection, and easy-to-use secrets management right out of the box. In this article we'll walk you through how to create a cron job on AWS using AWS Lambda and Serverless Framework, how to get the right alerts and security measures in place, and how to scale your cron jobs as needed. Take a look at our example repo for this article on GitHub if youd like to follow along. Lets dive in! Creating a cron job with AWS Lambda In this example well walk through a cron job that performs database rollovers. Our use case: we want to archive the past weeks data from a production database in order to keep the database small while still keeping its data accessible. We start by defining all the details of our cron job application in the `serverless.yml` file in the root of our repository:  Our function needs to connect to our production database, so we supply the secrets we need for that database via environment variables:  We then add the description of our function. We want it to have a single function called `transfer` which performs the database rollover. We want the `transfer` function to run automatically every week at a time when the load for our application is the lowest, say around am on Mondays:  Syntax for the Schedule expressions In our example above, the `transfer` handler gets run on a schedule specified in the `events` block, in this case via the `schedule` event. The syntax for the `schedule` event can be of two types: - `rate`  with this syntax you specify the rate at which your function should be triggered. The `schedule` event using the `rate` syntax must specify the rate as `rate(value unit)`. The supported units are `minute`/`minutes`, `hour`/`hours` and `day`/`days`. If the value is then the singular form of the unit should be used, otherwise youll need to use the plural form. For example:  - `cron`  this option is for specifying a more complex schedule using the Linux `crontab` syntax. The `cron` schedule events use the syntax `cron(minute hour day-of-month month day-of-week year)`. You can specify multiple values for each unit separated by a comma, and a number of wildcards are available. See the AWS Schedule Expressions docs page for the full list of supported wildcards restrictions on using multiple wildcards together. These are valid `schedule` events:  You can specify multiple schedule events for each function in case youd like to combine the schedules. Its possible to combine `rate` and `cron` events on the same function, too. Business logic for transferring database records At this point, the description of the function is complete. The next step is to add code to the `transfer` handler. The `handler.js` file defining the handlers is quite short:  The actual application logic lives in the `service/transfer_data.js` file. Lets take a look at that file next. The task that we want our application to accomplish is a database rollover. When it runs, the application goes through three steps: Ensures all the necessary database tables are created. Transfers data from the production table to a monthly table. Cleans up the data that has been transferred from the production table. We assume that no records with past dates can be added in the present, and that creating additional load on the production database is fine. We start by referencing the helper functions for each of the three tasks we defined above and initializing the utilities for date operations and database access:  The function code is quite straightforward: ensure the tables exist, transfer the data, delete the data, and log all the actions that are happening. The simplified version is below:  You can find the full version of the file in our GitHub repository. The helper function for creating the monthly table exports a single `create` function that essentially consists of a SQL query: ```javascript // database/create_month_table.js exports.create = async (client, week, year) => { await client.query(` CREATE TABLE IF NOT EXISTS weather_${year}_${week} ( id MEDIUMINT UNSIGNED not null AUTO_INCREMENT, date TIMESTAMP, city varchar() not null, temperature int not null, PRIMARY KEY (id) ); `) } ``` The `transfer_data` helper is similar in structure with its own SQL query: ```javascript // database/transfer_data.js exports.transfer = async (client, week, year) => { var anyId = await client.query(`select id from weather where YEAR(date)=? and WEEK(date)=?`, [year, week]) if (anyId.length == ) { console.log(`records does not exists for year = ${year} and week = ${week}`) return } await client.query(` INSERT INTO weather_${year}_${week} (date, city, temperature) select date, city, temperature from weather where YEAR(date)=? and WEEK(date)=? `, [year, week]) } ``` And finally, the cleanup of the data in the `cleanup` helper looks like this: ```javascript // database/cleanup_data.js exports.cleanup = async (client, week, year) => { var anyId = await client.query(`select id from weather where YEAR(date)=? and WEEK(date)=?`, [year, week]) if (anyId.length == ) { console.log(`cleanup did't needed, because does not exists records for year = ${year} and week = ${week}`) return } anyId = await client.query(`select id from weather_${year}_${week} limit `, [year, week]) if (anyId.length == ) { throw Error(`cleanup can't finished, because records are not transferred for year = ${year} and week = ${week} in`) } await client.query(` delete from weather where YEAR(date)=? and WEEK(date)=? `, [year, week]) } ``` With this, the core business logic is done. We also add a number of unit tests for the business logic that can be found in the `test` directory in our repo. The next step is to deploy our cron job. Deploying our cron job to AWS Both the application code and the `serverless.yml` file are now set up. The remaining steps to deploy our cron job are as follows: - Install the Serverless Framework. - Install the required dependencies. - Run the deployment step. To install Serverless Framework, we run:  To install our applications dependencies, we run:  in the project directory. We now have two options for how to run the deployment step. One option involves setting up AWS credentials on your local machine, and the other is to set up the AWS credentials in the Serverless Dashboard without giving your local machine direct access to AWS. Option : Use AWS credentials on the development machine This option works well if you have only one person deploying a sample cron job, or if the developers on your team already have access to the relevant AWS production account. We dont recommend this option for larger teams and production applications. Follow these steps: Make sure that the AWS CLI is installed locally. Try running `aws --version`, and if the CLI is not yet installed, run `pip install awscli`. Configure the AWS credentials for the AWS CLI by running `aws configure`. Once the credentials are set up, run `serverless deploy` to deploy the cron job. Option : Use the Serverless Dashboard to generate single-use AWS credentials for each deploy We recommend this option for teams with multiple developers. With this setup, you grant the Serverless Dashboard a set of AWS permissions, and for each deploy the Serverless Framework will generate a single-use credential with limited permissions to deploy your cron job. Before deploying, if you dont yet have an account, sign up for the Serverless Dashboard. Once your account is set up, create a new application using the Add button: , then using EC might be a better fit. In addition, when you have a very high volume of cron jobs running simultaneously, using EC might well be more cost-effective in the long run. You can find the full example that we walked through in our GitHub repo. Check out the details of Serverless Framework on the Serverless website. The Serverless AWS docs might be helpful, as well as the reference for the Serverless Dashboard. Find more examples of Serverless applications on our Examples page.",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fe2",
      "title": "AWS Lambda Performance Optimization & Monitoring with Tracing & Spans (Serverless Framework Pro)",
      "content": "Serverless applications are highly distributed, where each function performs a focused task and depends on countless other services for the remainder. For example, a REST API endpoint with a lambda function will have relatively little code, but it will have dependencies on other services like: Invoking other functions which provide shared logic across your service Interacting with infrastructure services like SES, SNS and DynamoDB Calling services like Stripe or Twilio using client SDKs Querying a database like RDS using an ORM like Sequelize If the REST API endpoint performance is degraded, it can be incredibly challenging to identify the root cause as it may be tied to the function code or to any one of the countless dependencies. As your applications grow, so do the service dependencies and the possible root causes of performance issues. To troubleshoot the performance of a AWS Lambda function, we need traces and spans that show us the transaction time of each dependency. Serverless Framework Pro provides both zero-config / zero-code automatic instrumentation and custom instrumentation to capture spans of the calls to dependent services. Your code is automatically instrumented for you and the Serverless Framework Dashboard provides a powerful invocation explorer to drill into and visualize span details. Lets have a look at how automatic AWS and HTTP instrumentation works, and how you can use custom spans for all other cases. While the examples are all node-js based, this is also automatically supported for Python, with other runtimes coming soon. Automatic AWS Spans One of the first challenges of troubleshooting performance issues is instrumentation. You have to identify all possible calls to dependent services and instrument the spans. In Serverless Framework services, the most common dependencies are AWS services like DynamoDB, SES, SQS, and S. In the example below, we use the AWS SDK to call the invoke method to invoke another lambda function. The call to `lambda.invoke`, is automatically instrumented to capture the AWS SDK method and the time span information. Without any modification to our usage of the AWS SDK, the calls are all automatically instrumented.  Automatic HTTP Spans In addition to automatically instrumenting spans in the AWS SDK, all HTTP spans are also automatically instrumented. For example, below we use the Stripe API to process a charge. Under the hood, the Stripe API key makes HTTPS API calls to Stripe. The Serverless Framework automatically instruments those API calls and captures the span details (start time / end time / duration), HTTP Status, HTTP method, response code, host name, and path.  Custom Span Instrumentation In the majority of the cases, when troubleshooting performance issues, the automatic AWS and HTTP instrumentation will be sufficient. However, to get full coverage of your serverless application, we may need to add custom instrumentation. For example, if you use an ORM like Sequelize to call RDS, this will not use the AWS SDK or HTTP. In this case, well add custom span instrumentation.  The `context.span` method allows us to decorate an async call which we want to instrument. Spans in the Dashboard Now that youve instrumented your lambda functions to capture spans, lets take a peek at what our data looks in the dashboard. !Spans in dashboard At Serverless, we love to dogfood - this is a screenshot of one of the services that powers the Serverless Framework Dashboard. You can see the duration, invocation details, and the span data in a single view. If your service was experiencing any kind of latency issues, youd immediately see the dependency with the longest duration. Get Started To get started, first make sure you are on the latest version of the Serverless Framework, which you can install with `npm install serverless --global`. Even though the framework itself is written in Node, the dashboard supports Python too. If you dont have a Serverless Framework account, register at https://app.serverless.com/, and if you havent yet logged in, run `sls login` on the CLI to login. Now run `sls` in your working directory. If you are new to Serverless Framework, this will start a new project. If you are running it from within an existing Serverless Framework project, then this will update your `serverless.yml` to work with the Dashboard and enable monitoring and tracing spans. Now deploy with `sls deploy` and youll immediately start seeing the invocations and spans in the dashboard as soon you invoke the functions. No additional configuration or instrumentation is needed, Serverless Framework monitoring and spans work out of the box. If you want to add custom spans these Node.js docs and Python docs will help you out.",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fe4",
      "title": "Running cron jobs in the cloud - Amazon EC vs AWS Lambda",
      "content": "Cron jobs are one of the things that have gotten harder, not easier, when moving to the cloud. The motivation to automate recurring tasks is still strong in the software community, but while companies have been transitioning their infrastructure towards cloud environments, theyve been falling behind on tooling for daily tasks. Previously, when companies hosted servers in their own data centers, scheduling a cron job to run on a spare machine was a -minute task. But with the move to the cloud, there are no longer any spare machines. Companies track infrastructure closely because the management of this infrastructure is now done automatically, and access to it is restricted, creating new barriers to automation in cloud environments. The first solutions to general automation in the cloud ran on Amazon EC: companies would spin up a machine and use it for cron jobs, or theyd install a layer of middleware on top of EC, such as Sidekiq. These solutions were unsustainable due to overspending on idle machines. Running cron jobs using Sidekiq and similar scheduling systems also meant that the software engineering teams had to maintain an application layer for scheduling the jobs, and this resulted in unnecessarily tight coupling of cron jobs to the business logic of the given applications. AWS Lambda is taking its place as the new standard for task automation in AWS environments. When used with the Serverless framework, AWS Lambda allows you to combine a great developer experience with the advantage of only paying for what you use, saving on compute costs. Of course, Lambda has its limitations, but in a large proportion of cases it can be a solution for recurring tasks and cron jobs in the cloud that is easier to develop for, more secure, and more observable than EC. In this article, well compare Amazon EC and AWS Lambda for running cron jobs and offer guidance for when to choose which of the two. Amazon EC vs. AWS Lambda for running cron jobs Cost and resource utilization Under EC, you must reserve an entire machine for your cron jobs at all times. Unless you have a very high and consistent number of cron jobs that you run, youre likely underutilizing your EC machine. With Lambda, AWS schedules your job once created and only charges you for the amount of time the job spends running. You pay for only what you use, and your costs are proportional to the number of cron jobs you run. This pricing model for AWS Lambda can be both a positive and a negative. If you run a small number of cron jobs, fewer than would use up an entire EC machine, youll pay less overall using Lambda. But if you run many scheduled tasks, or if your tasks have long execution times, the AWS Lambda charges may be higher than the equivalent computing capacity on EC. In this case it would be more economical to choose EC, perhaps especially so when using EC reserved instances. Software available for cron jobs and machine maintenance Here are some of the regular maintenance tasks youll need to perform on any EC machines you use for cron jobs: - Update the operating system. - Install security updates. - Clean up outdated temporary files. - Reboot the machine whenever AWS needs to migrate it to a newer infrastructure. In contrast, AWS Lambda is a fully managed service, so all these tasks are taken care of by AWS. You dont need to spend any time on them when using Lambda. But with Lambda you give up any flexibility around pre-installed software, operating system versions and available programming language runtimes. Deployment To ensure that your cron jobs deliver maximum value, they must be easy to update and iterate on. The deployment process forms a key part of the iteration cycle. Repeatable, traceable deploys allow you to add value faster and with more confidence. Just as in the cases of microservices, web applications and legacy software, you need a robust deployment process for your cron jobs as well, and this is very much attainable when running on either Amazon EC or AWS Lambda. On AWS Lambda, each function has a version identifier associated with it. Any change to the code creates a new version of the functionand this is the core of the deployment process. When using the Serverless Framework, running `serverless deploy` not only creates a new version of the function but makes all necessary changes to your AWS infrastructure to deploy a new version of your Lambda cron job. You can run the deployment manually as you see fit or, if you use Git Flow for your cron jobs, you can also run the deployment automatically via your CI environment whenever there is a merge to the default branch. This way you have a convenient and flexible deployment process for your cron job. When using EC for your cron jobs, a consistent deployment process requires more work. You most likely dont want your team members to have direct access to the production environment, so youll need a way to update the cron jobs on the EC machines remotely and automatically. One solution would be to use a configuration management system (like Chef Infra) to keep track of and update the cron jobs on your EC servers whenever developers make changes to the cron jobs code. Another option might be to create a versioned Docker container with your cron jobs and then set the EC machines to regularly pull the latest version of the container. In short, using EC machines for your cron jobs means you need to build the deployment automation yourself, while with AWS Lambda you get it out of the box. Secrets management Your cron jobs very likely need to connect to your backend systems, which means youll need to make sensitive credentials available to the cron job when it runs. As it happens, many teams who pay plenty of attention to handling secrets for their microservices and applications lack a secrets management strategy for their cron jobs. In an ideal world, youd be able to grant your developers all the flexibility they need to iterate on and test the cron jobs while creating zero security risks. When running cron jobs on Amazon EC, you can, for example, use a secrets store like Vault. With Vault, your cron jobs can dynamically get the credentials they need. The secrets dont get stored on the machine thats running the cron jobs, and if you change a secret, the cron jobs will automatically receive that change. The downside of implementing a solution like Vault, however, is the overhead of managing the secrets store. Youll need to set up the store itself, maintain the underlying server and see to getting the credentials from the store and into your cron job. With AWS Lambda, you can use a number of off-the-shelf services to handle secrets management. You can choose between AWS SSM and AWS Secrets Manager, or you can use the Serverless Frameworks secrets management functionality to take care of secrets without additional operational overhead. Check out our article on secrets management for AWS Lambda for a comparison of these three options. Overall, AWS Lambda has more options for secrets management that require less configuration and maintenance. Metrics and alerts When a cron job breaks, developers generally dont notice until it overloads another system or goes out of service. To prevent service disruptions from cron jobs not running or running incorrectly, it can be very helpful to set up a reliable metrics feed and alerts based on those metrics. The metrics and alerts make you aware of problems so that you can resolve them before they have any downstream effects on your infrastructure. Both EC and AWS Lambda allow you to export metrics to CloudWatch, and setting up alerts on those metrics is straightforward. On AWS Lambda, emitted CloudWatch events are generally tied to function executions and run times. CloudWatchs default metrics may not be right for monitoring infrequently running functions (like cron jobs), so you may need to adjust the metrics coming from your cron-job Lambda functions. With EC, however, the default CloudWatch metrics only monitor the machine itself, such the load average and the amount of memory used, offering essentially no visibility into the cron jobs running on the machine. If you use EC for cron jobs, you will certainly need to create and submit your own metrics to CloudWatch (or other metrics systems). AWS Lambda has a built-in metrics system thats more geared toward short-lived tasks like cron jobs. However, using CloudWatch can get expensive fast, and configuring the right alerts can be challenging for jobs that dont run often. To address this, the Serverless Framework provides pre-configured alerts that kick in when there is an unusual level of activity in your function, or when it generates a new exception. Independent of the metrics system you choose, getting visibility into how your cron jobs run and where they might have issues greatly reduces the risk of a job silently failing and impacting downstream services and infrastructure. EC vs Lambda: which one should you use for cron jobs? Weve covered all the ways in which AWS Lambda and EC differ in running cron jobs. Both of these services are cloud-native ways to automate tasks in your infrastructure. Deciding which one is the right choice for your company and your team depends on your particular use case, and whether it fits well with what AWS Lambda can do. If AWS Lambda can run your cron jobs without problems, it is very likely to be a more cost-effective and more easily manageable solution. And if you use Serverless Framework with AWS Lambda, you also get an out-of-the-box solution for secrets management, a number of built-in alerts and metrics and a great developer experience. Theres definitely still a place for EC in running cron jobs when the tasks have specific requirements that Lambda cant support, such as long-running jobs, jobs that require access to special resources like GPUs, or jobs that are written in runtimes not supported by Lambda. For these use cases, youll need to create your own solutions in concert with other AWS services for deployment, secrets management and alerting. Links and references - A scheduled cron job example using AWS Lambda and Serverless Framework. - Serverless Framework schedule event. - Deployment best practices for Serverless applications. - Secrets management for AWS powered Serverless applications.",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fe6",
      "title": "Structuring a Real-World Serverless App",
      "content": "As your Serverless app starts to grow, you reach a point where you are trying to figure out how best to organize it. In this post we'll share some of the best practices for organizing and managing large Serverless applications. Here are a few things we'll be covering: - Organizing your services in repos - Organizing Lambda functions - Sharing dependencies - Sharing code between services - Sharing `serverless.yml` config - Sharing an API Gateway endpoint - Deploying an entire app A quick reminder on the definitions before we get started. An app is a collection of services. Where a service is configured using a single `serverless.yml` file. Organizing services There are some very passionate arguments on the advantages and disadvantages of a monorepo vs multi-repo setup. We are not going to be focusing on their specific details here. But we want to pick a setup that allows us to: - Share code and config easily between services - Make it easy to create and configure new environments from the repos - And make our deployment process as simple as possible For starters, Serverless effectively requires you to adopt the infrastructure as code paradigm. This usually makes it so that your AWS resources and business logic code end up being closely coupled. But often you can draw a line between the resources that get updated frequently and the ones that aren't. For example, your Lambda functions and API Gateway endpoints get updated fairly frequently. While, resources like DynamoDB, Cognito, or S are less likely to do so. Additionally, your Lambda functions have all your business logic code. They need to be able to share code and config easily between themselves. So if you are creating a Serverless API backend, you'll have roughly two types of resources: your infrastructure resources and the Lambda functions for your API endpoints. It often ends up easier to keep them in separate repos. Why? Most of the code changes are going to happen in the repo with the Lambda functions. When your team is making rapid changes, you are likely to have many feature branches, bug fixes, and pull requests. A bonus with Serverless is that you can spin up new environments at zero cost (you only pay for usage, not for provisioning resources). So, a team can have dozens of ephemeral stages such as: prod, staging, dev, feature-x, feature-y, feature-z, bugfix-x, bugfix-y, pr-, pr-, etc. This ensures each change is tested on real infrastructure before being promoted to production. On the other hand, changes are going to happen less frequently to the infrastructure repo. And most likely you dont need a complete set of standalone DynamoDB tables for each feature branch. In fact, a team will usually have a couple of long-lived environments: dev and prod (and optionally staging). While the feature/bugfix/PR environments of the Lambda functions will connect to the dev environment of the resources. Here's a little diagram to illustrate the above setup. !Real-world Serverless app repo structure Organizing your Lambda functions Now that we have our repos organized, let's take a look at how we structure the code for our Lambda functions. Sharing dependencies Let's start by looking at how you'll set up your dependencies. We'll be looking at a Node.js example here. Continuing from the above example, let's have a look at how the repo for your Lambda functions:  The first question you'll typically have is about the `package.jon` is  \"Do I just have one `package.json` or do I have one for each service?\". We recommend having multiple `package.json` files. You could use something like Lerna or Yarn Workspaces here but we are keeping things simple. We want you to be able to use this setup as a starting point for your projects and leave that option up to you. We use the `package.json` at the project root to install the dependencies that will be shared across all the services. For example, if you are using serverless-bundle to optimally package the Lambda functions, or using the serverless-plugin-warmup to reduce cold starts, they should be installed at the root level. It doesnt make sense to install them in each and every single service. On the other hand, dependencies that are specific to a single service are installed in the `package.json` for that service. In our example, the `billing-api` service uses the `stripe` NPM package. So its added just to that `package.json`. This setup implies that when you are deploying your app through a CI; youll need to do an `npm install` twice. Once at the root and once in a specific service directory. Sharing code between services The biggest reason to use a monorepo setup is to be able to easily share common code between your services. Alternatively, you could use a multi-repo approach where all your common code is published as private NPM packages. This adds an extra layer of complexity. This only makes sense if you grow to the point where different teams are working on different services while sharing the same common code. In this scenario, making updates to the common code can potentially affect a very large number of services and Lambda functions. For this case it just makes a lot more sense to host your common code in versioned packages. This allows teams to explicitly decide when they want to update the common code. In our example, we'll try to share some common code. Well be placing these in a `libs/` directory. Our services need to make calls to various AWS services using the AWS SDK. And we have the common SDK configuration code in the `libs/aws-sdk.js` file. For example, we might want to optionally enable tracing through AWS X-Ray across all of our services.  Our Lambda functions will now import this, instead of the standard AWS SDK.  The great thing about this is that we can easily change any AWS related config and itll apply across all of our services. Sharing `serverless.yml` config We have separate `serverless.yml` configs for our services. However, we might need to share some config across all of our `serverless.yml` files. To do that: Place the shared config values in a common yaml file at the root. And reference them in your individual `serverless.yml` files. For example, we want to be able to use X-Ray, we need to grant the necessary X-Ray permissions in the Lambda IAM role. So we added a `serverless.common.yml` at the repo root.  And in each of our services, we include the lambdaPolicyXRay IAM policy:  Sharing an API Gateway endpoint A challenge that you run into when splitting your APIs into multiple services is sharing the same domain for them. You might recall that APIs that are created as a part of a service get their own unique URL that looks something like:  When you attach a custom domain for your API, it's attached to a specific endpoint like the one above. This means that if you create multiple API services, they will all have unique endpoints. You can assign different base paths for your custom domains. For example, `api.example.com/notes` can point to one service while `api.example.com/billing` can point to another. But if you try to split your `notes` service up, youll face the challenge of sharing a custom domain across them. In our example app, we have two services with API endpoints, `notes-api` and `billing-api`. Let's look at how to configure API Gateway such that both services are served out via a single API endpoint. In the `notes-api`, we will export the API Gateway resources:  And in the `billing-api`, we will import the above:  This allows us to share the same endpoint across these two services. Next let's look at how to deploy our app. Deploying the entire app Our services have a couple of interdependencies. This adds a bit of a wrinkle to our deployment process. Let's look at this in detail. First deployment Note that by sharing an API Gateway project, we are making the `billing-api` depend on the `notes-api`. When deploying for the first time, you need to ensure the `notes-api` is deployed first. If both the services are deployed concurrently, the `billing-api` will fail simply because the ARN referenced in its `serverless.yml` does not exist. This makes sense because we havent created it yet! Subsequent deployments Once all the services have been successfully deployed, you can deploy them all concurrently. This is because the referenced ARN has already been created. Adding new dependencies Say you add a new SNS topic in the `notes-api` service and you want the `billing-api` service to subscribe to that topic. The first deployment after the change, will again fail if all the services are deployed concurrently. You need to deploy the `notes-api` service before deploying the `billing-api` service. Deploying only updated services Once your application grows and you have dozens of services, you'll notice that repeatedly deploying all your services is not very fast. One way to speed it up is to only deploy the services thatve been updated. You can do this by checking if there are any commits in a service's directory. Upon deployment, you can run the following command to get a list of updates:  This will give you a list of files that have been updated between the two commits. With the list of changed files, there are three scenarios from the perspective of a given service. We are going to use `notes-api` as an example: A file was changed in my service's directory (ie. `services/notes-api`)  we deploy the `notes-api` service. A file was changed in another services directory (ie. `services/billing-api`)  we do not deploy the `notes-api` service. Or, a file was changed in `libs/`  we deploy the `notes-api` service. Your repo setup can look different, but the general concept still holds true. You have to figure out if a file change affects an individual service, or if a file change affects all the services. The advantage of this strategy is that you know upfront which services can be skipped. This allows you to skip a portion of the entire build process, thus speeding up you're builds. A shameless plug here, Seed supports this and the setup outlined in this post out of the box! Summary Hopefully, this post gives you a good idea of how to structure your Serverless application. We've seen the above setup work really well for folks in production. It gives you enough structure to help you as your app and team grows. While still allowing you to retain the flexibility to make changes along the way. Give this setup a try and make sure to share your feedback in the comments below!",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fe8",
      "title": "Tencent Cloud and Serverless Join Forces to Bring the Serverless Movement to China",
      "content": "A few months ago we met with a team from Tencent Cloud to discuss how we could work together to bring the serverless movement to China, and we immediately realized that we both share a similar vision... that serverless is the future of the cloud. We also recognized that Tencent is more ambitious about serverless than other cloud providers. From their leadership team to their engineers, everyone at Tencent impressed us with their commitment to innovating within the serverless space. Today we are excited to announce that Tencent Cloud is joining our cloud provider partner program as a Premier Partner. If you dont know Tencent, they are the largest internet company in Asia, providing services for hundreds of millions of people via a wide range of products, like QQ and WeChat. Gamers know them as the company behind Riot Games and League of Legends. Under the covers these popular internet services, and many other similar services from a variety of companies, are powered by the Tencent Cloud. By partnering with an infrastructure provider with the ambition and talent Tencent has, together, our companies can deliver a serverless cloud experience like no other cloud has, all to empower developers and large enterprises alike. Our shared goal is to enable developers to innovate more than ever, by offering them more use-cases to run on serverless infrastructure. We will also strive to make the developer experience of those use-cases as simple as possible. We want every developer to be able to harness the power of serverless cloud infrastructure, so they can innovate more and manage less. For enterprises, our collaboration will yield many operational tools that will enable enterprise software teams to not only innovate more quickly, but more safely. We will give teams advanced monitoring and observability features. We will also offer powerful debugging capabilities, streamlined deployment pipelines, security automation and policy enforcement to ensure developer teams are following organizational policies. Though our partnership is new, we've already built some things you can use today. We released three open-source, Serverless Framework projects that make it easy to deploy some of the most common serverless uses cases onto the Tencent Cloud: Express.js apps; static websites; and REST APIs. !Express.js applications graphic with Tencent Cloud and Serverless logos Deploy Express applications to Tencent Cloud. No need to change your application code or learn new things. We've created a simpler experience for delivering serverless APIs via the new Serverless Express.js Component. In seconds, you can deploy an auto-scaling, pay-per-use Express.js application on Tencent Serverless Cloud Functions. !Serverless Website component graphic with Tencent Cloud and Serverless logos We have something for front-end developers too. In seconds, you can deploy auto-scaling, pay-per-use, serverless websites, React and Vue.js applications. !Graphic of the Tencent Cloud Provider With very little configuration, you can now also easily deploy REST API endpoints that take advantage of the full range of Tencent API Gateway features (e.g. usage plans and throttling). You can also easily deploy Tencent Serverless Cloud Functions as scheduled jobs, and react to events on Tencent Cloud Object Storage, Tencent Message Broker and Tencent Cloud Kafka. As you try out each of these open-source projects, tell us how to improve your experience by logging issues and feature requests in the related Github repos. Our next steps will be to enable more use-cases to be built on serverless infrastructure, like types of data processing pipelines, AI and machine learning, workflow automation and more. Additionally, we will start to focus on advanced monitoring and debugging features, so developer teams have the appropriate level of observability to bring these use-cases into production, and the debugging power to identify the root causes of issues and fix them quickly.",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fea",
      "title": "Migrating Monitoring from IOpipe to Serverless Framework Pro",
      "content": "Overview You may have seen recent news that IOpipe was acquired by New Relic. As part of that process, IOpipe customers have to make changes in the next days to get any form of monitoring on their applications before New Relic sunsets the old product. So what better time to try the Serverless Dashboard for free? You'll get automatically-instrumented monitoring that requires no changes to your current function code along with CI/CD, deployment safeguards and more. With a quick account sign up and a two line change to your existing `serverless.yml` file you can setup monitoring (and other features!) for all your Serverless Framework services written in Node or Python for AWS. Interested? Let's see how to do it. Step - Prerequisites and Assumptions I'm going to assume that: You're using the Serverless Framework CLI v.. or later to deploy your applications You have AWS keys configured locally that you use to deploy your services You're ready to remove IOpipe (as it will soon be unsupported) Step - Signing Up for Serverless Framework Pro (For Free!) Serverless Framework Pro is free to use for up to a million monthly Lambda function invocations. Just create a dashboard account here. During the account creation process all you need to do is create an `app` and call it something related to the services you're trying to instrument. As part of this process you'll also create an `org` but more on that in a moment. Step - Configuring Your Service After you've created the account successfully, you can navigate into your service directory and open up your `serverless.yml`. At the top of the file add these two lines:  An `org` is an organization that your applications and services are contained within. You can have multiple `org`s or just one. You can also use organizations to help manage different teams with different levels of access to different services. The `app` is the applications that your services will be contained within. This might be a particular new product, or services that are somewhat related to each other. But think of this as an easy way to group Serverless Framework services together in a logical way. You will need to replace `theorgname` above with the name of your organization. To get that value you can click the upper right dropdown menu in the Serverless Dashboard and look for the value listed under \"Current Org\". You can also check the URL bar when signed into the dashboard and you should see the organization name right after `/tenants/` in the URL. To get the app name, you can either use the name of the application you just created in Step or you can create a new one with a name to match whatever application you're about to deploy. Step - Deploy Your Service After finishing Step , go ahead and try running the `serverless login` command. This should prompt you to login to the Serverless Dashboard and then automatically configure some access keys locally on your machine to deploy using the Dashboard. If you have any trouble with this make sure to upgrade your version of the Serverless Framework and feel free to take a look at the Getting Started guide. When this command completes you should be all set to run `serverless deploy` in your service directory and deploy your new service to AWS along with all the monitoring and debugging tools already baked into Serverless Framework Pro. When running `serverless deploy` don't be alarmed if you see a few new outputs or warnings in a Safeguards section - those are some of the automatically instrumented features that you're getting to help you align with serverless deployment best practices. Step - Monitor Your Service Then just test your function with `serverless invoke` or something like Postman to test out HTTP API endpoints. Once you fire off your first invocation after this new deployment just open up the Serverless Dashboard and review the function. Navigate into the application you just deployed your service in (the `app` value). You can do this by clicking on the tile for that application. For example - the `backup-app` in this screenshot: !The app tile view of the Serverless Dashboard Then, under the service name (in this case `demo-email-form-backup`) click the stage you just deployed to (be default the Serverless Framework deploys to the `dev` stage). !Where to find the explorer tab in the service view After this, you'll have a lot of options to monitor the service, configure notifications, review automatically-generated alerts, and review deployment history. But to review the recent function invocation just click the explorer tab: !The service view of the Serverless Dashboard with the explorer tab highlighted to click on From there, you'll be able to use the invocations explorer to review all of your function invocations. If your function has only run once (with that single invocation from earlier) then you'll probably only see one of the function invocations below. But you can use this view to sort through massive numbers of Lambda invocations to find the ones you're looking for using powerful search filters to find specific kinds of errors or other characteristics that you might be looking for. !The explorer view of the Serverless Dashboard When you click on the invocation you're debugging, you'll automatically have a variety of debugging information included. A bunch of the function metadata, memory utilization, tracing information for any AWS SDK calls or HTTP requests from common HTTP libraries, and the raw CloudWatch log details. !A successful function invocation with a long HTTP span You can use these details to help debug and optimize your functions. In the invocation above, for example, I might realize that I need to optimize a long-running HTTP call and see if there are ways for me to migrate to a better API provider or, if I manage the API behind that call, if I can optimize that service further. For invocations that fail, you'll also get automatic details about the error including a stacktrace to help direct you to the location of the failure in your code. !A failed function invocation with a stack trace The stack trace can help you jump immediately to the issue, fix it quickly, and move on with your development. Conclusions A migration from a core development tool like a monitoring solution can be tough, but we think that for those already working with the Serverless Framework that this is one of the easiest ways to get out-of-the-box monitoring and more for your applications. We're always trying to think about how the features we build can help meet your development needs and save you time previously stuck debugging. Have a question about how we compare to IOpipe or want more details? Leave a comment below or take a look at all the features included in Serverless Framework Pro.",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fec",
      "title": "Take the legwork out of API Gateway troubleshooting",
      "content": "Troubleshooting API Gateway Errors Debugging serverless applications requires a mind shift: in a traditional application stack, the entire request-response cycle exists in your application logs. With serverless applications, thats not the case. You have to connect the dots between API calls and AWS Lambda functions yourself, and that can involve a lot of thankless legwork. In this post, well show you how the Serverless Framework dashboards latest troubleshooting feature can eliminate the legwork, whether youre new to serverless application development, or an old hand. Misconfigured API Gateway Lets start with a common scenario: say you want to check the performance of a function youve written. You open your Serverless Framework dashboard, but you dont see an invocation for that function was invoked. You hit the API endpoint that should trigger it, and it responds with a status code: service unavailable. Now, errors can be hard to diagnose. Often, s are due to misconfigured API Gateways. That can be caused by an incorrect indentation in your Serverless.yml file, typos or spacing errors, or referencing a function that no longer exists. With multiple possible causes, you need the request and invocation logs to determine what actually happened. Getting the logs may not be a straightforward operation. To begin with, APIGW logs and Lambda function logs are not stored together. Remember that mindshift? You have to go into CloudWatch, where all your AWS logs are collected, and look for the logs separately. Its an annoying inconvenience if you have one API endpoint and one Lambda function in your application, but if youre using an API Gateway, and its being hit with a few hundred requests per second, its a real problem. You can search by request ID, but customers dont always include a request ID when they report an issue. Finding a log without a request ID is when the thankless legwork begins. With the Serverless Framework dashboards new requests explorer, you can sidestep that painful process and go straight to the logs and stacktraces you need. The explorer lets you locate requests by endpoint, method, status code and time range. Each request opens a detailed report, where youll find the log, and a link to the relevant function invocation report. You can skip the searching and focus all your time on the fun part: solving the problem. !Serverless Insights.png) Malformed Response If youve been developing serverless applications for a while, and youre good at avoiding the common mistakes that can trigger misconfigured API Gateways, you can still find yourself chasing down APIGW errors. Heres an example: Say that you log out a Lambda error so you can track it, but you forget that you have to respond to the APIGW. Again, that mind shift you need to make - your Lambdas have to be properly linked to your APIGW and you have to connect those dots yourself when you make a change to your Lamdba: the returned object needs a statusCode, body and headers attribute. If you leave one out, or format it incorrectly, the APIGW will return an Internal server error when you hit it. Using the requests explorer, you just ask for requests to this endpoint with status codes, and in seconds, youll be inside the log, youll see the malformed lambda proxy response message, and youll be refactoring your Serverless.yml. Without the requests explorer, youll still arrive at resolution, but if youre working at scale, and with an APIGW, youll be taking the long road to get there. Try the Serverless Frameworks requests explorer today, and take the legwork out of debugging your serverless applications!",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603fee",
      "title": "The State of AWS Lambda Supported Languages & Runtimes (Updated November )",
      "content": "Overview In the last few years a lot has changed with AWS Lambda supported languages and runtimes. With all of these changes there are some older runtimes reaching the end of their supported life and several new runtimes and new features to think about taking advantage of. Real quickly, let's make sure we're all on the same page with our terminology. By a `supported language` I'm talking about something like Node.js, Python, or Java. By a `runtime` I mean a specific version of that language like Python . or Java . Now let's take a look at some of the upcoming changes and what they mean for you. Managed Runtimes When AWS Lambda was launched you could only pick from a small set of AWS Lambda runtimes for your functions. While you could try and hack the underlying container somewhat to support additional runtimes, you ended up doing that at your own peril. Because of the popularity and demand for AWS Lambda, support for additional languages, and newer languages versions quickly followed. As of right now, there are several runtime versions of Node.js, Python, and Java. You can also use the Ruby, Go, and .NET Core runtimes. All of these runtimes don't require you to do anything other than specify what runtime you're looking for and you're good to continue writing your code, configuring your events, and pressing onwards. New Managed Runtimes In the last week or so, AWS also launched three new supported runtime versions for AWS Lambda: Node.js , Python . and Java . This means that you can take advantage of new language features and performance improvements in these runtimes just by reviewing the compatibility of these new versions and migrating your function code over. And of course, the Serverless Framework already has support for these runtimes! Just set your `runtime` in `serverless.yml` to `python.`, `nodejs.x` or `java` and you'll be good to go! For more information on these supported runtimes and AWS Lambda you can check out our Ultimate Guide to Lambda. So what's new with these runtimes? First of all, all these new runtimes rely on an Amazon Linux execution environment so keep this in mind if you're interacting with the operating system layer and might rely on something that differs between the OS versions. Additionally, each of these runtimes brings with it a bunch of new language features - let's take a look: Python. releases - Assignment expressions allow you to assign variables within expressions using the new `:=` also known as a \"walrus operator\"  The above would print `True` but requires a separate statement for the variable assignment. Now, you can use assignment expressions to do the same thing in a single line:  - Positional-only parameters introduce a new syntax with the `/` character to require some function parameters be specified positionally and prevents them from being keyword arguments. This can be combined with the `` character a function definition to require named arguments. For example:  When calling the function above, you must always provide the `positional_argument` positionally. The `positional_or_named_argument` can work either way and the `named_argument` must be named when you call the function. This allows you to change positional arguments at a later time and reserve some keywords for future possible arguments in your functions. - There are additional features and performance improvements for the `csv` module that might benefit folks using Lambda to parse csv files. - The new, fully-supported `asyncio.run()` method is now a stable part of the language to help manage coroutines A full summary of Python . release details can be found here. Node.js .x The Node.js .x runtime is the current Long Term Support (LTS) version of Node. This means that using it for current functions will give you the longest stable life for your code. You'll also get any of the minor updates as AWS pushes them into the runtime for security and performance reasons. Here are some of the new features and improvements in Node.js : - V has been updated to . and may be upgraded to . later in Node 's lifetime - Support for TLS . and other TLS improvements - A new experimental feature called a \"Diagnostic report\" that will let you generate a report on demand or when certain events occur - Potentially significant startup improvements that may give \"a ~% speedup in startup time for the main thread.\" You can review the full release details here. Java - This version introduces a native HTTP Client API that can make interacting with HTTP services easier - It also has additional support for TLS . - You can now use `var` to declare local variables (introduced in Java ) - Additional support for security and cryptography and various other language improvements and optimizations You can read more about the Java release details here. Developer-Provided Runtimes In addition to all the benefits of the AWS-managed runtimes, AWS also has support for brining your own custom runtimes. This process was enabled by the Lambda Layers feature which allows you to share code and dependencies across functions more easily. Using Lambda Layers you can also bring your own custom runtime like Rust or PHP. This is also supported using the Serverless Framework by using the `runtime` value of `provided` in your `serverless.yml` files and creating a Lambda layer that contains this runtime. The process of creating the layer is slightly different for each custom runtime you might want to work with, but an example of your `serverless.yml` file for PHP might look like this:  If you'd like an example on creating a PHP Lambda layer, you can take a look at this guide by Rob Allen. Runtime Deprecations With new runtimes coming out all the time you also have to watch out for the other side of the coin - runtime deprecations. AWS also has a few runtimes scheduled for deprecation in the coming months. You can always keep tabs on the deprecation schedule here, but it isn't always updated. Node.js . As of November , the next deprecation appears to be Node.js . and AWS plans to prevent creating new Node.js . functions on Jan , and prevent updating them on Feb , . It is probably a good idea to start testing and migrating functions in Node.js .x at this point. Python . Additionally, while Python . isn't listed on the runtime support policy page that AWS provides, it is ending it's supported life on Janurary , and AWS has mentioned elsewhere that a similar deprecation pattern will happen at that time. For each of these deprecated runtimes, existing functions should theoretically continue to respond to invocations - you just wont be able to update them after the final deprecation. Moving off these old runtimes is highly recommended. But if for some reason this is completely impossible you might want to consider how Developer-Provided Runtimes could technically act as a loophole to extend your ability to work with code written for these runtimes for a little longer.",
      "__v": 0
    },
    {
      "_id": "64e0892ab72e199dda603ff0",
      "title": "Resolve Serverless Errors the Easy Way- with Tags",
      "content": "Debugging Serverless Application Errors When a customer reports an issue that you cant replicate, but you still need to solve, what do you do? You can release a change with a log statement, then start monitoring logs to try and catch the customers function, and hope you see the error again. That can be a long and fruitless task. Weve been there ourselves! Thats why weve added tags to the Serverless Framework dashboards debugging tool, the explorer. !Tags in dashboard Our new tag filters can do the tracking for you. Now, when youre trying to debug an issue thats hard to track or hard to replicate, try this: log when youre in the error state, tag that log line, and output a JSON object that represents some state of your application. Open the explorer, filter on that tag and view the invocations the explorer finds for you. !Tags in dashboard Heres an example of how weve used it; the API that powers the Serverless Framework dashboard is monitored within the dashboard (yes, we eat our own dogfood). One of our customers reported a query error, but no actual errors were being raised. We created a tag, and used it to find their invocation logs, so we could study the state of our application during the time their functions were invoked. Instead of trying to track their invocations ourselves, we let the explorer do it. The time we spent was entirely focused on understanding the issue, not hunting for logs. If you want to try tags in the explorer, youll first need to add some to your Serverless.yml file. Tagging with NodeJS If youre using NodeJS, update serverless with npm update -g serverless Add this code to your Serverless.yml file:  Tagging with Python If youre using Python, add this:  If youd like to learn more about tagging Lambda functions, check out Jeremy Dalys blog post, How to tag your Lambdas for smarter serverless applications\". Learn more about debugging with the explorer, and open up your Serverless Framework Dashboard and explore tags for yourself!",
      "__v": 0
    },
    {
      "_id": "64e0892bb72e199dda603ff2",
      "title": "New Lambda Features - Pre-re:Invent - Nov.",
      "content": "Lambda Updates Galore! The AWS Lambda team has been busy sneaking in a bunch of updates before re:Invent this year. White I recently wrote about all the new runtimes that Lambda now supports, they've also added several substantial new features that allow sending asynchronous Lambda results along to a destination, having SQS FIFO queues as a Lambda event trigger, and providing greater control over how we interact with DynamoDB and Kinesis streams. Let's take a look at the pre-re:Invent Lambda feature announcements as of Nov , . AWS Lambda Destinations and Asynchronous Invocation Improvements AWS just recently announced the ability to have destinations for asynchronous invocations. This feature allows asynchronous Lambda functions to have their execution results sent to other AWS services like SNS, SQS, EventBridge and other Lambda Functions without having to wait around for the result to finish. So how exactly would this work and why does it matter? Imagine you want to be able to submit some tasks or jobs without having to wait to see if they succeed or fail, but you still want to be able to have some logic around what happens depending on how they go. Well, previously your best bet was either to orchestrate all that logic together in a single Lambda function or to use something like Step Functions. But now, AWS Lambda Event Destinations give you an arguably cleaner way to pull this off. When you invoke a function asynchronously you will rapidly retrieve a success or failure result to the caller. That initial success or failure indicates if the AWS Lambda function was successfully queued up and will almost always be a success. However, the actual result of that function might be something you want to handle and do something with. This is where event destinations come in: On success, you could send the results from an asynchronous Lambda to a downstream Lambda function to start another process, queue the results in SQS for some other worker, use SNS to create a fan-out style architecture, or send the results to EventBridge. All of these destinations add significant flexibility to how data flows through your architecture. On failure, destinations are also designed as a \"more preferred solution\" to SQS Dead Letter Queues (DLQs) because they offer additional function execution information like stack traces and can send to other destinations outside of SQS. In combination with newly released features that offer control over the maximum number of retries and the maximum event age for asynchronous Lambda functions this adds a lot of flexibility for error handling. So Why Does This Matter? Considering the relative expensiveness of services like Step Functions, Event Destinations seems to be an excellent way to reduce both the complexity and cost of your serverless applications. It should allow you to nuanced workflows that were previously reserved for folks who were either willing to write that nuance into custom Lambda Functions, or who were willing to pay for and create Step Function workflows. That's not to say Step Functions has no place, it is still a great tool to visualize and manage complex workflows, but for more simple architectural needs Event Destinations seem like a great fit. SQS First-In-First-Out (FIFO) Queues AWS has supported SQS standard queues as Lambda event sources for some time now, but they recently added support for First-in-first-out or FIFO queues. This means you can now have your SQS FIFO queues process data inside them in the same order that it went into those queues. So Why Does This Matter? In combination with the AWS Lambda Destinations, and the new support for configuring maximum retries and maximum event age this should start to provide powerful new ways to process data in or out of sequence with effective retry and failure logic. This can be critical in applications like auctions or ticket sales applications that need to review queued tasks sequentially in order to ensure that the system doesn't inadvertently produce some sort of conflict like overselling tickets or incorrectly processing bids. Improvements to Lambda Stream Interactions A very common Lambda use case is to process DynamoDB Streams or Amazon Kinesis Streams. Because of this, they've recently added more support for different ways of handling how Lambda functions processes these event sources. Stream Failure Handling AWS has just released new ways to handle failures when processing a stream. Previously, Lambda would try to process all the records in a batch and if it failed it would stop processing the data, return an error, and retry the entire batch of records until they are successfully proceed or they expire. Now, AWS is offering us several new methods of dealing with streams. Bisect on Function Error - When enabled, you can break the failed batch of records into two chunks and retry them separately. This will allow you to isolate where the bad data is and process the rest of the data successfully. Maximum Record Age - You can have your Lambda Function skip processing data records that are too old by using a a Maximum Record Age property between seconds and days. Maximum Retry Attempts - You can set another configuration property to specify how many times you actual want to retry processing - anywhere from to , retries. I will admit, I am very curious about the application that succeeds only on the ,th attempt. Parallelization Factors Additionally, for both DynamoDB and Kinesis Streams you can use a new \"parallelization factor\" that allows you to process DynamoDB and Kinesis shards with more than one concurrent Lambda function at a time. This can be very useful when data volumes are larger or processing records takes time. The parallelization factor can be set from to and is used to calculate the number of concurrent Lambda invocations allowed by multiplying the number of data shards. So, for a Kinesis stream with shards and a parallelization factor of you can use ( ) or concurrent Lambda invocations to process the data. So Why Does This Matter? Instead of writing your own complex error handling and retry logic to process common Lambda stream sources, you can start to incorporate AWS's new provided settings to do many of the same things. When combined with the new Event Destinations and improvements to handling asynchronous Lambda invocations results this will offer you a lot more flexibility in your Lambda workflows without having to write a bunch of custom data processing logic. Additionally, the new support for the parallelization factor means that you can significantly scale up the processing power for your streams without having to do something expensive like dramatically increasing the number of Kinesis Streams Shards. What's Next? With all these announcements happening before re:Invent, I can't wait to see what happens during the conference itself! Keep your eyes out for these AWS Lambda features making their way into the Serverless Framework soon! Are you attending re:Invent ? Want to get in touch with us? Sign up for our Happy Hour!",
      "__v": 0
    },
    {
      "_id": "64e0892bb72e199dda603ff4",
      "title": "EventBridge Schema Registry -- what it is and why it matters for Serverless applications",
      "content": "With the Midnight Madness event, the preInvent season is over and re:Invent has officially begun. AWS dropped a big announcement in the form of the EventBridge schema registry. This handy tool helps to document your event schemas, making it easier to share knowledge across teams. In this post, we'll look at: - Why you need a schema registry - What is the EventBridge schema registry and how does it work? - What are the remaining questions and issues with the schema registry Let's get started! Why do you need a schema registry? Before we get too far, let's understand why you would need a schema registry. First off, serverless architectures often are event-driven architectures. In event-driven architectures, different aspects of your business logic is triggered by the occurrence of events. Events can come from a variety of places. If you have a number of microservices in your application, you can use events to communicate across service boundaries, allowing for more loosely-coupled applications. If you're working with managed service offerings, such as DynamoDB with DynamoDB streams, you will receive an event whenever a record is updated. You can also use AWS services like EventBridge to receive status updates on things like ECS tasks starting or stopping or for resource creation notifications. Finally, you might receive third-party events such as webhooks from providers such as GitHub, Zendesk, or Stripe. These are similar to events from managed AWS services but a little less infrastructure-specific -- they may indicate a more application-centric event that occurred. Whatever the event type you're acting upon, your application will need to know the basic structure of the event. In the past, I've found that the documentation of events can be poor -- maybe they get updated once but never again. Maybe only a few fields are listed but not all of them. Types and required fields are barely listed, as well as the semantic meaning of the fields. This is a real problem! If these events are the main communication mechanisms across services and/or teams, you will need some understanding of what's available in there. Enter, the EventBridge schema registry. What is the EventBridge schema registry and how does it work? Amazon EventBridge is an event bus for shuttling events between systems and allowing reliable pub/sub functionality. You can send in your own application events, subscribe to AWS system events, or toss in third-party events. The EventBridge schema registry is a new service for automatically detecting and storing schemas for your events. This is pretty powerful and can help overcome the internal inertia around documenting events. There are two big features of the EventBridge schema registry that I find particularly powerful. First, the schema detection is automatic. This is pretty big -- you don't have to count on developers writing their schemas. Second, you can download code bindings for your discovered schemas. This is pretty huge, as you'll have an actual model that you can code against. For typed languages like Typescript or Java, you can get nice code completion in your IDE to help you work against that schema. That's a big win! What are the remaining questions about the schema registry? While the EventBridge schema registry is a pretty cool feature, there are still a few questions and issues you should think about. First, while the automation is useful, it probably won't be able to capture everything about your schemas. For example, even if you have a particular property defined as an integer, it may not be clear the meaning behind that number. If your OrderPlaced item has an \"Amount\" property, is that the amount before sales tax or after? Including discounts? With shipping? At some point, you have a responsibility as a developer to document this stuff beyond what can be automatically generated. Similarly, the automated schema detection may have trouble handling optional fields. If I have a property that's optional, will that \"detect\" a new schema version when I publish an event without it? This isn't really a new schema -- it's just a different example of an event that conforms to the schema. Second, schema detection can get expensive. It's currently priced at $. per million events ingested for discovery. Depending on how 'event-happy' your application is, that can get expensive. I think the AWS blog post announcing the schema registry has a good suggestion -- enable schema detection in testing environments but not in prod. I think this buys you a few wins: Your test environment will have lower traffic, so it won't cost as much. You can use the schema registry almost as an integration test to confirm that new events published conform to the schema. Finally, I would recommend using the schema registry as a starting point but not an ending destination. Use the schema registry to bootstrap the initial definition of third-party events but pull them into your documentation somewhere and add additional context to the event. This can reduce some of the initial labor while still adhering to best practices. Conclusion Event-driven architectures are growing in popularity, and we're still learning how to work with them. We don't have a tool like Swagger / OpenAPI for events yet that describes the shape of events and how to handle version changes. Tools like the EventBridge schema registry are definitely a step in the right direction. The sooner we can get developers thinking about events and schemas, the better they'll be in the long run as they work to integrate these events across their systems. I'm really excited to see how the schema registry and EventBridge evolve going forward.",
      "__v": 0
    },
    {
      "_id": "64e0892bb72e199dda603ff6",
      "title": "All the Serverless announcements at re:Invent",
      "content": "re:Invent is about to begin! And there is ever so much to track. If last year is any indication, we expect AWS to have a long list of serverless-centered announcements and launches. If you want to keep up, you've come to the right place. We're watching all the keynotes and announcements live as they happen, and compiling the \"what it is\" and the \"why it matters\" right here. Updating live all week! re:Invent announcements Latest: AWS API Gateway v for HTTP RDS Proxy Provisioned Concurrency S Access Points Amazon Managed Cassandra Service Pre-Invent Lambda Updates Eventbridge Schema Registry Most Exciting: AWS API Gateway v for HTTP RDS Proxy Provisioned Concurrency Pre-Invent Lambda Updates Eventbridge Schema Registry AWS API Gateway v for HTTP What it is Overall top down improvement of API Gateway in almost all aspects Why it matters While API Gateway v has served us well it hasn't been perfect. v solves some of the issues users have had with v such as improved latency, reduced cost, better CORS integration and support for JWT's amongst others. We have a detailed blogpost going through all the changes, so if you want to know more go check that out RDS Proxy What it is Moves connection handling from the Lambda layer to the RDS Proxy layer to help curb the issues around reaching connection maximums. Why it matters RDS has been a difficult service to use with Serverless for a number of reasons, and AWS has made numerous changes lately to try and correct these issues. This is one of the last and solves the problem produced by each Lambda function instance creating its own connection to the database. When at high Lambda concurrency this means you cna easily overwhelm a database with too many connections and essentially make the database inaccessible. Read on about the changes with RDS proxy in our very own detailed dive... Provisioned Concurrency What it is You can now spin up a pre-defined number of warm Lambda instances to bypass the cold start issue if latency is an issue Why it matters If you have workloads that are latency sensitive, then being able to have Lambda functions pre-warmed ahead of a certain known event that will invoked Lambda's means you can entirely bypass the cold start issue and make sure requests to Lambda will always begin execution as fast as possible. Read more about this new feature and how the Serverless Framework will support it at this blog post. S Access Points What it is An alternate method to specify access patterns to an S resource, whether thats a full bucket or just one key, as well as what that access is Why it matters S can get very complex to manage permissions for when you have many potential clients attempting to access that data. Being able to have a more specialised, app level permissions system makes managing this a lot easier and means things are more secure in the end for you and your data. Managed Cassandra What it is Cassandra is a very popular free and open-source, distributed, wide column store, NoSQL database management system designed to handle large amounts of data now fully managed on AWS Why this matters DynamoDB is great, but I have heard from many users already using Cassandra that don't want leave it behind. Now users can get the best of both worlds and communicate to a fully managed Cassandra backend from their Lambda functions and have it be managed for them just like a DynamoDB table with provisioned or on demand modes available and no need to manage clusters. !Managed Cassandra Announcement Pre-Invent Lambda updates There were a bunch of updates to Lambda released just before RE:Invent started. These include: AWS Lambda Destinations and Asynchronous Invocation Improvements SQS FIFO Queue trigger support Improved stream interaction; error handling and parallelization What it is AWS has made a few good changes to Lambdas right out of the gate by allowing for more flexibility in the Serverless architectures you build by tweaking the relationship between some of the event triggers and Lambda Why it matters These seemingly small tweaks open up a much broader set of use cases that maye have previously not been possible. All three changes listed add the capabilties of managing the interactions of three very useful event triggers in additional ways to meet the needs of users, and help make existing workarounds unnecessary. We have a dedicated blog post talking about these changes in detail, so please go check that out for the full skinny.. Eventbridge Schema Registry What it is The schema registry will scan the structure of events you send and receive in Eventbridge and document their structure automatically for you Why it matters Managing events and their structure is always a tricky business and traditionally relies on developers taking the time off of building things to document things. While Eventbridge is not a panacea its a good bootstrapped step. Check out the full blog post for all the details.",
      "__v": 0
    },
    {
      "_id": "64e0892bb72e199dda603ff8",
      "title": "Amazon RDS Proxy makes it easier to use SQL in Serverless",
      "content": "For Serverless users, preInvent and re:Invent has been like Christmas. We're getting a ton of new tools to help build Serverless applications, from Lambda Destinations to async invocations, to the EventBridge schema registry, to a ton of different ways to manage stream processing in Lambda. All of these are great, but the most impactful announcement for many users may be the Amazon RDS Proxy. This service, combined with some other recent improvements from AWS, makes it much easier to use relational databases in AWS. In this post, you'll learn the what and why about the Amazon RDS Proxy. We'll cover: - Why relational databases have been hard in Serverless - How Amazon RDS Proxy and other improvements are making relational databases work in Serverless - How to think about choosing a database in Serverless Let's get started! Why relational databases have been hard in Serverless The database question has been one of the bigger issues in Serverless for a few years. We've written on that numerous times, including posts on the data layer in Serverless, choosing a database in serverless, and why we were so excited about Aurora Serverless. Two years ago, there were a few major problems with using relational databases in Serverless applications: - Pricing model: Relational databases were priced hourly by instance size, whether you're using it or not. - VPC cold-starts: Relational databases should be network-partitioned in a private subnet of your VPC where it cannot be accessed from the public internet. However, this means that your Lambda functions need to be in a VPC, which meant occasional cold-starts of up to seconds. - Connection limits: Relational databases were built for an era of a lower number of long-running compute instances. They don't fit this world of a high number of hyper-ephermal compute instances. AWS Lambda users may run into connection limits when trying to connect to their relational databases. Fortunately, AWS has been listening, and each of these issues has been addressed. How Amazon RDS Proxy and other improvements are making relational databases work in Serverless In the past two years, AWS has worked hard to make relational databases work better in Serverless applications. First, AWS released Amazon Aurora Serverless. This is a serverless version of the proprietary Amazon Aurora database that can automatically scale up and down according to your usage. This release helped with the pricing model issues around using a relational database. Second, AWS announced improved VPC networking for AWS Lambda functions. This update greatly decreased the coldstart latency for Lambda functions that use a VPC. This makes it more acceptable to use VPC Lambda functions in user-facing applications. Finally, the Amazon RDS Proxy announced today handles the connection limits. Rather than managing connections in your Lambda functions, you can offload that to the Amazon RDS Proxy. All pooling will happen in the proxy so that you can handle a large number of connections in a manageable way. How to think about choosing a database in Serverless Given the updates in the last few years, how should Serverless developers go about choosing a database for their application? I don't think there's a clear answer, and these recent updates have made the decision more difficult. Lots of folks will reach for a relational database due to its familiarity, and that's fine! Making the shift to Serverless is a big change in itself, and using familiar tools like a relational database can ease the journey. That said, there are still a few issues with relational databases in Serverless applications. The pricing model is _better_, but it's still not perfect. Amazon Aurora Serverless scaling isn't as quick as it needs to be, and you're still not getting anything like the DynamoDB On-Demand pricing. Additionally, the Amazon RDS Proxy is priced on an hourly basis. Taking it a step further -- I'm increasingly convinced that you give up a lot of agility as soon as you add a single element to your architecture that is not pay-per-use. When all of your architectural components are pay-per-use, it makes it seamless to spin up temporary environments for development or testing. As soon as you add slower-moving, hourly-billed components to your infrastructure, it increases the cost and reduces the speed in which you can deploy test environments. For some, this is a totally reasonable tradeoff to make. The learning curve of understanding DynamoDB data modeling may not be worth the increased agility for development environments. Just be sure you're aware of the tradeoffs you're making as you're choosing your database.",
      "__v": 0
    },
    {
      "_id": "64e0892bb72e199dda603ffa",
      "title": "Provisioned Concurrency: What it is and how to use it with the Serverless Framework",
      "content": "AWS Lambda pretty much single-handedly kick-started the serverless movement we find ourselves in. A compute service with automated scaling and complete elimination of machine or container maintenance. However, some of the characteristics of the service made it a little less than desirable for certain workloads. If you were trying to use Lambda in a use case that was very latency sensitive, cold starts were probably your greatest concern. Cold starts have also been the biggest issue pointed out by detractors of service as to why you need to be cautious about adopting Lambda as your primary compute platform. However, thankfully, AWS has heard the concerns and has provided the means for us to solve the problem. If you have just deployed a Serverless service, or none of your functions have been invoked in some time, your functions will be cold. This means that if a new event trigger did occur to invoke a Lambda function, a brand new micro VM would need to be instantiated, the runtime loaded in, your code and all of its dependencies imported and finally your code executed; a process that could take - ms (or longer depending on the runtime you choose) before any execution actually started. However, after execution, this micro VM that took some time to spin up is kept available for afterwards for anywhere up to an hour and if a new event trigger comes in, then execution could begin immediately. Before now, if you were trying to use techniques to create warm Lambda instances, this was a tricky exercise. It was a little difficult to exactly control how many warm instances you wanted simultaneously and you then had to execute the Lambda you wanted with some kind of branching logic that determined whether this was a warm up execution or an actual execution. It was rather ugly. But it helped folks step past the cold start issues to some degree. However, AWS has now launched Provisioned Concurrency as a feature. It does pretty much the same thing as those Serverless Framework plugins that try to keep a certain number of warm functions running by allowing you configure warm instances right from the get go. In addition, there are no code changes needed. All we need to do is set a value as to how many provisioned instances we want for a specific function, and the AWS Lambda service itself will ensure to always have that quantity of warmed instances waiting for work! Combine this with the auto scaling features of Lambda and we now have the means to respond rapidly to traffic as well as automatically accommodate more traffic as it comes in. AWS Console This setting can be made very simply in the AWS Console. Go to the function in the Lambda service, scroll all the way to the bottom and set it at what you want the minimum provisioned concurrency to always be. Easy as that. Serverless Framework Of course, we dont really want to dip into the console if our service is built with the Serverless Framework, so instead, we can change one setting on our functions definition to add provisioned concurrency to that function.  In the example above, the `hello` Lambda function will always have warm instances ready to go to handle incoming HTTP requests from API Gateway. AWS API However, it doesnt end there. You can even go so far as to write a simple Lambda that you run on an hourly basis with a pre-determined schedule in mind. If you are like a lot of organsiations, you will have busy spikes you know about well in advance. In these situations, you may not want `provisionedConcurrency` all the time, but you may want it during those known spikes. Provisioned Concurrency can be set via the AWS SDK:  Now you have the means to schedule the provisioned concurrency whenever you choose and so optimise the cost efficiency of it. Conclusion This single feature released by AWS gives those needing greater control over ensuring lower latencies exactly the tool they needed while keeping Lambda Serverless. You are not provisioning hardware or networks, runtimes and operating systems, but tweaking the settings that directly affect the end result in a measurable and predictable way. And this single feature opens Serverless up even further to more use cases and makes it far more competitive in the world of application development.",
      "__v": 0
    },
    {
      "_id": "64e0892bb72e199dda603ffc",
      "title": "re:Invent  - AWS API Gateway v for HTTP",
      "content": "AWS just announced support for HTTP APIs using API Gateway v. While API Gateway v has technically been around for a little while, until today we've only been able to use it create Websocket APIs. Included in this announcement are a variety of new features and performance improvements over the existing HTTP Gateway functionality. Let's take a look at some of the details. What Are the Changes in V? Price Changes First up, there are some very impressive pricing and performance numbers coming in with this new release. Off the top, AWS says that the general cost of using v will be % cheaper and have % lower latency than v. Because current pricing for v is about $. per million requests it seems v should be about $ per million requests, a pretty substantial change. That means that API Gateway, previously one of the larger cost sticking points for serverless applications just got a serious upgrade in how likely people are to be able to adopt it cost-effectively. Resource Changes In addition to the general performance improvements, it looks like the sheer number of AWS resources that you need for an API is changing dramatically. The Single-Resource API API Gateway V is notoriously verbose for the number of resources you need to correctly configure even a simple HTTP API and apparently AWS knows it. In v, you have the option to create a simple, single-resource API endpoint with minimal configuration. This single-resource API can route all requests in and pass them along to a backend to handle itself. This sort of feature seems to indicate that AWS wants to make it easier to enable different kinds of serverless architectures beyond the microservice-style approach. The change opens up more use cases like Express.js, Flask, and other similar frameworks and tools that may prefer to setup routes themselves in the application code and not in the API Gateway configuration. Other HTTP APIs While this single-resource option exists, you do also still have the option to dive into new AWS Resources within v to setup configuration for things like JWT authorizers, API stages, and Open API (formerly known as Swagger) specifications for more complex APIs. Authorizers and JWT Configuration Yes, I just said JWT authorizers in more custom v APIs! When adding authentication to APIs created with API Gateway v you have a few different options ranging from API Keys to configuring things like Lambda authorizers. With v, HTTP APIs should have some direct support for JSON Web Tokens (JWTs). If you're familiar with JWTs, they are essentially encoded tokens with scope metadata that are signed cryptographically to verify and authenticate a user and the scope of access they should be granted. They're typically generated through a client-side process that is then verified by the backend of your service. With HTTP APIs through API Gateway v there was support for this through AWS Lambda Authorizers which took that incoming JWT, manually decoded it with a JWT library, verified it with the JWT provider and then passed through the Lambda Function ARN that the requesting function could then pass the request to. Now, at first glance, it looks with v we'll have the ability to use some AWS configuration to handle parts of this process for us without having to write these custom authorizers. AWS will then return the scopes to our application for us to determine how to allow the user to act. This means we don't have to write all the custom code in our Lambda authorizers ourselves! Cross-Origin Resource Sharing (CORS) One of the other features added in API Gateway V is more support for HTTP APIs returning CORS headers. This is essential when returning data in order to enable the requests to be returned to frontend applications on domains that don't share a domain name. What this new feature appears poised to do is to allow us to stop writing things like this in or Lambda functions:  And instead just return the body and have the Access-Control-Allow-Origin header taken care of for us for example. What Do the Changes Mean? AWS is investing heavily into enabling serverless applications that leverage API Gateway by making it cheaper, more performant, enabling more use cases, and reducing the amount of code we have to write to get common use cases taken care. What do you think of the announcement? Are there use cases that you see being enabled from it? Let us know or give me your thoughts directly!",
      "__v": 0
    },
    {
      "_id": "64e0892bb72e199dda603ffe",
      "title": "Deploying Your First Knative Service with the Serverless Framework",
      "content": "Deploying Your First Knative Service with the Serverless Framework One of the biggest ongoing conversations that I see when talking about modern microservice architectures is people asking \"Should I be running that on containers or serverless?\". Well, that's not entirely true. In fact, it is usually more of a vehemently opinionated response about why I should be using one or the other. My favorite example of this ongoing conversation is probably Trek's Serverless vs. Containers Rap Battle: The somewhat surprising conclusion (considering the format of the discussion) is that both approaches are perfectly suited to different use cases. How Kubernetes and Serverless Make Each Other Better We can take this a slight step further and say that both architectural patterns provide us insight on the limitations and potential improvements of the other. Kubernetes has a reputation for operational complexity that Serverless infrastructure like AWS Lambda aim to eliminate entirely. The broader community around Kubernetes is constantly innovating to create tools like Knative that address these concerns and simplify the experience for developers and operators. Serverless technologies on the other hand, have a reputation for provider-imposed limitations such as cold starts and runtime length limits. Many of these concerns are starting to be addressed or are now solved problems on some platforms. At Serverless, I think it's safe to say we think this conversation is a legitimate one and we want to contribute to it with new tools that support the best of both worlds. Because of this, the Serverless Framework now supports integrating with Knative - a tool to help build serverless applications on top of Kubernetes. We think that Knative can be a logical choice for many workloads, especially those that require multi-cloud portability either due to internal or regulatory requirements. Getting Started There are a few prerequisite steps to getting started with the Serverless Framework Knative plugin. First, you'll need a Kubernetes cluster with Knative installed. Because of the open source nature of Kubernetes you have a lot of different options for this. You might choose to install it on any of a plethora of cloud providers or even in your own data center. For this demo, we'll leverage Google Cloud Platform. Creating a Kubernetes Cluster on the Google Cloud Platform To get your Kubernetes cluster up and running in GCP, you'll need to create a Google Cloud Platform account. Create Your GCP Account Go to https://cloud.google.com/ and create an account. As of this tutorial, Google offers a $ credit towards using GCP. I'll try and keep you within that credit allotment and the Google default limitations but keep in mind that while Kubernetes clusters can scale up and down, they still have a minimum node count of three and will be on even when they aren't in use. As the last step in this guide, I'll show you how to delete your cluster. Create a Project in GCP After you have an account up and running, you can create a project using the Google Cloud Console here. I named mine `sls-kubernetes-project` to keep things straight: !Screenshot of Google Cloud Project UI Make sure that you keep a hold of that value whatever you name yours because we'll be using it later. Install and Configure the Google SDK To create your Kubernetes cluster and interface with it you'll need to use the Google SDK. It will provide a nice CLI interface to do everything you need. Depending on your operating system, you can get started here. If you followed the installation instructions for the SDK you probably also authenticated it with your Google Cloud account. If not, you can do this with: `gcloud auth login` When going through this process it should prompt you to select a project. Make sure to select the project you just created, `sls-kubernetes-project` in my case. After that, let's set some environment variables to make creating our cluster a bit easier. We'll set one for our cluster name, our cluster zone (where we're deploying to in GCP), and our project name.  I've used `slsknative` for my cluster name, you can use `knative` or something else that doesn't conflict with any other clusters you might have running and follows the naming conventions for a cluster. Now make sure we set our project as the default in our Google Cloud CLI settings. You can check this with `gcloud config list`. If part of the output includes your project name in there like this you're good to go: `project = sls-kubernetes-project` Otherwise, set the project config with this command after you set the $PROJECT environment variable: `gcloud projects create $PROJECT --set-as-default` Next, let's enable some of the APIs for the services we're going to use on Google Cloud:  After this command completes, we should be ready to create our Kubernetes cluster! Create Your Kubernetes Cluster Now for the hard part (sort of, Google makes the surprisingly easy). You'll use the following command to create a Kubernetes cluster in Google Cloud:  So, what's this doing? Well we're using GCP to create a new cluster and passing in some standard configuration to create a cluster that will work with Knative. First, we add some addons like Istio that work well with Knative. We also specify the machine types we want to be in our cluster. I'm using slightly smaller machine types of `n-standard-` because Kubernetes clusters have a minimum of three nodes and as of this demo Google Cloud limits newly-created accounts to vCPUs in a single region. You can spin up a more robust cluster with larger instances, but you might end up needing to activate the account and make sure your limits are increased. You'll notice that I also have auto-scaling enabled in this command, but in this case I might end up hitting some of those account limits if I scaled too far. After the cluster finishes creating, you'll need to grant yourself admin permissions to administrate it. You can do that with this command:  Once you have those administrator permissions, you'll be able to use `kubectl` to interact with the cluster and install Knative. If you've already installed Docker on your machine before you may see a warning about `kubectl` here or later on that looks like this:  Just make sure that you restart your terminal at this point. Likely, GCP changed your path in the installation process so you will then find `kubectl` at `~/google-cloud-sdk/bin/kubectl`. If it didn't, just make sure you're using the Google Cloud SDK `kubectl` installation or some other recent installation. The easiest way to verify this is to enter `which kubectl` and confirm that it references the location in the Google Cloud SDK folder. Installing Knative on Our Cluster So now we're ready to install Knative on our Kubernetes cluster! First, we'll run this command which helps avoid race conditions in the installation process:  Then we can actually complete the install with this command:  This will get all the Knative goodies we need into our Kubernetes cluster. While it installs, we just need to wait for a few minutes and monitor the installation of the Knative components until they are all showing a running status. We do that with these three commands:  Run them each every few minutes and then confirm that all of the results have a status of running. When that's complete, you should have Knative up and running in Kubernetes in the Google Cloud! Using the Serverless Framework and Knative Now that we've got our cluster and Knative setup we're ready to start using the Serverless Framework! First, make sure you have at least Node.js + installed on your local machine. Then, if you still need to install the Serverless Framework run the following `npm` command to install it on your machine:  Next up we need to create a new Serverless Framework project with the `knative-docker` template and then change directories into that project:  Because were using the `serverless-knative` provider plugin we need to install all the dependencies of our template with `npm install` before we do anything else. This will download the provider plugin that was listed as a dependency in the `package.json` file. Next, let's take a look at the `serverless.yml` file in our project which looks like this:  This is the Serverless Framework service definition which lists Knative Serving components as `functions` with their potential event sources as `events`. You might be asking, this looks too simple. How is the Serverless Framework connecting with my cluster? Well, by default, we're using the `~/.kube/config` that was created on your machine when you setup your cluster. To get other developers started you'll also need to make sure they have access to your Kubernetes cluster and have their own kubeconfig file. Also, one critical part of the above is the Docker Hub section. At the moment, that section allows you to specify credentials so that your local Docker image and code in the `code` directory can be sent into Docker Hub and used by Knative. In order to enable it to work you'll need to have a Docker Hub account and set the docker environment variables locally. On Mac you can set those environment variables like this:  Once the Docker Hub credentials are set as environment variables we can deploy a service to our Kubernetes cluster:  After the process finishes, invoking our new service is as easy as:  And congratulations! After you see a response, you've just deployed your first Serverless Framework service using Knative, Kubernetes and Google Cloud! Now, if you need to remove the Knative Service you can use:  This should remove the Knative service but keep in mind that your Kubernetes cluster is still running! If you'd like to remove the cluster to save yourself some money you can run this command:  That should delete your cluster, but to be safe make sure to also confirm that it worked by checking inside of the GCP UI for your cluster. Now there's a lot more you can do as you continue to work with Knative. You'll probably want to try customizing your Docker containers with more interesting services, and integrate your Knative cluster with events from sources like Google Cloud Pub/Sub, Kafka, or AWS Simple Queue Service. There's a lot of possibilities and we can't wait to see what you do with it! Are you interested in guides on particular event sources or topics related to Knative and Serverless Framework? Leave us a comment below!",
      "__v": 0
    },
    {
      "_id": "64e0892bb72e199dda604000",
      "title": "The Serverless Framework Knative Integration",
      "content": "Modern Microservices - Containers and Serverless Over the last decade a lot has changed in the Cloud Computing landscape. While most of the application workloads were deployed as monolithic applications on dedicated servers or VMs in early , we are now seeing a shift towards smaller and self-contained units of application logic which are deployed individually and together make up the whole application. This pattern of application development and deployment is often dubbed \"Microservice Architecture\". Its adoption was greatly accelerated when Docker, a container creation and management software was first released in early and Google decided to Open Source Kubernetes, a container orchestration system. Nowadays complex applications are split up into several services, each of which deals with a different aspect of the application such as \"billing\", \"user management\" or \"invoicing\". Usually different teams work on different services which are then containerized and deployed to container orchestration systems such as Kubernetes. Given that such software containers are self-contained and include all the necessary libraries and dependencies to run the bundled application, AWS saw a potential to offer a hosted service based on such containerized environments where individual functions could be deployed and hooked-up to existing event sources such as storage buckets which in turn invoke the function whenever they emits a events. It announced their new service offering called AWS Lambda in . While initially invented to help with short-lived, data processing related tasks, AWS Lambda quickly turned into the serverless phenomenon where applications are now split up into different functions which are executed when infrastructure components such as API Gateways receive a request and emit an event. We at Serverless, Inc. invested heavily in this space and released the Serverless Framework CLI, our Open Source tooling which makes it easier than ever to deploy, manage and operate serverless applications. Given the huge adoption of serverless technologies due to their properties such as cost-, management, and resource-efficiency, Google decided to open source Knative, a Serverless runtime environment which runs on top of Kubernetes in . Since its inception several companies joined the Knative effort in making it easier than ever to deploy and run serverless workloads on top of Kubernetes. Containers vs. Serverless Given all those new technologies developers are often confused as to which technology they should pick to build their applications. Should they build their application stack in a microservice architecture and containerize their services to run them on top of Kubernetes? Or should they go full serverless and split their application up into different functions and connect them to the underlying infrastructure components which will invoke the functions when something happens inside the application? This is a tough question to answer and the right answer is: \"it depends\". Some long-running workloads might be better suited to run in containers, while other, short-lived workloads might be better deployed as a serverless function which automatically scales up and down to if not used anymore. Thanks to Knative there doesnt have to be the question of \"either containers, or serverless\". Knative makes it possible to use both, \"container and serverless\" workloads in one and the same Kubernetes cluster. Announcing the Serverless Knative provider integration Today were excited to announce the Serverless Framework Knative provider integration! Our `serverless-knative` provider plugin makes it easy to create, deploy and manage Knative services and their event sources. This first beta release comes with support to automatically build and deploy your functions as Knative Serving components and to use them as event sources via the Knative Eventing component. All such workloads and configurations can be deployed on any Kubernetes cluster, whether its running in the cloud, on bare metal or your local machine. While working on this integration we focused on ease of use and therefore abstracted some of the rather involved implementation details away into a cohesive developer experience our Serverless Framework users are already familiar with. Are you excited and want to learn more? Take a look at our tutorial to get started with your first service!",
      "__v": 0
    },
    {
      "_id": "64e0892bb72e199dda604002",
      "title": "How to Troubleshoot Serverless API’s",
      "content": "Building APIs is an order of magnitude the most common use case we see for Serverless architectures. And why not? Its so easy to combine API Gateway and AWS Lambda to create API endpoints that have all the disaster recovery and load management infrastructure you need by default. Combine that with the Serverless Framework and creating them is as easy as:  But how do we go about debugging and troubleshooting our APIs? CloudWatch within AWS does (sort of) give us easy access to our Lambda logs, and we can turn on API Gateway logging. But this doesnt provide us all the info we need if our API begins to have trouble. This is pretty much the entire reason why we created Serverless Framework Pro, as a way to help users of the Serverless Framework to monitor and debug their Serverless services; APIs being chief among them. And if this is the first time you are hearing about this, let me introduce you to the the Serverless Framework Pro dashboard with a minute Youtube video to get you up to speed. If you would like to know how to connect one of your services to the dashboard, make sure you have the most recent version of Serverless installed (`npm i -g serverless` or if you use the binary version `serverless upgrade`) and then run the command `serverless` in the same folder as your service. You will be walked through setting everything up. Log to CloudWatch When you are trying to debug you need to have data in order to help you determine what may have caused any problems. The easiest way to do that is to make sure you use your runtime's logging method when you need to. For example, in a NodeJS Lambda, we can capture any errors that come up when we make calls to other AWS resources such as DynamoDB, for example. Writing code that logs out the appropriate error data in this case may look something like this:  With this arrangement, it means if for some reason our query to DynamoDB errored out, looking at the logs would indicate exactly why. And the same pattern can be applied to almost all types of code that has the possibility of erroring out while executing. Aggregate monitoring Before we can troubleshoot any specific errors, often it can be hard to tell if any errors are happening in the first place! Especially when you are dealing with a busy production system, it can be hard to tell if your users are experiencing any errors and this is where Serverless Framework Pro comes into its own with the service overview screen. By just glancing at the charts provided here, you can immediately see if any API requests or Lambda invocations have returned as errors and in some way affected your users, even if they themselves are not aware of it. !Image showing error bars With the image above, I dont need to wait for a user to complain or report an error, I can instantly see that some errors start happening around pm. But it doesnt end there. It would be even better if I am not required to be watching these charts and I just get notified if something happens. This is where the Serverless Framework Pro notifications come into it. By going into my app settings, and choosing notifications in the menu, I can configure to have notifications sent to an email address or several, a Slack channel, call a webhook or even send the notification to SNS so I can have own Lambda function, for example, process those notifications as I want. !Notifications options You can configure these per service and per stage and have as many notification configurations as you wish; perhaps dev stages report via email since they arent critical but errors in production always go to a Slack channel for the whole team. Retrieving error details Since I am now able to see and be alerted to errors, I need some way to help me figure out what the error is and how to fix it. This becomes relatively easy with Serverless Framework Pro again. !Overview showing errors You start off with an overview screen such as this and I see some errors. Let me click on that !Errors List Now I can see some summary information about the errors within that time frame. Let me select one to drill down further !Stack trace and logs Scrolling down a bit on the next view I can see that Serverless Framework Pro is giving me a stack trace of the line of code in my handler that threw the error so I know exactly where to look. And because of my detailed `console.log` lines, my CloudWatch log shows me the data related to the error. (Obviously I deliberately generated an error for demo purposes here, but the same applies for actual errors as well). NOTE: CloudWatch logs are pulled in from your AWS account. They are not stored anywhere within Serverless Framework Pro, so when I open this detailed view, Serverless Framework Pro makes a request to your AWS account to retrieve the logs. If you delete the CloudWatch log from your account it wont be visible here either. Prevention is better than cure Up till now weve been looking at how to react to errors. But we can even take it one step further and keep our eyes out for issues that may cause a problem later. For example, if we have Lambda functions that generally run for a certain amount of time, say between and ms, and suddenly there is a spike where our Lambdas are running for over ms, this could indicate a potential problem brewing; perhaps some downstream provider is having issues and if we could get some warning ahead of time we could perhaps head that off at the pass. The same thing could apply for invocation count. Maybe we usually get a very steady flow of activity on our Lambda invocations and any sudden spike in invocations is something we need to know about. Serverless Framework Pro already creates these alerts for you automatically and you can choose to have notifications of these alerts sent to you using the notifications system shown before. Performance tweaking Troubleshooting doesnt have to be all about errors. We may need to meet certain performance criteria, and Serverless Framework Pro gives us ways to assess this too. Assessing execution time Every Lambda function can have a memory size value set. But this setting is not just for memory and also affects CPU and network allocation as well in a linear way; if you double memory you double effective CPU and network. By clicking through to the functions section on the menu on the left, and then selecting a specific function, you can see duration statistics with dashed vertical lines for deployment. Now you can immediately see how a change you makes affects the average execution time of your invocations after a deployment. !Function Duration Change And you can do exactly the same for memory usage... SDK and HTTP requests Often in a Lambda we need to make requests to other AWS services via the AWS SDK or even HTTP requests out to other rd party services, and these can have definite impact on the performance of our endpoints. So being able to gauge this impact would be really useful. Again, Serverless Framework Pro makes it possible to investigate this. Within the detailed view of a Lambda, we can see the spans section that will indicate to us if our outgoing requests are slower than they should be. Remember the issue with third party services mentioned above? Well, with spans we can see how long requests can take and then take appropriate action. !Spans for AWS SDK Pushing data at runtime However, not all the data we want to look at is as vanilla and easy to capture as we have seen so far. Sometimes we need to be able to analyse metrics and data that is only available at runtime. Which is why the Serverless Framework Pro SDK incorporates a number of features to help track this data a little easier. By default, Serverless Framework Pro overloads the context object at runtime, and provides some additional functions to use for runtime data capture. All these options are documented on the Serverless website and include options for Node and Python runtimes. Capture Error There may be cases where we would like to know about a potential error without actually returning an error to the end user making the request. So instead we can use the captureError method:  As you can see from the above, we just push an error message out but ultimately return a response. And our monitoring will show it as an error. !Captured Errors Capture Span And we can do the same for capturing any code that may take time to execute. We can wrap that code in our own custom span and see the performance data made available to us:  The above produces the following span: !Custom Span of the Hash You can immediately tell, just looking at that, that your focus for any optimisation needs to be on that HASH span. Trying to optimise anything else wouldnt make sense. Capture Tag Lastly, there exists a way to capture key-value pairs from invocations at run time that can be filtered for in the explorer view. Maybe an example will make this a little easier to grasp. You have built a checkout process that captures a users credit card details and then passes those details onto a third party payment provider. A lot of us will have built such functionality in the past. And usually what happens is that the response, after passing those details, will indicate success or failure and actually even explain why it failed; lack of funds, expired card, declined by bank, etc. We can tag these various states to make it possible for us to search through these easier. It basically lets you pass a key, a value and additional context data if you need it:  This allows you to find all invocations that relate to a specific customer ID so if we ever needed to find the very specific logs from the payment provider processing the card details we can easily filter by that customer ID. -- Serverless Framework Pro has a generous free tier for anyone building a Serverless application to use. It requires nothing more than signing up here. If you would like to see these features in action, then feel free to [sign up for our webinar)[https://serverless.zoom.us/webinar/register/WN_GpfDRsT-qsUmovARuvrg] on February.",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda60408c",
      "title": "Improving CLI Usability in Airflow",
      "content": "Introduction: In the world of open source, I recently had an exciting opportunity to contribute to the Airflow project, an essential tool for orchestrating and scheduling data workflows. During my contribution, I encountered an issue related to the `--state` CLI flag in the `airflow dags list-jobs` command. In this blog post, I want to share my experience and the steps I took to enhance the CLI usability. The Problem: The `--state` flag lacked information about the available arguments it could accept. This meant that users were left in the dark, unsure of what values they could pass in. To make the Airflow CLI more user-friendly, I aimed to provide suggestions for valid state arguments. Proposed Solution: To address this issue, I suggested adding keyword arguments to the `--state` flag, which would guide users on the acceptable state values. I modified the code as follows:  By dynamically generating the `metavar` and `choices` based on the available `DagRunState` values, users would receive auto-suggestions for valid states when using the CLI. This improvement not only enhances the usability of the `list-jobs` command but also reduces the effort required for users to determine the acceptable state arguments on their own. Conclusion: Contributing to open-source projects like Airflow has been an incredible learning experience for me. It allows me to give back to the community and help make the tools we use better for everyone. I encourage you to consider contributing to open-source projects as well. It not only sharpens your skills but also enables you to connect with like-minded individuals who share your passion for technology and collaboration. Stay tuned for more exciting open-source adventures, and don't forget to connect with me on LinkedIn to join the conversation! Happy coding and contributing! Issue Link: https://github.com/apache/airflow/pull/event- LinkedIn: https://www.linkedin.com/in/rohan--anand/",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda60408e",
      "title": "Diving Deep into NPM Ecosystem",
      "content": "Overview Hey, We are back with a blog on the most used package manager. In this blog, we will try to learn one of the package managers of JavaScript and it is also a default package manager for Node.js runtime environment. In this blog, I will cover all the things related to NPM and will describe What, Why, and How \"NPM\" is used in development. At the end, I will also provide a few links related to NPM where you get even more good ideas regarding the things we discussed. Let's JUMP!! What is NPM? The acronym of NPM is \\\\N\\\\ode \\\\P\\\\ackage \\\\M\\\\anager. Firstly it is not a single so-called software/application we can able to download but it is come up with Node.js which is a run time environment for executing JavaScript code. While downloading the Node we can get the NPM also. It is a library and registry for JavaScript software packages. It is also the world's largest software registry. Before getting directly into its usage, working let's get an idea about packages. A software package typically contains related programs that can perform different functions. In simple terms, if are using some part of code again and again in your development you can make it as a Module and use that wherever it needs. But these modules which we create are up to our system only!!!. It would be super cool if all the developers(Js) chose one particular place to search for other developer modules and even launch their modules which they think the particular module is a common use case in the development, it reduces the time. In simple words the moment when you have a problem in your mind there is a solution available. Why NPM? Node.js has a lot of inbuilt modules. As it has a lot of modules there is a need for someone to manage all of those. This is where NPM came into the picture. Imagine having to manually download libraries such as React, Bootstrap, and Styled Components to get your project running. If the library size is too long then it consumes a lot of system memory. Youd have to check each packages version number and get the correct dependencies as well. Seems super time-consuming right !!? We no longer have to manage third-party packages for our project manually. Its all been made easy with NPM. In simple words, it manages all the packages of one particular project. It also has a super facilitated command line interface. Installation As I said early we can't install the NPM package separately. We can able to get that by installing node.js. By using the CLI command we can even make a simple check should the package manager is installed successfully or not we can cross check the version of the NPM exists.  Mostly we can handle the NPM ecosystem through the command line only. The actual installation of the Package & usage. Let's imagine we need the package from the NPM to be used in one of our projects. For that, we need to install the particular package in our system either globally or locally mostly it depends on the type of package we are installing. If we are using one of the packages most frequently good to install that one globally. The installation of any package is quite simple and short.  The above is the simple command for installing. With a few modifications, we can get the most out of the installation.  The above command states that we can even install a specific version of the package by placing the version number after \"@\" Conclusion. In this blog, I just tried to give a basic idea of what is NPM and why I didn't get deep into it but I try to explain to you in my upcoming blogs stay tuned my beautiful readers. I hope you learn something new through this blog. To get a visual idea of NPM working and use case I highly recommend the below video to start. %[https://www.youtube.com/watch?v=RmjUYmJk] That's it about the blog. Do share your valuable feedback in the comment section feel free. Do connect with me on other socials to learn from each other. Twitter LinkedIn",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda604090",
      "title": "Google's Heading in AI: Thoughts and Personal Opinions",
      "content": "Artificial Intelligence (AI) has taken center stage in Google's strategic approach for many years, which was on full display at the Google I/O conference in . The tech giant is constantly pushing the boundaries of what is achievable in AI. With a variety of new products and features unveiled at the event, we can discern the direction Google is moving in terms of AI development. One of the most groundbreaking announcements was the unveiling of PaLM , Google's next-generation large language model (LLM). As an evolution of its predecessors, PaLM is built on advances in compute-optimal scaling, scaled instruction-fine tuning, and improved dataset mixture. The model is engineered to be fine-tuned and instruction-tuned for various purposes, and it's already been integrated into over Google products and features. Among the products utilizing PaLM are Bard (available in around countries, excluding EU and others), a tool that boosts productivity and fuels curiosity, and Search Generative Experience, which improves the process of online search by offering AI-powered snapshots of key information. Also noteworthy is Codey, a version of PaLM fine-tuned on source code to function as a developer assistant, providing a broad range of Code AI features including code completions, bug fixing, source code migration, and more. It's evident that Google is using AI to improve a wide range of user experiences, from general productivity to more specialized tasks like coding. In the realm of image and video generation, Google has made remarkable strides with the launch of Imagen and Phenaki. Imagen, a family of image generation and editing models, is being incorporated into multiple Google products including Google Slides and Android's Generative AI wallpaper, showcasing Google's ambition in the realm of AI-powered visuals. Phenaki, on the other hand, is a Transformer-based text-to-video generation model that can synthesize realistic videos from textual prompt sequences, presenting vast possibilities for the future of video content creation. ARCore's Scene Semantic API is another innovative tool announced at the conference, enabling users to create custom AR experiences based on the features in the surrounding area. This underlines Google's commitment to improving AR experiences through AI technologies. In the field of speech recognition, Google launched Chirp, a family of Universal Speech Models trained on million hours of speech. Chirp enables automatic speech recognition (ASR) for + languages, emphasizing Google's dedication to inclusivity and accessibility in AI. Lastly, the launch of MusicLM, a text-to-music model, highlights Google's intention to revolutionize the music industry through AI. MusicLM can generate seconds of music from a text prompt, offering a fascinating glimpse into the future of AI-assisted music creation. Thoughts In my opinion, Google's AI development direction seems to be characterized by two main themes: improving user experiences across various domains and pushing the boundaries of what AI can achieve. The company is clearly not just focusing on one aspect of AI but is instead leveraging it in every possible way to create better, more intuitive products and services. However, it's important to consider the potential challenges and ethical considerations as AI continues to advance. Issues such as data privacy, the risk of AI systems being misused, and the potential impact on jobs and society at large are all valid concerns that need to be addressed. As we move forward into this new era of AI, it's essential for companies like Google to take the lead in ensuring that these technologies are developed and used responsibly. The end",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda604092",
      "title": "Boost Your App's Performance: -Factor Methodology Explained",
      "content": "Introduction Welcome to my blog! In our rapidly changing tech world, businesses must build and maintain scalable apps to stay competitive. The -Factor methodology provides valuable tips for creating modern, cloud-native applications. In this article, we'll discuss the benefits of using the -Factor principles and demonstrate how they can enhance your apps' performance, maintainability, and adaptability. Let's get started! What is a -factor app? A -Factor app is a method for developing scalable, maintainable, and cloud-native applications by following twelve best practices. It was created by Heroku co-founder Adam Wiggins in as a methodology for building applications that can easily scale and be deployed in a cloud-based environment. These practices include managing your codebase, handling dependencies, configuring settings, working with backing services, managing processes, and more. Here are some examples that had accepted this -factor app principle: Heroku, Netflix, and Shopify are well-known companies that have implemented the -factor app methodology in their applications. ) Codebase :  Admin Processes: Admin processes in a -factor app are one-time tasks or management jobs that run separately from regular app processes, using the same codebase, environment, and settings to maintain consistency and ease of maintenance. Example: Running database migrations or data cleanup tasks as separate processes ensures that they do not interfere with the running app. Resources : The -Factor App official website: https://factor.net/ The Twelve-Factor App on GitHub: https://github.com/heroku/factor The -Factor App: A Java Developer's Perspective: https://dzone.com/articles/the--factor-app-a-java-developers-perspective The -Factor App Methodology Explained: https://www.digitalocean.com/community/tutorials/the--factor-app-methodology-explained Building -Factor Microservices on AWS: https://aws.amazon.com/blogs/compute/building--factor-microservices-on-aws/ Factor App By KodeCloud : https://kodekloud.com/courses/-factor-app/ Conclusion : In conclusion, adhering to the -Factor methodology can significantly enhance your app's performance, maintainability, and adaptability. By following these best practices, you can build scalable, cloud-native applications that can easily adapt to changing business requirements and technological advancements. And it's a wrap-up . I hope you have learned something from this blog. If it's helpful to you then do like , follow me on Hashnode, Twitter, and GitHub subscribe to my Hashnode newsletter so that you don't miss any future posts. Thanks for reading and have a great day!",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda604094",
      "title": "The 'O' Complexity (cheat-sheet)",
      "content": "Introduction In this blog post, we will see time complexity and delve into the average time complexities of various common data structures for insertion, deletion, and traversal operations. Understanding these complexities will empower you to make informed decisions when choosing the right data structure for your specific use case, ultimately leading to more efficient and scalable code. For each data structure, we will explore the average time complexities associated with insertion, deletion, and traversal operations. It is important to note that the time complexities mentioned are averages and may vary in certain scenarios. O(n) - Insertion, Deletion, Traversal By the end of this blog post, you will have a solid understanding of how different data structures perform when it comes to these essential operations, enabling you to choose the most suitable data structure for your specific needs. So, let's dive in and unlock the world of time complexity in data structures! Array:  - Traversing all the vertices and edges in a graph, represented using an adjacency list, takes linear time. It involves visiting each vertex and its adjacent vertices. Please comment out if anything left or yiu want to add in this blog, will be happy to add.",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda604096",
      "title": "Let's end the mobile wars. Android is better.",
      "content": " In the ongoing debate between Android and iOS, I firmly stand on the side of Android. Google's mobile operating system offers a myriad of advantages that make it a superior choice for smartphone users. From its open nature to its diverse range of features and options, Android provides an unparalleled user experience. In this article, we will delve into the reasons why I believe Android outshines iOS and the iPhone. Choice and Flexibility: One of the fundamental strengths of Android lies in its versatility and freedom of choice. Unlike iOS, which operates within a closed ecosystem, Android allows users to customize their devices according to their preferences. With Android, you have the power to personalize your home screen, use different launchers, and install third-party apps from various sources. This level of customization empowers users to create a truly unique and tailored experience. Plus, Android devices come in a wide range of models and price points, offering greater accessibility to users across different budget ranges. From affordable options to flagship devices, Android caters to diverse needs and ensures that users can find a device that suits their requirements. Always-On Display: Android has been ahead of the game when it comes to the Always-On Display feature. While iOS introduced this feature with iOS , Android users have been enjoying it for almost a decade. The implementation of the Always-On Display on Android devices is superior, offering more customization options and useful functionalities. For instance, Pixel devices provide the At a Glance widget that displays essential information on the home and lock screens, such as date, time, weather, and battery level. This feature enhances convenience and efficiency by providing quick access to vital information without unlocking the device. Google Assistant: When it comes to virtual assistants, Google Assistant takes the lead over Siri. While Siri may perform adequately for basic tasks, Google Assistant offers advanced controls, superior voice recognition, and extensive integration with various devices. The AI-powered Google Assistant excels in understanding complex queries, controlling smart home devices, and providing accurate information. Features like real-time voice typing, live translation, and seamless integration with other Google services make Google Assistant an invaluable companion. Notifications: Android's notification system is renowned for its efficiency and organization. From status bar icons that indicate unread notifications to the Smart Reply feature that allows quick responses without opening apps, Android excels in managing notifications. The Quick Settings panel provides a centralized location for managing and interacting with notifications, further streamlining the user experience. In contrast, iOS notifications can become overwhelming and cluttered within the Notification Center, especially if you receive a high volume of notifications daily. Fingerprint Scanners: While Face ID on iPhones is an impressive technology, Android's widespread adoption of fingerprint scanners offers a more versatile and convenient unlocking method. Whether through capacitive or in-display scanners, Android devices allow users to unlock their phones securely without the need to look at the screen. Certain scanners even offer additional functionalities, such as controlling UI elements like the Quick Settings panel. Fingerprint scanners provide accessibility benefits, ensuring that users can unlock their devices even when wearing glasses or masks. Faster Charging: Android devices have embraced fast charging technology, with an increasing number of phones supporting W or higher charging speeds. This rapid charging capability allows users to top up their devices in under minutes, significantly reducing downtime. Some Android devices even offer super-fast wireless charging above W. While longevity concerns exist with such high charging speeds, it remains an undeniable advantage for those who prioritize quick charging capabilities. USB-C Standard: The ubiquity of USB-C connectivity in Android devices presents a significant advantage over iOS's proprietary Lightning standard. USB-C offers faster data transfer speeds, improved charging capabilities, and a wider range of compatible accessories. The convenience of using a single cable for charging and data transfer is further enhanced by the fact that many other devices, such as laptops and tablets, also adopt USB-C. This compatibility simplifies device integration and reduces the need for multiple cables and adapters. Lower Average Prices: Android offers a vast selection of devices across various price ranges, making it more accessible to a broader audience. While Apple's iPhone SE provides an affordable option, Android smartphones provide more choices for those seeking greater value for their money. From budget-friendly devices to high-end flagship models, Android offers a diverse range of options that cater to different budgets and preferences. Plug-in Features: Android's ability to decouple features from system updates allows users to benefit from new functionalities without waiting for a full OTA (Over-The-Air) update. Play System changes enable users to add new features through application installations from the Play Store. This flexibility ensures that Android users can enjoy enhanced functionality promptly. In contrast, iOS still relies on full system updates for feature additions, potentially leaving users behind if they don't update automatically. Various Form Factors: Android devices provide a wider range of form factors compared to iPhones, offering users more choices to suit their preferences. While Apple produces a limited lineup of devices each year, Android manufacturers like Samsung offer diverse options, including foldable and flip smartphones. Whether you prefer a traditional slab design, a compact phone, or an innovative folding device, Android's versatility ensures that you can find a device that matches your individual needs and style. Customizability: Android's customizability remains a significant advantage over iOS. While iOS has introduced more customization options over the years, Android still reigns supreme in this regard. Android users can easily change launchers, customize their home screen layouts, and apply various themes and icon packs to personalize their devices fully. The freedom to tailor the user interface to individual preferences fosters a sense of ownership and uniqueness. Expandable Storage: Another advantage of Android devices is the ability to expand storage through microSD cards. Many Android smartphones offer a dedicated slot for expandable storage, allowing users to increase their device's storage capacity at a fraction of the cost of upgrading to a higher storage variant. This feature is particularly beneficial for users who frequently store large media files, such as photos, videos, and music. File Management: Android provides users with a more robust file management system compared to iOS. Android devices allow users to access the device's file system directly, making it easier to manage and organize files. Users can create folders, move files between directories, and browse their device's storage with greater flexibility. This level of control over file management enhances productivity and simplifies tasks such as transferring files between devices or organizing media libraries. Default Apps: Android offers the flexibility to set default apps for various functions such as web browsing, email, messaging, and more. This means that users can choose their preferred apps to handle specific tasks, ensuring a personalized experience. In contrast, iOS restricts users to Apple's default apps for certain functions, limiting the freedom to use third-party alternatives. Multitasking and Split-Screen: Android provides robust multitasking capabilities, including split-screen mode, which allows users to run two apps simultaneously side by side. This feature is particularly useful for productivity, as it enables users to multitask efficiently by viewing and interacting with multiple apps simultaneously. While iOS has introduced limited multitasking features, Android offers a more comprehensive multitasking experience that enhances productivity and efficiency. Widget Support: Android's widget support is a notable advantage over iOS. Widgets provide users with at-a-glance information and quick access to app functions directly from the home screen. Android widgets come in various sizes and styles, offering versatility and customization options. Widgets can display information such as weather updates, calendar events, news headlines, and more, providing a convenient and personalized user experience. Intuitive Sharing Options: Sharing content between apps is seamless and intuitive on Android devices. Android's sharing menu allows users to quickly share content, such as photos, links, or files, with a wide range of compatible apps. The sharing menu is customizable, allowing users to prioritize their preferred apps and streamline the sharing process. This streamlined sharing experience enhances productivity and makes it easier to collaborate and interact with various apps and services. Default Apps for Voice Assistants: Android allows users to set their preferred default voice assistant, such as Google Assistant or Amazon Alexa. This flexibility enables users to access their preferred voice assistant with a simple voice command or button press, enhancing the overall voice interaction experience. In contrast, iOS limits users to Siri as the default voice assistant, limiting options for those who prefer alternative voice assistants. Open Ecosystem: Android's open ecosystem fosters innovation, encourages competition, and promotes app development. The Google Play Store offers a vast selection of apps from various developers, providing users with a diverse range of options. The open nature of Android allows developers to create innovative apps that push the boundaries of what is possible on a smartphone. Additionally, Android's open-source nature allows developers to customize and modify the operating system, leading to unique custom ROMs and modifications that cater to specific user preferences. Community and Support: The Android community is vibrant and active, offering a wealth of resources, forums, and support for users. Whether you need troubleshooting assistance, want to explore new apps and customization options, or seek advice from fellow users, the Android community provides a rich ecosystem for knowledge sharing and collaboration. The community-driven nature of Android fosters a sense of camaraderie and empowers users to make the most out of their devices.  Wrapping up In conclusion, Android's open nature, extensive customizability, expandable storage options, robust file management, and intuitive sharing capabilities make it a superior choice for many users. The platform's multitasking capabilities, widget support, and default app customization further enhance the user experience. Additionally, Android's open ecosystem, diverse app selection, and active community contribute to a dynamic and innovative environment. While iOS and the iPhone have their strengths, Android's flexibility, choice, and ability to cater to individual preferences make it the preferred option for those seeking a more personalized and versatile smartphone experience. It's just a question of thinking. iOS is just iOS, nothing more, while with Android you have thousands of operating systems based on it. Android is open-source.",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda604098",
      "title": "DevsInTech Chronicle",
      "content": "Greetings, DevsInTech community! Another incredible month has passed, filled with engaging events, knowledge-sharing sessions, and entertaining activities. We hope this newsletter finds you well, ready to dive into the exciting highlights from the past month. So, grab your favourite beverage, sit back, and let's rewind and relive the amazing moments together!  Check out our social handles and Follow us on socials. And join our Discord and be part of our community to learn, grow and upscale yourself with other community members. GitHub: https://github.com/devs-in-tech Twitter: https://twitter.com/devs\\_in\\_tech LinkedIn: https://www.linkedin.com/company/devsintech-community Discord: https://discord.com/invite/gFmxBuZp Website: https://devsintech.netlify.app/ YouTube: https://www.youtube.com/@TechWithSusmita Hashnode: https://devsintech.hashnode.dev/ Daily-dev Squad: https://app.daily.dev/squads/devsintech/fceIGBDjMejFnyycFPo\\_yscXQtcDQYQf-zw Happy coding!  DevsInTech Community.",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda60409a",
      "title": "Learn Simple Animation Using JS",
      "content": "Overview Hello readers we are here with another blog on an interesting topic, you will learn how to make a simple animation using one of the most popular programming languages of all time \"JavaScript\". Animation makes a lot of difference in designing any website of our choice. Through this, you will get a basic understanding of how to animate HTML elements. What is animation? !Question Mark GIFs | Tenor As for flow let's spend some time on what is animation. There are plenty of definitions for this but here we go In simple language, we are making the element/objects appear to move. But again making complex animation is quite hard just by using only programming languages. So, in this blog, we are only focusing on getting you an overview understanding of animation. Let's dive !! Slide an Element In this section, we will learn how to slide an element. For this, we will imagine like after clicking the button we want to slide the element in any one of the directions(UP, DOWN, LEFT, RIGHT) First design a button and a div element the purpose of these two elements is after clicking the button the div element has to slide any one of the directions accordingly.  We named the button Right for explanatory purposes we are trying to slide the div element to the right side. Style the div element of your choice. The final result will be.  After scaling the element Play Around As of now, these are the basic animations in JavaScript my small advice would just be to mix all these things and try to create another weird animation to see what will happen. I am telling you this is the best way to get a better understanding of the things we are currently learning. !Children-playing GIFs - Get the best GIF on GIPHY Wind-up That's it about the blog hope you learn and get to know something new from my blog. If you like the blog and want content like this please follow the DevinTech community and be involved in our amazing community we are thrilled to be a part of your journey. Discord -DevsInTech Twitter - DevsInTech If you want to make the connection with the writer of the content here you GO. Twitter - Malavi Pande",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda60409c",
      "title": "Arc Browser: What exactly is it?",
      "content": " Welcome back, everyone! Today we're about to embark on a journey of discovery: we're taking a deep dive into the mysterious, enigmatic, and quite possibly, fantastic world of the Arc Browser. Hold on to your computer chairs, because this is going to be a wild ride! A New Kid on the Block Just when you thought you knew all there was to know about web browsers, here comes Arc, strutting its stuff on the digital catwalk. This plucky newcomer to the browser scene has been making some noise recently, promising to be \"everything you care about, all in one place\". Bold words, Arc, bold words indeed. If I had a nickel for every browser that promised to change my life, I'd have... well, a lot of nickels. Now, let's cut to the chase. Arc is currently only available for macOS users, which could be very bad for our Windows-loving friends out there - like me. But fear not, Arc has announced plans to release a Windows version sometime in the fall (October/November) of (here's a unprecise countdown - not sure about days/hours/minutes). So hang in there! The Arc is coming. The Arc Experience Stepping into the Arc browser feels like stepping into a s-themed techno party, complete with a splash screen that invites you to \"Meet the Internet again\". The setup process is a bit like signing up for a secret club - you need an invite, there's a mandatory account creation process, and you're even asked to pick a color theme. This color, you'll find out later, is used for organization. It's like being in kindergarten again, but this time, instead of coloring books, we have the world wide web. Once you're in, you'll find yourself face-to-face with Arc's slightly unusual UI. Everything lives in a sidebar to the left of the screen, including the URL bar, tab list, favorites, and navigation. You can hide this sidebar with a click, and then hover on the left side of your screen to show it back! It's kind of like that over-enthusiastic puppy that won't leave you alone - a little annoying at times, but kind of endearing. Beyond the Surface But what's a browser without some cool features? Arc has a few tricks up its sleeve that make it stand out from the crowd. It's kind of like that quiet kid in class who suddenly reveals they're a chess prodigy, an opera singer, and a black belt in karate. Who knew, right? For starters, Arc has a Spotlight-style interface for accessing pretty much every browser feature. This lets you do things like pin tabs and view history simply by typing. You can even move and switch tabs without lifting your fingers from the keyboard. It's like being a web-browsing ninja, executing commands with stealth and precision. Arc is also pretty keen on keeping things tidy. By default, any tabs you don't pin will be closed after hours. There's a separate space for pinned tabs and \"Today's tabs\", and you can hit a \"Clear\" button at any time to sweep your old tabs into the digital dustbin. Another cool feature is \"Spaces,\" which allows you to organize your tabs. You can swipe between spaces as easily as navigating back and forth on a web page. Each space can have its own theme, and tabs can be shifted between spaces as needed. You can also apply profiles within themes, which lets you associate different browser data with different spaces. This is super handy for switching between personal and professional accounts. Not All Rainbows and Unicorns Now, it wouldn't be fair to sing Arc's praises without acknowledging its drawbacks. It's a bit like a movie review - you've got to discuss both the mind-blowing special effects and the less-than-stellar acting. According to multiple sources, the UI and tab management might not be everyone's cup of tea. Chrome extensions don't always work as advertised, and the browsing experience isn't always buttery smooth, despite some good benchmark results. Finally, energy impact and CPU usage could be a concern for some users. The Verdict? Arc is definitely an interesting addition to the browser landscape. It brings some innovative ideas to the table and seems willing to take risks to reinvent the browser experience. It's not perfect, and it might not be for everyone, but it certainly offers some features that could make your internet journey more efficient and enjoyable.",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda60409e",
      "title": "Supabase: What is it & Getting started",
      "content": "What is Supabase? Supabase is an open-source Firebase alternative that aims to provide developers with a Firebase-like experience using an enterprise-grade platform. It combines a suite of open-source tools, emphasizing the use of existing tools and communities with MIT, Apache , or equivalent open licenses. If an appropriate tool doesn't exist, the team behind Supabase develops and open sources it themselves. Supabase is a hosted platform, meaning you can sign up and start using it without needing to install anything on your local machine. However, if you prefer, you can self-host and develop locally. Key Features of Supabase Supabase offers a range of features that aim to provide a comprehensive backend solution for developers: Database: Supabase provides a full Postgres database for every project. It supports real-time functionality, database backups, extensions, and more. Authentication and Authorization: Supabase offers a suite of identity providers and APIs that allow you to add and manage various types of logins to your project, including email and password, passwordless, OAuth, and mobile logins. Auto-generated APIs: Supabase auto-generates REST and GraphQL APIs, enabling real-time subscriptions. Functions: Supabase supports server-side functions, including database functions and edge functions, which execute your code closest to your users for the lowest latency. File Storage: With Supabase, you can store, organize, transform, and serve large files. The storage is fully integrated with your Postgres database and supports Row Level Security access policies. AI & Vectors: Supabase allows you to store and search embedding vectors, a feature that comes in handy when working with AI-based applications. Realtime Listening: This feature enables you to listen to database changes, store, and sync user states across clients, broadcast data to clients subscribed to a channel, and more. The philosophy behind Supabase's approach to client libraries is modular, meaning that each sub-library is a standalone implementation for a single external system. Supabase supports a range of client languages, including JavaScript (and TypeScript), Flutter, C, Go, Java, Kotlin, Python, Ruby, Rust, Swift, and even the Godot Engine (GDScript). Getting Started with Supabase Now that we've covered what Supabase is and its key features, let's walk through how to get started with a simple project using Supabase and React. Setting up a Supabase Project First, you'll need to create a new project in the Supabase Dashboard. After your project is ready, you'll create a table in your Supabase database using the SQL Editor in the Dashboard. As an example, we'll create a `countries` table with some sample data using the following SQL statement:  This statement will create a new table called \"countries\" with two columns: \"id\" and \"name\". Then, it inserts the names of three countries into the table. Creating a React App Next, create a React app using a Vite template. You can do this by running the following command in your terminal:  This command creates a new React application using Vite, a modern front-end build tool, with the project name \"my-app\". Installing the Supabase Client Library The Supabase client library, `supabase-js`, provides a convenient interface for working with Supabase from a React app. Navigate to your newly created React app and install `supabase-js` by running the following command:  This command navigates into your \"my-app\" directory and installs the `supabase-js` library. Querying Data from the App Now, we'll set up a way to query data from the Supabase database and display it in your app. In `App.jsx`, create a Supabase client using your Project URL and public API (anon) key. Then, add a `getCountries` function to fetch the data from the `countries` table we created earlier and display the query result to the page. Here is a simple implementation:  This code creates a new Supabase client with your project URL and public API key, fetches the country data from the database, and displays it in an unordered list. Starting the App Finally, start your app by running the following command in your terminal:  Then, navigate to http://localhost: in a web browser, and you should see the list of countries displayed on the page. Wrapping Up Supabase is a powerful open-source alternative to Firebase that offers a suite of features for developing web and mobile applications. It's built on top of a stack of open-source tools and is designed to give developers a Firebase-like experience. Whether you're looking for a real-time database, authentication services, auto-generated APIs, or file storage capabilities, Supabase has you covered Moreover, it supports a variety of languages and platforms, making it a versatile choice for many developers. The step-by-step guide above will help you get started with your first Supabase project, providing a basic understanding of how to create a project, add data to your database, and query that data from a React app. Happy coding!",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040a0",
      "title": "Difference Between Reschedule Mode and Deferrable Flag in Airflow Sensors",
      "content": "Motivation Recently my PR for adding the `difference between Deferrable and Non-Deferrable Operators` got merged in `apache-airflow`, you can see it here - PR Link. So I thought to explain it through a blog. Introduction Airflow sensors are a special kind of operator that is designed to wait for something to happen. When sensors run, they check to see if a certain condition is met before they are marked successful and let their downstream tasks execute. There are two modes available in Airflow sensors, \"poke\" and \"reschedule\". In this blog, we will discuss the difference between the \"reschedule\" mode and the \"deferrable\" flag in sensors. Reschedule Mode When the sensor mode is set to \"reschedule\", if the criteria are not met, then the sensor releases its worker slot and reschedules the next check for a later time. This mode is best if you expect a long runtime for the sensor because it is less resource-intensive and frees up workers for other tasks. Between each wait\\_interval, when the sensor is not checking your criteria anymore, the slot is released, and your sensor gets the status \"up\\_for\\_reschedule\". Meanwhile, your other tasks can run. \"up\\_for\\_reschedule\" means your sensor is going to be rescheduled at a later time, or more specifically at the `current date + wait_interval`. Deferrable Flag The \"deferrable\" flag is used to mark a task as deferrable, which means that it can be deferred to a later time if it fails to run. When a task is marked as deferrable, it is put in a \"up\\_for\\_reschedule\" state if it fails to run. The task is then rescheduled for a later time, or more specifically at the `current date + wait_interval`. The \"deferrable\" flag is not a common behavior in sensors, and it is only used in a few of them. Which mode should you use? If the expected runtime of your sensor is short or if the wait\\_interval is short like less than a minute, go with the \"poke\" mode. If the expected runtime is quite long, then go with the \"reschedule\" mode. Code snippet to understand better Below is the example code snippet that demonstrates the difference between the \"reschedule\" mode and the \"deferrable\" flag in sensors:  In this example, we have two sensors, one with the \"reschedule\" mode and one with the \"deferrable\" flag. The `FileSensor` operator waits for a file to appear in a specified directory before it is marked successful and lets its downstream tasks execute. The `waiting_for_file_reschedule` sensor uses the \"reschedule\" mode, which means that if the criteria are not met, then the sensor releases its worker slot and reschedules the next check for a later time. This mode is best if you expect a long runtime for the sensor because it is less resource-intensive and frees up workers for other tasks. The `waiting_for_file_deferrable` sensor uses the \"deferrable\" flag, which means that if the task fails to run, it is put in a `up_for_reschedule` state and rescheduled for a later time. The \"deferrable\" flag is only used in a few sensors. Conclusion It's worth noting that bugs and errors in the sensors may be masked by timeouts, which however may be mitigated by properly written unit tests. Some overhead is added to the scheduler, as such polling intervals may not be too frequent, and a separate process is spawned. In conclusion, the \"reschedule\" mode and the \"deferrable\" flag in sensors are used to free up resources and reschedule tasks for a later time if they fail to run. The \"reschedule\" mode is best for long-running sensors, while the \"deferrable\" flag is only used in a few sensors. It's important to choose the appropriate mode based on the expected runtime of your sensor and the wait\\_interval",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040a2",
      "title": "Is Web Development still relevant in this AI world",
      "content": " Introduction So, what exactly makes AI so popular in the tech industry? Do you know the role of AI in web development? What is the future of AI? How does it impact the web development process? Does it boost revenue generation? I am pretty sure that all these questions are running through your mind right now and would be wondering whether to start learning web development or not, To help answer them, I have showcased all the important benefits of AI in this write-up. So, read this blog carefully and you'll know AI's importance and role in . And also get to know the winner of the battle between Humans and Artificial Intelligence in the tech space. Role of AI in Web Dev Currently, the world of web development is fuelled by the collaboration of artificial and human intelligence. AI cant do the job alone, and humans no longer need to. Most AI use cases in web development complement the work human programmers do and help the development process progress more efficiently. The role of AI can be summarized as follows: It is an essential technology in the modern world that is built upon the assumption that you canLet the Machine Play Your Role. For example, production, recycling, development, design, transportation, infrastructure production and more can be controlled by machines instead of humans. It emphasizes the development of smart and intelligent machines that act like humans. For example, an AI-enabled machine can help a doctor identify diseases from an image, it can control both air and ground traffic, and so on. It boosts user experience by creating strong human connections by improving social listening, data analysis, and facial recognition. This allows companies to know their customers' behaviour at an advanced and intimate level. Its inbuilt with virtual robots and innovative technology to provide efficient functioning just like humans. For example, the Amazon Echo, popularly called Alexa, is a virtual assistant developed by Amazon. Will AI replace Web Developers AI is not going to replace web developers any time soon. The current technology isnt designed to write a whole project from scratchits designed to complement human programmers and make their jobs a little easier. Other types of artificial intelligence are made to design solutions based on data, but at a scale that humans could never achieve alone. They help us achieve things that werent possible before, or help optimize tasks we were already doing so that humans can focus on the jobs that AI is still very incapable of. Some ways through which we can use AI are: Use AI to help with a job, not to do a job for you. One of the most important things to remember is human intervention is always needed when dealing with AI-written code. Theyre branded as assistants because they need to work with you, not for you. Think about how AI can help users. Using artificial intelligence for web development doesnt just mean using coding assistants. They can also be used to help users interact with websites and brands in new and more efficient ways, for example, customer experience and support chatbots. Delegate low-level work to AI to free up your time. When used correctly, AI can help programmers spend more time working on new and complex tasks, and less time on basic maintenance. This helps increase productivity and the value of the work you output.  Conclusion Although web development and AI have been working co-dependently, theres no reason for website developers and designers to feel a threat from artificial intelligence. AI can never replace the ideas of the human mind. It can only perform repetitive and monotonous tasks. However, website designers and developers must learn about artificial intelligence to truly utilize what it has to offer. Because shortly, AI would be a bigger part of web development, considering how fast its growing. Therefore, being well-versed in the field has now become essential for aspiring web developers. Thus, instead of seeing AI as a threat that might replace web development, businesses should view AI as an empowering tool that can aid in everyday tasks that require time and effort.",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040a4",
      "title": "The Power of LearnInPublic: How it Can Benefit Programmers",
      "content": "%[https://twitter.com/swyx/status/?s=] Introduction Are you tired of studying in private and not seeing the benefits of your learning? Have you ever considered the idea of sharing your learning process with others?  Do you want to earn recognition from people in your field? If your answer to these questions was a resounding yes then this blog is just perfect for you!!! The practice of LearnInPublic or Learning In Public could be the key to unlocking your full potential as a programmer. In this article, we will explore the power of learning in public, the benefits it can bring to programmers, and how to get started on your learning journey. So get ready to step out of your comfort zone and join the community of learners who are actively sharing their knowledge and experiences.  Liked This Blog? Do react and comment with your thoughts on the points discussed above. Make sure to follow me : Twitter Hashnode Medium GitHub LinkedIn",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040a6",
      "title": "DevsInTech Chronicle",
      "content": "Hello, DevsInTech community! Welcome to our monthly newsletter where we share the latest news, events, opportunities, and projects related to our awesome developer community. Let's dive in! ",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040a8",
      "title": "Managing Python Version Dependency in Google Cloud Composer",
      "content": "Introduction Google Cloud Composer is a managed Apache Airflow service that allows users to orchestrate and manage workflows in the cloud. However, one common challenge faced by users is when they need to use a Python package that requires a higher version of Python than what is supported by the Composer environment. This article will guide you through the process of managing Python version dependencies in Google Cloud Composer. Understanding the Issue In some cases, you may have seen that your DAG (Directed Acyclic Graph) may rely on a Python package that requires a specific Python version, such as Python &gt;= ..  cluster to execute the tasks specified in the DAG. The KPO will spawn a pod from the custom Docker image, ensuring the correct Python version and dependencies are available during execution. Conclusion Managing Python version dependencies in Google Cloud Composer can be achieved through containerization. By building and deploying a custom Docker image that includes the desired Python version and dependencies, you can ensure compatibility and execute your DAGs smoothly. This approach provides flexibility and enables the use of Python packages that require higher Python versions, expanding the capabilities of your workflows in Google Cloud Composer.",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040aa",
      "title": "Navigating the IT Landscape: Choosing between Containerization and Virtualization",
      "content": "Docker Docker is an open-source platform that enables developers to automate the deployment, scaling, and management of applications using containerization. It provides a way to package software and its dependencies into standardized units called containers. Containerization The process involves encapsulating the necessary libraries and dependencies to run the application into a single cohesive unit called an image. These images will then be used to run containers. The containerization technology enables us to run multiple instances of single or multiple containers on a single host machine, eliminating the need for intermediary software like a hypervisor in virtual machines. Unlike in virtualization wherein the virtual machines are assigned the necessary specs based on the application's requirements by the hypervisor, here the operating systems feature is used to isolate containers and manage resource allocation allowing them to run consistently and independently. Virtual machines The virtual machine is software, also known as a hypervisor or virtualization platform, installed on the host machine. This software creates and manages virtual machines by emulating the hardware components and providing the necessary virtualization capabilities. Virtual machines (VMs) are like virtual computers that imitate the hardware and operating system of a physical machine. Just as you can install different operating systems on separate computers, you can run multiple VMs on a single physical server. Each VM operates independently and can run its applications. Virtualization Virtualization involves the creation of virtual instances of physical resources within a system. By introducing a virtualization layer between the operating system and hardware, users are able to run multiple virtual servers or machines, each with their own operating system, on a single physical machine.  are a good fit for older applications or situations where complete separation between applications is necessary. They enable running different operating systems on a single physical machine, which is helpful for testing and development purposes. VMs also allow hosting multiple applications on the same hardware, making better use of available resources. Conclusion: As technology continues to evolve, the decision between Docker containers and virtual machines becomes a reflection of our individual aspirations and the specific requirements of our applications. Docker empowers agility and seamless deployment, while virtual machines offer isolation and stability. By understanding the nuances of both options, We can use the technology that best suits our needs.",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040ac",
      "title": "Software Libraries: What, Why &How",
      "content": "Overview If you're familiar with writing code in any of the programming languages then you will come across the word library once in a while and they are super easy to use by just importing with one line of code. As we even deep dive there is something called a framework that kicks you with confusion. People always face some sort of uncertainty while experiencing it. But I promise you don't worry if you have a good idea about both of these or not in this article I will help you. So without further due let's start. !Let'S Get Started GIF - Despicable Me Minions Lets Get Started - Discover & Share GIFs No Doubt Some people use these two terms interchange but it's quite wrong because let's think there is a twin in the family even though they look the same should we call both of them by the same name? No right! The same applies here to people who are the inventors of libraries & frameworks using different names to distinguish means they are different for sure. This confusion rises mainly because of some weird behaviour of a few libraries & frameworks where they violate some conventional rules & regulations. But Library != Framework What is library The technical definition -&gt;\"It is a set of code that was previously written that can be called upon when building your code\". But it was as simple as it was you will get more clarity through visualization. Just stick with me throughout the blog. Imagine there is a problem !Spongebob Rainbow Gif - IceGif Jack has a piece of code that he has to write in different projects so often and he was just really frustrated by the writing code again and again and wanted a solution for this. One of his friends suggested, \"separately writing that repeated code in one single file (module) and importing it whenever you want will save time, it even makes the entire more readable\". Steps create a separate file & dump the code  With this, I am wrapping up the library blog . I just explain everything from scratch hope you understand the scene behind every library and learned something new. Wind-up If you like the blog and want content like this please follow the DevinTech community and be involved in our amazing community we are thrilled to be a part of your journey. Discord -DevsInTech Twitter - DevsInTech If you want to make the connection with the writer of the content here you GO. Twitter - Malavi Pande",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040ae",
      "title": "Cloud Computing : A Beginner's Journey into the Future of Technology !",
      "content": "Introduction In this fast-paced digital era, businesses are continuously seeking ways to optimize their operations, enhance scalability, and boost efficiency. Among the many technological advancements that have revolutionized the modern business landscape, cloud computing stands out as a pivotal solution. The cloud has transformed how we store, process, and access data, providing numerous benefits to organizations of all sizes. I am Kelvin here and, In this Blog, we will deep dive into the world of cloud computing, explore its key components, and uncover the advantages it offers for businesses and individuals alike. What is Cloud Computing? To grasp the significance of cloud computing, let's start with the fundamentals. Cloud computing refers to the delivery of various computing services, including storage, servers, databases, software, analytics, and more, over the Internet. Unlike traditional computing methods that rely on physical servers and local networks, cloud computing enables users to access these services through the internet, from any location, at any time. It offers unparalleled flexibility and eliminates the need for maintaining on-premises hardware, saving businesses valuable resources.  Tools: CI/CD tools like Jenkins, GitLab CI/CD, or CircleCI automate the process of building, testing, and deploying updates to the SaaS application, facilitating a smooth development workflow. API Gateway: API gateways manage and expose APIs, enabling seamless communication between different components of the SaaS application and facilitating integrations with external systems. Security and Encryption Tools: Various security tools are employed to secure the SaaS application and protect user data. SSL certificates, firewalls, and encryption mechanisms are some examples. By leveraging these tools and technologies, developers can create a robust and scalable infrastructure for Software as a Service application, delivering efficient and seamless services to users over the internet. The Advantages of Cloud Computing \\. Enhanced Flexibility and Scalability Cloud computing offers unparalleled flexibility, allowing businesses to scale their resources up or down as needed. Whether it's handling seasonal spikes or accommodating business growth, the cloud can seamlessly adjust to fluctuating demands, ensuring optimal performance and cost efficiency. \\. Cost Savings By adopting cloud computing, organizations can significantly reduce their capital expenditure on hardware, software, and data center maintenance. The pay-as-you-go pricing model of cloud services enables businesses to pay only for the resources they consume, eliminating unnecessary costs. \\. Improved Collaboration and Accessibility With cloud-based applications and data storage, teams can collaborate more effectively, regardless of their physical location. Cloud computing enables real-time access to data and applications, promoting collaboration, and boosting productivity. \\. Enhanced Security and Reliability Leading cloud service providers invest heavily in security measures to protect their clients' data. These providers employ encryption, authentication, and other robust security protocols, ensuring that data is safeguarded from unauthorized access and potential threats. Additionally, cloud services often have built-in redundancy and backup mechanisms, enhancing data reliability and availability. \\. Automatic Updates and Maintenance Cloud service providers handle software updates and maintenance tasks, freeing businesses from the burden of managing these processes. This ensures that applications and services are up-to-date and functioning optimally at all times. Conclusion Cloud computing has undoubtedly emerged as a game-changer in the world of technology. Its ability to provide scalable, cost-effective, and efficient solutions has reshaped the way businesses operate. From startups to multinational corporations, cloud computing has opened up new possibilities for growth and innovation. Embracing the cloud empowers businesses to stay competitive, agile, and future-ready in a dynamic and ever-evolving digital landscape. As technology continues to advance, cloud computing will remain at the forefront of digital transformation, driving success for businesses and unlocking the potential of the future. And it's a wrap-up . I hope you have learned something from this blog. If it's helpful to you then do like , follow me on Hashnode, Twitter, and GitHub subscribe to my Hashnode newsletter so that you don't miss any future posts. Thanks for reading and have a great day!",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040b0",
      "title": "Trying out Brave",
      "content": "I love Firefox. But because it's not based on Chromium, it doesn't have Chrome extensions, the latest Chrome features, etc. My difficulty was mainly the lack of available extensions on their official Addons store, so I decided I needed a Chromium browser. Chrome was not an option, so I decided to go with Brave browser. Using Brave browser I went to the Brave website and installed the browser, but at first, I wasn't happy about it. All the web crypto nonsense, ads when it's made for privacy, the useless \"Talk\" feature that is basically a Jitsi Meet wrapper, and other annoying stuff. The \"import from browser\" feature and the setup were quite default. Some things I didn't like were, for example, how chrome-like it was. Also, there were no options to remove the built-in VPN (as I have my own, Proton VPN - I'll maybe make an article on it later). I immediately noticed that it had a \"Private browser with Tor\" feature, which I thought was a great idea, because why have a separate browser for Tor when you can have it on your normal browser? (edit: after trying it, I noticed it was extremely slow and didn't load CSS for some pages) I wish there was more personalization, and that the browser wasn't so much dedicated to Crypto - you have an icon in the toolbar about it, a button in the options menu, entire huge sections about it on the settings, and some default options about it that I don't like. Conclusion I have decided to stick with Firefox as my main browser, and use Brave for other stuff. Stay tuned for more articles about interesting stuff! ",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040b2",
      "title": "Optimizing Performance in MERN Stack Apps: Best Practices and Tools",
      "content": "In today's fast-paced digital landscape, where user expectations are higher than ever, the performance of web applications plays a crucial role in determining their success. As developers and businesses strive to deliver seamless and lightning-fast experiences, mastering performance optimization in MERN (MongoDB, Express, React, Node.js) Stack applications has become a critical skill. In this blog post, we will delve into the world of MERN Stack development, exploring the significance of performance optimization, its impact on user experience and SEO, and the best practices and tools that can empower you to create high-performing, responsive, and efficient web applications. Whether you are a seasoned developer or just starting your journey with MERN Stack, our goal is to equip you with the knowledge and resources to unlock the true potential of your MERN Stack apps and leave your users delighted with blazing-fast performance. Let's dive in! Optimizing Performance in MERN Stack Apps : PWAs are web applications that provide a native-like experience, even with limited or no internet connectivity. By implementing PWA principles, MERN Stack applications can achieve faster load times, offer offline access, and enhance user engagement. ",
      "__v": 0
    },
    {
      "_id": "64e08931b72e199dda6040b4",
      "title": "Prompt Engineering",
      "content": "Basic understanding of Prompt Engineering with examples. Introduction I heard about prompt engineering through a LinkedIn post which was talking about new practices in AI, Prompt Engineering is a concept within the field of natural language processing (NLP) and artificial intelligence (AI). Prompt Engineering is a way to design more instructive prompts to guide the behavior of the model. It takes input texts that instruct the model on specific things or tasks to perform. The main goal of prompt engineering is to achieve the desired outputs from language models by carefully designing the input prompts, it means you need to provide clear specifications, output styles, and biases. Real-life examples of prompt engineering is a customer support chatbots Is it a technology? No, it's not a standalone technology, but it reflects how language models are utilized for various applications. It is used in such a way that we can get desired results on giving specific tasks. Example I'll be using chatGPT to show you this example, so be ready with OpenAI API key/secret key from https://platform.openai.com/account/api-keys.  Best Practices Be explicit and specific: Clearly define the task or query in the prompt to avoid ambiguity. Use context: Incorporate context and relevant information in the prompt to guide the model effectively. Control output: Add keywords or instructions to influence the style and tone of the response. Mitigate biases: Design prompts that discourage biased or harmful outputs. Experiment with temperature and max\\_tokens: Adjust temperature and max\\_tokens parameters to fine-tune the model's creativity and response length. Iterate: Prompt Engineering is an iterative process, due to no rules for output information, we should test different inputs to see their outputs. Conclusion Prompt Engineering is becoming a way for normal people to use AI effectively without having Ph.D. in ML/AI. Just by designing well-crafted prompts, anyone can explore the capabilities of language models.",
      "__v": 0
    },
    {
      "_id": "64e08932b72e199dda6040b6",
      "title": "What is Butter: a smooth meeting platform for facilitators",
      "content": "In the digital age, the way we work, learn, and interact has dramatically changed. Virtual collaboration tools have become a necessity, and among them, Butter.us has emerged as a platform that promises to make online collaboration as smooth as its namesake. This post delves into the workings of Butter its features, and how it can revolutionize the way we conduct virtual meetings, workshops, and training sessions. What is Butter? Butter is a virtual collaboration platform designed to bring structure, energy, and joy to your online interactions. Unlike traditional video conferencing tools, which are primarily designed for conversations, Butter is built to facilitate planning, running, and recapping engaging and interactive collaborative sessions. The platform seeks to empower facilitators, moderators, and hosts by providing them with a suite of tools to engage participants, all in one place. How does it work? At its core, Butter focuses on three aspects: planning, engagement, and recap. Planning: Butter allows users to prepare sessions that practically run themselves. It encourages structured sessions with a time-boxed agenda, pre-loaded tools, and a reusable team library, leading to better outcomes. Engagement: The platform goes beyond the typical meeting format by incorporating features designed to keep participants engaged. These include emoji reactions, sound effects, a hand-raise queue, polls, flashcards, music, and GIFs. This interactive environment combats meeting fatigue and fosters more dynamic and engaging sessions. Recap: Butter also places emphasis on ensuring that the outcomes and takeaways of a session are easily accessible. It allows users to summarize sessions and capture outcomes in seconds, with options to access and share recordings, personal notes, chat logs, and poll results from any session through a single dashboard. What sessions can Butter facilitate? Butter is not built for those meetings that \"could have been an email\". Instead, it is a tool for facilitating highly-collaborative sessions like workshops, training sessions, bootcamps, courses, events, interactive webinars, social events, or any other session where you need to keep people energized and productive. While you can use Butter for everyday meetings, its designed to be much more than a standard meeting tool. Key features Butter integrates various facilitation tools into one window, eliminating the need to juggle multiple apps and tools during a session. You can access polls, flashcards, videos, music, Miro, MURAL, Google Docs, and more, all within the Butter window, keeping everyone focused on one window and one outcome. It also provides a workspace for all your teams sessions. This workspace is a place to create rooms, share templates, and access recaps, which helps save on session setup time, create team-wide consistency, and keep track of your teams sessions. In addition, Butter comes with community-made agendas, activitys, and tool templates that users can add to their Butter library in seconds. This feature is handy for those who dont want to start from scratch and would rather use proven templates to plan their sessions. Pricing Butter.us offers a free plan that allows you to run group sessions for up to minutes with as many as participants. One-on-one sessions are unlimited and free. Wrapping up Butter stands out in the crowded market of online collaboration tools with its focus on facilitation and engagement. Its unique features, user-friendly interface, and affordability make it a promising solution for anyone looking to transform their virtual meetings into interactive, productive, and fun sessions. Regardless of whether you're a teacher aiming to make your online classes more engaging, a project manager looking to make team meetings more productive, or an event host trying to bring energy to a virtual social event, Butter could be the tool you need to smooth out your online interactions.",
      "__v": 0
    },
    {
      "_id": "64e08932b72e199dda6040b8",
      "title": "How to Deploy React JS Applications for Free on Firebase",
      "content": "ase In this article , Ill show you how to deploy react js applications for free on Firebase, First lets understand what is Firebase What is Firebase Firebase is a Backend as a Service that started as a YC startup and grew up into a next-generation app-development platform on Google Cloud Platform. Firebase frees developers to focus crafting fantastic user experiences. You dont need to manage servers. You dont need to write APIs. Firebase is your server, your API and your datastore, all written so generically that you can modify it to suit most needs. Yeah, youll occasionally need to use other bits of the Google Cloud for your advanced applications. Firebase cant be everything to everybody. But it gets pretty close. Getting Started Before deploying your app on Firebase, you need to have Firebase project and a React project (create-react-app) Build Your App for Production Open your project folder cd your-project and build your app for production using command below  The JavaScript and CSS files will be inside the build/static directory. Install Firebase Tools Once you built your app then you can install Firebase tools that will allow you to deploy your React app. You can install the tools by running  Login to Firebase Make sure youre in the root directory of your React app and run the following command to login to firebase in your terminal  If youre not logged in, youll be asked to enter the credentials for your google account. Initialise Firebase Now that youre logged in, you will need to initialise Firebase in your React app. You can do it by running the following command  You will then be prompted with a series of questions and configuration options. Choose Hosting: Configure and deploy Firebase Hosting sites Choose Use an existing project Select the firebase project that you created What do you want to use as your public directory? (public) build Configure as a single-page app (rewrite all urls to /index.html)? No Now it is time to deploy our app Deploying Our React JS App on Firebase Run the following command to deploy your app:  Firebase will then give you a unique URL where your deployed app is located (e.g. react-app.web.app).Thats all there is to it!",
      "__v": 0
    },
    {
      "_id": "64e08932b72e199dda6040ba",
      "title": "What is React Native and Why you should use it as an App Developer ?",
      "content": "r ? According to Statista, React Native is the most popular technologies for cross-platform software development of mobile applications for two years in a row. Releasing an app on both App Store and Google Play is every businesss desire. But the trick is, before shipping a mobile app to the market, you have to choose a technology stack. Is it going to be React Native, Flutter or some other technologies ? What is React Native and Why you should use it ? React Native is a popular open-source library that runs on JavaScript.It is written with the combination of XML -Esque markup and JavaScript which is also known as JSX. React Native primarily focuses on the native rendering of applications that is majorly compatible with Android and iOS. Pros of React Native Large developer community Simplified UI Fast Applications Native rendering Performance Hot-reloading Popular apps made using React Native SoundCloud Instagram Facebook Flipkart Tesla Skype Wix Pinterest Conclusion Now, did you get the answer to your queries related to React Native App Development? Hope you have got a crystal-clear picture to what React Native is all about and why you should go for it for creating your mobile apps.",
      "__v": 0
    },
    {
      "_id": "64e08932b72e199dda6040bc",
      "title": "Turning Your Website Into A Mobile App using React Native",
      "content": "ive Every day more and more people spend their time on mobile platforms, especially when it comes to shopping and entertainment. Successful companies such as Amazon that initially launched only a website, but now they have also established their mobile presence. There are ,,,+ mobile users around to world, and If you want to make your business successful, you should have mobile app beside your website. React Native is a JavaScript framework for writing real, natively rendering mobile applications for iOS and Android. Its based on React, Facebooks JavaScript library for building user interfaces, but instead of targeting the browser, it targets mobile platforms. But in this article we are going to convert our website's web view into app !Turning Your Website Into A Mobile App using React Native Step : Installing Node JS Node.js is a platform built on Chrome's JavaScript runtime for easily building fast and scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. Go to nodejs.org and download and install the the latest stable version (LTS) in your computer. \"nodejs package includes both node and npm\" Step : Installing Expo Expo is a framework and a platform for universal React applications. It is a set of tools and services built around React Native and native platforms that help you develop, build, deploy, and quickly iterate on iOS, Android, and web apps from the same JavaScript/TypeScript codebase. The quickest way to install expo is using NPM, just open your terminal and write  Step : Initialising our app Open your terminal and write  This will generate a simple one screen app using React Native. Once your app is create, navigate to your app folder using  and run  On your phone, download and install Expo Go app, after installing open it and press \"Scan QR Code\" on the \"Projects\" tab of the Expo Go app and scan the QR code you see in the terminal or in Expo Dev Tools. Step : Converting our website to App Open your terminal and install react-native-webview by running the command  Once it is done, open your app's folder in any code editor such as VS code and in app.js paste the following code and save it  Step : Building Android and IOS App To build Android and IOS app your just need to write below command in your terminal For Android : expo build:android For IOS : expo build:ios You can view your app in your expo dashboard at expo.io That is it, We are done :) I know, it is a bit confusing but, Hope you understand it. If you have any issues please let me know :)",
      "__v": 0
    },
    {
      "_id": "64e08932b72e199dda6040be",
      "title": "How to Deploy React JS Applications for Free on Firebase",
      "content": "ase In this article , Ill show you how to deploy react js applications for free on Firebase, First lets understand what is Firebase What is Firebase Firebase is a Backend as a Service that started as a YC startup and grew up into a next-generation app-development platform on Google Cloud Platform. Firebase frees developers to focus crafting fantastic user experiences. You dont need to manage servers. You dont need to write APIs. Firebase is your server, your API and your datastore, all written so generically that you can modify it to suit most needs. Yeah, youll occasionally need to use other bits of the Google Cloud for your advanced applications. Firebase cant be everything to everybody. But it gets pretty close. Getting Started Before deploying your app on Firebase, you need to have Firebase project and a React project (create-react-app) Build Your App for Production Open your project folder cd your-project and build your app for production using command below  The JavaScript and CSS files will be inside the build/static directory. Install Firebase Tools Once you built your app then you can install Firebase tools that will allow you to deploy your React app. You can install the tools by running  Login to Firebase Make sure youre in the root directory of your React app and run the following command to login to firebase in your terminal  If youre not logged in, youll be asked to enter the credentials for your google account. Initialise Firebase Now that youre logged in, you will need to initialise Firebase in your React app. You can do it by running the following command  You will then be prompted with a series of questions and configuration options. Choose Hosting: Configure and deploy Firebase Hosting sites Choose Use an existing project Select the firebase project that you created What do you want to use as your public directory? (public) build Configure as a single-page app (rewrite all urls to /index.html)? No Now it is time to deploy our app Deploying Our React JS App on Firebase Run the following command to deploy your app:  Firebase will then give you a unique URL where your deployed app is located (e.g. react-app.web.app).Thats all there is to it!",
      "__v": 0
    },
    {
      "_id": "64e08935b72e199dda60411c",
      "title": "Building a Full Stack Web YouTube Clone with Next, IPFS, The Graph,  Solidity, and Livepeer",
      "content": "eer Every day more and more people are transiting to Web. The demand for developers is increasing and skills in blockchain development are among the most in-demand in the tech industry. The greatest approach to improving your Web skills is to use them to create projects. In this article, you are going to build a full-stack YouTube clone on top of the Polygon blockchain using the below tech stack. - Frontend framework: Next.js - Smart contracts: Solidity - Ethereum web client library: Ethers.js - File storage: IPFS - Querying data: The Graph - CSS Framework: TailwindCSS - Ethereum development environment: Hardhat - Layer blockchain: Polygon - Video Infrastructure: Livepeer You can find the final code of the application here. Prerequisites Before you start with the tutorial make sure you have Node.js v or greater, and the Metamask browser extension installed on your machine. Setting up Next.js App The first step is to set up a next.js app and install the required dependencies. In order to do that, you would need to run the below command in your terminal.  The following command creates a new directory named `web-youtube`, then navigates to that directory and creates a Next.js app. Once the project is created successfully, run the following command to install a few other dependencies.  - `react-icons` is an icon library that we will be using in the app. - `plyr-react` is a video player component with rich plugins and functionalities. - `moment` is a JavaScript date library for parsing, validating, manipulating, and formatting dates. - `ipfs-http-client` is used to upload videos and thumbnails to IPFS. - `ethers` is an Ethereum client literary that will be used to interact with the smart contracts You can also run the below command to install Hardhat as a dev dependency in your project.  Initialize local Ethereum environment Next, it is time to initialize a local smart contract development using Hardhat. To do that, simply run the below command in your terminal.  The above command will scaffold the basic Solidity development environment. You should see below the new files/folders generated in your project directory. `test`: This folder contains a test script written in Chai and it is used to test the smart contract. `hardhat.config.js`: This file contains the configuration for Hardhat. `scripts`: This folder contains a sample script to show to deploy a smart contract. `contracts`: This is the folder that includes the files in which we write our smart contract code. Adding TailwindCSS Tailwind CSS is a utility-first CSS framework for building user interfaces rapidly. We will be using it for styling our applications. Run the below command to install tailwindcss and its dependencies.  Once the dependencies are installed, we need to initiate the Tailwind CSS. To do that, run the below code in your terminal.  The above command will generate two files named `tailwind.config.js` and `postcss.config.js`. Next, open the project in any code editor and replace the code inside `tailwind.config.js` with the below code.  At last, add the tailwind directives for each of Tailwinds layers to the `./styles/globals.css` file.  You can also check if Tailwind CSS is integrated successfully by updating the code inside of the `pages/index.js` file.  Save the file and run `npm run dev` to start the next.js app and you should see a similar page. !https://t.p.clickup-attachments.com/t/ebccb-c--c-c/asdas.png The smart contract Now that the project setup is completed, we can start writing smart contracts for our application. In this article, I will be using Solidity. In the contracts folder, create a new file named `Youtube.sol` and add the following code to it.  Modifying the Hardhat configurations Now, we need to do some modifications to the Hardhat configuration file in order to deploy our smart contract. Open `hardhat.config.js` in your code editor and update the module.exports object to the below code.  To deploy our contract, we need a private key. Open Metamask in your browser and click on three top right and choose account details. !https://cdn.hashnode.com/res/hashnode/image/upload/v/zqJIHPs.png Then, click on \"Export Private Key\". You will be prompted to enter your Metamask password. Enter your password and click on Confirm. !https://cdn.hashnode.com/res/hashnode/image/upload/v/sZqhCme.png You should see your private key inside a box in red color. !https://cdn.hashnode.com/res/hashnode/image/upload/v/inCLsSj.png Create a `.env` file in the projects root directory and add your private key.  Compiling smart contracts with Hardhat Now that our smart contract is completed, let's go ahead and compile them. You can compile it using the command below.  if you have encountered Error HH: `Plugin @nomicfoundation/hardhat-toolbox requires the following dependencies to be installed`. Run the below command to install hardhat dependencies  Once the package is installed, re-run the above compile command. After the compile successfully completes, you should see a new directory named `artifacts` created in your projects directory. Artifacts contain the compiled version of our smart contract in JSON format. This JSON file contains an array called ABI. ABI or Application Binary Interface is what we need to connect our client (Next app) with our compiled smart contract. Deploying smart contract on Polygon Now, we can deploy our smart contract on a Polygon Mumbai. We have already added RPC and Metamask private keys, so we don't need to do it again. However, you need some $MATIC in order to deploy a smart contract. Navigate to https://faucet.polygon.technology/ and paste your wallet address. Click on confirm and you should receive . MATIC in your wallet. !https://cdn.hashnode.com/res/hashnode/image/upload/v/U-mHSxlS.png By default, Metamask doesn't have Polygon blockchain in the list of networks, so we need to add it manually. Go to Metamask settings and choose to Add a network manually. Use the below information to add Polygon Mumbai to the Metamask.  Save it and you should see . MATIC on your Metamask wallet. !https://cdn.hashnode.com/res/hashnode/image/upload/v/SumAetq.png Next, replace the code inside `scripts/deploy.js` with the below code. ``` // We require the Hardhat Runtime Environment explicitly here. This is optional // but useful for running the script in a standalone fashion through `node `. // // When running the script with `npx hardhat run ` you'll find the Hardhat // Runtime Environment's members available in the global scope. const hre = require(\"hardhat\"); async function main() { // Hardhat always runs the compile task when running scripts with its command // line interface. // // If this script is run directly using `node` you may want to call compile // manually to make sure everything is compiled // await hre.run('compile'); // We get the contract to deploy const YouTube = await hre.ethers.getContractFactory(\"YouTube\"); const youtube = await YouTube.deploy(); await youtube.deployed(); console.log(\"YouTube deployed to:\", youtube.address); } // We recommend this pattern to be able to use async/await everywhere // and properly handle errors. main() .then(() => process.exit()) .catch((error) => { console.error(error); process.exit(); });  npx hardhat run scripts/deploy.js --network mumbai  YouTube deployed to: xAEfbeefFbFD  npm install -g @graphprotocol/graph-cli ``` Once installed, run `graph init` in order to initialize the subgraph in the project. You will be prompted with some questions. You can follow the below code for the answers:  Make sure to update the contract address, name, and ABI. Next, lets declare the schema for our application. Replace the code inside of `schema.graphql` inside of the indexer directory with the below code.  Now, replace the code inside of `you-tube.ts` with the below code.  Navigate to the indexer directory and run `yarn codegen` to generate code from your GraphQL operations and schema. Building Subgraph Before we deploy the subgraph, we need to build it. To do that simply run the below command in your terminal.  Next, in order to deploy our subgraph, we need to create an account on The Graph. Deploying Subgraph Go ahead and create an account and then navigate to https://thegraph.com/hosted-service/dashboard. Click on Add Subgraph button. !https://cdn.hashnode.com/res/hashnode/image/upload/v/jqaFEVXm.png On the next, screen fill in information related to your subgraph and create Subgraph Button at the bottom of the screen !https://t.p.clickup-attachments.com/t/fa-bab-e-b-fbce/XDDmbEm.png Once the subgraph is created, copy its access token as we would need it later. In your terminal run `graph auth` and choose hosted service. In the deploy key, paste the key that you copied earlier. At last, run the below command to deploy your subgraph.  If everything goes fine, you should see your subgraph link similar to the below output.  The Frontend Now that we have completed smart contracts, it is time to work on the front end of the application. Lets start with the Authentication of the app. Authentication The first step is to set up authentication in our app that allows users to connect their wallets. Create a new folder named `landing` inside of the pages folder and create a new file inside of it named index.js. This file will have the code for the landing page in our application, which will also allow users to connect their wallets. Erase everything inside of `index.js` in the page directory and inside import the `Landing` file to the file. Here is what your index.js file should look like.  Now, on the landing page, we will create a simple hero component with connect wallet button that will allow users to connect their wallets and access our application. Add the below code to the landing page. I have already added comments so you can understand them properly.  If everything goes fine you should see a similar screen. You should also be able to connect your MetaMask wallet. !https://cdn.hashnode.com/res/hashnode/image/upload/v/AJGoGQEew.png Uploading videos Now that users are able to connect their wallets, it is time to add upload video functionality to our app. Create a new folder in the pages directory named `upload` and add a file named `index.js`. Inside of the file adds the below code. Again I have already added the comments on the code, so I hope that helps you to understand it.  And you should see a similar screen if you navigate to http://localhost:/upload. !https://cdn.hashnode.com/res/hashnode/image/upload/v/UFstyJpw.png This is a basic upload page, for now, we just have the inputs and save their value of them in the state. Before working on the handle submit function, create a new folder named `utils` and inside of it create a file named `getContract`. This file will be used to interact with your contract on the upload page. Add the below code to it and make sure to replace the contract address with your contract address.  Integrating IPFS (Web Storage) Now we need an IPFS client to upload thumbnails. There are many services that offer IPFS service, but for this tutorial, we will use web.storage. Create a new account in web.storage and then navigate to the Tokens page. !https://cdn.hashnode.com/res/hashnode/image/upload/v/mwVXLFdf.png Create a new token and copy the generated token as we will need it later. In the utils folder, create a new file named `saveToIPFS.js` and add the following code inside of it. ```jsx // importing axios import axios from \"axios\"; const saveToIPFS = async (file) => { // create a new multipart form data const formData = new FormData(); // add file to the form data formData.append(\"file\", file); var config = { method: \"post\", url: \"https://api.web.storage/upload\", headers: { Authorization: `Bearer WEB_STORAGE_TOKEN`, \"Content-Type\": \"text/plain\", }, data: formData, }; // Posting the form data to the IPFS API const response = await axios(config); // returning the CID return response.data.cid; }; export default saveToIPFS; ``` This is the file which we will use in order to upload thumbnails to the IPFS. Next, we need to integrate Livepeer in order to upload the videos and serve them through Livepeer CDN. Integrating Livepeer Livepeer is a decentralized video processing network and developer platform which you can use to build video applications. It is very fast, easy to integrate and cheap. In this tutorial we will be using Livepeer to upload videos and serve it. Navigate to https://livepeer.studio/register and create a new account on Livepeer Studio. !Untitled.png Once you have created an account, in the dashboard, click on the Developers on the sidebar. !Screenshot_--_at_.._PM.png Then, click on Create API Key, give a name to your key and then copy it as we will need it later. !livepeer-key.png Now back to the code, go ahead and install `livepeer.js` by running the below command in your terminal  Livepeer.js is a JavaScript SDK with ready-to-use hooks that allows us to quickly upload video, serve videos and connect to Livepeer Studio. Once the package is installed, create a new file inside of the root directory named `livepeer.js` and add the below code to initialise a Livepeer client.  Make sure to replace the `YOUR_API_KEY` with the key which you just copied from the Livepeer dashboard. And also replace the code inside of `_app.js` in the page directory with the below code.  And that is it, you can now use Livepeer to upload assets/videos. Back, let's import `saveToIPFS` and `Livepeer` by adding the below code on top of the component  You can also add the below Livepeer hook after the reference.  The above hook will be used in order to upload videos to the Livepeer. Next, add the below functions to the upload page after the Livepeer hook.  I have already commented on each line of the code so you can understand what is going on. Save the file and BOOM!! We are done with the upload functionality. You should now be able to upload videos to the contract. !https://cdn.hashnode.com/res/hashnode/image/upload/v/BUSgXVGH.png Connecting with The Graph In order to fetch videos from The Graph, we need to set up a graphQL client. Create a new file named `client.js` in the root directory and add the following code inside of it.  Make sure to replace the URI with your graph URL. And also replace the code inside of `_app.js` in the page directory with the below code.  In the above code, we have wrapped our code with `ApolloProvider` and provided the client which we created earlier as a prop. Fetching videos from Blockchain Create a new file named `index.js` inside of a new folder named `home`. And for now you can add the below code to the file. ``` import React, { useEffect, useState } from \"react\"; import { useApolloClient, gql } from \"@apollo/client\"; export default function Main() { // Creating a state to store the uploaded video const [videos, setVideos] = useState([]); // Get the client from the useApolloClient hook const client = useApolloClient(); // Query the videos from the the graph const GET_VIDEOS = gql` query videos( $first: Int $skip: Int $orderBy: Video_orderBy $orderDirection: OrderDirection $where: Video_filter ) { videos( first: $first skip: $skip orderBy: $orderBy orderDirection: $orderDirection where: $where ) { id hash title description location category thumbnailHash isAudio date author createdAt } } `; // Function to get the videos from the graph const getVideos = async () => { // Query the videos from the graph client .query({ query: GET_VIDEOS, variables: { first: , skip: , orderBy: \"createdAt\", orderDirection: \"desc\", }, fetchPolicy: \"network-only\", }) .then(({ data }) => { // Set the videos to the state setVideos(data.videos); }) .catch((err) => { alert(\"Something went wrong. please try again.!\", err.message); }); }; useEffect(() => { // Runs the function getVideos when the component is mounted getVideos(); }, []); return ( {videos.map((video) => ( {video.title} ))} ); } ``` Save the file and you should see a similar output. !https://cdn.hashnode.com/res/hashnode/image/upload/v/JZTlIzT.png As you can see for now we are just fetching the video title. So let's create a reusable component to display the videos nicely. Create a folder named `components`, and then create a new file named `Video.js` inside of it. Add the below code the file. It is a very basic video component.  Import the Video component to the home file and replace the map function with the below code. ``` {videos.map((video) => ( { // Navigation to the video screen (which we will create later) window.location.href = `/video?id=${video.id}`; }} ))} ``` Save the file and now you should see a nice-looking homepage, similar to below image. !https://cdn.hashnode.com/res/hashnode/image/upload/v/PiCmF_MwI.png Video page Now that we are able to fetch the videos in the home screen. Let's work on the video page where the user will be redirected if they click on any video component. Create a new file in the components folder named `Player` and add the below code to it. We are using Livepeer player to create a video player component.  Create another file in the same directory named `VideoContainer` . Imagine this component as the left side of the youtube video page, which contains a player, video title, upload date, and description. Add the below code to the file.  At last create a new folder named video inside of pages folder and create a new file `index.js` of it. For now you can add the following code to the file. ``` import React, { useEffect, useState } from \"react\"; import { useApolloClient, gql } from \"@apollo/client\"; import Video from \"../../components/Video\"; import VideoComponent from \"../../components/VideoContainer\"; export default function VideoPage() { const [video, setVideo] = useState(null); const [relatedVideos, setRelatedVideos] = useState([]); const client = useApolloClient(); const getUrlVars = () => { var vars = {}; var parts = window.location.href.replace( /[?&]+([^=&]+)=([^&])/gi, function (m, key, value) { vars[key] = value; } ); return vars; }; const GET_VIDEOS = gql` query videos( $first: Int $skip: Int $orderBy: Video_orderBy $orderDirection: OrderDirection $where: Video_filter ) { videos( first: $first skip: $skip orderBy: $orderBy orderDirection: $orderDirection where: $where ) { id hash title description location category thumbnailHash isAudio date author createdAt } } `; const getRelatedVideos = () => { client .query({ query: GET_VIDEOS, variables: { first: , skip: , orderBy: \"createdAt\", orderDirection: \"desc\", where: {}, }, fetchPolicy: \"network-only\", }) .then(({ data }) => { setRelatedVideos(data.videos); const video = data?.videos?.find( (video) => video.id === getUrlVars().id ); setVideo(video); }) .catch((err) => { alert(\"Something went wrong. please try again.!\", err.message); }); }; useEffect(() => { getRelatedVideos(); }, []); return ( {video && ( Related Videos {relatedVideos.map((video) => ( { setVideo(video); }} key={video.id} ))} )} ); } ``` Save the file and click on any videos on the home screen. You should be redirected to the video screen similar to the below page. !https://cdn.hashnode.com/res/hashnode/image/upload/v/aBmEGvHak.png Search Functionality Now that we are almost completed with apps functionality. Lets also add a search functionality. In the components folder, create a new file named `Header.js`. For now, you can add the below code.  This is a very simple component that is divided into parts. On the left we have a logo of the application, on the middle we have declared a input that users can type to search and at the last we have a icon that navigates users to the upload screen. Back to the home page (`pages/home/index.js`) import the Header component and add if after line number  Now you should see a header component in the home page. !Untitled .png align=\"left\") Declare a new state on the home page after line to capture the value in the search screen.  You can also update the Header component to set the value of the input in the above useState.  Lets also update the `getVideos` function to search videos in case there is some value in the state.  In the above function, we just added a `where` object to search for videos in case there is a value in the state. Finally, update the useEffect function to also run the function if there is a change in the search state.  And now if you search anything, you should see the videos automatically filters. Yayy !https://cdn.hashnode.com/res/hashnode/image/upload/v/qXYUoFJFK.png Whats Next? If you have come this far, it means that you have very passionate about building Web applications. here are a few other functionalities/improvements which you can add to the application if you are interested. - Allowing users to search for videos based on video category. (Check this repo, if you need a reference) - Trying to use Arweave instead of IFPS and see how it works. - Trying adding light mode to the application and allow users to toggle - You can also make the application responsive Conclusion That is it for this article. I hope you found this article useful, if you need any help please let me know in the comment section or DM me on Twitter. Let's connect on Twitter and LinkedIn. Thanks for reading, See you next time",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda60411e",
      "title": "Why GitHub as CI/CD",
      "content": "CD Today I am going to tell you about one amazing feature of Github which is CI/CD of course, which I use daily for my integration and deployment works, but why I chose Github?, plenty of brilliant and dedicated tools are there for Integration and Deployment. Brief intro of Github as a CI/CD tool:- Continuous integration (CI) automatically builds, tests, and integrates code changes within a shared repository. Continuous deployment (CD) automatically deploys code changes to customers directly. . GitHub provides all the required tools and various platform extensions in one place, so you can save time.",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda604120",
      "title": "DevOps",
      "content": "ps What is DevOps? In simple words, it is a combination of development (dev) and operations (ops). It is a method through which a team aims to integrate software development and operations to deliver applications and services at high velocity. DevOps has key principles, listed as Automation of the software development lifecycle Collaboration and communication Continuous improvement and minimization of waste Hyperfocus on user needs with short feedback loops What is the goal of DevOps? The main goal of DevOps is to establish an environment where delivering applications and services becomes faster and can occur more frequently. DevOps is a set of agile practices to improve collaboration between development and operation teams, so it eliminates traditional silos among them which we can say is an important goal of DevOps. Why DevOps? It provides continuous delivery of software. It enables better collaboration between teams. DevOps make deployment easy. It provides better scalability and efficiency. It provides more security and error fixation at an early stage. Through DevOps tools, human intervention got minimized, less chance of error. DevOps Lifecycle - DevOps lifecycle is divided into six phases which provides us with more clarity Management of Source Code - In this phase owners and the development team discuss project goals and makes a plan, then code for the project is made. Continuous Build and Test - This phase deals with making tools, then arranging and combining codes from different sources or repositories to build complete application. Then the application gets tested with selenium or other testing tools to check product quality. Continuous Integration - when the build and test phase is complete then new features get integrated automatically into the existing codebase. Continuous Deployment - In this phase, software or product is packaged and deployed from the development server to production server, once deployed to production server then operations start performing their task such as provisioning and configuring servers. Monitoring - This phase deals with the identification of issues in specific releases and understanding their impact on users. Product/Software Release - Through passing all the above phases and business requirements, it gets released in the market. Popular DevOps tools - Build Tools - Build tools are commonly known as programs that automate the process of building an executable application from source code. This building process includes activities like compiling, linking and packaging the code into an executable form. Build automation involves scripting or automating a wide range of tasks that software developers perform in their daily activities. Ex- Terraform, Gradle, Maven, Scala-oriented Build Tool (SBT) etc. Version Control Tool - It is the practice of tracking and managing changes to software code. Ex- GitLab, GitHub, BitBucket etc... Continuous Integration and Deployment Tool- (discussed in DevOps lifecycle section) Ex- Jenkins, TeamCity, GitLab, and CircleCI are among the popular CI/CD tools. Configuration Management Tool - Configuration Management is a method of ensuring that systems perform in a manner consistent with expectations over time. Ex- Chef, Puppet etc. Containerization Tool - containerization is a software deployment process that bundles an application's code with all the files and libraries it needs to run on any infrastructure Ex- Docker.",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda604122",
      "title": "DevOps (Intro to GitLab)",
      "content": "b) Introduction:- In this article, I will be focusing on GitLab, which I am frequently using for CI/CD. Competing with other tools in the field of CI/CD, GitLab has got some more user-friendly features, which help users to create a pipeline hassle-free. What is GitLab:- It is a DevOps software package that combines the ability to develop, secure, and operate software in a single application. It is a web-based git repository providing open-source free and private repositories. It provides monitoring for a project after the project is released. It also provides a good level of security. Nowadays to use GitLab pipelines we need to provide a credit card so that it can verify the user. Why should we use GitLab:- There are plenty of reasons to use GitLab over other software, but I will be focusing on a few important ones below... GitLab is the single platform to provide the entire DevOps cycle - This is the first software that has all DevOps cycles on a single platform, it means development, QA, product and operation teams can collaborate more efficiently on a single platform. Open core - GitLab core is open source or free but at a higher level, GitLab has both open source and proprietary features, commonly open features provided by the community edition and proprietary features by the enterprise edition. Higher Security - It has many security features integrated into it such as Dependency scanning, Static Application Security Testing (SAST), Dynamic Application Security Testing, and Container scanning. It has security checks built into the merge request(MRs). Transparency - We can build any software and DevOps infrastructure with our teams by maintaining honest and open relationships with each other. GitLab vs GitHub:- GitLab is open source and freely available for community edition whereas GitHub is not open source. GitLab is a cloud-native application whereas GitHub is a platform to share your project work codes with the public. GitLab is more secure whereas GitHub is less secure and doesn't have a security dashboard, License compliance. It provides an issue tracker, group milestones, time tracking, provides monitoring but GitHub doesn't have all these features.",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda604124",
      "title": "Apache Spark Installation",
      "content": "on On Windows Go to this page to download Apache Spark on its official page, this will download as a ' .tgz ' file, you need to unzip it. Put its extracted files inside a folder in C-Drive(recommended).  Download 'winutils.exe' as per your Spark version by going through the below link of GitHub. Put this file inside a folder named Hadoop in C-Drive(recommended).  In this step, you need to set the user variable and system variable path for spark and Hadoop. a) Copy the path of the 'winutils.exe' file(I kept it inside a folder named Hadoop) and add its home directory to the user variable as 'HADOOP\\_HOME'.  On Ubuntu Execute the below commands one by one  How to start and stop the master and slave in single command -  How to start and stop master ",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda604126",
      "title": "Web Scrapping using Python",
      "content": "on Introduction Web scraping is an automatic method to obtain large amounts of data from websites. Generally, web scrapping is used to obtain large amounts of data to train models in Machine Learning. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular APIs or even creating your code for web scraping from scratch. Languages for Web Scrapping Following are some popular languages for web scrapping:- Python (most recommended) Javascript Java C++ Libraries used for Web Scrapping using python BeautifulSoup  Scrapy  Selenium  Requests  Urlib  Lxml  Mechanical Soup ",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda604128",
      "title": "Web Scrapping demo project",
      "content": "ct Introduction In this project, I'm going to show you my web scrapping project which is of course very small but will give you an insight into web scrapping. I've done web scrapping over a FlipKart item (hp Pavillion laptop), you can choose your own, using some libraries and making a function for it. Codes ",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda60412a",
      "title": "Web Scrapping & Apache Airflow",
      "content": "ow please check my previous blog on the installation of apache airflow Introduction In this blog, I'm going to tell you how to schedule your project with Apache Airflow, so that you don't have to trigger your function/projects on your own, it will be taken care of by DAGs. Brief Introduction of Airflow and DAGs Apache Airflow is used for the scheduling and orchestration of data pipelines or workflows. Orchestration of data pipelines refers to the sequencing, coordination, scheduling, and managing of complex data pipelines from diverse sources. DAGs (Directed Acyclic Graphs) are a set of tasks to be executed at some interval as scheduled. It is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies. Codes To follow the below codes, you need to check out my previous blog 'Web Scrapping demo project' as I will be taking reference of functions inside that project. Below are the codes for DAGs...  If you want to insert an email notification in the above codes so that you can get notifications then use the below codes... ",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda60412c",
      "title": "Data pre-processing",
      "content": "ng Introduction In this blog, I will share my experience with data pre-processing, what is meant by data pre-processing and why we need it, how I encountered many unknown errors in making models with large data, and will show all images of my errors through which you can understand the need for data pre-processing. What is data pre-processing The method of converting raw data into useful data, that can be used further to analyze or predict.  don't just look at numbers as they aren't pointing at a good model but I got rid of 'nan' loss and . accuracy. Upon model, I'm still working.",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda60412e",
      "title": "Covid- Data Analysis(India)",
      "content": "dia) In this analysis, the dataset is taken from the below link. https://www.kaggle.com/datasets/imdevskp/corona-virus-report Introduction This dataset is available with Indian cases only, so the data analysis will show the different states of India and their covid cases. In this analysis, I will show you how to fetch data from the drive and get the path of your file to perform analysis over it using the pandas Dataframe. In this analysis, I will be visualizing the data and plotting them for better understanding. Analysis    !Output of above command            Thank You...",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda604130",
      "title": "Jovian",
      "content": "an Introduction Jovian is a cloud platform for storing and managing Jupyter notebooks, code, and data. To use Jovian, you will need to install the `jovian` Python library and authenticate your account using an API key. Below is the process to do it. Process Install the `jovian` library by running the following command in a cell:  Once the installation is complete, import the `jovian` library and call the `jovian.authenticate()` function, as follows:  This will prompt you to enter your API key, which you can find in your Jovian account settings. Once you have authenticated your account, you can use the `jovian` library to save and share your notebooks. For example, you can use the `jovian.commit()` function to save a snapshot of your notebook, as follows:  This will create a new version of the notebook and save it to your Jovian account, along with any code and data files associated with the notebook. You can then use the Jovian platform to view and manage your saved notebooks, as well as share them with others. Google Colab Notebook When I tried to do the above things in Google Colab then I got this result.....  Summary You don't need Jovian in Google Colab, use it in Jupyter notebook or other notebooks.",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda604132",
      "title": "nsetools (National Stock Exchange)",
      "content": "e) Introduction It is a Library that works without any setup requirement, used for collecting/extracting real-time data from National Stock Exchange (India). Installation and Upgrade  Usage with examples In this section, we will see usage and cover all APIs that nsetools offers.          Thank You....",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda604134",
      "title": "Dev Retro ,",
      "content": ", In the daily life of Software Engineers, the only common friend is open-source websites or communities like stackoverflow etc. These websites help us a lot when we are stuck alone with a problem. Thanks to all the open sources for being with us throughout every journey. Challenges and Lessons:- Challenge (learning new techs and tools) - At the start of , I had to learn some techs and language to get allocated to projects, we got basic training for it but it was not enough to get started, so I took help from different open projects, tutorials and a few YouTube videos. Cleared my concepts through small projects, and get a hold of those techs and their tools. Lesson learned -&gt; search more about that topic over which you want a hold of, and make some projects using that to test your skills. Achievement -&gt; Learned those techs and tools within time and made projects to showcase my skills. Challenge (lead a team for a new project) - I was in my training phase, trying to make a small project with member team, and the time was given as one month. I figured out the path or roadmap for the project and assigned roles to everyone with time limits. The problem was the code that we wrote, the code was messy and needs to align with the aim, so I spent more time reviewing the codes and rectifying if there were errors. The main problem was waiting for us at the deployment stage, but I figured out with different approach, I chose different platforms to host any application and tested each one of them, discussed with each member, and we proceeded with the best possible way. Lesson learned -&gt; Discuss more with your team member to get new and unique ideas, ask them about other unconventional approaches also, talk about each other doubts and problems in accomplishing the task, and take responsibility if anything happens in the project. Achievement -&gt; Our team was recognized as the best team because of our project out of six teams. Challenge (Client interviews) - Got a chance to get interviewed for a new role of data engineer, I had done a couple of projects and had good knowledge of some data engineering tools but the requirements for that role were slightly different from my expectation. The time before my first interview, I spent more time learning that required techs and tools, made a small project with that techs to showcase my skill, and took the help of a senior in making some real-time projects also. Lesson learned -&gt; Take the help of seniors, take some time before any interview to revise your strong and weak points if required then learn new techs also. Achievement -&gt; Cleared all four rounds of interviews and got into the project successfully. Challenge (Learn more out of interest) - As for getting into any project, you need to focus on that particular field and its related techs but I got fascinated by some other techs also which I am not using in real-time but wanted to learn. With doing my work every day, I need to take out some time to learn that tech and to do hands-on. I started to learn, got basic knowledge of it, and made errors, but got over them quickly. Then I made a small project with that tools and showed them to other mates, got appreciation from them, and now looking to make more projects and go deeper into it. Lesson learned -&gt; Never limit yourself to fewer techs and tools, go and search out different tools and if that is fascinating then learn it. Achievement -&gt; Learning new techs and making projects through them, and also indulging in research work.",
      "__v": 0
    },
    {
      "_id": "64e08936b72e199dda604136",
      "title": "Broken DAGs issue(Airflow)",
      "content": "w) Introduction In this blog, I'm going to point out one error that I used to encounter very often while scheduling DAGs and its solution, this blog will also give you a little insight into DAGs and Airflow. Note:- I have installed Airflow through Docker Images, I'm using Apache Airflow .. version so this solution should be working perfectly for ..+ versions of apache airflow, not sure for versions older than ... Apache Airflow Apache Airflow is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows. Airflow's extensible Python framework enables you to build workflows connecting with virtually any technology. A web interface helps manage the state of your workflows. DAGs DAGs. In Airflow, a DAG  or a Directed Acyclic Graph  is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies. A DAG is defined in a Python script, which represents the DAGs structure (tasks and their dependencies) as code. Broken DAGs  Here, I have installed the 'joblib' library in my virtualenv and also kept this lib in requirements.txt but still getting this error. Why???? Temporary Answer :- Because your airflow isn't able to configure this library, to get it configured you need to specify this library in docker-compose.yml file inside the 'environment' tag under '\\_PIP\\\\_\\ADDITIONAL\\_REQUIREMENTS: ${}'.  Download the docker-compose.yml file from here:- docker-compose.yml For more information check this link:- env-variables-docker-compose after adding the library to the docker-compose.yml file you need to do below things:-  if changes do not reflect by the above command then use the below command:-  This will install all the required python lib(S) along with Airflow services. Best Anwer :- The below link will take you to the discussion section... %[https://github.com/apache/airflow/discussions/] if not going through the above link then do the below things... ```python In order to add custom dependencies or upgrade provider packages you can use your extended image. Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml and uncomment the \"build\" line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:|version|} build: . python ```",
      "__v": 0
    },
    {
      "_id": "64e08937b72e199dda604138",
      "title": "NASDAQ data pipeline through GCP",
      "content": "CP Introduction This blog gives a brief knowledge of how a pipeline is made and works to fetch, and analyze it further as per requirement, I will be doing this by taking GCP into account. Pre-requisites `google-cloud` library should be installed by using `pip install google-cloud` and you should have a GCP account with admin access(to avoid any restrictions) or a service account with required privileges. You should have a project(project\\_id needed), a BigQuery table, and PubSub(topic). Procedure Import all necessary libraries  We need to keep logs of every activity.  We will store data in BigQuery tables, and for that, set up a BigQuery client.  We need to define a function to process the incoming data  Setup PubSub client  Now, define your cloud function  Last, define a function to fetch Nasdaq data ",
      "__v": 0
    },
    {
      "_id": "64e08937b72e199dda60413a",
      "title": "Job Listing with GCP",
      "content": "CP Introduction This blog will give you a good insight into the pipeline to fetch jobs around the US or your preferred location, saving them into BigQuery tables, and iterate this process for you every day or at specified intervals. Pre-requisites You should have a GCP account with admin access or a privileged service account to perform tasks. I will be using Python language for it. Procedures This project consists of three `.py` files named as `scraper.py` , `server.py` , `scheduler.py` . codes for `scraper.py`  codes for `server.py`  codes for `scheduler.py` ",
      "__v": 0
    },
    {
      "_id": "64e08937b72e199dda60413c",
      "title": "Covid cases fetching with GCP",
      "content": "CP Introduction This blog is all about a pipeline that fetches daily covid data as it is orchestrated by apache-airflow and saves the data into the BigQuery table. Pre-requisites You should have a GCP account with admin or privileged access, and apache-airflow installed or you can use cloud composers also(GCP). Procedures This project consists of two files named as `function.py` which contains a couple of functions to fetch and save the data, `dag.py` which contains codes for airflow. I will also give a `Dockerfile` , in case you need this project to deploy. codes for `function.py`  codes for `dag.py`  codes for `Dckerfile`  Save to local If you don't want to save the data into tables instead you want the data into `.csv` format saved directly into your local directory then just modify `function.py` by changing `save_cases` function with the below codes. ",
      "__v": 0
    },
    {
      "_id": "64e08937b72e199dda60413e",
      "title": "Limitations of 'Pandas'",
      "content": "s' Introduction Pandas is a popular and powerful library for data manipulation and analysis in Python. It provides easy-to-use data structures and data analysis tools for handling and manipulating numerical tables and time series data. The two primary data structures in pandas are the Series (-dimensional) and DataFrame (-dimensional). The Series object is similar to a column in a spreadsheet or a one-dimensional array, while the DataFrame object is similar to a table in a relational database or a two-dimensional array. Pandas provides a wide range of tools for data manipulation and analysis, including data filtering, aggregation, and transformation. It also provides functionality for handling missing data and working with time series data. Realizations Recently, I came across a situation in an ongoing project where I had to look for an alternative to pandas, this happened when the data was Petabyte and even more. My approach was to convert the table data into pandas DataFrame and then start analyzing and further processing, but you can't just easily convert the petabyte data into DataFrame for further processing, it cost you time and even system crash, it is also not a good practice to use pandas for a huge dataset. Limitations Pandas is a popular and powerful library for data manipulation and analysis in Python. However, it has some limitations when dealing with very large datasets. Some of these limitations include: Memory limitations: Pandas stores data in memory and can consume a lot of memory when working with large datasets. This can cause performance issues and even crash the program. Slow performance: Pandas can be slow when performing certain operations on large datasets, especially when working with data that does not fit in memory. Limited support for distributed computing: Pandas is not designed for distributed computing and does not have built-in support for working with data that is spread across multiple machines. Alternatives If possible then try to use SQL queries(static or dynamic) for dealing with large datasets in a table before going for the below alternatives. Alternatives to Pandas for working with large datasets include: Dask: Dask is a parallel computing library that can be used to work with large datasets that do not fit in memory. It allows you to perform operations on data in a distributed environment, which can be faster and more memory-efficient than using Pandas alone. Apache Arrow: Apache Arrow is a columnar in-memory data format that is designed to work with large datasets. It can be used with a variety of libraries, including Pandas, to improve performance when working with large datasets. Vaex: Vaex is a python library that allows you to perform out-of-core (data is read from disk as needed) and lazy computations on large datasets. This allows you to work with datasets that are much larger than the available memory. PySpark : PySpark is a python library that allows you to perform distributed data processing using the Apache Spark framework. PySpark allows you to work with large datasets in a distributed computing environment, making it a good alternative for data processing and analysis at scale.",
      "__v": 0
    },
    {
      "_id": "64e08937b72e199dda604140",
      "title": "V's of Big Data",
      "content": "ta In the world of Big Data, if you are entering for the very first time then definitely you'll come across the five V's named Variety, value, volume, velocity, and veracity of Big Data. What are they? Why are they important to understand or to start with? Do we skip these V's to learn Big Data? Are they really important? ... These were the questions when I started learning Big Data but Let me tell you that every small thing in this topic will lead you to understand Big Data more clearly. Variety - This means the diversity or range of data that we are getting as structured, semi-structured, and unstructured. Value - It refers to the important insight or meaning that we are generating from the data for better understanding and effective operations. Volume - It means the amount of data that we are fetching, managing and analyzing at a particular time interval. Velocity - This is an important V in Big Data because when we are dealing with huge and complex data then receiving it from different sources takes a certain amount of time, so, lesser time means better velocity. Veracity - It refers to uncertainty in data because sometimes the data we got are messy and becomes very difficult to control its accuracy and quality. Apart from these five V's of Big Data, there are few others V's you can encounter on Internet but they aren't important V's to consider, you can neglect them or if have ineterest then go for it. But as per my working experience I can say that above V's are important to keep in mind to get started with Big Data.",
      "__v": 0
    },
    {
      "_id": "64e08937b72e199dda604142",
      "title": "Google Cloud Dataflow",
      "content": "ow Introduction Google Cloud Dataflow is a fully managed, cloud-native data processing service that allows you to build and run data processing pipelines. It is designed to handle batch and streaming data sources with ease and scale to accommodate the demands of large-scale data processing. The service is based on Apache Beam, a unified programming model for batch and stream processing, and provides a simple, intuitive interface for building data processing pipelines. The pipeline is defined in code, and then Dataflow takes care of automatically executing the pipeline on a managed cluster of virtual machines. Benefits Below are major benefits which make Dataflow more popular. Scalability:- Dataflow can scale automatically to handle large data processing demands, with the ability to spin up and down worker nodes as needed. Managed service:- Dataflow is a fully managed service, which means that you do not need to manage the underlying infrastructure. This reduces the operational burden of running large-scale data processing pipelines. Flexibility:- Dataflow can handle batch and streaming data sources, and supports a variety of programming languages, including Java, Python, and Scala. This makes it a flexible solution for a wide range of data processing needs. Integration with other GCP services:- Dataflow integrates seamlessly with other Google Cloud services, such as BigQuery, Cloud Storage, and Pub/Sub, making it easier to build and manage data processing pipelines that span multiple services. Pros Few important advantages I have written down below : Efficient data processing: Dataflow provides a high-level programming model that allows you to focus on the logic of your data processing pipeline, rather than the underlying infrastructure. This can result in more efficient data processing compared to a manual approach. Cost-effective: Dataflow is a fully managed service, which means that you do not need to invest in the underlying infrastructure. This can result in cost savings compared to a manual approach, especially for large-scale data processing pipelines. Cons Below are cons which I have found in my working experience, can differ from others : Steep learning curve: Although Dataflow provides a simple interface, building and managing large-scale data processing pipelines can still be a complex and challenging task, especially for those with limited experience with data processing. Limited customization options: Dataflow is designed to handle common data processing tasks, but may not be suitable for custom or highly specialized data processing needs. Limitations Language support: Currently, Dataflow only supports a limited number of programming languages, including Java, Python, and Scala. This may limit its usefulness for those who prefer to work with other programming languages. Performance: Dataflow can be slower than other data processing tools, especially for smaller data processing tasks. This is because Dataflow is designed for large-scale data processing, and the overhead of the managed service can impact performance for smaller tasks. Conclusion Google Cloud Dataflow is a powerful and flexible solution for large-scale data processing. It is a fully managed service that provides a simple and intuitive interface for building data processing pipelines and integrates seamlessly with other Google Cloud services. However, building and managing data processing pipelines with Dataflow can still be a complex task, and the limited language support and potential performance overhead may limit its usefulness for certain use cases.",
      "__v": 0
    },
    {
      "_id": "64e08938b72e199dda604144",
      "title": "How  I  automated  'Monitoring Job'",
      "content": "b' My friend is a Software Engineer at a recognized MNC, working as Support Engineer on a project, I have been watching my friend doing a monitoring job specifically in that project from last month. His job is to check every mail, their criticality, if it is high/medium/low then he has to forward it to the responsible team to fix it as soon as possible, also in every mail he has to check some more parameters like from which team or client this error/alert is coming.  method is made to send a mail with a message and subject.  The below code loop through all emails and do operation like checking the subject and body, then based on cases it forwards the mail.  Conclusion I tried the above automation by observing only one kind of monitoring job, this is a basic example of what an automated monitoring job of the above-explained task looks like, there will be some change that needs to be made above to meet the exact automation.",
      "__v": 0
    },
    {
      "_id": "64e08938b72e199dda604146",
      "title": "Sentiment Analysis of 'hashnode' Articles",
      "content": "es Introduction I'm going to tell you how you can see an article's sentiment(rated from - to ). This blog is focusing on fetching the data from a hashnode account, storing it in a BigQuery table and using NLP to measure the sentiment of each article through its title. Pre-requisites You should have a GCP account, with a service account(recommended). Must have a hashnode account through which you can access Personal Access Token.  The above-explained things are just basic but we can explore more through them. Let me know if you have any ideas to explore this topic more.",
      "__v": 0
    },
    {
      "_id": "64e08938b72e199dda604148",
      "title": "Building a Full Stack Web TikTok Clone with React Native, Livepeer, and Lens Protocol",
      "content": "col Building real-world projects in Web is important for anyone looking to understand and develop decentralized applications. It's essential to gain hands-on experience in using the latest technologies and frameworks to build a functional application. By working on projects like a Tiktok clone, you can learn about various aspects of Web, such as social graphs, data querying, video infrastructure, and wallet authentication. These skills can be applied to develop more complex and sophisticated decentralized applications, paving the way for the future of the internet. In this tutorial, you are going to build a full-stack Tiktok clone using the below tech stack. Mobile framework: React Native Social graph: Lens Protocol Querying data: Lens API Video Infrastructure: Livepeer Wallet Authentication: WalletConnect You can find the final code of the application here. Prerequisites Before you start with the tutorial make sure you have Node.js v or greater and Expo CLI installed on your machine. Setting up React Native app To get started, you need to set up a React Native app and install the required dependencies. To do this, simply run the following command in your terminal:  This command creates a new React Native app using Expo CLI. The process may take some time depending on the speed of your machine and internet connection. Once the project has been created successfully, run the following command to install additional dependencies.  `react-native-walletconnect` provides a way for our app to connect with a user's crypto wallet using WalletConnect. This is important since we want to authenticate users with their wallets. `@react-native-async-storage/async-storage` provides a simple way to store key-value data in our app. We can use this to save different data such as auth tokens, user IDs, etc. `@apollo/client` is a package that allows us to easily connect your app to a GraphQL server (Lens API). `graphql` is a query language for APIs that works seamlessly with `@apollo/client`. `@livepeer/react-native` is a package that provides components and hooks to make it easier to work with Livepeer's video infrastructure in our React Native app. `@react-navigation/native` is a library for implementing navigation in React Native apps `react-native-screens` provides an easy way for managing screens and transitions in React Native apps `react-native-safe-area-context` is used for handling safe area insets in our app `@react-navigation/material-bottom-tabs` is a package for implementing bottom-tab navigation in our app `expo-media-library` is used for accessing and managing media assets (such as videos) on a user's device Yeah, I know the list is pretty long. But hey, we gotta have all these libraries for our app to work smoothly. I mean, we are building a Tiktok clone, so we gotta make sure it's on point! Setting up the navigation Now we can move forward and add navigation to our app. We will be using react-navigation which is a widely used navigation library for react native apps. To get started, create a new folder called `screens` inside the root directory of the project. Then, create a new file inside that folder called `Login.js`. For now, you can add the simple code snippet below to the `Login.js` file, which will just display the words `Login Screen`:  Next, create a new file called `routes.js` in the root directory and add the code below to it. This code imports the Login screen we just created and adds it to the navigation container:  Finally, replace the existing code inside app.js with the following code to import and define the route file  With these steps, we've set up a basic navigation structure for our app, and we can now easily add more screens and navigation options as needed. Setting up GraphQL Client Let's now set up our GraphQL client which will allow us to interact with Lens API. We'll be using Apollo GraphQL, a popular and efficient way to set up a GraphQL client. GraphQL is an open-source query language that offers flexible and user-friendly syntax to describe data requirements and interactions. As an alternative to REST, you can create GraphQL requests that include data from various sources from a single API call. To get started, create a new folder named `clients` inside the root directory. Inside the `clients` folder, create a new file named `apollo.js` and add the following code to this file:  Setting up Livepeer Livepeer is a decentralized video processing network and developer platform which you can use to build video applications. It is very fast, easy to integrate and cheap. In this tutorial, we will be using Livepeer to upload videos and play them back. As mentioned earlier, Livepeer will be used as the video infrastructure in our application. It automatically transcodes and serves the videos that users upload for seamless playback. Navigate to https://livepeer.studio/register and create a new account on Livepeer Studio.  { ... on RelayerResult { txHash txId } ... on RelayError { reason } } } `; ``` Then, inside of the `upload.js` screen, you can use `useMutation` to post the metadata to Lens API:  Whats Next? So, you've made it this far! That's awesome and it tells me that you're enthusiastic about creating Web apps. Now, if you're feeling up for it, I have some ideas on how you can take your app to the next level: How about giving users the ability to search for other users and videos? It could make your app more user-friendly and convenient. You could also experiment with using Arweave instead of IFPS and see how that affects your app's performance. If you're looking to make your app more comprehensive, why not add some extra screens such as a user profile section? This could give your app more depth and allow users to personalize their experience. And finally, if you want to get your app ready for prime time, don't forget to make the like and comment buttons actually work! Adding these functionalities will make your app more engaging and keep users coming back for more. These are just a few ideas, but the possibilities are endless. Don't be afraid to get creative and have fun with it! Conclusion That is it for this article. I hope you found this article useful, if you need any help please let me know in the comment section or DM me on Twitter. Let's connect on Twitter and LinkedIn. Thanks for reading, See you next time",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda604060",
      "title": "Cool Javascript APIs you didn't know",
      "content": "Hey there, coding enthusiasts! Are you tired of using the same old JavaScript APIs? Well, we've got some great news for you! We're here to introduce you to five awesome JavaScript APIs that you might not have heard of before. Let's get started! The SpeechRecognition API  Are you tired of typing out long commands or code lines? Well, you can now take a break from the keyboard and use your voice with the SpeechRecognition API! This API enables you to control your web application using voice commands, making it easier and faster for you to get things done. It's like having your own personal assistant!  The Intersection Observer API Do you struggle with detecting when an element is in view on your website? The Intersection Observer API is here to save the day! This API simplifies the process of detecting when an element enters or exits the viewport. Say goodbye to complex code and hello to a simple solution! Look at this:  The Web MIDI API Do you love music and coding? The Web MIDI API combines both of these passions, allowing you to connect your web application to MIDI devices such as keyboards and synthesizers. You can even send and receive MIDI messages to control and manipulate sound in real-time. Here's an example of how you can use the Web MIDI API:  The DeviceOrientation API Do you want to create a web-based game that uses the device's orientation to control the gameplay? The DeviceOrientation API is here to help! This API enables you to access the device's gyroscope and accelerometer data, giving you precise information about the device's orientation and movement. You can use this information to create immersive and interactive gaming experiences that are sure to impress. ```javascript window.addEventListener('deviceorientation', event => { const alpha = event.alpha; const beta = event.beta; const gamma = event.gamma; console.log(`Device orientation: alpha = ${alpha}, beta = ${beta}, gamma = ${gamma}`); }); javascript const supportedPaymentMethods = [ { supportedMethods: 'basic-card', data: { supportedNetworks: ['visa', 'mastercard', 'amex'], supportedTypes: ['credit', 'debit'] } } ]; const paymentDetails = { total: { label: 'Total', amount: { currency: 'USD', value: '.' } } }; const options = {}; const request = new PaymentRequest(supportedPaymentMethods, paymentDetails, options); request.show() .then(paymentResponse => { // Process the payment }) .catch(err => { console.log('Payment request failed:', err); }); ``` In conclusion, these five JavaScript APIs are awesome tools that can help you create better and more efficient web applications. So go ahead and give them a try! With these APIs in your toolbox, you're sure to impress your colleagues and friends alike. Happy coding!",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda604062",
      "title": "Make your own blog with hashnode!",
      "content": "Hi there everyone! Tired of complicated blogging platforms that require a computer science degree just to use them? Do you want to focus on writing great content instead of fighting with technicalities? Then let me introduce you to Hashnode! Hashnode is a user-friendly platform that allows you to create your own blog in minutes: you don't need to worry about hosting, security, or even design. All you have to do is sign up, choose a name for your blog, and start writing! But wait, there's more! Hashnode offers some fantastic features that make it stand out from other blogging platforms. For starters, you can customize your blog's domain name to match your brand or personal style. Plus, Hashnode provides a built-in analytics dashboard to help you track your traffic and audience engagement. Or you can even change the FULL CSSS of your blog! Hashnode also has a very good community of developers and bloggers who use the platform to share their knowledge and connect with like-minded individuals. You can join discussions, participate in challenges, and even collaborate with other bloggers to grow your network. And if you're worried about the technical side of things, don't be. Hashnode uses modern web technologies and is constantly updated to provide the best experience possible. You can focus on creating amazing content, and Hashnode will take care of the rest. For example, hashnode's new editor, Neptune, has AI features built in! So what are you waiting for? Get started! hashnode.com",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda604064",
      "title": "Tips to write clean, maintainable code",
      "content": "Most people strive to write code that is clean and easy to maintain. However, with deadlines looming and pressure to deliver results, it's easy to fall into bad habits that lead to messy and unmaintainable code. In this article, I'll share some tips on how to write clean, maintainable code that not only makes your life easier but also benefits your colleagues and future self. Write code that is easy to read One of the most important aspects of clean code is readability. Code that is difficult to read can lead to confusion, errors, and frustration for everyone involved. To ensure your code is easy to read, use descriptive variable names, write comments to explain tricky sections and break up long blocks of code into smaller, more manageable sections. Follow coding standards Every development team should have coding standards in place to ensure consistency in coding style and formatting. Following these standards makes it easier for developers to read and understand each other's code, which is especially important when working on large projects or in a team environment. Additionally, adhering to standards can help reduce the occurrence of bugs and errors. Keep it simple Simplicity is key when it comes to clean code. Instead of trying to impress your colleagues with complex algorithms and intricate designs, focus on writing code that is simple and easy to understand. This will make it easier to maintain and debug in the future, saving you and your team valuable time. Don't repeat yourself (DRY) Repeating code is a common mistake that leads to bloated, difficult-to-maintain code. Instead of copying and pasting code blocks, consider creating reusable functions or classes for your project. This not only makes your code more maintainable but also reduces the risk of errors and inconsistencies. Test your code Testing is an essential part of the development process that ensures your code works as intended. By writing tests for your code, you can catch errors early on and prevent them from causing bigger problems down the line. Additionally, tests make it easier to maintain and refactor your code in the future. Refactor regularly As projects grow and evolve, it's essential to regularly review and refactor your code to ensure it remains clean and maintainable. This involves identifying and removing unnecessary code, improving code readability, and updating code to adhere to new coding standards. By making small, regular improvements to your codebase, you can prevent technical debt from accumulating and avoid major headaches in the future. Writing clean, maintainable code is essential for any software developer looking to create high-quality, reliable applications. By following these tips and making a conscious effort to write code that is easy to read, simple, and reusable, you can save yourself and your team time, effort, and frustration in the long run. Remember, clean code is not just good for the present, but also for the future of your project.",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda604066",
      "title": "Explainable AI (XAI)",
      "content": "Introduction Explainable AI (XAI) is a framework for designing and developing artificial intelligence (AI) systems that can be understood and interpreted by humans. The goal of XAI is to make AI systems transparent and explainable so that end-users can understand how a decision was made, and experts can identify errors or biases in the system. XAI is becoming increasingly important as AI is used in high-stakes decision-making processes, such as medical diagnosis or financial risk assessment, where the consequences of a wrong decision can be significant.  Explainable AI tools available in GCP Below are some of the Explainable AI tools and services available in GCP: Cloud AutoML: Cloud AutoML is a suite of machine learning tools that allow users to build custom models without needing extensive AI expertise. It includes a range of Explainable AI features, including model interpretation and explanation. AI Platform: AI Platform is a cloud-based platform for building, testing, and deploying machine learning models. It includes a range of Explainable AI tools and services, such as model explanation, interpretation, and fairness evaluation. TensorFlow: TensorFlow is an open-source machine learning framework developed by Google. It includes a range of Explainable AI features, including model interpretation and visualization. Cloud AI Explainability: Cloud AI Explainability is a tool that helps users understand how AI models make decisions. It provides insights into the factors that contribute to model predictions, allowing stakeholders to identify potential biases or errors. Limitations As XAI is evolving day by day, its limitations are also reducing gradually but still has the following limitation which needs to be considered: - Complexity: XAI is limited by the complexity of the underlying machine learning models. As models become more complex, it can be difficult to identify which inputs and decisions led to a particular output, making it challenging to provide a clear explanation. Trade-Offs: In many cases, XAI requires trade-offs between model performance and explainability. For example, adding interpretability features to a model can often result in a loss of accuracy. Domain-Specific: Explainable AI methods are often domain-specific, which means that they are designed to provide explanations for a specific type of model or task. This can limit their applicability across different domains. Limitations of Data: Explainable AI requires access to large amounts of high-quality data to train models, and this can be a limitation in certain domains where data is scarce or sensitive. Conclusion In conclusion, as AI continues to evolve, the importance of Explainable AI cannot be overstated. With GCP's robust suite of Explainable AI tools and services, users can ensure that their AI systems are transparent, interpretable, and fair, providing a solid foundation for building trust in AI.",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda604068",
      "title": "Chrome extensions you must have in your computer",
      "content": "As someone who spends some time on the internet, I know how important it is to have a reliable browser that can keep up with your needs. And when it comes to browsing the web, Google Chrome is one of the most popular and versatile options available. Here are my top must-have Chrome extensions: AdBlock Plus - This extension is a must-have for anyone who's tired of being bombarded with ads while they're trying to browse the web. AdBlock Plus blocks most ads from appearing on your screen, making your browsing experience faster and more streamlined. Bitwarden - With so many websites and accounts to keep track of, it's easy to forget your passwords. Bitwarden takes care of that problem by storing all your passwords in one place and automatically filling them in when you need them. Bonus: It's open-source! Grammarly - Whether you're writing an email, a social media post, or a blog post, Grammarly can help you improve your writing by catching grammar and spelling mistakes in real time. It's like having a personal editor right in your browser. Honey - If you're an online shopper, Honey is a must-have extension. It automatically finds and applies coupon codes to your online purchases, saving you money with minimal effort. I personally do not shop much, but it works well. Manganum - Finally, if you want to make your browsing experience more enjoyable and productive, try Manganum. This extension replaces your new tab page with a beautiful, customizable dashboard that includes whatever you want. Plus, you get a sidebar on sites that you can show when you hover the right corner of the page, which includes ChatGPT, Google Keep and much more. Happy browsing!",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda60406a",
      "title": "Getting started with Hashnode Rix",
      "content": "Rix is an AI chatbot made by Hashnode that lets you \"chat\" with programming languages, popular libraries, etc. with accurate responses based on the library's documentation, to prevent hallucinations. I have chatted for some time with Rix, and it works quite well. You can also submit your library by clicking \"Submit doc\" at the top, which is very useful. The interface is also very simple, with a sidebar on the left and a chat on the right. Rix is based on Hashnode's DocsGPT, which lets you add an AI chatbot to your open-source library's documentation. Getting started with Rix Start by going to https://hashnode.com/rix. This is Rix's website. If you don't have any, create a Hashnode account. On the modal, follow all the steps indicated. After creating an account, select a programming language or library in the sidebar on the left. Start chatting with the bot on the right side. Enter your prompt on the input, and watch the response showing on the screen. Here's an example chat I had with the \"General Chatbot\": `python def hello_world(): print(\"Hello, World!\") `",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda60406c",
      "title": "Making a auth system in SECONDS with Authflow!",
      "content": "Let's be honest, making secure authentication systems is HARD. Really hard. You need to think of security, compatibility, ease-of-use, and much more. It can take days, if not weeks, to develop an authentication system that meets all these requirements. But what if I told you there was a way to implement a secure and easy-to-use authentication system in just a few minutes seconds? Introducing Authflow  a simple and straightforward way to add authentication to your website. With Authflow, you can implement a login system in just a few steps, without worrying about the security and compatibility issues that often arise when developing your own system from scratch. How does Authflow work? First, you redirect your users to the Authflow login flow. Then, Authflow loads the \"Continue with Google\" button on their website. The user logs in, and their information is sent back to an intermediary page on your website, where it is stored in cookies. Finally, the user is redirected to the \"logged in\" page. It's that simple. To get started with Authflow, all you need to do is add the following script to your website:  And then, initiate the login process using the following code:  That's it! With just a few lines of code, you can implement a secure and easy-to-use login system on your website. But that's not all. Authflow also offers some additional features that you can use to further enhance your authentication system. For example, with the Authflow Button API, you can create a simple login button on your website. And with Authflow Instant, you can quickly and easily add a login with Google button to your page, providing a convenient way for users to log in. And this is really useful: If you're using Node.js, you can use the \"Zarran API\" to create your own authentication and authorization server. It has many features built-in, plus there's also a password-username login if your users don't have a Google account. (btw, who doesn't?) Get started now at https://authflow.glitch.me/!",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda60406e",
      "title": "We need to rethink email.",
      "content": "Hi everyone! Today I want to talk about an issue that's been a problem on my side for a while now: email. As essential as it is for our daily communication, email has some major flaws that we need to address. Here, I'll explore the problems with email, some possible solutions, and the potential impact of using AI to rethink how we communicate. Let's start by acknowledging the most obvious problem with email: it's overwhelming. We receive countless messages each day, many of them irrelevant or even spam. It's easy to miss important messages, and it can take a lot of time to sort through the clutter. On top of that, responding to emails can be a tedious task, especially when we have to answer the same questions or provide the same information multiple times. Newsletters get desorganized. And, let's admit, it's really difficult to unsubscribe from some.  Another problem with email is that it can be a breeding ground for miscommunication. Without the benefit of tone or body language, it's easy to misinterpret someone's message or intention. The lack of context can also make it hard to understand what's being discussed. And since email is often used for formal communication, people may feel pressure to be more formal or guarded than they would be in person, leading to stilted or awkward conversations. So, what's the solution? Well, there's no one-size-fits-all answer, but there are some possible ways we could improve email communication. One idea is to use AI to help us manage our inboxes. For instance, an AI-powered email client could sort messages by importance or topic, highlight emails that require an immediate response, and even draft replies for us. But that also has problems, like privacy concerns. Another solution is to use different tools for different types of communication. For example, we could use email for longer, more formal messages, instant messaging for quick back-and-forth conversations, and video calls for face-to-face meetings. Using different communication tools depending on the context we can avoid some of the problems associated with each tool. Another issue to consider is the human element. Communication is more than just exchanging information; it's about building relationships, establishing trust, and showing empathy. AI can't replace these crucial aspects of communication, at least not yet. While it can help us manage our inboxes more efficiently, we still need to be mindful of our tone, language, and how our messages might be perceived by others. It's clear that we need to rethink how we use email. By experimenting with different communication tools and incorporating AI into our workflows, we can make email more efficient and effective. But we also need to be mindful of the potential downsides and make sure we're not sacrificing the human connection that makes communication meaningful. As always, the key is balance, and by finding the right balance, we can make email work for us rather than against us. !Unread",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda604070",
      "title": "A simple guide to Brain.js",
      "content": "Brain.js is a JavaScript library that allows you to create and train neural networks in the browser. With it, you can build predictive models, image classifiers, and even chatbots without having to learn complicated algorithms or dive into the weeds of deep learning. So how does Brain.js work? At its core, Brain.js is a simple implementation of a type of neural network called a \"feedforward network.\" These networks consist of layers of interconnected nodes, or \"neurons,\" that are trained to recognize patterns in input data. Here's a simple example: let's say you want to create a neural network that can recognize handwritten digits. You could create a feedforward network with an input layer that takes in images of handwritten digits, a hidden layer that processes the image data, and an output layer that predicts which digit is being shown. To train this network, you would feed it a large dataset of labeled images (i.e., images that have been tagged with the correct digit). The network would then adjust the weights of its connections between neurons until it could accurately predict the correct digit for new images it hasn't seen before. Now, I know what you're thinking: this all sounds very complicated. But fear not! Brain.js makes it easy to create and train these kinds of networks with just a few lines of code. Here's an example of how to create a simple feedforward network in Brain.js:  In this example, we're training a network to learn the XOR function, which outputs only if one of its inputs is (but not both). We pass in an array of training data, where each input/output pair represents a specific case. Once the network is trained, we can pass in new inputs and get predicted outputs using the `run` method. Pretty simple, right? But Brain.js has a lot of other tricks up its sleeve as well. For example, you can add more layers to your network, customize the activation function of each neuron, and even save and load trained models to disk. But perhaps the coolest thing about Brain.js is how easy it is to use with other JavaScript libraries and frameworks. You can integrate it into your React or Vue app, use it with Node.js and Express, or even build a browser-based AI that runs entirely in the client. And if you're feeling adventurous, you can even use Brain.js to generate text using a technique called \"recurrent neural networks.\" These networks allow you to train a model to predict the next word in a sentence based on the previous words, which can lead to some surprisingly coherent (and sometimes hilarious) output. Of course, like any machine learning tool, Brain.js has its limitations. It's not the fastest or most powerful library out there, and it can struggle with larger, more complex datasets. But for small-to-medium-sized projects, it's hard to beat Brain.js for ease of use and accessibility. So why not give it a try? Who knows, you might just end up building the next big thing in AI! ",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda604072",
      "title": "A simple guide to Brain.js",
      "content": "Brain.js is a JavaScript library that allows you to create and train neural networks in the browser. With it, you can build predictive models, image classifiers, and even chatbots without having to learn complicated algorithms or dive into the weeds of deep learning. So how does Brain.js work? At its core, Brain.js is a simple implementation of a type of neural network called a \"feedforward network.\" These networks consist of layers of interconnected nodes, or \"neurons,\" that are trained to recognize patterns in input data. Here's a simple example: let's say you want to create a neural network that can recognize handwritten digits. You could create a feedforward network with an input layer that takes in images of handwritten digits, a hidden layer that processes the image data, and an output layer that predicts which digit is being shown. To train this network, you would feed it a large dataset of labeled images (i.e., images that have been tagged with the correct digit). The network would then adjust the weights of its connections between neurons until it could accurately predict the correct digit for new images it hasn't seen before. Now, I know what you're thinking: this all sounds very complicated. But fear not! Brain.js makes it easy to create and train these kinds of networks with just a few lines of code. Here's an example of how to create a simple feedforward network in Brain.js:  In this example, we're training a network to learn the XOR function, which outputs only if one of its inputs is (but not both). We pass in an array of training data, where each input/output pair represents a specific case. Once the network is trained, we can pass in new inputs and get predicted outputs using the `run` method. Pretty simple, right? But Brain.js has a lot of other tricks up its sleeve as well. For example, you can add more layers to your network, customize the activation function of each neuron, and even save and load trained models to disk. But perhaps the coolest thing about Brain.js is how easy it is to use with other JavaScript libraries and frameworks. You can integrate it into your React or Vue app, use it with Node.js and Express, or even build a browser-based AI that runs entirely in the client. And if you're feeling adventurous, you can even use Brain.js to generate text using a technique called \"recurrent neural networks.\" These networks allow you to train a model to predict the next word in a sentence based on the previous words, which can lead to some surprisingly coherent (and sometimes hilarious) output. Of course, like any machine learning tool, Brain.js has its limitations. It's not the fastest or most powerful library out there, and it can struggle with larger, more complex datasets. But for small-to-medium-sized projects, it's hard to beat Brain.js for ease of use and accessibility. So why not give it a try? Who knows, you might just end up building the next big thing in AI! ",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda604074",
      "title": "Top design trends as of",
      "content": "Hi! In this article, I'm going to explore some of the latest design trends that are taking the creative world by storm. As we know, design trends are constantly evolving, and it's essential to keep up with them to create visually appealing designs that capture the attention of the audience. The design industry has seen a lot of changes in recent years, and it's exciting to see how designers are experimenting with new techniques and styles to create innovative and dynamic designs. So, let's dive into some of the most significant design trends of . Abstract illustrations Abstract illustrations are gaining popularity as a way to communicate complex ideas in a simple and visually appealing way. These illustrations can be anything from geometric shapes to bold, colorful patterns. They offer a fresh take on traditional illustrations, which often feature realistic imagery. The great thing about abstract illustrations is that they are versatile and can be used across a range of mediums, from web design to print media. They are also excellent for branding, as they can be easily adapted to represent a company's values and ethos. D designs D designs have been around for a while, but they're still going strong. Thanks to advances in technology, D design is becoming more accessible, and designers are using it to create stunning visuals that bring designs to life. One of the biggest advantages of D designs is that it allows designers to create immersive experiences that engage users in a new and exciting way. From product design to virtual reality, they are opening up a whole new world of possibilities. Hand-drawn illustrations Hand-drawn illustrations are another trend that is gaining momentum. As more and more designs become digital, there is a growing desire for handcrafted elements that add a personal touch to the design. Hand-drawn illustrations can be used to add character and charm to any design, from logos to packaging. They can also be used to create bespoke illustrations that are unique to a particular brand or product. One great example of hand-drawn illustrations is Mailchimp (taken from their GDPR page):  Gradients Gradients have been around for a while, but they are still a popular design trend. They offer a subtle way to add color to a design and can be used to create a range of effects, from subtle shading to bold and vibrant color schemes. One of the great things about gradients is that they can be used to create a range of moods and emotions. For example, a warm gradient can create a sense of comfort and security, while a cool gradient can create a sense of calm and tranquility.  Be warned that you should not use too many gradients, as they can create noise. Minimalism Minimalism is still going strong in , and for good reason. Minimalist designs are simple, elegant, and effective. They offer a clean and tidy aesthetic that is perfect for straightforwardly conveying information. One of the great things about minimalism is that it can be used across a range of mediums, from web design to packaging. It's also an excellent way to create a strong and memorable brand identity. Dark mode The dark mode is a design trend that has been gaining popularity in recent years. It's a way to reduce eye strain and make designs more accessible, particularly in low-light environments.  Design trends are constantly evolving, and it's critical to stay up-to-date with the latest styles and techniques. From abstract illustrations to minimalism and dark mode, there are a lot of exciting design trends to explore in . So, whether you're a seasoned designer or just starting, embrace these design trends and experiment with new styles and techniques to create visually stunning and engaging. ",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda604076",
      "title": "We need to find Alternative Captachas",
      "content": "The internet is an incredible tool that has revolutionized how we live our lives. It has made it possible to connect with people from all over the world, access information at a moment's notice, and complete transactions from the comfort of our own homes. However, the ease with which we can do all of these things has also made it easier for malicious actors to exploit the system. One of the most common ways they do this is through bots, which are automated programs that can perform tasks at a much faster rate than a human can. To prevent bots from accessing sensitive information or performing malicious actions, most websites often CAPTCHAs, which are challenges that only a human can complete. But as AI becomes more advanced, CAPTCHAs are becoming easier and easier to bypass. Researchers have found that bots can solve some CAPTCHAs with up to % accuracy. This means that most CAPTCHAs are no longer an effective barrier against bots. For example, a simple program using GPT (with image recognition to recognize things) can easily bypass a modern reCaptacha. Plus, there are multiple CAPTCHA-solving services online. We need to find alternative captchas that are difficult to pass for bots, but easy to pass for humans. We also have the problem of accessibility. Sound captchas are easy to pass with voice recognition software. To address these challenges, researchers and tech companies are exploring alternative CAPTCHAs that are more secure and accessible, but this isn't easy: most captchas are easy to bypass or very difficult to implement. Cloudflare Turnstile, for example, although a good Captcha, can be bypassed by more advanced bots. ReCAPTCHA is the same: it can be easily bypassed by an object recognition program.",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda604078",
      "title": "Building a NFT Ticketing app with Paper and Next.js",
      "content": "NFT ticketing is a new method of selling tickets for various events such as concerts, sports games, and festivals. In this system, tickets are represented as NFTs (non-fungible tokens) which are unique digital assets that cannot be duplicated or exchanged for anything else. When attendee purchases an NFT ticket, they are essentially acquiring a one-of-a-kind digital asset. These tickets are minted on blockchains such as Ethereum, which makes it simple to track ownership and securely transfer them between ticket holders. This means, attendees no longer have to be concerned about fake tickets or scams. Why companies are moving towards NFT ticketing NFT ticketing is gaining traction among companies due to several significant reasons, including: Improved Security: NFT ticketing offers a high level of security and authenticity that traditional paper or digital tickets cannot match. Each NFT is unique and recorded on a blockchain, making it almost impossible to counterfeit or duplicate. Potential for Increased Revenue: NFTs are digital assets that can be bought, sold, and traded on secondary markets, creating the potential for ticket resales at higher prices. Enhancing the Fan Experience: NFTs can grant access to exclusive content, such as backstage meet-and-greets or VIP lounges, improving the fan experience. Additionally, fans can hold onto their NFT tickets as collectibles, providing a memorable experience. Environmental Sustainability: NFT ticketing can reduce environmental impact by eliminating the need for paper tickets, which is becoming increasingly important for environmentally conscious consumers. What is Paper? Paper is a developer platform for NFT commerce. It allows you to accept credit card payments for NFTs, enable users to connect to your app with just their email, and airdrop NFTs at scale. In this tutorial, we will be building an NFT ticketing application with Paper. You can find the final code of the application here. Here is what the application would look like: Prerequisites Before you start with the tutorial make sure you have the latest version of Node.js installed on your machine. Setting up Next.js App The first step is to set up a next.js app and install the required dependencies. In order to do that, you would need to run the below command in your terminal.  The above command creates a new directory named `nft-ticketing` then navigates to that directory and creates a next.js app. The Smart Contract You can write and deploy a smart contract using Solidity or thirdweb. thirdweb is a platform that provides a suite of tools for creators, artists, and developers to easily build, launch and manage a Web project. In this tutorial, we will do both. Using solidity and also thirdweb. Using thirdweb If you prefer to use Solidity, you can safely skip this step. We can use thirdweb to create and deploy an NFT drop. Head over to thirdweb.com and connect your wallet. Once connected, you will be navigated to the explore page below.  Whats Next? So, you've made it this far! That's awesome and it tells me that you're enthusiastic about creating Web apps. This tutorial was just to help you get started, now I recommend you turn this into a real-world application. Don't be afraid to get creative and have fun with it! Conclusion That's all for this article. I hope you found it useful. If you need any help, feel free to leave a comment or send me a dm on Twitter. Let's connect on Twitter and LinkedIn. Thanks for reading! See you next time.",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda60407a",
      "title": "A simple guide to Brain.js",
      "content": "Brain.js is a JavaScript library that allows you to create and train neural networks in the browser. With it, you can build predictive models, image classifiers, and even chatbots without having to learn complicated algorithms or dive into the weeds of deep learning. So how does Brain.js work? At its core, Brain.js is a simple implementation of a type of neural network called a \"feedforward network.\" These networks consist of layers of interconnected nodes, or \"neurons,\" that are trained to recognize patterns in input data. Here's a simple example: let's say you want to create a neural network that can recognize handwritten digits. You could create a feedforward network with an input layer that takes in images of handwritten digits, a hidden layer that processes the image data, and an output layer that predicts which digit is being shown. To train this network, you would feed it a large dataset of labeled images (i.e., images that have been tagged with the correct digit). The network would then adjust the weights of its connections between neurons until it could accurately predict the correct digit for new images it hasn't seen before. Now, I know what you're thinking: this all sounds very complicated. But fear not! Brain.js makes it easy to create and train these kinds of networks with just a few lines of code. Here's an example of how to create a simple feedforward network in Brain.js:  In this example, we're training a network to learn the XOR function, which outputs only if one of its inputs is (but not both). We pass in an array of training data, where each input/output pair represents a specific case. Once the network is trained, we can pass in new inputs and get predicted outputs using the `run` method. Pretty simple, right? But Brain.js has a lot of other tricks up its sleeve as well. For example, you can add more layers to your network, customize the activation function of each neuron, and even save and load trained models to disk. But perhaps the coolest thing about Brain.js is how easy it is to use with other JavaScript libraries and frameworks. You can integrate it into your React or Vue app, use it with Node.js and Express, or even build a browser-based AI that runs entirely in the client. And if you're feeling adventurous, you can even use Brain.js to generate text using a technique called \"recurrent neural networks.\" These networks allow you to train a model to predict the next word in a sentence based on the previous words, which can lead to some surprisingly coherent (and sometimes hilarious) output. Of course, like any machine learning tool, Brain.js has its limitations. It's not the fastest or most powerful library out there, and it can struggle with larger, more complex datasets. But for small-to-medium-sized projects, it's hard to beat Brain.js for ease of use and accessibility. So why not give it a try? Who knows, you might just end up building the next big thing in AI! ",
      "__v": 0
    },
    {
      "_id": "64e0892fb72e199dda60407c",
      "title": "AI. Copyright?",
      "content": "As technology continues to advance at an incredible speed, we are faced with a myriad of ethical questions that were once unimaginable. One such question that is increasingly prevalent in the tech world today is the issue of AI and copyright. Can AI systems create truly original content? The answer is a resounding yes. AI can be programmed to analyze vast amounts of data and generate new content based on that information. From music to art and even literature, AI systems can create all sorts of original works. However, the real question is whether this content can truly be considered original or not. Can a machine really create something that is entirely unique and not simply a rehash of existing content? Plus, some studies reveal that sometimes AI gives you content used in training. This question leads us to the issue of copyright law and AI. Under current copyright law, any original work of authorship is automatically protected by copyright from the moment it is created. This includes basically any creative work. But, as with many legal issues related to AI, it is unclear how copyright law should apply to content created by a machine. Who owns the rights to AI-generated content? The person who programmed the AI system, the company that owns the system, the persons who made the training text, or the machine itself? This ethical dilemma poses a serious challenge to society. If an AI system can create original content, should it be allowed to profit from it? Is it ethical for a machine to create something that a human could have created and profited from it? On the other hand, if we do not allow AI systems to create and profit from their content, are we limiting the potential of this technology? Could we be missing out on groundbreaking discoveries and works of art if we do not let AI systems create? As AI technology continues to advance, the issue of AI and copyright will only become more pressing. It is likely that we will see changes to copyright law in the future to address this issue. Maybe we can create a new category of copyright protection specifically for AI-generated content? This could involve giving ownership rights to the person or company that owns the AI system, while also ensuring that the original creator of the content is properly attributed. Give your thoughts on the comments. ",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda60407e",
      "title": "Getting Started with Data Engineering: A Step-by-Step Guide",
      "content": "Are you interested in becoming a data engineer but don't know where to start? In this post, I'll walk you through the basics of data engineering using a simple project that involves fetching data from GitHub and visualizing it through Looker. Steps:- Here's what you need to do: Select a programming language: Data engineering requires you to be proficient in at least one programming language. Python is a popular choice because it has a rich set of libraries that can be used for data manipulation, transformation, and analysis. Choose a cloud provider: Several cloud providers offer data engineering services such as AWS, GCP, and Azure. For this project, we'll be using GCP because it provides an easy-to-use service called BigQuery for storing and analyzing large datasets. ETL: ETL stands for extract, transform, and load. This is the process of taking data from one source, transforming it to meet your requirements, and loading it into a destination. In our project, we'll be using an API provided by GitHub to extract data about the top repositories with the highest stars. Load data into BigQuery: Once we have the data, we need to load it into BigQuery so that we can analyze it. We'll be using the BigQuery Python client library to do this. Visualize the data: Finally, we'll be using Looker to visualize the data. Looker is a business intelligence tool that allows you to create interactive dashboards and reports. But wait, there's more! To automate this process and make it repeatable, you'll need an orchestration tool like Apache Airflow or Apache NiFi to schedule and execute your data pipeline. If you're interested in seeing the project in action, you can check it out here: https://github.com/rohan/Github-repo-extraction-GCP Results  Data engineering can seem overwhelming at first, but with a little bit of practice and patience, you'll be able to master it in no time. Good luck!",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda604080",
      "title": "Creating a Mastodon Bot with Python",
      "content": "Mastodon is a decentralized social network that allows users to create their own instances and communicate with users on other instances. With a Mastodon bot, you can automate tasks such as posting updates, replying to mentions, or even creating interactive chatbots. In this tutorial, we'll be using Python and the Mastodon.py library to create a Mastodon bot. Before we get started, let's go over a few prerequisites. You'll need to have Python installed on your system, as well as the Mastodon.py library. You can install Mastodon.py using pip:  You'll also need to have a Mastodon account and access to an instance. If you don't have an account yet, you can create one on any instance that allows new registrations. For bot accounts, I highly recommend https://botsin.space/, managed by BotWiki. Once you have everything set up, let's dive into the code. First, we'll need to import the Mastodon.py library and create an instance of the Mastodon class. We'll also need to authenticate with the Mastodon API using our access token, which can be obtained by going to the \"Settings\" page on your Mastodon instance and clicking \"Development\".  With our Mastodon instance set up, we can now start interacting with the Mastodon API. Let's start by posting a new status update.  That's it! You've just posted a new status update to Mastodon. Let's take a closer look at how this works. The `status_post` method takes a single argument, which is the text of the status update. When you call this method, Mastodon.py sends a POST request to the Mastodon API's `/api/v/statuses` endpoint with the status update data. The Mastodon API then creates a new status update on your behalf and returns the updated status data, which Mastodon.py uses to generate a Status object. Now let's say you want your Mastodon bot to reply to mentions. To do this, we'll need to use the Mastodon API's streaming API, which allows us to receive real-time updates for various events, including mentions.  In this code, we've defined a function called `handle_mention` that takes a Status object as its only argument. We check if the status update contains a mention of our bot username, and if so, we use the `status_post` method to reply to the user who mentioned us. Finally, we start streaming for mentions using the `stream_user` method. This method takes a callback function as its only argument, which is called every time the Mastodon API sends us a new event. That's it! You've just created a Mastodon bot using Python and Mastodon.py. With a little bit of creativity, you can use this bot to automate tasks and create interactive experiences on Mastodon. Here's the final code:  In the next blog post, we'll make a bot with much more powerful capabilities, and test one in the wild! Before we wrap up, let's take a look at a few tips and tricks for creating a successful Mastodon bot: Respect users' privacy: Mastodon is built on a foundation of privacy and security. Make sure your bot respects users' privacy by only accessing data that's necessary for its functionality. Stay within the API limits: Like any other API, the Mastodon API has limits on how often you can make requests. Make sure your bot stays within these limits to avoid being rate-limited or banned. Be responsive: If your bot is designed to respond to users, make sure it does so in a timely manner. Users on Mastodon expect a fast and responsive experience, so make sure your bot is up to the task. Have fun!: Mastodon is a community-driven network that values creativity, humor, and fun. Make sure your bot reflects this by injecting some personality and humor into its interactions. Creating a Mastodon bot with Python is a fun and rewarding experience that allows you to automate tasks, provide unique value to users, and engage with the Mastodon community. With Mastodon.py and a little bit of creativity, the possibilities are endless. So go ahead, create your own Mastodon bot and see where it takes you! Happy bot-making! ",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda604082",
      "title": "Using 'Pylint' for Python Code Analysis in GitHub Actions",
      "content": " As a Python developer, it's important to ensure the quality and maintainability of your code. One way to do this is by using Pylint, a popular code analysis tool for Python. In this blog post, we'll go over how to use Pylint in GitHub Actions to automatically analyze your Python code. Prerequisites A GitHub account A repository with Python code A requirements.txt file with `Pylint` listed in it. Setting up Pylint with GitHub Actions Create a new file in your repository named `.github/workflows/pylint.yml`. Add the following code to the file:  This YAML file defines a GitHub Actions workflow that will run Pylint on all Python files in your repository. Commit and push the changes to your repository. GitHub Actions will now automatically run Pylint on all Python files in your repository whenever you push changes. You can view the Pylint results in the Actions tab of your repository. Conclusion Using Pylint in GitHub Actions is an easy and effective way to ensure the quality and maintainability of your Python code. By automatically running Pylint on your code, you can catch errors and potential issues early, allowing you to fix them before they become a problem. With just a few simple steps, you can set up Pylint in your repository and start analyzing your code today.",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda604084",
      "title": "Firefox: my recent experience",
      "content": "As someone who used to be a diehard Chrome user, I never thought I would switch to another browser. But after having problems with Chrome and moving to Firefox, I have to admit that I was probably wrong. The first thing that struck me about Firefox was how customizable it is. While Chrome has a few options for changing the appearance and layout of the browser, Firefox takes it to a whole new level. I was able to easily tweak everything from the toolbar buttons to the size of the tabs. It's amazing how much difference these small changes can make in terms of usability and comfort. Another thing I appreciated about Firefox was the privacy features. While Chrome has made some strides in this area, Firefox goes above and beyond. There are built-in options for blocking trackers and fingerprinting, as well as a strict mode for blocking all third-party cookies. It's reassuring to know that my browsing history isn't being tracked by dozens of companies without my knowledge. I also found Firefox to be faster and more efficient than Chrome in some ways. For example, when I had a lot of tabs open, Chrome would start to slow down and use a lot of memory. Firefox, on the other hand, handled multiple tabs with ease and didn't seem to have as much of an impact on my computer's performance. This was really my main reason for making the switch: I opened simple tabs in my browser (e.g. socket.io) and it just stopped working. The design is also really cool, I like the loaders, UI dropdowns, and more. Of course, there were a few things I missed about Chrome. For example, I found Firefox's bookmarks system to be a bit clunky compared to Chrome's. And some websites that I use regularly seemed to work better on Chrome than on Firefox. Also, there were some experiences I didn't like with Firefox dev tools, for example on Chrome, when you add a CSS property, sometimes It shows you a list of useful options. On Firefox, you don't have that.  But I was impressed with Firefox and would definitely recommend it to others. In conclusion, my recent experience with Firefox has been eye-opening. I never thought I would consider switching from Chrome, but Firefox has made a compelling case for itself. If you're looking for a customizable, privacy-focused, and efficient browser, give Firefox a try. You might be surprised at how much you like it.",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda604086",
      "title": "Automatically Document Your Database in Markdown",
      "content": "If you're working with databases, you know how important it is to keep track of the schema and column information for each table. One way to do this is to manually create documentation in a format like Markdown, but that can be time-consuming and error-prone. Fortunately, with a few lines of code in Python and MySQL, you can automatically generate Markdown documentation for your database tables. How it Works The Python script connects to a MySQL database and queries the schema to get a list of all the tables. It then loops through each table, retrieves the column information, and generates a Markdown table with the column names, types, nullability, key constraints, default values, and any extra information. The script then writes the Markdown table to a separate file for each table, with the filename matching the table name. By default, the script overwrites any existing files with the same name, but you can modify the code to append the Markdown table to the file instead. Scripts  The script then prompts the user to input a description for each table and appends it to the end of the Markdown file for that table. The `with open(f\"{table_name}.md\", \"a\") as f` block opens the Markdown file in \"a\" (append) mode, allowing the `f.write(table_md)` statement to append the Markdown document to the file instead of overwriting it. After appending the Markdown document and the user-provided description, the script reads the contents of the file and prints it to the console using `f.seek``()` and `print(``f.read``())`. Usage To use the script, you'll need to have Python and MySQL installed on your machine and have the necessary credentials and permissions to connect to your database. You'll also need to modify the `config` dictionary at the beginning of the script to match your database connection details. Once you've set up the script, simply run it from your terminal or IDE of your choice. It will generate a separate Markdown file for each table in your database, with the column information formatted as a Markdown table. Conclusion Automatically generating Markdown documentation for your database can save you time and reduce the risk of errors in your documentation. With the Python and MySQL script provided above, you can easily generate Markdown tables for all the tables in your database, making it easier to keep track of your schema and column information. Give it a try and see how it can improve your workflow!",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda604088",
      "title": "Mastodon Bots series: Part : More complex interactions.",
      "content": "Welcome back to part two of our Mastodon bot tutorial series! In the first part, we went through the basics of creating a Mastodon bot using Python and Mastodon.py. We created a simple bot that posted a new status update and replied to mentions in real time. Now, it's time to take things up a notch and explore more complex interactions. In this article, we'll dive deeper into the Mastodon API and explore some advanced features, including interacting with timelines, searching for content, and posting media. So grab your favorite beverage and let's get started! Interacting with timelines A timeline in Mastodon is a stream of status updates from a particular user or a group of users. There are several types of timelines available in Mastodon, including the home timeline, which displays status updates from users that you follow, and the local timeline, which displays status updates from users on your Mastodon instance. Let's say you want your Mastodon bot to retweet all the status updates from a particular user. To do this, we'll need to interact with the Mastodon API's timelines endpoint. Boosting all the status updates from a particular user  In this code, we've defined a function called `retweet_user_timeline` that takes a username as its only argument. The function retrieves the status updates from the user's timeline using the `account_statuses` method and loops through each status update, using the `status_reblog` method to retweet the status update. Searching for content The Mastodon API also allows you to search for content on Mastodon using keywords and hashtags. Let's say you want your Mastodon bot to search for all the status updates that contain the hashtag python and like them. To do this, we'll need to interact with the Mastodon API's search endpoint. Like all the status updates that contain the hashtag python  In this code, we've defined a function called `like_python_hashtag` that searches for all the status updates that contain the hashtag `python` using the search method and loops through each status update, using the `status_favourite` method to like the status update. Posting media Finally, let's explore how to post media to Mastodon using Python and Mastodon.py. The Mastodon API allows you to attach images, videos, and other media types to your status updates. Post a new status update with an image  Here, we've defined a function called `post_status_with_image` that takes an image path as its only argument. The function uses the `media_post` method to upload the image to Mastodon and obtains the media ID. Then, it uses the `status_post` method to post a new status update with the media ID attached to it. Polls Another way to make your bot more interactive is to use user input to trigger certain actions. For example, you could create a poll bot that allows users to vote on various topics. To do this, you could use the Mastodon API's polling feature, which allows users to create polls with up to four options.  This has more interactivity, as it looks for words and mentions. Run the code and your bot will be able to create polls in response to mentions! Just make sure to mention your bot's username followed by the word \"poll\" and the poll options, separated by spaces. For example, if your bot's username is `my_bot` and you want to create a poll with the options \"cats\", \"dogs\", and \"birds\", you would send a mention to `@my_bot poll cats dogs birds`. Your bot will then create a poll with those options and reply to your mention with the poll attached.  And there you have it! With these advanced features, you can create more complex and interactive Mastodon bots using Python and Mastodon.py. Tips and tricks Before we wrap up, let's go over some tips and tricks to help you create a successful Mastodon bot: Use descriptive status updates: When posting status updates, make sure they're descriptive and relevant to your bot's purpose. This will help users understand what your bot is doing and engage with it more effectively. Use emojis and memes: Mastodon users love emojis and memes, so don't be afraid to use them in your bot's posts. Just make sure they're appropriate and relevant to your bot's purpose. Engage with users: Interact with Mastodon users by responding to their posts and mentions of your bot. This will help build a community around your bot and encourage more people to use it. Be consistent: Make sure your bot is consistently posting updates and responding to users. This will help establish your bot's presence on Mastodon and keep users interested. Test your bot thoroughly: Before releasing your bot to the public, make sure to test it thoroughly to catch any bugs or issues. This will help ensure that your bot is reliable and effective. Stay within Mastodon's guidelines: Make sure to review Mastodon's guidelines and stay within them when creating your bot. This will help ensure that your bot is allowed on the platform and doesn't violate any rules. By following these tips and tricks, you can create a successful Mastodon bot that users will love to interact with.",
      "__v": 0
    },
    {
      "_id": "64e08930b72e199dda60408a",
      "title": "BLOG-A-THON Challenge: A Month-Long Journey",
      "content": "Are you a blogger looking for a challenge? Do you want to showcase your writing skills and connect with other bloggers? Then our Blog-a-thon event is perfect for you! In this month-long event, we are inviting bloggers from all over the world to participate in a series of writing challenges and prompts. This month, we will provide themes or topics for bloggers to write about on their blogs. By the end of the month, you will have written at least blog post, each of them challenging you to think creatively and stretch your writing abilities. Why Participate The Blog-a-thon event is a great opportunity to challenge yourself, improve your writing skills, and connect with other bloggers. By participating, you'll: Expand your writing skills and creativity as you explore different topics and themes Build your online presence and connect with new readers and fellow bloggers Strengthen your writing voice and style as you receive feedback from others Gain exposure and promotion as we share your posts across our social media channels Have fun and join a supportive and engaging community of like-minded individuals who are passionate about blogging. What is Blog-A-Thon? Blog-a-thon is a blogging challenge brought to you by DevsInTech in association with Hashnode, a free developer blogging platform. We have tracks for May namely- `Development`, `Artificial Intelligence`, `Low Code Tools` and `Hackathons and your experiences`. You can write a blog for all or either of them by adding the tag `DevsInTechBlogs`. At the end of the month, we will announce one winner, who will be rewarded with Hashnode swags and a shoutout from DevsInTech's socials. Tracks for this month Development: Explore the latest trends, techniques, and best practices in software development. Share your insights on programming languages, frameworks, tools, and methodologies to help fellow developers enhance their skills and stay up-to-date with the ever-evolving world of development. Artificial Intelligence: Dive into the world of Artificial Intelligence and explore its various applications, techniques, and advancements. Share your knowledge on machine learning, deep learning, natural language processing, and other AI-related topics to help fellow enthusiasts stay informed and inspired in this rapidly growing field. Low Code Tools: Discover the power and potential of low code tools in streamlining and simplifying software development. Share your insights on various low-code platforms, their features, and how they can be utilized to create applications with minimal coding efforts. Help fellow developers understand the benefits and best practices of using low-code tools to enhance productivity and efficiency in their projects. Hackathons and your Experience: Share your personal experiences and insights from participating in hackathons. Discuss the challenges you faced, the lessons you learned, and the impact these events had on your personal and professional development. Inspire fellow enthusiasts by showcasing the skills you gained, the projects you created, and the connections you made during these intense and collaborative competitions. How to participate? To participate, write a blog on Hashnode and add the tag `DevsInTechBlogs` to your blog on Hashnode Share your blog on Socials by tagging Hashnode and DevsInTech Do not forget to link your social profiles (Twitter and/or LinkedIn) to your Hashnode profile so that you can be contacted if you win. Please join our socials If you write a blog but are not joined on our social then you will be not selected as a winner. All you have to do is write a blog on Hashnode and add the tag `DevsInTechBlogs` to it and publish it. If you do not add the said tag, your entry will not be considered. Can I write multiple blogs? YES! The more you write, the better chance of you winning. But note that you are writing on only one blog for each track. For example, if you have a blog on Development, then you can't submit another blog for the same track but you can submit blogs on the remaining tracks. When does this end? The challenge ends on May , . What will this get me? Cool Hashnode swags and a chance to build your personal brand through technical blogging! Join Our Socials Twitter Discord Linkedin Hashnode All the best to all participants and see you after the month .",
      "__v": 0
    }
  ]