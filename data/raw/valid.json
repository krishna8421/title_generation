[
  {
    "_id": "64e0892bb72e199dda604004",
    "title": "Announcing Serverless CI/CD",
    "content": "Announcing Serverless CI/CD Today we are excited to announce the general availability of Serverless CI/CD in Serverless Framework Pro. Serverless CI/CD is a continuous integration and deployment service you can use for free by signing up for a Serverless Framework Pro account and following the getting started guide. We built Serverless CI/CD because serverless developers need a CI/CD service optimized for serverless workflows. As developers, we want to focus on developing and deploying often, but too much time is spent on managing the CI/CD pipeline. Here are some of the highlights. Stages & Environments When we deploy a service we dont just deploy to production directly every time. More often, well have multiple environments setup for each stage of our deployment pipeline. While we deploy the same service to each stage, each stage will be deployed to different AWS Accounts and use different configurations. Serverless CI/CD heavily leverages existing Serverless Framework Pro features like outputs to share variables across services, parameters to set secrets/variables, and providers to connect to your AWS account. Whether you are deploying from the CLI or from Serverless CI/CD the right environment and configurations are used depending on the stage submitted. Preview deployments from pull requests !Pull Request Status Preview deployments enable you to automatically test and deploy a preview version of your service for every pull request. So now you can get a live preview to test the live code and test integrations. Its also self-cleaning. After your branch is merged and deleted, Serverless CI/CD will automatically un-deploy your service. Now your environment can stay lean and clean. Branch deployments !Branch deployment status Branch deployments enable you to deploy all commits from a branch to a specific stage. For example, you can deploy everything from the master branch to a staging stage, and everything from the prod branch to the prod stage. This enables you to leverage your existing git flow to review and promote changes from stage to stage. Mono-repo support !Mono repo trigger directories In a microservices architecture we end up with many serverless.yml files in a single repo, but we dont want to redeploy all services just because one file changed. With the mono-repo support, you can specify trigger directories, so you can deploy services only if specific files change. Getting Started in Steps Youll need a Serverless Framework Pro account, so sign-up for a free account now. Sign-up for free Serverless Framework Pro account Before you get started, youll need an AWS Account, Github repo with an existing Serverless Framework project, and permissions on your Github organization to install the Serverless Framework Pro app. https://serverless.com/framework/docs/dashboard/cicd/ The Free tier of Serverless Framework Pro includes one concurrent build and you can test and deploy as often as you would like. It also comes with simple & powerful monitoring and troubleshooting features too. If you need more concurrent builds you can upgrade to the Team tier and buy additional concurrent builds a la carte.",
    "__v": 0
  },
  {
    "_id": "64e0892bb72e199dda604006",
    "title": "A Guide to Preview Deployments with Serverless CI/CD",
    "content": "In this guide, I'll explain what Framework Pro preview deployments are, why you might want to take advantage of them and then show you how to enable these on your own applications. I'll use a version of a song vote counting service I created called Serverless Jams to help illustrate this and you can follow along with me step by step. So What Are Preview Deployments? You might be familiar with the concept of preview deployments from frontend tooling like Netlify that allow you to create a preview of frontend builds before they are merged into a production website. Well, we at Serverless thought - \"Why don't we have that for our backends?\" In the past, the answer to this question has been simple - it costs too much. In the days of expensive, sprawling backend infrastructures it took far too much money and developer time to replicate these environments effectively for testing and staging - let alone for every PR. But with serverless applications, managed services, and microservice architectures that's no longer the case. Take a typical AWS microservice with the serverless framework. What resources are included? Probably some of these: - Lambda Functions - API Gateway Endpoints - DynamoDB Tables - SQS Queues - SNS Topics - S Buckets - SSM Parameters What characteristics do these resources share? They're virtually all pay-as-you-go or pay-per-use/invocation style services or can be configured very cheaply. This means, we can create an entire separate stack of our infrastructure for every feature branch without spending much at all. Why Use Preview Deployments? The preview deployments benefits for frontend code are clearly apparent - you get some preview URL of a deployment and you can see what changed and make sure it looks great. So what do we really get out of doing something similar for the backend? Let's take a look. Automated Tests When you create your Preview Deployment, the Framework Pro CI/CD system will still run any automated tests you have setup to run against the deployment. This gives you all the same benefits you might otherwise have waited until a staging environment to check against. They Can Supplement Existing CI/CD You can use Preview Deployments in combination with Serverless CI/CD for stages like production and staging. Or, you can use them in addition to whatever existing CI/CD tools you have. Already using another tool for your workflow? Great! You can still add Preview Deployments without disrupting any of your existing processes! A Clean Environment for Code Review Because you're spinning up an entire set of infrastructure, it's open season for your code reviewers to play with the API endpoints and infrastructure resources. They can run manual tests to confirm the deployment meets expectations, run API contract tests against the API endpoints that are created or integrate the feature branch into local frontends for a fuller test experience. This can be especially helpful when you want validation from a frontend team on the expected functionality for a new API. And because the preview branch is discrete from other environments they don't have to worry about stepping on any toes during the review. After this, they can go straight back to the PR, and make sure that any issues and feedback they discover are addressed before the PR is even merged into a staging environment. Automated Spin Up and Spin Down Because these resources are all spun up automatically by opening the PR, there's no manual process for the developer or reviewers to create a full environment to test against. The best part is that when the PR is finally closed or merged you can configure your deployment to automatically remove the infrastructure resources that were created in AWS. How to Use Preview Deployments So how do we configure all this? In this section, I'll take you through how to get started with preview deployments. You'll be able to follow along with every step by cloning my Serverless Preview Deployments project and using it in your own Framework Pro account. Prerequisites In order to get started with Preview Deployments, you'll need a Framework Pro account. You can get a free account for personal use here and configure it using these steps. As of this post, preview deployments require you to: - Have your code in GitHub - Be deploying services to AWS - Be using either Node or Python - Be using a recent version of the Serverless Framework (I'm using v..) With all of these perquisites met, you should be able to configure your repository to use preview deployments. Setting Up Preview Deployments First, you'll need to follow some steps to get any CI/CD working with your Framework Pro account. The steps are documented here but let's walk through them together. . Setup a default provider When using preview deployments, we'll be deploying to a stage name based on the feature branch name. This helps avoid conflicts with resource names because feature branch names should be unique. If we are able to work in an environment with multiple AWS accounts, we want to them make sure we use an appropriate, non-production account usually for the deployment of these preview branches. Because preview branches could be created at any time with any potential stage name, we do not have the opportunity to select the correct AWS connection to associate with these automatically generated stages. We can, however, add providers to our Framework Pro accounts and we can even set a provider as a default. It is this default account that will be used by any preview deployment, or in fact any deployment with a stage name we have not yet manually configured. Adding a provider is super simple and this two minute YouTube walks through setting up that connection. . Get Your GitHub Repo Setup For this example, I'll use a version of Serverless Jams - a vote counting system for different coding-related songs. We'll open a feature branch PR to add some functionality to Serverless Jams after we create it in our own GitHub repo. Go ahead and sign in to GitHub and take a moment to create a new GitHub Repository. You can make it public or private, just give it a memorable name because we'll have to find it later. I'll call mine `preview-deployments-test`: !Screenshot of new repo creation page on GitHub Then copy down the git URL for your repo for later. Mine is: `https://github.com/fernando-mc/preview-deployments-test.git` We need to do this step because we'll need our own repository in our own GitHub account to push code to and then configure with Preview Deployments later on. After you create that repo, you can clone the code we'll be using: - Run `git clone https://github.com/fernando-mc/serverless-preview-deployments.git` to get the code - Enter the project directory with `cd serverless-preview-deployments` - Then change the git origin with `git remote set-url origin `, for example, mine would be: `git remote set-url origin git@github.com:fernando-mc/preview-deployments-test.git` - You can confirm that you've correctly updated the remote URL with `git remote -v` - Then run `git push origin master` to push this code to your own repo. I'm jumping through all these hoops to make sure this repo is yours and yours alone and not associated with mine in any way. That way I can make sure you don't inadvertently open feature requests against my repo, which will unfortunately fail for you! With this repo setup, we now need to create and configure an App in Framework Pro. . Configuring Your App in Framework Pro Navigate to the Framework Pro dashboard and click the \"Create App\" button. Go ahead and give your app a name in the required box, then add the service name as it is in your serverless.yml (`serverlessjams` if you haven't changed it). Clicking on `deploy` then gives you the opportunity to choose an AWS provider as the default for the app; you can leave this blank. What you will see are some commands. Copy and paste the `org` and `app` properties to your serverless.yml, make sure to run `serverless login` if you haven't yet so that the Serverless Framework CLI tool has access to your Serverless Pro account and then just run deploy.  With that initial deployment out the way, it makes it a lot easier to edit the CI/CD settings. By clicking the menu to the left of your now deployed service's name, you can go to settings which has the option to go to CI/CD settings. There should be an option to connect to GitHub or BitBucket. Click whichever is appropriate! !Screenshot of connecting GitHub If you have multiple organizations associated with your account, you'll need to pick the one you put the repo in. !Screenshot of selecting the Org or User account in GitHub After this, you will at least need to grant permissions to access the repo we just created. !Screenshot of selecting repositories in GitHub At some point, you might also have another screen or two prompting you to install the Serverless Application in GitHub. After you're done with this process, you can head back to the Framework Pro dashboard and you should now see a repo dropdown to pick from: !Screenshot of connecting the repo in the Framework Pro dashboard After you select the repo your using, preview deployments should be configured by default as shown below: !Screenshot of the preview deployment settings You could also configure deployments for other branches (like whenever changes are made in master). But for now, just save the settings and continue. Testing Preview Deployments In this example, we'll have a `master` branch in your GitHub repository. When a PR is made against the `master` branch from a feature branch, we'll want to create a Preview Deployment for that PR. To set this all up, go back to your code, and run `git checkout -b preview-test-feature` to create a new feature branch. Then, in the `backend/vote.py` file change the integer on line from `` to ``. This will have us vote by instead of by . Now, add and push the changes: - Run `git add .` to add your changes - Then commit the change `git commit -m \"vote by \"` - And finally push them to the feature branch at the origin with `git push origin preview-test-feature` The output of this should include something like this:  If you see that, you can click on the link to automatically open a PR. Otherwise, visit your repo in GitHub and open a PR manually: !Screenshot of the open a pull request button Then, actually open the PR: !Screenshot of pull request creation page After you open the PR, you should see the GitHub checks running on your PR like this: !Screenshot of the preview checks run in GitHub by Serverless If you see a failure like this click the \"Details\": !Screenshot of a failed build in GitHub This should direct you to the deployments section of the Framework Pro dashboard. From there, you can review the log for your preview deployment to see what happened: !Screenshot of a failed build log in Framework Pro If everything was successful you should see a success message in GitHub !Screenshot of successful preview deployment in GitHub Clicking the \"Details\" link inside of the Serverless check should direct you to the Framework Pro dashboard where you can review the logs from the build. Finally, after the deployment is completed, you'll see any API endpoints and other relevant resources that were created. At this point, any reviewer could copy and paste those API endpoints and test them out. If I were reviewing, I could test out the new API using something like Postman or even copy the API endpoint directly into the frontend and test it out within the UI. Let's give that a shot now for fun. Copy the base of the API endpoint from the log, in my case: `https://myjahkk.execute-api.us-east-.amazonaws.com/preview-test-feature/` Then, paste it into the `frontend/app.js` file where the `REPLACE_ME` value currently is on line so it becomes the new `endpoint_url_root`. Then, change directories into the `frontend` repository and run `python -m http.server` to start a webserver for the frontend and open up localhost:. It should look something like this: !Screenshot of the frontend application From there, you can try using the app! Make sure to enter your number in with the plus sign and the country code at the beginning. After you submit a vote you'll see that it is incremented by instead of . From here, we could keep testing the API endpoints, go and comment on the PR and suggest any changes we needed made. When we're done, we can either merge the PR or close it. At that point, Its also self-cleaning. After your branch is merged or deleted Serverless CI/CD will automatically remove your service. Bear in mind, you will have to delete the branch, not just close the PR for the infrastructure to be removed. What Next? So by now I hope I've convinced you of the utility and possibilities of preview deployments. Next, try taking a look at the other features of Serverless CI/CD like branch deployments, and testing. If you have suggestions about what you want from Serverless CI/CD next let us know in the comments below!",
    "__v": 0
  },
  {
    "_id": "64e0892bb72e199dda604008",
    "title": "Promotion Pipelines",
    "content": "Serverless application development is great for rapidly spinning up solutions to problems; you complete a little configuration in a serverless.yml file, add some Lambda code to glue it all together and hit `sls deploy`. However, this isn't really going to fly when you have multiple team members working on the same code. You can't have multiple developers deploying code at the same time to the same infrastructure, potentially overwriting what each is doing. The first step to resolving this issue is to allow each developer access to their own AWS account, possibly using the AWS Organisations service that makes managing multiple child accounts to the organisations main account an easy to manage process. But what happens when this all needs to come together preferably in some kind of staging environment? Use stages for environments The Serverless Framework has a feature you may already be familiar with; stages. You can deploy a Serverless Framework to AWS by passing it a stage that can differentiate one stack from another and this means you have the means to have various environments to deploy to. `serverless deploy --stage staging` Now if all services are deployed to the staging environment it means we have some way to do some quality assurance on an application and a collection of services before we deploy this to production. We can run our integration tests, perform some manual confirmation with stakeholders and customers, and then we just do a ... `serverless deploy --stage prod` ... right? This in itself has problems. Its great we can break things up as far as environments are concerned, but we probably need more reliable ways to make sure that code gets pushed to the right environments just as a part of the development flow of your developers. Lets tie in some branching strategies In a more traditional development environment, the software development lifecycle will include a strategy of using branches to indicate the relative promotion of code to staging and development environments. And to that you would also have a CI/CD backend being triggered off of merges into specific branches. Merging into a develop branch deploys code into a staging environment. Merging into master ends up deploying into the production environment. And deployments cannot be made in any other way. What if we can get these kinds of strategies to work for our Serverless applications? It will help us solve some of these problems Enter Serverless Framework Pro CI/CD This is actually very simple to do with Serverless Framework Pro's CI/CD feature. There are just a couple of pre-requisites to get up and running: Need service (at least an initial version) with a serverless.yml file and an `app` and `org` set within that serverless.yml The service needs to already be pushed to a Github repo with all the branches you want to auto-deploy to different environments already created; if you want to deploy from the develop branch to a develop environment/stage, that needs to be created already. Lastly we also need to deploy at least once to make sure the service is added to Serverless Framework Pro. With that out of the way, lets go to our service in the Serverless Framework Pro Dashboard: !Enable Service D Image As we can see, there's a handy `enable` link we can click on. !Enable Github Now we just need to connect Serverless Framework Pro to our Github account and follow the Github OAuth prompts to give the required permissions to enable Serverless Framework CI/CD to deploy our services for us. Once we are connected: Choose the repository we want to deploy from. Select the base directory the service sits ... yes we can do mono repo things ... more in a future blog post And now comes what we have all been waiting for. Scroll down the the section labelled `branch deploys`: !Branch Deploys UI This is where we can now link the branches in our repository to stages in our application, and those stages are linked to profiles that help us control how and to which AWS accounts we deploy to. And that's pretty much. I did say it was easy. Once we match our branch to the right stage, every merge into those branches will automatically begin deployment.. !Queued Deployment To get started head to the Serverless Framework Pro Dashboard and sign up for free!!",
    "__v": 0
  },
  {
    "_id": "64e0892bb72e199dda60400a",
    "title": "Managing Stages and Environments",
    "content": "Since day , the Serverless Framework has had the concept of stages; the ability to create different stacks of the same service. This concept works really well when you need to provide different types of environments for the software development lifecycle of your team or organisation, as it allows you to deploy development code to a development environment using a development stage: `serverless deploy --stage develop` This does come with a few issues, however. How do you manage different environment variables between the various environments? What if you wanted to deploy to multiple AWS accounts? The Serverless Framework Dashboard uses features called Providers and Parameters to allow you to manage exactly that. Lets dive in! Initial setup Let's get started with the basic setup we need. Its pretty quick! First, go to the Serverless Framework Dashboard, and create a new account if you haven't got one yet or log into your existing account. If you created a new account, it will prompt you to give your org a name. You can name it anything you like and don't worry, you can create additional orgs later for free if you need one specially named. Once done, you can click the `create app` at the top right and since we are talking about adding an existing Serverless Framework service, go ahead and choose that option. Here just add the app name you wish to create and the name of the service you are going to deploy. Click the deploy button and you will be prompted to create or choose a Provider. Provider's is a feature to help manage your connection to ... well ... a provider like AWS. Once you have that complete, you just need to copy and paste the small yml snippet with the org and app properties into your serverless.yml, save the file and deploy. !Create App Form When we deploy our up, if we didn't set a stage at deploy time with `--stage stagename`, it would have defaulted to the `dev` stage so you may something like this. !Dev stage deployed Howeveer, what if we want to deploy multiple stages? How do we manage that? Separate AWS accounts Separating our various environments, such as development and production, into alternate AWS accounts is a pretty common practice. If we want our development environment to deploy to an entirely different AWS account to our production environment, we can do so by first of all adding that alternate AWS account to our org. Go to the org settings section clicking `org` on the left,then choose the Providers tab. Here you can add a link to any and all AWS accounts you may want to assign to any of your stages going forward. You can even choose a default provider which we recommend setting to an AWS account you don't mind someone accidentally deploying something to; in other words, not your production AWS account. Once you have added the additional AWS accounts, you can head back to the app screen, and if you have any deployed services (which you should after the instructions above), you will see them here. What we want to do is create a new prod stage and assign our prod only AWS provider to it before we deploy. We do this by clicking the menu icon to the right of the service name, choosing \"add stage\" and then giving the name prod. Clicking on our new prod stage with a grey \"pending\" icon we can switch to the provider tab and choose which of the providers we want to allocate to this yet to be deployed stage. Now, when we do deploy with `serverless deploy --stage prod`, that deployment process will use the associated provider to get temporary credentials to our prod AWS account and do what it needs to do. !Product provider allocated to production stage But there are more benefits built in by default as well. Because you can now do deployments to AWS via the Serverless Framework Dashboard, you no longer need to distribute Access Keys and Secrets to developers so that they can deploy from their local machines. When a deployment is done via the dashboard, at deployment time the Serverless Framework requests temporary access credentials created via the provider you just setup. Once deployment is complete, those credentials are no longer in use. Much better for security! Parameters Your application needs configuration data. Whether that's to connect to data sources or third party API's, it needs these details for the running of your application. However, these details often differ depending on whether you are running in the development environment or in production, or even locally. Thankfully, the Serverless Framework Dashboard has a feature to help us solve that. Open up the settings for a service as we did previously you should see a menu with options for CI/CD, Provider and Parameters. Switching to Parameters we are able to add a collection of key/value pairs, with the values stored encrypted. !Parameters list for a service These parameters are made available to ALL stages within it. This is a great place to put defaults that are always shared across all stages or perhaps just some sane values to make sure deploys don't error no matter what. As mentioned though, we do want to be able to set unique parameters for stages themselves. So lets go back to the apps screen and click through to any of our deployed stages, and we should see the parameters tab: !Parameters list for a stage It is here that we can see that the parameters we had added at the service level filter through, but hovering over the `inherited` label, we can now override this inherited value with a custom one for our stage. We could even add any parameter we need for this stage from scratch if we so desire! Now at deployment time, these values are avaialable to be used in our serverless.yml file:  The `${param:}` syntax retrieves the value stored against the key at runtime. But combined with the existing variables syntax of the Serverless Framework, I can also make sure that local development has the required values:  If the param does not exist, as may happen in a local environment, the default value after the `,` is used instead. Lets make the infrastructure team happy If you were a user of the previous dashboard, you may have noticed that the Safeguards feature has been removed. It is not gone, however. We moved Safeguards into a plugin where you can choose to add it to your project or not and continue to add organisational policies to your services that are evaluated at deployment time. You can find out more at the plugins GitHub page With everything we've looked at, imagine looping in Serverless Framework CI/CD which uses all of these features by default. It can help you manage a seamless software development lifecycle across multiple stages and deployment scenarios. All you need to get started is to go the Serverless Framework Dashboard and sign up!",
    "__v": 0
  },
  {
    "_id": "64e0892bb72e199dda60400c",
    "title": "Announcing Support for AWS HTTP APIs",
    "content": "AWS HTTP API support just landed! !Example serverless.yml HTTP API configuration By introducing the HTTP API service (still in beta) last December, AWS offered us a lighter, cheaper, faster and in general better designed alternative to REST APIs. More importantly, HTTP API is way easier to configure and can also be created by importing an Open API definition file. Whats not to love? How HTTP APIs are different from REST APIs? HTTP APIs truly embody the \"less is more\" ethos -- they have fewer configuration options but support catch-all routing (which is not possible with REST APIs), built in JWT authorization, global rules for CORS headers and automatic deployments which make the deployment of production APIs dead simple. At this point, we can direct endpoints to either trigger AWS Lambda or to another URL endpoint, but theres no integration with other AWS services. With that said, HTTP APIs are still in beta and have several limitations as highlighted below: - No support for Usage plans and API Keys as with REST APIs. - No wildcard subdomains - Request and response transformation non existent - No caching, validation etc. (have to be implemented in lambda logic) - Cannot deploy edge-optimized or private API's. All deployments are regional and public - While we can enable simple access logs and receive CloudWatch metrics, there is no AWS X-Ray support or ability to propagate logs to Kinesis Data Firehose For full outline of differences see Choosing Between HTTP APIs and REST APIs section in AWS documentation. However, the service is fast improving and as it moves towards GA, we expect a lot of these to be addressed. How to configure a Rest API backed by HTTP API with Serverless Framework? As HTTP API differs from API Gateway in many parts, and only very basic Rest API configuration, can be easily translated from API Gateway to HTTP API, we decided to propose a new event (httpApi) for HTTP API case, which should be attached to functions in the traditional way:  Configuring Routes Configuring a basic route for a specific method is as simple as: `httpApi: GET /get-something` Your API Gateway path can also include parameters: `httpApi: GET /get-something/{param}` We can also configure a method catch-all route for a specific path: `httpApi: /get-something` Or define one catch-all route, and handle all request from scope of single lambda function: `httpApi: ` Note: When configuring catch-all route, you may still redirect requests for specific paths to different lambdas by configuring intended dedicated routes If there's a need to customize the `httpApi` event further (with configuration options mentioned below), then event configuration should be outlined in object form as below:  Configuring JWT authorizers When configuring plain routes, we configure a publicly accessible API. If theres a need to restrict access to whole API or some endpoint, we must rely on JWT authorizers, as currently its the only access restriction method currently supported by HTTP API. Fortunately, AWS Cognito User Pools are perfectly suited to this purpose. To add User Pools, we need to configure authorizers in the provider.httpApi.authorizers section, where we list JWT authorizers by name as follows:  Having that, we need to indicate endpoints for which we want to restrict access with configured authorizers:  If we need to provide authorization scopes, then endpoint configuration should be extended as:  Configuring CORS If you intend to consume API endpoints in a browser then most likely you need CORS headers. CORS headers can be configured just globally for all API endpoints, and in Serverless Framework you may configure them in two ways. The first one is to set following and rely on Framework defaults:  Itll ensure following headers: - `Access-Control-Allow-Origin`: `` - `Access-Control-Allow-Headers`: - `Content-Type`, `X-Amz-Date`, `Authorization`, `X-Api-Key`, `X-Amz-Security-Token`, `X-Amz-User-Agent` - `Access-Control-Allow-Methods`: - `OPTIONS`, and all the methods defined in your routes (`GET`, `POST`, etc.) If you need more fine grain customization you can configure each header individually with following setup:  Note: Any not configured header will fallback to Framework default What's Next? HTTP APIs are subject for further extensions in the framework. Notably the ability to configure access logs and ability to share the same API across different services will be published soon. We are also planning to add an option to reference an Open API spec in your `serverless.yml` functions config. Please follow this GitHub issue for updates. If you approach any issues or want to propose an improvement do not hesitate sharing that by either commenting under this issue or opening a new dedicated report. Have fun! We hope that introducing support for AWS HTTP API in Serverless Framework will springboard your serverless development process and we can't wait to see what you build with it!",
    "__v": 0
  },
  {
    "_id": "64e0892bb72e199dda60400e",
    "title": "Serverless Auth with AWS HTTP APIs",
    "content": "New AWS HTTP APIs Earlier this week, we announced support for AWS HTTP APIs and talked a bit about what is possible with them. If you'd like to learn more about the AWS HTTP API and the new event source we've added integrate with it check that post out. In this post, however we'll jump in to using the new AWS HTTP APIs with one of the new features they offer - the JSON Web Token integration. I'll show you how to use Amazon Cognito to add authentication and authorization to your AWS HTTP API endpoints. You can choose to follow along with examples in either Node.js or Python and towards the end, I'll show how you could modify the examples in order to work with a tool like Auth or Okta instead of Amazon Cognito. Let's get started! Setup In this guide, we will create an Amazon Cognito User Pool, App Client, and Domain all from scratch in the `resources` section of `serverless.yml`. You can choose to use either the Node.js or the Python version of the code. Run one of the following commands to get started: - For Node.js - `git clone https://github.com/fernando-mc/aws-http-api-node-cognito.git` - For Python - `git clone https://github.com/fernando-mc/aws-http-api-python-cognito.git` After you have the code, make sure you've also installed the Serverless Framework, setup and configured the AWS CLI, and (optionally) created a Framework Pro account. Deploying the Project With the repository cloned, change directories into the repository and make sure you're on the same level as the `serverless.yml` file. Then you can make a few changes to the demo code: Either configure your own `org` and `app` name with Framework Pro or remove the `org` and `app` from the top of `serverless.yml`. Update the `DOMAIN_SUFFIX` value in the provider environment section to something unique. I recommend you use something like your name and favorite mythical animal. After that, save the file and run `serverless deploy`. This should deploy all the Amazon Cognito resources required as well as all the parts of our new HTTP API. After the deployment completes, you should see two API endpoints in the output:  Copy your endpoints down and then try using the GET endpoint by pasting it into your browser or a tool like Postman. You should see this result: `{\"message\":\"Unauthorized\"}` Similarly, if you try to send JSON data to the POST endpoint you should see the same result. This means these endpoints are protected and will only work with a valid JSON Web Token! In order to get this, we'll need to generate one using the Cognito User Pool Hosted UI. Log into the AWS Console and navigate to the Cognito section of the dashboard. Make sure you're in the same region you deployed your service to and click Manage User Pools: !Image of the Amazon Cognito Dashboard entry page From there, click on the user pool you created: !Image of the Amazon Cognito User Pools And then navigate to the \"App Client Settings\" page: !The Cognito App Client Settings And then scroll down to find the \"Launch Hosted UI\" button. !The Hosted UI Button Then sign up for your own account on the hosted UI !The Hosted UI Sign Up Modal After you sign up, you should be redirected to a non-available localhost page. Copy the URL out of your browser. It should look something like this:  That URL contains two JSON web tokens, an `id_token` and an `access_token`. They each serve different purposes, but either can be used in this case to verify against the API. Grab the `id_token` by copying everything after the `id_token=` and before the `&access_token`. You can inspect the JSON web token on a site like jwt.io. Just paste the token in the debugger as shown below: !Decoding the JWT in jwt.io From here, you can open up something like Postman and set the Authorization section of the request as shown below before testing the GET endpoint: !Postman example sending a request with authorization You'll need to select the Type of Bearer Token and paste your token into the text box. Keep in mind that you'll need to copy it exactly! You can't have extra spaces, new lines, or a trailing `&` character that you might have copied accidentally. It should return a nice juicy response containing all the fun information you might want about the token's owner in the `message.requestContext.authorizer.claims`. Importantly if you try again with the `access_token` you'll get a different set of information in the response. These two tokens are designed for different purposes and as such they contain different sets of information. Cognito Alternatives I included Cognito in this service to make it easier to demonstrate without including third party services. However, you could also easily replace Cognito with something like Auth by removing the `resources` section from `serverless.yml` and then replacing the values in the `provider` section under the `httpApi` and `authorizers`. The updated `httpAPi` section would look something like this:  This JWT integration simply requires that you send either an `id_token` or `access_token` in via the `Authorization` header with the value of `Bearer `. AWS will then take care of validating the token against the provided `issuerUrl` and `audience`. Here are two examples that have a more simplistic configuration like this: HTTP API with Node.js HTTP API with Python Simply clone either repository and follow most of the same steps shown in the earlier section. You'll be able to skip setting the `DOMAIN_SUFFIX` environment variable as you'll already have configured and created your own resources to replace the User Pool Domain. You will also need to figure out how to generate the `id_token` or `access_token` on your own using the other provider in order to test the integration. Now What? Congratulations! At this point, you deployed and tested your AWS HTTP API and it's ability to authenticate users who want to access an endpoint! In the future, you may want to learn how to manage scopes and permissions with the `access_token`. But for now, you can start to use this new tool to shave hundreds of lines of JWT verification code out of your AWS HTTP API projects! You can also start to evaluate the limitations of the AWS HTTP API to see if it is ready to support your existing API Gateway workloads. Have questions about the guide? Get in touch or leave a comment below!",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda604010",
    "title": "CI/CD for monorepos",
    "content": " When building serverless applications as a collection of Serverless services, we need to decide whether we are going to push each service individually to our version control system, or bundle them all together as a single repo. This article is not going to go into the details about which is better or not, but all our posts so far seem to show examples of services all stored in individual repositories. What this article is going to demonstrate, however, is that deploying services from within a single monorepo is easily doable within Serverless Framework Pro's CI/CD solution. Getting started You need to make sure that the services you are deploying are bundles together in a repo in seperate subdirectories off the root of the repo. You can see a simple example repo here to see how this is structured to make sure that yours matches as closely as possible. Really the biggest part to take note of is that all services are off the root of the repo as seperate sub-directories and a folder with shared code is also off that root. This just simplifies configuration later. Once you have a repo setup (or you've cloned the sample repo), make sure each service in each subdirectory have the same app and org settings to connect to the dashboard and that those changes are also pushed to the repo. The last step before we walk through setting up the monorepo deployment is to ensure that we have our connection to our AWS account all squared away, especially if you have a brand new dashboard account. Here is a minute video that shows you how to easily and quickly connect to AWS using Providers. With that out of the way, lets get cracking! First deploy The best way to get started is to just deploy first and get all your services deployed to AWS and created within the dashboard. Here are the steps to follow: Make sure you have credentials for the CLI to communicate to your Serverless account by running `serverless login` in the CLI and completing the login process. If the app property you have added to your services' serverless.yml files does not yet exist, click the `create app` button and choose to add `an existing Serverless Framework project` During that process you can create or choose a new Provider Once the app is created, go back to your project on the CLI, make sure to `cd` into the first service and run `serverless deploy --stage [stageyouwanthere]`. The `--stage` is optional since it will always default to a value of `dev` unless you specify otherwise. Repeat by `cd`ing into each sub-directory and deploying each service into AWS. Even if these services are already deployed, you can deploy again. As long as nothing but the `org` and `app` properties have changed, all that this new deployment does is add these services to your dashboard account. Connecting to GitHub or BitBucket Now that everything is added to the dashboard, lets click the menu option to the right of the one of our service names and choose settings: !Select service settings option In the settings menu, select CI/CD and you should see the CI/CD configuration open up. If you are doing this for the first time, you have probably not connected to GitHub or BitBucket before, so just click the connect option and follow the prompts. Once you have completed that process, you will need to choose the repository for your monorepo from the dropdown list, and since this is a monorepo, the CI/CD settings will also ask you to choose the right base directory for this specific service. Go ahead and do that! !Select repo and base directory Setting up automated deployments Since we have the basic connection all set up now, lets scroll down to the branch deploys` section. This is where you can now configure which branch in your repo deploys to which stage or environment. Most repos have a main branch (or master) and this is often selected as the prod stage. You can however add a develop branch that can deploy to the dev stage as in the image: !Branch deploys with prod and dev Just add any additional branch to stage mappings you want to have. This configuration will then trigger an automated deployment as soon as any code changes are made to the branch you configure; for example if a developer creates a PR to the develop branch, one that PR is merged it will automatically trigger a deployment of your service to the dev stage if configured like the image above. You will need to repeat the above process for each sub-directory within your monorepo. More advanced configuration Selective deployments By default, if you have multiple services of a monorepo configured and you merge a change anywhere in that repo, all services will redeploy, just in case. There's no way for the system to know if there are any dependencies between the services so it cannot assume that only that one service that had a change should be redeployed. In other words, if you had services and you made a change to just one, seperate redeployments will occur, just in case. However, you can configure it differently. If you open the CI/CD settings for one of the services and scroll down to expand the `build settings` section, you should see numerous options to help you maximise the efficiency of your CI/CD pipeline for a monorepo. !Build settings By default, the `Only trigger builds on selected file changes` option is not selected and means that this service will always be redeployed with any change to the git repository, even if there were no changes to the code of this service. If you only want this service redeployed when its own code is edited, check the box and you should see something like: !Trigger directories configuration Automatically, the directory of the current service is selected. From this point on, `servicea` will only be re-deployed when its own code is edited. Dependency deployment But what if you actually did have services that had dependencies on each other. So in the example with `servicea`, we could in fact link it to `serviceb` and configure it so that `servicea` will always be re-deployed when `serviceb` is also edited. Just by adding a reference to the correct service directory, I can ensure this will happen: !Service B added I could of course do this for any number of services that `servicea` depends on and vice versa. But what if I have some kind of shared folder that `servicea` uses. Because we reference a directory structure in our configuration, you can point to any path in your monorepo to be watched for changes. In the example repo, we have a directory called `shared` that stores a number of classes and functions (or at least it could) that are re-used by multiple services. If I change anything in `shared`, multiple services need to redeploy. I can accomplish this just by adding the path to `shared`: !Shared directory added In the image above, `servicea` will be deployed on merges to Github if changes are detected in directories `servicea`, `serviceb` or `shared`. And I can configure any service with any specific arrangement of dependencies I need, providing me a ton of great flexibility to deploy what I need under the right circumstances. Monorepo deployments are much simpler to manage using Serverless Framework Pro CI/CD. But if you do have any feedback for us or want to just share, please hop into our Slack channels or Forums and let us know. Or you can even DM me on Twitter if you have any questions.",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda604012",
    "title": "AWS Lambda Destination Support",
    "content": "What Are Lambda Destinations? We first wrote about Lambda Destinations when AWS announced support for them right before re:Invent . Essentially, destinations are the ability for asynchronous Lambda invocations to have their execution results sent to other AWS services without needing to wait for the Lambda execution to finish. Previously, you would have to wait for a Lambda success or failure or need to leverage something like Step Functions. Now, you can invoke functions asynchronously, and send the results of the invocations to different places depending on success or failure. Today, we're excited to announce support for Lambda Destinations in the Serverless Framework! Make sure you've upgraded to `v..` or higher and let's look at how to add them. You can watch the video below for an abbreviated version or work through the entire guide! Adding Lambda Destinations The first step to adding Lambda destinations is to decide what resources you would like to use as a destination on a success or failure. Currently, event destinations can be configured as another Lambda function, an SNS topic, SQS queue, or Amazon EventBridge. You can either create these resources in your `serverless.yml` file or you can reference already-existing resource ARNs. Inside of `serverless.yml`, you'll add a `destinations` section to a function that you want to configure Event destinations with:  Inside of the `destinations` configuration you can add either an `onSuccess` or `onFailure` or both. The value for those bits of configuration can either be another function defined in the same `serverless.yml` file or an ARN of the destination resource. Now let's look at a few configuration examples. You can find full-fledged example services of the below snippets on GitHub here. Function Event Destinations from the Same Service One of the easiest ways to configure event destinations with your Lambda functions is to refer to other Lambda functions that you're already creating in your service in the same `serverless.yml` file. For example, in the below snippet, the functions section creates a `helloStarting` function that then uses the `helloSuccess` and `helloFailure` functions as destinations:  This configuration has the benefit of creating a per-stage destination automatically as you deploy your service across stages like `dev` and `prod`. ARN-based Destinations If you already have existing resources for your event destinations, you can also reference them using the ARN value of those destinations. For example:  In the above example, we've already created an SQS queue called `successQueue` and a Lambda Function called `failureFunction` and we've configured our function to deliver success and failure event to each of them, respectively. There is one potential issue with this sort of configuration though. You may not want to send success and failure notifications for every application stage to the same place. Let's take a look at at least one way of fixing this. Stage-based Destinations If you want separate destinations depending on the stage of your service you can use a few different methods to accomplish this. Framework Pro Parameters One of the simplest ways to accomplish this would be to leverage Framework Pro Parameters and load them in at deployment time:  This way, the framework will load the relevant parameter at deployment time from Framework Pro. Stage Variables in Event Destinations You can also forego Framework Pro and create destinations resources with the stage as part of their name. For example, a destination queue for success messages in the `dev` stage might become `successQueue-dev` and in prod `successQueue-prod`. Then, you can create the final ARN at deployment time based on the stage:  This sort of configuration assumes you've already created all the resources required to configure with the destinations. Testing the Destinations So how would we test one of these examples? First, clone the examples repo from GitHub here. There are several examples in that repo, but change directories into the `arn-based-event-destinations` and we'll go from there. First, we can create a single SQS queue for failure and success events to end up in. Run `aws sqs create-queue --queue-name destinationQueue` to create a queue Then, copy the output queue url into the following command to get the QueueArn:  ``` Now that we have the ARN value, replace the existing `onSuccess` and `onFailure` ARNs inside of the `serverless.yml` file. You should end up with something like this:  From here, we can run `serverless deploy` to set everything up. When the deployment is finished, we can invoke our function with events that will cause the function to succeed and fail and then review the results in the SQS queue. Run this a few times to create a few successful events:  Then, you can run this one a few times to cause some failures:  If you've changed the service or the function name before you deployed you'll need to make sure to update the `--function-name` value in the commands. When you're done, you can review the results in the queue you created.You'll need to replace `` with your queue URL in the following command:  If you need the url, you can always run the `aws sqs list-queues` command. It might take a moment for the messages to make their way to the SQS queue, but when they are there you should see something like this:  And that's it! You've successfully configured and tested your Event Destinations. Now What? Now that you've configured your first event destination you can use them to save yourself from architectures that have to wait for Lambda Functions to finish their invocation and then return a response. This means you can avoid situations where one function is doing nothing, and waiting for the service or function it called to respond. Instead, you can use Event Destinations to process any successes or failures appropriately once the Lambda Function has done it's job. If you didn't have enough fun with the example above, or the other examples in GitHub, there are also plenty of other ways to configure Event Destination resources! You can: Store ARN values in SSM and pull them in with the SSM variable syntax: `${ssm:paramName}` Reference ARN outputs from other services with Framework Pro Outputs and the outputs variable syntax: `${output:my-service.myOutputKey}` We hope this helps you get started with Event Destinations in the Serverless Framework! Have questions about Event Destinations? Get in touch or leave a comment below!",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda604014",
    "title": "The Official Guide to AWS HTTP APIs",
    "content": "The Official Guide to AWS HTTP APIs We've already discussed AWS HTTP APIs in the past few weeks, but there's a lot to learn. So we decided to create The Official Guide to AWS HTTP APIs - a comprehensive guide that we will keep updated with all the latest best practices and how to use HTTP APIs with the Serverless Framework. This guide will follow the same format as our existing guides like the ones on the API Gateway, DynamoDB, and Lambda. As we develop it, we want to hear from you! What examples and use cases do you want us to include? What remaining questions do you have for us given our previews guides and integration with the AWS HTTP APIs? What do you want to see us change? Let us know in the comments below! As things change, keep an eye out for blog announcements here and updates to the guide!",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda604016",
    "title": "Announcing Troubleshooting Monolambdas with Express.js and Flask",
    "content": "Troubleshooting Serverless APIs While it might feel like Express.js or Flask are more \"Monolithic\" approaches to serverless, it's actually a very common pattern for many different applications. And we're excited to announce that you can now deploy your Express.js and Flask microservices with the same automatic monitoring and debugging features as traditional Serverless Framework microservices. While Serverless Framework Pro previously had automatic monitoring and troubleshooting tools integrated out of the box, we lacked support for monolambda microservices. Developers using Express.js, Flask, Lambda API, or other development frameworks were unable to take advantage of many of the tools we offer to help review function invocations, sort invocations by API endpoint and more. That changes today. There's a lot of information on the Serverless Blog and in our dashboard documentation about how you can leverage Framework Pro if you haven't already had a chance to use it. But for now, let's take a look at some of the features you now have access to with monolambda applications. What's New for Monolambdas? Because monolambda applications output lots to the same Amazon CloudWatch log stream, debugging monolambda applications has historically been a huge nuisance. In order to find the single API request you were looking for you'd have to dig through hundreds of unrelated logs from other API requests just to find the invocation and API endpoint you were trying to debug. With our automatic monolambda monitoring and troubleshooting tools that's no longer an issue. You can review the API requests for all the endpoints across your monolambda application: !Errors Overview This starting point gives you an at-a-glance view into the successful and failed requests across your API endpoints. If you want a deeper look at particular endpoints, you can sort them by the API route in question: !Endpoint Overview The best part about this? For Express.js, Flask, and Lambda API, all these API routes are automatically sorted out for you. You don't have to instrument a single endpoint yourself. For frameworks outside of this list that want the same experience, you can leverage the Serverless SDK's setEndpoint functionality to get a similar experience. When you find the invocation you're looking for, you'll get the same information you're used to seeing from the Framework Pro explorer monitoring: !Endpoint Overview Configuring Your Existing Serverless Monolambda Prerequisites So how do you get started with these new features? First, update your version of the Framework to the latest version. If you installed it with NPM you can use `npm update -g serverless`. You'll need v.. or greater of the Framework and v.. or greater of the Framework Pro plugin. Next, in order to add the automated troubleshooting, you'll need to have already created a Framework Pro account and add the `org` and `app` values inside of your `serverless.yml` file. You may need to create a new `app` for your service. Existing Express.js and Flask apps If you've already created your own Express.js or Flask app and deployed it previously with the Serverless Framework, all you need to do now is run `serverless deploy` again and test out some of your endpoints. In the Framework Pro Dashboard for that service you should see all your logs and troubleshooting capabilities for each route you test in your monolambda. Keep in mind, that the routes will only start to appear after you run requests against them. The two line configuration change and zero changes to your application code should trigger a new deployment with the automatic Monolambda troubleshooting and monitoring instrumentation. Creating a simple monolambda app If it's your first time deploying a monolambda application with the Serverless Framework you can follow the steps below for an Express.js or Flask app before deploying and testing the new functionality. Express.js Assuming you've already installed the latest version of the Serverless Framework globally you can start your new Express.js project by installing a few dependencies.  Then, you can create an `index.js` file that contains your Express.js app code: ```js // index.js const serverless = require('serverless-http') const express = require('express') const app = express() app.get('/hello/:name', function (req, res) { const name = req.params.name res.send(`Hello ${name}!`) }) module.exports.handler = serverless(app) ``` Next, you'll have a `serverless.yml` file:  This will setup an Amazon API Gateway proxy endpoint for the `app` function which will allow any custom routes to be handled by your Express.js application. You can do this by creating a single function `app` with a handler of `index.handler` pointing towards the `handler` function we just created inside of the `index.js` file. You'll also need to make sure that the `org` and `app` values are included in the file and reference your Framework Pro account. From there, just run `serverless deploy` and you should get a new endpoint to test out: !New Endpoint From there, just load up the endpoint in to your browser and test out the `hello/name` route: !New Endpoint After you test the endpoint out, you should see it appear in the Framework Pro Dashboard under the explorer for that service: !New Endpoint From there, you can add new routes, test them, and monitor and troubleshoot the rest of your application! Flask Let's try the same thing with a simple Flask application. To start, I'll assume you have Python installed, along with the updated Serverless Framework version from earlier and Node/NPM. First, run `echo Flask > requirements.txt` to create a `requirements.txt` file that you can use to install Flask and other dependencies when deploying to AWS. Then, create an `app.py` file that contains your Flask routes:  Next, create a `serverless.yml` file that you can use to deploy the app. It will have a single function that is configured using `wsgi_handler.handler` as the handler because we will be using the `serverless-wsgi` plugin to deploy our Flask application. It will also need the same HTTP events we configured earlier.  If you compare to the Express.js application, you'll also notice that we have an additional `custom` and `plugins` section. These are to allow us to configure the plugins we need to deploy Python dependencies with `serverless-python-requirements` and to deploy Python monolambda apps with `serverless-wsgi`. Make sure to update the `app` and `org` names with your own Framework Pro configuration. From there, we'll need to install these plugins with:  After we install these plugins, we can deploy our application with `serverless deploy`! You may also need to install Docker in order to use `serverless-python-requirements`. After your service has deployed, you should see a new endpoint to use: !New Endpoint Flask Then, you can test the endpoint in to your browser and see how the `hello/name` route works: !New Endpoint Flask Test Then, you will see the new endpoint appear in the Framework Pro Dashboard with the route you used: !Explorer Flask Now, you can add new Flask routes, test them all out and continue to monitor and troubleshoot your applications! What Next? Well, if you are just starting with Flask or Express.js or you're not sure how to get it working with the Serverless Framework on AWS Lambda, you can look at these guides on creating your own applications with them: - Deploy a REST API using Serverless, Express and Node.js - Build a Python REST API with Serverless, Lambda, and DynamoDB If you'd like a more full-fledged example application to review, you can look at an example \"Survey Service\" that contains a handful of entities like customers, surveys and responses to surveys. It then takes these entities, stores them in DynamoDB and makes them accessible via different API routes. I've created the service with both Express.js and Node.js and Python and Flask.",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda604018",
    "title": "Announcing Serverless Components GA",
    "content": "Today, we're bringing Serverless Framework Components out of beta, and introducing several new features, including a \"serverless dev mode\" that enables you to develop on the cloud, via an experience that looks and feels local... Check out the video overview and register for our Serverless Components Webinar, if you would like to join us. Serverless Components are a Serverless Framework feature that enables you to deploy applications and use-cases on auto-scaling, pay-per-request, serverless cloud infrastructurewithout a lot of infrastructure knowledge. An example is Serverless Express, one of a handful of Components that are part of today's release. You can use it to rapidly build Express.js applications on AWS Lambda and AWS HTTP API, to deliver an API that auto-scales massively, and only charges you when it runs ($. and $. per request). Here are the new features of the GA release, which you can use with Serverless Express and other Components, like Serverless Website, AWS DynamoDB, and more... Fast Deployments When it comes to development, speed is a killer feature. Serverless Components is now powered by our innovative Components Engine, which performs all deployments, and reduces deployment time to under seconds. !Serverless Components Fast Deployments With fast deployments, it becomes much easier to develop directly on real cloud services, rather than maintain a local emulation of those services. Now, you can develop on the same cloud infrastructure your application will use in production, without compromising on development velocity. Dev Mode Getting logs from cloud services while developing on them has previously been slow and difficult. Serverless Components features a new \"Dev Mode\", which speeds up the feedback cycle during development, just run `serverless dev` in your Component. First, \"Dev Mode\" watches your code, detects changes and auto-deploys it rapidly using our Components Engine. Second, when you interact with your application using \"Dev Mode\", transactions, logs and errors stream from your application to your CLI in real-time. !Serverless Dev Mode It looks and feels as fast as if your application is running locally. Advanced Functionality Every Component is rich in advanced functionality. For example, the Express Component can set up a custom domain for you, as well as a free AWS ACM SSL certificate. This Component also ships with canary deployment support, so you can roll out code changes that affect a subset of your HTTP requests. Just merge in your new experimental code, and set the percentage of requests you wish for it to receive. Every Component now stores its state automatically in the cloud, so you can easily collaborate on them and run them in CI/CD. Components also feature better support for staging. Pass in a `--stage` flag to deploy a separate instance of your Component. Lastly, Components export outputs, which are saved in the cloud and therefore easy to reference as inputs for other Components. You can even use outputs from Components in different stages. Components are free to use with the Serverless Framework. Check them out at https://github.com/serverless/components.",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda60401a",
    "title": "Announcing HTTP API Troubleshooting",
    "content": "Troubleshooting HTTP APIs After we announced support for HTTP APIs in the Serverless Framework we saw a lot of enthusiasm around the benefits of the new HTTP APIs. People were excited about the possibility for significant cost reduction and performance improvement. But, there was still the question of effectively troubleshooting your Lambda infrastructure in combination with the new HTTP API. Because of this, we're excited to announce newly released monitoring and debugging support for HTTP APIs. Now you can get automatically instrumented monitoring and debugging tools on top of your HTTP APIs right out of the box. Let's see how with a simple service. Setting up Troubleshooting First, make sure you've already done a few things: - Installed the Serverless Framework `npm install -g serverless` - Created a free Framework Pro account After this, you should be able to create an HTTP API pretty easily. First, let's create a new project directory and create a `serverless.yml` file in it: - `mkdir http-api-project` - `cd http-api-project` - `touch serverless.yml` Then, add this to your `serverless.yml` file:  Make sure to replace the `org` and `app` values with the ones for your Framework Pro account. From there, you can create a new `handler.py` file: - `touch handler.py` And then add this Python code inside the file:  After you make sure to save both `serverless.yml` and `handler.py` you can run `serverless deploy` to deploy your HTTP API. From here, just open up the URL in a browser and refresh the page a few times. The URL should look something like this: `https://whsmxql.execute-api.us-east-.amazonaws.com/hello` When you load it up in the browser you should see this: !Text saying hello friends, in a web browser After you refresh the page a few times, open up your Framework Pro Dashboard and navigate to your app and service. You should now see the recent logs: !The Framework Pro Overview section And that's it! You've just setup your HTTP API with monitoring and alerting capabilities. What Next? Now that you know how to setup a basic HTTP API with monitoring you're ready to continue developing your HTTP APIs. As you dive into it, you might be interested in some of our other guides on HTTP APIs: - Our Official Guide to AWS HTTP APIs covers important essentials and context around the newer HTTP APIs - Serverless Auth with HTTP APIs is an introductory tutorial to getting started with HTTP API authorizers - Also check out this example of a more complex multi-entity \"Surveys service\" using DynamoDB and Python - Or, if you prefer Node, this example of the same multi-entity \"Surveys service\" using DynamoDB and Node.js",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda60401c",
    "title": "Serverless Azure Functions V - Linux, Python & .NET Core Support",
    "content": "V Release We're excited to announce the official v release of the Serverless Azure Functions plugin for the Serverless Framework. This version includes some exciting new features and fixes which we think will both simplify your development experience and enable you to do more with Azure Functions. Feel free to check out our full changelog, but here are the highlights: - Linux Support - Python Support - .NET Core Support - Simplified runtime configuration - No more `x-azure-settings` (still backwards compatible) - Automated integration tests - Invoke the APIM endpoint - `sls info` and `sls deploy --dryrun` - Configurable logging verbosity - Resource group tagging Linux Support You can now deploy a Linux function app with the following flag in your configuration:  The default `os` is still `windows` for all Function Apps (except Python, which does not allow Windows Function Apps). Python Support The updated Python template is now included in the Serverless Framework. Simply run:  A mentioned above, Python Function Apps can only run on Linux, so if you're deploying a Python Function App, you'll be forced into using Linux, regardless of your specification in `serverless.yml`. We highly recommend creating a virtual environment in your local development and make sure you add the name of your environment to the `exclude` section within `serverless.yml`. .NET Core Support In order to deploy a .NET Core Function app via the Serverless Framework, you also need to have the .NET Core CLI installed. The `package` lifecycle event invokes the `dotnet build` command to compile the function app. Simplified Runtime Configuration Rather than pinning to a specific patch/minor version of Node, and trying to determine if that version is supported by Azure Functions, we simplified the `provider.runtime` property. Here are the valid values: - `nodejs` - `nodejs` - `python.` - `python.` - `python.` - `dotnet.` - `dotnet.` This is the recommended approach from the Azure Functions team as well. No more `x-azure-settings` The feature you've all been waiting for... We've flattened out the `function` configuration so that it no longer needs the `x-azure-settings` object to build the function bindings. Before  After  We did, however, make this backwards compatible, so you can still use `x-azure-functions` if you want to for some reason. No judgments here. Automated Integration Tests Since we added support for two new runtime languages and an additional operating system, our ability to manually test deployment possibilities was quickly diminishing. We are using Clover to automate the deployment, invocation and cleanup of function apps, as well as make assertions on the output of the commands. These integration tests are on a timer that runs twice a day, and runs in a GitHub workflow on the plugin repo. Here are links to our workflows for .NET, Python and Node integration tests. Invoke API Management Endpoint The plugin allows for a deployment of an API Management instance, and previously, you'd have to copy/paste into Postman or your browser to test the APIM endpoint. Now, you can simply invoke it directly via the CLI by running:  Info Command The `info` command is a way to view a quick summary of your deployed resources. Run  and you'll see something like:  Dry-run Deployments Similar to the `info` command, we wanted a way for you to get info on what the deployment _will_ be like. We added the `--dryrun` option to the `deploy` command so tha tyou can take a look at the Azure resources that will be deployed with the current configuration. Run:  and you'll see the exact same format as the `info` output, but based on what your current configuration would generate. Tagging Resource Group Resource group tags can be an important part of Azure governance. Previously, any deployment would overwrite any tags that existed on the resource group. Now, the deployment will check if any tags exist as well as add any that were included in `serverless.yml`, which can be included like so:  Conclusion Thank you to the many of you that have used the plugin, provided valuable feedback and even pull requests back to the repo. Feel free to reach out with any questions, issues or feature requests by posting an issue. Until next time ",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda60401e",
    "title": "Serverless Express – Easy APIs On AWS Lambda & AWS HTTP API",
    "content": "TLDR - Take existing Express.js apps and host them easily onto cheap, auto-scaling, serverless infrastructure on AWS Lambda and AWS HTTP API with Serverless Express. It's packed loads of production-ready features, like custom domains, SSL certificates, canary deployments, and costs ~$. per request. If you simply want to host a common Express.js Node.js application, have it auto-scale to billions of requests, and charge you only when it's used, we have something special for you... Announcing Serverless Express, a Serverless Framework offering enabling you to easily host and manage Express.js applications on AWS Lambda and the new AWS HTTP API, which is % faster and % cheaper than their initial API Gateway product. Serverless Expess is a pure Express.js experience and it's perfect for those that want to focus on apps, not infrastructure complexity. Here are the highlights: - Easy, Safe, Performance - Includes the optimal infrastructure pattern for cost, performance & scale. - Never Pay For Idle - No API requests? No cost. Averages ~$. per request. - Zero Configuration - Add your Express app, then deploy (advanced config options are available). - Fast Deployments - Deploy changes to the cloud in seconds. - Real-time Logging - Rapidly develop on the cloud w/ real-time logs and errors in the CLI. - Canary Deployments - Deploy your app gradually to a subset of your traffic. - Custom Domain + SSL - Auto-configure a custom domain w/ a free AWS ACM SSL certificate. - Team Collaboration - Collaborate with your teamates with shared state and outputs. Here is how to get started and deliver a Serverless Express.js based API with a custom domain, free SSL certificate and much more! You can also check out our Serverless Fullstack Application boilerplate, which includes Serverless Express in a real-world example that features a database, website using React and more. Set-Up Serverless Express is a Serverless Framework Component (i.e premium experiences for popular serverless use-cases) and you'll need to install Node.js and the Serverless Framework CLI to use it. Install Node.js here. Then run this command to install Serverless Framework.  Next, install the Serverless Express template:  Lastly, Serverless Express deploys onto your own Amazon Web Services account, so you'll need Access Keys to an AWS account you own. Follow this guide to create those. After you have created AWS Access Keys you can add them directly to an `.env` file, or reference an AWS Profile in a `.env` file, within the root of the template you installed.  You can also reference an AWS Profile in a `.env` file like this.  If you don't include a `.env` file, the Serverless Framework will automatically look for a `default` AWS Profile in the root folder of your machine. Also, Serverless Framework has a built-in `stages` concept. If you change the `stage` it will deploy a totally separate copy of your serverless application.  Even better, you can use different `.env` files for each `stage` by simply using this convention:  One lastoften overlookedstep is to install the Express.js dependency, by running `npm i` in the template. Deployment Now, you are ready to deploy. The template should work out-of-the-box, so run this command to get up and running...  Serverless Express will provision all of the infrastructure and upload your code to it, in a matter of seconds. Though, the first deployment always takes longer than the rest. You should see your teminal return the following: !Serverless Framework Express.js Development Most like to run their Express app locally, and you can absolutely boot up your Express app locally, as you always would. However, local emulations are never the same as running it on real serverless infrastructure, resulting in surprising bugs when you push to production. Further, you will most likely end up using other cloud resources with your Express.js API, and you want to be sure everything works together well. So we wholeheartedly recommend you develop on the real cloud environment (AWS Lambda)and Serverless Express comes with some powerful features to help you do that, via an experience that looks and feels local. Serverless Express features fast deployments and real-time logging from your live AWS Lambda. To get started, simply run:  Now, every time you save, your Serverless Express will quickly push your changes to the cloud. Further, if all API requests, log statements and errors will stream into your terminal. It should look like this: !Serverless Framework Express.js Advanced Configuration Serverless Express may be easy, but that does not mean it isn't powerful or customizable. It features the best possible defaults, but when you are ready for more, there is a ton of possibility. This tutorial was written with Serverless Express version .., which at the time of writing, supports all of the following configuration options. There is a ton of possibility here!  Setting Up A Custom Domain Registered With AWS Route & SSL Certificate Here's how to easily set up a custom domain and SSL certificate on AWS Route. You can also follow the next section to add a custom domain registered outside of AWS Route. To set up a custom domain purchased on AWS Route, make sure you it is in a \"registered\" status and within the same AWS account your Express.js application is running in. Once this domain's status goes from \"pending\" to \"registered\", simply add the following configuration to your `serverless.yml`...  Serverless Express will then add your custom domain to your API as well as automatically set-up an SSL certificated with it, so that you can have a production-ready Express.js API. Don't forget to use `.env` files for different `stages` to use different domains for different environments.    Setting Up A Custom Domain Registered Outside Of Route & SSL Certificate If your domain is not on AWS Route, you will have to set this up manually because the component does not have access to your registrar. Here are the general steps involved: Create an AWS ACM certificate for your domain. Make sure you set the \"Additional Names\" field to `.yourdomain.com` as well to include all subdomains as well. After you create the certificate, it should be in a `PENDING_VALIDATION` status. Now you will need to validate your domain. We suggest you follow the DNS steps by adding the validation CNAME record you see on the AWS console to your domain via your registrar dashboard. After you add the validation record, it might take a while, but eventually the certificate should change status to `ISSUED`. Usually it takes around minutes. Add your domain to the `serverless.yml` file as shown above and deploy. This step is important as it adds your domain to API Gateway. Notice the regional url that is returned as an output. Copy this URL, get back to your registrar and add another CNAME record with your domain or subdomain name and a value of this regional url. This ensures that your domain points to that cloudfront URL. After around mins, your SSL certificate and domain should all be working and pointing to your URL. Keep in mind that if you change the `name`, `stage`, `app` or `org` properties in `serverless.yml`, this would result in a completely new instance with a new cloudfront url. This allows you to setup different domains for each stage or instance Bundling Your Express App (Webpack, etc.) By reducing your code size, your Express app will actually perform better in the AWS Lambda environment, resulting in a faster API. A great way to reduce your code size is to bundle it with Webpack, Parcel, or others. To do this, you can modify the `src` input to run a `hook` script before deployment, like this:  Canary Deployments At scale, when you want to push changes out to a small set of users, Serverless Express offers easy Canary Deployments out of the box! This enables you to push out a version of your app (containing code changes you deem risky) which is only served to a percentage of traffic that you specificy (-%). This allows you to test big changes with little risk. To perform a canary deployment, first update your code with the potentially risky change. Next, set a traffic weighting in your `serverless.yml` `inputs`:  This tells Serverless Express to serve the new (potentially risky) code to % of the API requests, and the old (stable) code to the other % of requests. Run `serverless deploy`. After deployment is complete, % of your requests will be randomly handled by the new experimental code. You can slowly increment the percentage over time, just continue to re-deploy it. If things aren't working, revert your code to the old code, remove the `traffic` configuration option, and deploy. If things are working, keep the new code, remove the `traffic` configuration option, and deploy. Wrapping Up Our goal is to offer the best Serverless Express.js experience possible. We have packed years of serverless experience into Serverless Express so you and your team don't have to configure, manage and automate the underlying infrastructure, and we've barely touched on the tremendous power Serverless Express offers. As always, you should focus on your application, not infrastructure. That is the Serverless Way! If you want to learn more, check out these resources: Serverless Express - This is the repo for Serverless Express and it contains lots of additional documentation. Serverless Components - You will most likely want to include a database, custom permissions role, website and more with your Express.js app. Composition of serverless infrastructure is what Components are all about, so check out all of the neat things you can do via the Components Documentation. Serverless Fullstack Application - Here is a real-world example of how to use Serverless Express within the context of a fullstack application that features a database, website, authentication, authorization and more. It's a great starting point.",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda604020",
    "title": "Serverless is the ultimate place to experiment",
    "content": "As developers, we are used to being able to play around with dev stuff on our machines. Try a few things. Experiment with new libraries and code. But when it comes to Serverless and things being deployed into the cloud, don't we need to be more careful? Nope! With just a few minor practices and a little knowledge, Serverless becomes an awesome playground so instead of having to constantly read what others have done, we can instead just try something ourselves. And if we hit a road block, or it doesn't go as intended, we can just start from scratch! All hail the free tier Due to the fact that AWS has the best performing Functions as a Service product, Lambda, as well as the most diverse suite of managed services to use with your functions, it has become the defacto platform for building Serverless applications. This has another advantage, however. AWS provides a lot of permanent free tiers for many of the services that you would use when building a Serverless application. This means that you get a pretty generous amount of resources per month that last beyond the usual year limit. These include: Lambda: M free requests per month and , GB-seconds of compute time per month DynamoDB: WCUs and RCUs of provisioned capacity, GB of data storage, . million stream read requests from DynamoDB Streams API Gateway: Only for months but is so cheap after for development purposes may as well be free. Stacks are ephemeral One of the other advantages we have is that if correctly configured, our Serverless services we configure will deploy into CloudFormation stacks that can be created and destroyed on a whim. If we build each service to be as autonomous as possible, it means we have no external dependencies we have to make sure are up and running first.",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda604022",
    "title": "The Serverless Framework Knative Component",
    "content": "Serverless is an architectural pattern with the underlying goal of delivering software that has radically low operational cost. It achieves this by prioritizing building applications on next-generation cloud services that auto-scale and never charge for idle time. While every organization wants to reduce operational cost, many are unable to use all of the cloud services that have serverless qualities. The main reasons are that those services are unable to support an organizations use-case, or those services may not offer the level of control an organization may need over their environment. To solve this, many have been innovating open-source serverless platforms since , as an alternative option. Some of these are Apache OpenWhisk, Kubeless, and more recently Knative. These platforms offer similar auto-scaling characteristics, different functionality and make self-hosting their environment possible. Additionally, they all run on Kubernetes and are a great fit for organizations that leverage Kubernetes already. The Serverless Framework has integrated with almost all open-source serverless platforms, since their beginning, to offer developers a single, easy way to build serverless apps, regardless of whether they are hosted on self-hosted or hosted serverless services. !Serverless Framework Knative Component Today, the Serverless Framework is improving its support for Knative with the first official Knative Serverless Framework Component. This Component, developed in collaboration with Red Hat, can deploy and manage containerized applications on serverless Knative infrastructure easily, cheaply and scale massively, all via the Serverless Framework. You can use the Serverless Framework Knative Component to deploy applications written in any language, framework, or idiom youre familiar with. The Quick-Start guide features multiple templates you can run to deploy Express.js, Go and Java-based applications, easily. To get started, try these commands via NPM: `npx serverless init knative-express-starter` `npx serverless init knative-go-starter` `npx serverless init knative-quarkus-starter` This Component supports two ways of building container images out of your source code: The Kubernetes mode will use Kaniko for building a container image from your source code and user Docker Hub as a registry for handing over the container image to Knative. The credentials for a Docker Hub account need to be added to the configuration. This mode requires a Kubernetes cluster on which you are allowed to run Pods in privileged mode. The OpenShift mode builds the container image with OpenShift's SI mechanism and uses the OpenShift internal registry for image hand-over, which does not require any extra security setup. This mode works only on OpenShift with OpenShift Serverless installed for running Knative services. The mode is autodetected from the connected cluster. OpenShift mode is used by default when you have configured a connection URL to an OpenShift cluster; otherwise, Kubernetes mode is used. Red Hats OpenShift Serverless Platform, based on Knative, comes with extra benefits, focusing on enterprise use cases. These benefits include extensive testing covering the platforms supported by OpenShift, from on-premises installations on bare-metal systems to AWS, Azure, GCP, Openstack, VMWare, and soon even Mainframes, supporting hybrid cloud deployments and providing customers flexibility and portability for serverless workloads. Combined with Red Hat's extensive experience building Kubernetes Operators, OpenShift Serverless is distributed as an operator, available to every OpenShift customer, and can be installed using the Operator Lifecycle Management (OLM) and the built-in OperatorHub available in OpenShift. For developers, a consolidated web console provides an intuitive experience and a general solution for connecting several event sources to your serverless applications, all via OpenShift. Check out the Serverless Framework Knative Component to get started. Learn more on the RedHat Blog",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda604024",
    "title": "Serverless Framework V",
    "content": "We recently released our first (minimal) set of breaking changes in the Serverless Framework, in over years, prompted by deprecating support for old Node.js versions. These breaking changes are included in the new v release. Here is a quick article to detail the breaking changes and how they may impact you. Deprecating Support for Node.js Version and You will now need Node.js v or higher installed locally to use the Serverless Framework. At the time of this release, AWS Lambda supports only Node.js v or higher as well. Dropping old Node.js versions allows us to upgrade dependencies with potential security vulnerabilities, making the Serverless Framework more secure. Run the Locally Installed Version by Default If you have the Serverless Framework CLI installed locally within your Service (i.e. project) folder, that version of the Framework will be run, instead of a globally installed version. Otherwise, if the Framework is not installed loclly, but is installed globally, it will default to that. AWS HTTP API, Lambda Integration Amazon's new API Gateway product, \"HTTP API\", has updated its initial payload format (v.) for its AWS Lambda integration. We've decided to use the new payload format (v.) as the default format. More info here: https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-develop-integrations-lambda.html AWS ALB Config Change Support for `providers.alb.authorizers[].allowUnauthenticated` setting was removed. Please now rely on `providers.alb.authorizers[].onUnauthenticatedRequest` instead. Moving Forward Generally, we have avoided breaking changes to avoid creating unnecessary churn. We continue to feel this way. At the same time, we have decided to adopt true semantic versioning for the Serverless Framework. Again, we don't intend to make large, breaking changes in the future. But we want to be more clear when that is the case. As always, many thanks to our community for their continued support. Continue making amazing serverless applications! Learn more about the V release here",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda604026",
    "title": "Container Image Support for AWS Lambda",
    "content": "AWS Lambda is easy to use and manage; the execution environment has a specific runtime on a known environment and you can just send code to it and it runs. Nice! This has served us well over the years. The biggest problem with that status quo, however, is when you want to achieve a use case outside these predetermined environments. Perhaps you want to perform some form of processing using a library that is not included by default into the lambda environment? Or even use your own runtime that is not provided? AWS tried to help solve some of these issues with the introduction of Lambda layers which was useful but still quite limited. The real problem was that, while Lambda by default is great for pick up and run with little to no maintenance, flexibility was sacrificed to achieve that simplicity. In December, , we released some basic Docker container support and recently we have expanded on that to make it a lot easier for users to make use of this new feature. The container entirely encapsulates your Lambda function (libraries, handler code, OS, runtime, etc) so that all you need to do after that is point an event at it to trigger it. And the Serverless Framework makes this incredibly easy to do:  Because we are pointing at an existing container definition that contains everything the Lambda needs to execute, including the handler code, the entire packaging process now occurs in the context of the container. AWS uses your docker configuration to build, optimise and prepare your container for use in Lambda. Bear in mind, this isnt just Proprietary Ks in the background. This is still very much the Lambda micro-VM architecture and your container, while wholly custom, is packaged in a way to prepare and optimise it for use in that environment just like a regular Lambda. AWS claims that cold start times should see no significant impact, but I think its safe to assume that it is possible to configure things in such a way as to make cold starts longer, so taking care and testing thoroughly may be needed. Especially since container images can be up to GB in size; we have seen that package sizes can affect cold start times in the past. And this brings about the biggest downside of using your own docker containers. While this new feature is definitely needed and will provide a great amount of flexibility to the platform and Serverless development in general, it really should be seen as a last resort. Why? One of the great selling points of Serverless development is that you can spit out a solution, and the underlying managed services manage everything for you; from infrastructure to networks, OSs to runtimes. Now with docker support, you can ratchet that back a notch and take back management of the OS and runtimes, which may be required in some situations. But if you can use the pre-built, prepared environments, it's still advisable to do so to reduce the amount of work you may need to do in managing these environments; it's one of the reasons most of us started building applications with Serverless to begin with! Let the framework do all the heavy lifting If you would like to make use of the docker support but still allow the framework to do a lot of the work for you, we have you covered. We recently added the ability for you to define a Dockerfile, point at it in your serverless.yml and have the Serverless Framework do all the work of making sure the container was available in ECR and that it was all setup and configured as needed with Lambda. One pre-requisite before we get started is that we need to make sure we have docker CLI installed on our local machine. You can grab the instructions to do this for your own environment on Docker's own documentation. To get the ball rolling lets use the added starter template to make things a little easier: `serverless create --template aws-nodejs-docker --path aws-nodejs-docker-demo` This will generate a boilerplate with some basic setup already configured for us in our serverless.yml. Let's go take a look at some key sections. In the provider section you should see something new here:  What this does is tell the framework what the image reference name is (`appimage`) that we can use elsewhere in our configuration, and where the content of the docker image resides with the `path` property; a Dockerfile of some type should reside in the specified folder. Our Dockerfile now does the work of specifying where the executable code is for our function. ```dockerfile FROM public.ecr.aws/lambda/nodejs: COPY app.js ./ You can overwrite command in `serverless.yml` template CMD [\"app.handler\"] ``` The `CMD` property defines a file called `app.js` with a function called `handler`. If you look at the contents of our service's directory, we have a file called app.js and inside it has that exact function name. All good so far. However, we still need to configure the function itself that will be created in Lambda, and the event that will trigger it.  Note we use the same value for `image.name` above as we do for the image when we defined it; `appimage`. It can be anything you want as long as you use the same value to reference it. You can also attach any event you need to this container-based version, and it will work just like the non-container version. Tada! Re-using the same container for multiple functions Sometimes you may actually want to use the same function container for multiple functions defined in your serverless.yml. You can store all your function handlers in a single container and then reference them individually within the serverless.yml, effectively overwriting the `CMD` property as you need:  By adding the `command` property, we are telling the framework that for this specific function, the code is still in the `app.js` file, but the function name is `greeter`. We also have the `entryPoint` property. This is related to the base image we reference in our Dockerfile. Taking a look once again at the first line of our Dockerfile:  Our base image that our container is built from is one from AWS. If we use this as our base image then we will always have  If you use a different base image for your own dockerfile then be sure to use the correct `entryPoint` value. Other than that, that's it! We are now able to generate our containers, deploy them to ECR and execute functions. However, if you want to centralise creation of docker images outside of the Serverless Framework and just reference them in the serverless.yml, that capability is available too! Building our docker container manually for Lambda We can build our docker container ahead of time specifically for Lambda and just reference it in our serverless.yml. To start, let's get a small list of requirements out of the way: Ensure Docker CLI is installed: https://docs.docker.com/get-docker/ Ensure AWS CLI is installed: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html We need to use Docker itself to prepare the docker container and then the AWS CLI to push our newly minted container to AWSs ECR service for use in Lambda. It's just a case of following the steps below. Login Docker to AWS ECR  Just substitute the right region and account ID as needed, and you should see the message \"Login Succeeded\". Setup a lambda ready Docker image The easiest way is to rely on base images as provided by AWS. Check AWS ECR Gallery for a list of all available images. You can pull the chosen image via:  e. g. Node.js image (at the time of writing this post) can be pulled as:  The basic configuration for that image is as follows:  Now we can build our image.  Create a repository for corresponding lambda image in AWS ECR service The create repository command is image specific and will store all its versions. We suggest naming the repository the same as the image  Link local image to AWS ECR repository and push it  Here note the returned image digest. We will need to reference the image in our service configuration Point lambda to AWS ECR image And finally, in our serverless.yml we point the lambda to the pushed image by referencing its uri and digest as returned by the last docker push command  And thats it. Pretty easy to get docker containers up and running in the Lambda environment. If there are any questions around using this new feature please make sure to drop by our Community Slack Workspace or our forums. If you spot any issues then also please drop by the project on GitHub to create an issue.",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda604028",
    "title": "Components - Improved Credential Handling & Faster Deployments",
    "content": "Serverless Framework Components enable developers to deploy popular serverless use-cases onto AWS Lambda more easilyand now more securely and quickly. Components are an optional feature in Serverless Framework, and they use an innovative cloud engine, hosted by our company. If you log in to use Components, you can take advantage of a handful of compelling features. Improved Credential Handling Since the model of Components is different from how Serverless Framework has worked traditionally, and it involves passing your source code and AWS account credentials through our hosted engine, we've improved how we handle and protect your credentials, if you choose to use Components. These improvements involve requiring use of our new Providers feature. Providers allow you to give us access to an AWS IAM Role in your AWS account, which we can assume to generate temporary credentials to perform deployments with. You control the AWS IAM Role, so you can revoke our access to your AWS accounts at any time. You can easily set a default Provider for your entire Organization in our Dashboard, or set specific Providers to use with each Service or Stage. This makes it easy for teams to assign which AWS accounts their applications and application stages deploy to. By requiring use of Providers for Components, we hope to make it easier and safer for us to deploy serverless applications into your AWS accounts, and enable use of Components' great features like the upload caching described below. Check out the Components documentation to learn more about how they handle source code and credentials. Check out the Providers documentation to learn more about them. % Faster Deployments Through Upload Caching One of our big missions at Serverless Inc., is to help developers develop directly on the cloud, so they face no surprises when they deploy to production. A big obstacle to this is that deploying changes to the cloud is simply too slow. Current infrastructure-as-code-tooling can cause developers to wait - minutes until their infrastructure and minor code changes are live on the cloud. Additionally, depending on your internet connection speed, a lot more time is required to merely upload a lot of code to services like AWS Lambda. Due to the pandemic, more developers are working from home, and working with slower internet connection speeds as a result. Our cloud engine changes this. It's capable of performing infrastructure modifications within seconds, and it now comes with built-in upload caching which dramatically reduces the amount of code uploaded upon each deployment. Now, when you deploy any Serverless Component use-case that involves source code, only the files you have modified are uploaded, and the rest is cached. In our tests, this reduces deployment speeds for commmonly-sized projects by %. As a benchmark, when using the Serverless Express.js Component, with an application source code size of MB, a .MB/s internet connection upload speed, deployment speed typically takes around seconds. With upload caching, and modifying a couple of files (which is the majority of deployments), deployment speed is reduced to seconds. If a developer is making code changes to their Express.js application a day, this saves them over an hour of deployment time. Components are free to use for single developers making infinite projects. Check out the Components documentation to get started.",
    "__v": 0
  },
  {
    "_id": "64e0892cb72e199dda60402a",
    "title": "Safer Credential Handling In Serverless Components",
    "content": "In January, concerns were raised regarding how our Serverless Components service used AWS credentials stored in a users default profile. These concerns were valid and caused additional confusion about how Serverless Framework handles credentials. In response, we immediately updated our documentation and added clearer CLI prompts for Components users. Now, after working with customers to ensure a successful migration path, we have removed local AWS profile access from our Components service completely. Ultimately, we could have designed this feature better. Were truly sorry about that. Heres what happened, and how weve fixed it. What We Did Wrong Since , the Serverless Framework's model has been to deploy serverless applications from your machine directly to your own cloud infrastructure account using your locally stored credentialsand in no way has that open-source functionality ever changed. However, we launched a new service last year called Serverless Components. Its an optional, premium service, which requires creating an account on serverless.com and logging in from the CLI in order to deploy your applications. Components use a cloud engine for deployment, which our company hosts within our own secure AWS accounts. This enables rapid deployments for Component users through innovations like source code caching to help developers get changes to the cloud, fast. To perform deployments, the Components engine requires access to users' source code and Amazon Web Services account credentials. Many users requested that we automatically import AWS credentials stored in their default AWS Profile, which is a convention supported by AWS tools, the Serverless Framework, and others. But the Components service is different from those tools because it introduced an intermediary into the deployment process. This is where we stumbled. Users who didnt explicitly specify credentials with Components could have passed credentials from their default profile through our cloud engine unknowingly. The Components service is also built into the Serverless Framework CLI, adding additional confusion as to how credentials were being handled by the Framework itself. We documented how using the Components service differs, but ultimately we should have scrutinized these design decisions more thoroughly. !Serverless How We Fixed It Every service and feature we build at Serverless, Inc. must empower developers to move fast safely, securely, and with full confidence in the tools theyre using. That means, no surprises... ever. To remove the risk of any further confusion, we have removed all local credential handling from the Components service. Now, Components require assuming an AWS IAM Role, which Components can use to generate temporary access credentials to perform deployments. Further, Component users must explicitly set up their IAM Roles in the Serverless Dashboard, via the new Providers feature. !Serverless Lastly, were working to remove the Components service from the Serverless Framework CLI, to better separate concerns and make credentials handling more explicit. If you have any questions, concerns, or feedback, please reach out to us at support@serverless.com.",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda60402c",
    "title": "Setup and Build Your First Web . Application With React, Hardhat, Solidity, and Metamask",
    "content": "The greatest approach to improve your Web . skills is to use them to create coding projects. But building them from scratch and adding different libraries can be challenging. This is why in this article we will be creating a simple full-stack application using React, Hardhat, Solidity, Ethers.js, and Metamask which can be used as a boilerplate for our future projects. Setting up Git Git is a source code management technology used by DevOps. It is a free and open-source version control system that is used to efficiently manage small to extremely big projects. In this project, we will be using Git to track all the changes in the project. Creating a Git repository The first step would be creating a git repository. In this article, I will be using GitHub, but you can also use similar services like GitLab or Bitbucket. Head over to the GitHub website and click on the button Create a new repository. Enter the name of your project select the visibility (Public or Private) and click on the button Create repository. !GitHub Adding Git to the project Now that we have created the git repository, we can add git to our project. Create a new folder on your computer and open it on a terminal. Now go back to your browser and copy the code which is provided by default on your project repository in GitHub. It should look similar to the below code  Paste it on your terminal and that is it, you have successfully added git to your project. Setting up the frontend using React To get started, we can use the command below to create a simple react application.  Once it is completed, your folder structure should look like this.  Now that our react application is created we can install some packages such as `ethers.js`, `chai`, and `hardhat`. Run the below command to install those packages using yarn.  And that is it for now, we will come back to our react application to setup `ethers.js`. Configuring an Ethereum Development Environment Next, we need to setup the Ethereum Development Environment, we can simply use Hardhat for this. Open up your terminal and run the command below.  Once completed, you should see below new files/folders generated on your project directory. `test`: This folder contains a test script written in Chai and it is used to test our smart contract `hardhat.config.js`: This file contains the configuration for Hardhat `scripts`: This folder contains a sample script to show to deploy a smart contract `contracts`: This is the folder, which includes the files, in which we write our smart contract code. Modifying the Hardhat configurations If you are looking to deploy your smart contracts on a Testnet then you have to first get an RPC link. Go ahead and create an account on alchemy.com and then in the dashboard click on \"Create App\", name your app and choose Ethereum Goerli as the network.  with our compiled smart contract. Deploying smart contract on a local blockchain Now, we can deploy our smart contract on a local blockchain using Hardhat. Simply rename `sample-script.js` to `deploy.js` in your scripts folder. And then run the below code to deploy your smart contract.  If it was successful, you should see an output similar to below code.  You can also check the logs from the terminal which you opened previously.  In the above logs, we have the Contract address, Gas used, and the address of the one which deployed the smart contract. The next step would be connecting Metamask to our Local Hardhat Blockchain Node Connecting Metamask to Hardhat Blockchain Node Download and install the Metamask extension in your browser and complete the onboarding process. Once it is completed click on networks and choose Localhost !image.png Once you did, click on the avatar image on Metmask and choose \"Import Account\". !image.png Copy any private key from the account that was logged into your terminal and import it to Metamask. !image.png And, that is it, we have connected our local blockchain to Metamask. Connecting the Front-end with Smart contract Now we can start connecting the front-end of our application with smart contract. In your main directory run the command below to start the react app.  Now we can start with allowing users to connect with Metamask in your react app. Connect with Metamask Replace the code in your `app.js` with the below code.  In the above code, we have a `useEffect` which calls `connectWallet` function every time the app loads. And using ethereum from the window object we are connecting to our app with Metamask. Save the file and reload your app, you should see a MetaMask popup asking you to connect Wallet. !image.png Fetching greetings from smart contract Now that we have connected Metamask we can work on fetching greetings from the contract. But before that, we need to import ABI and ether.js in our app.js file. After getting the account you can call a new function called and here is the code for it.  In the above code, we are checking if used have Metamask installed and then get its provider, and using ether.js we are reading the contract and fetching the greets. Finally, this how your `app.js` should look like.  The process of sending greeting is also similar, you just need to call that function using ether.js And that is it. You can also push your final code to GitHub and you have a basic full-stack application that can be used as a boilerplate for your other projects. Conclusion That is it for this article. I hope you found this article useful, if you need any help please let me know in the comment section. Let's connect on Twitter and LinkedIn. Thanks for reading, See you next time",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda60402e",
    "title": "The Complete Roadmap and Resources to Become a Web Developer in",
    "content": "Every day more and more people are transiting to Web. The demand for developers is increasing as Crypto use grows at an exponential rate. Skills in blockchain development are among the most in-demand in the tech industry. It's difficult to find a proper/clean roadmap and resource to get started with Web because it's so new. In this article, I'm going to give you a roadmap and some of the best resources on the internet that will definitely help you get your first job in Web. Basics of computer science Before getting into Web and Blockchain development, It is better to have good knowledge about the basics and fundamentals of computer science. I highly recommend the CS course by Harvard University. !image.png It is completely free, and after completing this course you will have a broad and robust understanding of computer science and programming. Basics of Blockchain Now it is time to learn the basics of Blockchain. Jumping directly to Web without knowing the basics of blockchains is possible but it could be difficult, therefore I recommend understanding the basics of blockchains and how they work this way you can understand web without difficulty. Here is a video from the Coding Tech YouTube channel that explains the blockchains technology in hours. %[https://www.youtube.com/watch?v=qOVAbKKSH] The basics of blockchains include topics such as \"How a blockchain works\", \"What is DeFi and how it works\", \"What is the Decentralized web\", \"What are Token economies\" etc. The above video covers all of them Web and Blockchain Terms The blockchain ecosystem is very vast and has a lot of confusing words and terms. To make your learning path straight, I recommend understanding the most important of them cause you will % need them in the future. I have made a list of terms and I hope you find it useful. %[https://twitter.com/SuhailKakar/status/] You don't need to memorize all the terms, you just need to have an idea and understand them. Programming language When it comes to building decentralized applications, Solidity and Rust are quite popular programming languages. You can choose any of them and build your application using that language. Solidity Solidity is an object-oriented, high-level programming language for creating smart contracts on the blockchain that automate transactions. The language was created by participants of the Ethereum project when it was proposed in . This language is mostly used to make smart contracts on the Ethereum blockchain. I have chosen Solidity and currently building most of my Web projects on top of it. If you want to learn to write dApps on top of the Ethereum blockchain. Solidity is a pretty good choice. Resources Here are the best resources that can help you learn Solidity. Buildspace This is the place to go if you're a developer who's interested in crypto but doesn't know where to begin. Buildspace is a great place to start learning and creating some cool projects. CryptoZombies CryptoZombies is an interactive school that teaches you all things technical about blockchains. It teaches you about Solidity and Ethereum blockchain. Solidity by Example It is a great collection of more practical examples, incl. source code and supporting videos. Rust Rust is an ideal smart contract language: It is type-safe, memory safe, and free of undefined behaviors. If you want to build an application on the Solana blockchain, Rust is a very good choice. !image.png This language itself has many features that make writing Rust code both more ergonomic and easier. Interacting with blockchain Once you understood how to write smart contracts, it is time to connect them with the front end of your application. There are many libraries that does that but the most popular are Ethers.js, Web.js, and Web.py. All of these libraries aim to be a complete and compact library for interacting with the Blockchain. Web.js The Web.js library is the primary JavaScript library you'll use while creating Web applications. web.js is a set of libraries that let you use an HTTP or IPC connection to communicate with a local or distant Ethereum node. Ethers.js Similar to Web.js, Ether.js is a JavaScript library allowing developers to easily interact with the Ethereum blockchain and its ecosystem. According to their docs, It was originally designed for use with ethers.io and has since expanded into a more general-purpose library. If you are looking to learn Ether.js, Dapp University has recently made a great tutorial about it. %[https://www.youtube.com/watch?v=yknVpHTCk] Web. py If you are a python, this library might come in handy for you. Web.py is a Python library for interacting with Ethereum. It helps you help with sending transactions, interacting with smart contracts, reading block data, and a variety of other use cases Dapp University has also made a great tutorial on building blockchain applications using Web. py %[https://www.youtube.com/watch?v=pZSegEXtgAE] Development environment When it comes to writing smart contracts, you can either use Remix IDE or your own local development environment. Remix IDE If you are new and want to learn, Remix is a great choice. It allows you to develop and deploy smart contracts just from your computer browser. You don't need to install or set up any other software. It also has dark mode and a rich set of plugins with intuitive GUIs. Local Development environment However sometimes when you are building a complex project, Remix wouldn't be enough and you might need a local development environment. There are many tools that can help you with this but some of the popular tools are Hardhat, Truffle, Brownie, and Foundry. Hardhat The most popular choice for many developers including me is Hardhat. Hardhat is a development environment to compile, deploy, test, and debug your Ethereum software. !image.png It has its own local blockchain and it is quite easy to set up. Truffle Another popular choice is Truffle. According to their docs, it is a world-class development environment, testing framework, and asset pipeline for blockchains using the Ethereum Virtual Machine (EVM), aiming to make life as a developer easier. Truffle is mostly a GUI tool that makes managing your project much easier and it is maintained by the team at Consensys, a popular blockchain software technology with headquarters in Brooklyn, New York. Brownie If you are a python developer, then this tool is for you. Brownie is a Python-based development and testing framework for smart contracts targeting the Ethereum Virtual Machine. It is a very robust and easy-to-use framework for developing Ethereum smart contracts. Foundry Last but not least, we have foundry. Foundry is a blazingly fast, portable and modular toolkit for Ethereum application development. It is written in Rust. It has many features including a fast compilation pipeline, fast remote RPC, and flexible debug logging, and it is portable. SDKs Building functionalities from scratch can be a little time-consuming, therefore we can use Web SDKs to make the development process much faster. Some of the popular SDKs are Moralis and thirdweb. thirdweb thirdweb lets you build web apps easily. It supports many blockchains such as Ethereum, Polygon, and Avalanche. !image.png With thirdweb you can build NFT projects, marketplaces, tokens, NFT drops, and much more. Moralis Moralis' SDK provides the most comprehensive, easy-to-use Web SDK. Think of it as Firebase on Web, it offers everything that the user needs to create, host, and grow great dApps in one place. %[https://youtu.be/txHnWDRB] Paper Paper is a developer platform for NFT commerce. It allows you to accept credit card payments for NFTs, enable users to connect to your app with just their email, and airdrop NFTs at scale. Testing Testing plays a vital role in the blockchain. Since smart contracts are non-editable, you have to test your code before deploying. Among all, I recommend learning Chai. Chai is a BDD / TDD assertion library that can be paired with any javascript testing framework. It is often used along with Mocha. Chai provides clean syntax that almost reads like English Build Projects Now it is time to build projects. Start from a basic project such as creating our own token and then continue to build larger and larger projects. The Web ecosystem is quite big, you can either build an NFT collection, build a DeFi, or you can clone an existing Web application and build a Web version of it Apply for a job Once you build your projects, make them live and push your code to GitHub (if possible), and then start working on your CV and cover letter. Yes, you can :) If you feel that you are ready for the job, you can apply for a blockchain/Web developer. In case you aren't sure where to apply for. Here is a list of sites where you can find Web jobs. %[https://twitter.com/SuhailKakar/status/] Web development is a vast ecosystem. As a beginner who wants to get into web development, it could be a little hard to focus and learn in a proper way. If you are a self-thought developer, it's your responsibility to find a standard structure to follow. It is better to stick to a set of guidelines. Alchemy University Alchemy University is the ultimate ecosystem for learning how to build and interact with web. Alchemy University is completely free and one of the best resources that can help you get started with Web development. !image.png  Extra Resources Solana Developer Resources This developer reference explains the fundamental concepts for creating Solana apps. LearnWebDAO It offers a free program that will turn you into web developers. From the basics to advance concepts, they cover everything. They also have an active community on discord. Dapp University This is among the best YouTube channels that teach you about Web & Blockchain and building cool Web projects. useWeb useWeb is a learning platform for developers to explore and learn about Web. It has a lot of the latest resources, tutorials, challenges, tools, courses, and boilerplates that can help you in your Web journey. Smart Contract Best Practices Smart contracts are complex and have the authority to allocate high-value resources between complex systems with huge financial loss at risk. This document provides a baseline knowledge of security considerations. Solidity by Example It is a great collection of more practical examples, incl. source code and supporting videos. Conclusion That is it for this article. I hope you found this article useful, if you need any help please let me know in the comment section. Let's connect on Twitter and LinkedIn. Thanks for reading, See you next time",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda604030",
    "title": "Moving from Medium!",
    "content": "Hi everyone! I've made the leap from Medium to a new platform and I'm ready to have some fun! No offense to Medium, it's a great platform and has served me well, but this new space is where the real adventures lie. With multiple reactions and endless possibilities, I'm ready to explore and have a blast! So, I'll be moving all my Medium posts to this new home and I can't wait to see what the future holds. Join me on this journey and let's make some memories!",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda604032",
    "title": "Sending WhatsApp messages with Python and PyAutoGUI",
    "content": " The project Hi everyone! Today Im building a text-to-whatsapp python app Bellow, I have the code, and I will explain everything in the comments. Oh, but first run the following commands:  And here we have our code:  To use this code, you must have Whatsapp installed and a x (width x height screen. If you dont have a screen of that dimensions, open WhatsApp, execute the following code, and immediately position your mouse over the close button and the send button of WhatsApp. The program sleeps seconds after execution. Then, change the X and Y coordinates of the pyautogui.click() functions to the results.  Thanks for reading!",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda604034",
    "title": "Reading the latest Whatsapp messages using python and pyautogui",
    "content": " Introduction Hi everyone! I had built this some time ago this code and now I want to share it with everyone. My idea was to open WhatsApp, select the phone number, and read the messages, but then I needed to do this: Open WhatsApp using WhatsApp's web protocol Wait seconds Focus on WhatsApp's window Drag the cursor to select the latest messages (unfortunatly, can't use ctrl+a to select everything) Copy everything Get the clipboards content In the full code, the coordinates below may not be correct in your case, so remember to update them:  But first, you need to install the pyautogui (for automating) and the webbrowser (to open whatsapp) libraries:  The code Finally, heres the full code:  Thank you for reading!",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda604036",
    "title": "Connecting my website with Google Sheets",
    "content": "When I used my old website, there was something I remember I didn't like about it: When making my new website, I was going to make the same error, but then I thought that it would be cool to connect it with Google Sheets. Looks easy, right? Well, the difficult part was getting a service that actually worked. Some were paid, others had a limit. So I decided to go with opensheet. It's quite easy to use, only some fetch requests.  Looked great! Then I did the same thing for my social media links:  Add a very simple loader effect, and it's done! It is quite useful and a huge time-saver, but we always have problems with loading speed.",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda604038",
    "title": "Apache-airflow installation",
    "content": "Installation of Apache Airflow (The safe way) As in the documentation of apache-airflow, they provided a lot of ways of installation on different OS but when it comes to installing it to your machine, you encounter many errors. So, In this blog, I am going to share my few installation processes and their files, which I found very useful and easy. Use \"virtualenv\" to do this task (highly recommended) As the process of installation involves many lib and modules with different versions sometimes, for this reason, we should use virtualenv. Install with PyPI:- This should be the first way to install Airflow on your machine. Use pip commands for this process.  above codes will do all the required things automatically, if got any error then checkout StackOverflow or any other community to get help. Install with Docker:- In this process of installation, we need some files such as reuirements.txt, Dockerfile, and `docker-compose.yaml` which includes all configurations like DB, ports etc. You should have the Docker desktop application on your machine to get started further. Here, I am attaching files that I had modified slightly for my use, but you can add more according to your requirements. Below is the `docker-compose.yaml` file which you should copy but better if you look into the official yaml file because I've removed a few things from inside which aren't of my use. ```yaml Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version . (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL. WARNING: This configuration is for local development. Do not use it in a production deployment. This configuration supports basic configuration using environment variables or an .env file The following variables are supported: AIRFLOW_IMAGE_NAME - Docker image name used to run Airflow. Default: apache/airflow:.. AIRFLOW_UID - User ID in Airflow containers Default: Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode _AIRFLOW_WWW_USER_USERNAME - Username for the administrator account (if requested). Default: airflow _AIRFLOW_WWW_USER_PASSWORD - Password for the administrator account (if requested). Default: airflow _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers. Default: '' Feel free to modify this file to suit your needs.  version: '.' x-airflow-common: &airflow-common In order to add custom dependencies or upgrade provider packages you can use your extended image. Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml and uncomment the \"build\" line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:..} build: . environment: &airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg://airflow:airflow@postgres/airflow For backward compatibility, with Airflow \"docker-compose up -d\" the next time if you want to run airflow in localhost. You can also start the airflow-webserver container in your Docker desktop directly.",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda60403a",
    "title": "Making my BETA website design",
    "content": "Step . Font selection To begin this project, I carefully selected two fonts: Playfair Display and Inter, which I felt captured the modern and beautiful aesthetic I desired for the beta redesign. Step . Prototype in Figma After that, I started making a prototype in Figma with a design that was clear, organized, and fast. The results were satisfactory, and I liked the design. The prototype website featured my logo in a small font at the top, a header, a description, projects with a Show More button (a significant upgrade from the original website), and a slider with social links (another significant upgrade). !The resulting figma sketch. Looked quite nice, for me! Wow, for me looks nice! Now, the most difficult part: make it work. As usual, I coded everything on Glitch.com: It's a nice interface, with autosave, and HTTPS domains, and works fast. After working some time creating the initial preview with Glitch, I got an honestly nice result: !The HTML+CSS+JS website I made I was quite happy with the result. Splide worked well with the slider: some small customization and I was good to go. Step . Make it work I added multiple things to actually make the website work: I added a title animation (the one saying \"Howdy\"). Moving letters provided some outstanding ones, and were easy to integrate. I decided to go with . I also added a waving hand APNG from emojipedia. I also added a language picker I connected my spreadsheet with data to the website (see this post) I also added much more features and I am adding more features, so check out my website!",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda60403c",
    "title": "Diving into WebAuthn",
    "content": "As a developer, you may have already heard about the new standard in online authentication - Web Authentication (WebAuthn). This new standard offers a more secure and user-friendly alternative to the traditional password-based authentication system. So, let's dive into the steps of integrating WebAuthn into your website or application.  Setting up authentication credentials: The first step is to set up authentication credentials on the user's device. This could be a security key or biometric factor, which will be used for authentication when accessing the website. Here's an example of how to create a new public key credential:  Registering credentials with the website: Once the user's authentication credentials have been set up, the next step is to register them with the website. This process creates a secure connection between the user's device and the website. That's easy!  Logging in with WebAuthn: Finally, when the user logs in to the website, they will be prompted to use WebAuthn for authentication. The user will then use their registered biometric factor or security key to authenticate themselves.  And there you have it! By following these steps and code examples, you'll be able to integrate WebAuthn into your website or application in no time. Here's the full code example of registering and logging in with WebAuthn: ",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda60403e",
    "title": "Making a simple live stream with javascript",
    "content": "Hey there! If you're looking to add a live stream feature to your website, Javascript can be a great tool to achieve this. In this post, I'll share some code examples to help you get started. To create a live stream, you'll need to use the Media Recorder API. This API lets you capture streaming data from the user's camera and microphone, and then send it to a server for broadcast. Here's an example of how to use the Media Recorder API to capture video and audio from the user's device:  Once you have the streaming data, you can send it to a server for broadcast. To do this, you'll need to set up a server-side script that can receive the streaming data and broadcast it to viewers. Here's an example of how to broadcast a live stream using Node.js and the `socket.io` library:  In this example, the server listens for connections from viewers and broadcasts the streaming data to all connected clients using the `socket.io` library. This is just a basic example to get you started. You can customize and extend this code to meet your specific requirements. Here's the full code:   In this example, the client-side code captures the live stream using the Media Recorder API. The captured stream is then sent to the server as a blob of data using the `socket.emit` function. The server-side code is built using the `express` framework, the `socket.io` library, and the `http` module. The server listens for connections from clients and broadcasts the received stream data to all connected clients using the `socket.broadcast.emit` function. It's important to note that this example is just a starting point, and you'll likely need to customize and extend it to meet your specific requirements. With this complete code example, you should now have a good understanding of how to create a live stream using JavaScript. I hope this helps you get started on your project! Good luck!",
    "__v": 0
  },
  {
    "_id": "64e0892db72e199dda604040",
    "title": "Windows Updates",
    "content": "Dealing with Windows can be a frustrating experience, as I recently found out. I was happily reading a blog post when, suddenly, my mouse stopped working, my screen went black, and my computer restarted. Without me doing anything, it was \"Updating Windows... % Done!\" As a result, my unsaved work was lost in the process, and my computer's performance seemed to have been affected as well (I have heard of Windows slowing down your computer after updates, but not like this!). What's worse, there was no warning that this was about to occur. This serves as a reminder to always save your work in case Windows decides to do its own thing. It's a lesson I won't soon forget!",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda604042",
    "title": "Let's make a conferencing app with Javascript!",
    "content": "In the previous article, I talked about making a live stream with javascript. Now, we're going to see how to make a real conferencing app that uses WebRTC and sockets! Setting up the WebRTC Connection The first step in creating a video conferencing app is to establish a WebRTC connection between the participants. WebRTC is a technology that enables real-time communication in the browser, and it provides the necessary APIs to create peer-to-peer connections. Here's an example of how to get access to the user's camera and microphone and set up a WebRTC connection using JavaScript:  In this example, we use the `navigator.mediaDevices.getUserMedia` function to get access to the user's camera and microphone. The returned stream is then assigned to a `` element on the page to display the local video. Next, we create an `RTCPeerConnection` object to set up the WebRTC connection. The `peerConnection.addStream` method is used to add the local stream to the connection, and the `peerConnection.onicecandidate` event is used to receive the ICE candidates, which are used to establish the connection. Exchanging SDP Offer and Answer Once the WebRTC connection is set up, the next step is to exchange the SDP offer and answer between the participants. SDP (Session Description Protocol) is a protocol used to describe multimedia sessions, and it's used to negotiate the media and network settings for the WebRTC connection. Here's an example of how to create an SDP offer and send it to the other participant:  In this example, we use the `peerConnection.createOffer` method to create an SDP offer, and then set it as the local description using the `peerConnection.setLocalDescription` method. Finally, we send the offer to the other participant. To receive the SDP offer and send an SDP answer, we can use the following code:  In this example, we use the `peerConnection.setRemoteDescription` method to set the received SDP offer as the remote description. Next, we create an SDP answer using the `peerConnection.createAnswer` method and set it as the local description using the `peerConnection.setLocalDescription` method. Finally, we send the answer to the other participant. Displaying the Remote Video Once the WebRTC connection is established, we can start streaming video between the participants. To display the remote video, we need to use the `RTCPeerConnection.ontrack` event. Here's an example of how to display the remote video:  In this example, we use the `peerConnection.ontrack` event to display the remote video. The `event.streams[]` property contains the remote stream, which we can assign to a `` element on the page. Using a Server for Signaling Finally, to complete our video conferencing app, we need a way to exchange messages between the participants. To do this, we can use a web server to handle the signaling. The signaling messages include the SDP offer and answer, as well as the ICE candidates. Here's an example of how to send and receive signaling messages using WebSockets:  In this example, we use a WebSocket connection to send and receive signaling messages between the participants. The `socket.addEventListener(\"open\")` event is used to send the offer or answer to the other participant, and the `socket.addEventListener(\"message\")` event is used to receive the signaling messages and update the remote description or add the ICE candidate. Conclusion and full code In this article, we have seen the steps to create a simple video conferencing app using JavaScript, WebRTC, and a web server. With these technologies, it's now easier than ever to create real-time communication applications in the browser. I hope you found this article helpful and informative, and I encourage you to try building your own video conferencing app! Here's the full code:   ",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda604044",
    "title": "The beginner's guide to encryption",
    "content": "Encryption, the word that strikes fear into the hearts of many developers, but it's actually not as complicated as it sounds. Think of it like a secret code that you used to send to your friends when playing hide and seek (except now it's used to protect your data). The idea is to scramble the original message into a coded form that only the intended recipient can read with the right key. But what exactly is encryption and why should developers care? In simple terms, encryption is like locking up your diary with a key to keep your secrets safe from curious eyes. It's used to protect sensitive information from cyber criminals, hackers, and other malicious entities. There are various forms of encryption: In this beginner's guide, we'll focus on symmetric encryption, and specifically the Advanced Encryption Standard (AES) algorithm, one of the most widely used encryption standards in the world. Here's a quick code example of symmetric encryption using AES in Python:  In the code above, we first create a function `encrypt_message` that takes in a message and password and returns the encrypted message. We use the password to generate a secure key using the `base` library, which encodes the password into a URL-safe string. This key is then used to initialize an instance of the Fernet class from the `cryptography` library, which provides the encryption and decryption functions. Next, we create a function `decrypt_message` that takes in an encrypted message and password and returns the decrypted message. This function works similarly to the `encrypt_message` function, using the password to generate the secure key and using the Fernet class to decrypt the encrypted message. So why is encryption important for developers? With the increasing number of data breaches and cyber attacks, encryption is essential in ensuring the confidentiality, integrity, and availability of sensitive information. It's like having a knight in shining armor to protect your precious data.",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda604046",
    "title": "Best practices for securing your web apps",
    "content": "Securing your web application is a crucial step in ensuring the safety of your users' data and your business. With the increasing frequency of data breaches and cyber attacks, it's important to make sure your web application is as secure as possible. Here are some best practices to keep in mind when securing your web application: Use HTTPS HTTPS (Hypertext Transfer Protocol Secure) is a protocol for secure communication over the internet. It encrypts data so that it cannot be intercepted or tampered with. By using HTTPS, you can protect the sensitive information transmitted between your web application and your users, such as login credentials and financial information. To implement HTTPS, you can purchase an SSL (Secure Socket Layer) certificate from a trusted certificate authority and install it on your web server. Here's an example of how to set up HTTPS using Node.js and Express:  Validate user input Validating user input is one of the most important security measures you can take to protect your web application. Malicious users can use loopholes in your validation to inject malicious code, steal sensitive information, or perform other security attacks. You should validate all user input on both the client-side and the server-side. On the client-side, you can use JavaScript to validate user input before it's sent to the server. On the server-side, you should validate all user input again to ensure it's secure. Here's an example of how to validate user input using Node.js and Express:  Use prepared statements Prepared statements are a way to separate the logic of your web application from the data it uses. They can help protect against SQL injection, which is a type of security attack that exploits vulnerabilities in the way data is processed by a web application. To use prepared statements, you can use a database library, such as MySQL or PostgreSQL, that supports them. Here's an example of how to use prepared statements in a Node.js and MySQL application:  By using prepared statements, you can ensure that user-supplied data is safely escaped and separated from the SQL code, reducing the risk of SQL injection attacks. Keep software up-to-date Keeping your web application software up-to-date is important for security. As new security vulnerabilities are discovered, software updates will often include patches to fix these issues. By staying up-to-date, you can ensure that your web application is as secure as possible. Use a web application firewall (WAF) A web application firewall (WAF) is a security tool that monitors incoming traffic to your web application and blocks malicious requests. A WAF can help protect against common security attacks, such as SQL injection, cross-site scripting (XSS), and cross-site request forgery (CSRF). There are many WAFs available, both commercial and open-source. Some popular open-source WAFs include ModSecurity and NAXSI. By following these best practices, you can help secure your web application and protect your users' data. Remember to always prioritize security, and never compromise it for convenience or ease of development.",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda604048",
    "title": "How to implement lazy loading in your website",
    "content": "As web applications grow larger and more complex, it's important to consider how to optimize the user experience for slow internet connections or underpowered devices. One way to achieve this is by implementing lazy loading, which is a technique that delays loading of images or other content until it is needed. Here's a simple implementation of lazy loading using the Intersection Observer API:  The Intersection Observer API makes it easy to detect when an element enters or leaves the viewport, which is ideal for lazy loading. In this example, we query all `img` elements on the page and observe each one. When the observer determines that an image has entered the viewport, it sets the `src` attribute to the value stored in the `data-src` attribute, effectively loading the image. Note that this is just one example of how to implement lazy loading, and there are many other techniques and libraries available. No matter what approach you take, the goal is to make your web applications fast and responsive, regardless of the network or device conditions. For example, one of the easiest ways to lazy load images in HTML is by using the `loading=\"lazy\"` property:  It's also recommended to lazy load content. An easy and fast way to use this is, for example, to make multiple fetches. Or you can even use WebSockets!",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda60404a",
    "title": "Getting started with graphQL subscriptions",
    "content": "GraphQL is a powerful and flexible query language for APIs that allows clients to specify exactly what data they need. One of the lesser-known features of GraphQL is subscriptions, which allow for real-time updates to be pushed from the server to the client. Here's an example of how you can set up a GraphQL subscription using the `subscription` type in your schema:  In this example, the `newMessage` subscription is defined as a field on the `Subscription` type. It takes a single argument, `roomId`, and returns a `Message` type. The `@resolve` directive is used to specify the function that will be used to resolve the subscription. To actually subscribe to the `newMessage` subscription, you can use a GraphQL client library, such as Apollo Client or Relay, to open a websocket connection to the server and send a subscription query. Here's an example using Apollo Client: ```javascript const subscription = gql` subscription newMessage($roomId: ID!) { newMessage(roomId: $roomId) { id text room { id name } } } `; const observer = client.subscribe({ query: subscription, variables: { roomId: \"\" } }); observer.subscribe({ next: data => console.log(data) }); ``` With this code, the client will open a websocket connection to the server and subscribe to the `newMessage` subscription, passing the `roomId` variable as an argument. The server will push new messages to the client in real-time as they become available, and the `next` callback will be called with the updated data. In conclusion, GraphQL subscriptions allow for real-time updates to be pushed from the server to the client, making it easy to build dynamic and responsive web applications. Whether you're building a chat app, a real-time dashboard, or any other type of application that requires real-time updates, GraphQL subscriptions are a great tool to have in your toolbox.",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda60404c",
    "title": "Websockets: Real-Time updates",
    "content": "Real-time updates are a must-have feature in many modern web applications. From chat applications to online marketplaces, providing users with the ability to receive immediate updates is crucial to creating a seamless experience. One way to implement real-time updates in a web application is by using WebSockets. WebSockets provide a full-duplex communication channel between the client and server, allowing for real-time data transfer in both directions. In this article, we'll go over how to use WebSockets to implement real-time updates in a web application. Getting started with websockets To get started with WebSockets, you'll need to use a WebSockets library, such as `socket.io` for Node.js or `Pusher` for PHP. These libraries provide a convenient API for working with WebSockets, making it easy to get up and running quickly. Setting Up the Server To set up the server for WebSockets, you'll need to create a WebSockets endpoint and configure the server to listen for WebSockets connections. The following code sets up a WebSockets endpoint using Socket.io in Node.js:  This code sets up a WebSockets endpoint using the socket.io library and listens for connections. When a user connects, the code logs a message to the console. Setting Up the Client Next, you'll need to set up the client to connect to the WebSockets endpoint. The following code sets up a WebSockets connection using it in the browser:  This code sets up a WebSockets connection to the server and listens for the `connect` event. When the client successfully connects to the server, the code logs a message to the console. Sending and Receiving Messages With the server and client set up, you can now send and receive messages over the WebSockets connection. The following code sends a message from the client to the server:  And the following code receives the message on the server:  This code sends a message from the client to the server using the `emit` method, and the server receives the message using the `on` method. When the server receives the message, it logs the data to the console.",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda60404e",
    "title": "How does Machine Learning actually work?",
    "content": "Greetings, fellow humans! Today, we'll delve into the fascinating world of Machine Learning and learn how it works. But don't worry, we won't make it as dry as a calculus lecture. So, let's get started! What is Machine Learning? Machine Learning is a type of Artificial Intelligence that allows machines to learn from data without being explicitly programmed. In essence, we're trying to teach machines to learn from experience, similar to how we humans do it. There are three primary types of Machine Learning: Supervised Learning, Unsupervised Learning, and Reinforcement Learning. Supervised Learning Supervised Learning is the type of Machine Learning where we train machines on labeled data. The machine learns to map input data to output data by identifying patterns in the data. A classic example of supervised learning is image classification, where the machine is trained to recognize images and label them correctly. Here's an example of how you can train a machine to classify images of cats and dogs using the popular TensorFlow library:  Unsupervised Learning Unsupervised Learning is where machines learn from unlabeled data. The machine tries to find patterns and structure in the data without any predefined labels. A common application of unsupervised learning is clustering, where the machine groups similar data points together. Reinforcement Learning Reinforcement Learning is a type of Machine Learning where machines learn through trial and error. The machine interacts with an environment and receives rewards or punishments based on its actions. The goal is for the machine to learn the optimal set of actions to take to maximize the reward. An example of Reinforcement Learning is training a machine to play a game, like tic-tac-toe.  Thanks for reading!",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda604050",
    "title": "Why switch to duckduckgo",
    "content": "Are you tired of being tracked by big tech companies every time you search for something online? Are you fed up with targeted ads following you around the internet? Well, it's time to ditch Google and make the switch to Duckduckgo - the privacy-focused search engine that puts you in control of your online experience. Still not convinced? Here are reasons why you should switch to Duckduckgo: Your searches are private When you use Google, every search query you make is logged and analyzed to build a profile of you. This profile is then used to serve you targeted ads and recommendations. With Duckduckgo, your searches are completely private. They don't track you, store your data or sell it to advertisers. It's like having a secret search engine just for you. No filter bubble Google's search results are tailored to you based on your browsing history, location, and other data they have on you. This means you only see what they think you want to see. Duckduckgo, on the other hand, shows you the same results as everyone else. This means you get a more objective view of the world, free from the \"filter bubble\". Instant answers Duckduckgo has a feature called \"Instant Answers\" that provides answers to your queries right on the search results page. For example, if you search for \"minify JS\" it will instantly show you a box to minify your code on the top. No need to click through to another website. Bangs! One of the coolest features of Duckduckgo is \"Bangs!\". This allows you to search other websites directly from the Duckduckgo search box. For example, if you want to search for a product on Amazon, just type `!a` followed by your search query. Prefer Wikipedia? Use `!w`. There is actually a bang for everything! Bangs are a real time-saver. Customizable interface Duckduckgo lets you customize the look and feel of the search engine. You can change the background, theme, URL style, and much more. It's a small thing, but it makes the search engine feel more personal. No more annoying ads Have you ever searched for something on Google and then seen ads for that product follow you around the internet? It's creepy and annoying. With Duckduckgo, you don't have to worry about targeted ads. They don't track you, remember? It's just better Duckduckgo is just a better search engine. It's fast, reliable, and provides great search results. Plus, you get all the privacy benefits we've already talked about.  So, there you have it - reasons to switch to Duckduckgo. If you care about your privacy and want a search engine that puts you first, then it's time to make the switch. The future belongs to those who duck for it today. Take back your privacy. duckduckgo.com",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda604052",
    "title": "Introduction to PWAs (Progressive Web Apps)",
    "content": "Progressive Web Apps (PWAs) are a new way of delivering web content that combines the best of both worlds - the ease of use of native apps and the accessibility of traditional websites. PWAs are designed to provide users with a fast and seamless experience, even on slow networks or when offline. To get started with PWAs, it's important to understand the key features that make them different from traditional websites. Some of these features include: Offline support: PWAs can work offline or with limited connectivity, providing users with a seamless experience even when their internet connection is poor. Home screen icons: PWAs can be installed on a user's device and accessed just like a native app, with a dedicated icon on their home screen. Push notifications: PWAs can send push notifications to users, even when the app is not currently open. So, how do you build a PWA? It starts with creating a web app that meets certain technical requirements, such as having a service worker, a web manifest file, and HTTPS. From there, you can enhance your PWA with features like offline support, push notifications, and a home screen icon. Here's a code example of a basic service worker in JavaScript:  In this example, we're using the `navigator` object to check if the `serviceWorker` API is supported in the user's browser. If it is, we use the `window.addEventListener` method to wait for the page to load and then call `navigator.serviceWorker.register` to register our service worker file (`/sw.js`). This is just a basic introduction to PWAs, but there's a lot more to learn and explore in this exciting area of web development. Whether you're building a new web app or looking to enhance an existing one, PWAs are a great way to provide users with a fast and seamless experience. Here's a great starter for you to remix: ~easy-pwa on Glitch ",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda604054",
    "title": "Teachable Machine: Make your own AI!",
    "content": "Hi there everyone! Today, I'm going to introduce you to the magical world of AI and how you can create your very own AI using Teachable Machine, all while using the ever-so-popular programming language, JavaScript. Now, let me start by saying that making your own AI might sound like a daunting task, but fear not! Teachable Machine is here to save the day. This cool little tool, created by people at Google, allows you to train your own machine learning model really easily. Believe me, really easy. So, what is Teachable Machine, you ask? Well, let me tell you. Teachable Machine is a web-based tool that allows you to train a machine learning model to recognize images, sounds, and poses. This means that you can teach your computer to recognize things like a cat meowing, a dog barking, or anything you want. But don't worry, you're not making this AI: !AI WILL DESTROY THE WORLD IN A DECADE | Funny disney jokes, Funny ... To get started with Teachable Machine, you'll need to have a basic understanding of JavaScript. But don't worry, you don't need to be a JavaScript master. You just need to be able to copy and paste some code snippets. And if you're anything like me, copying and pasting is your specialty. !Teachable Machine's website with a arrow next to the \"Get started\" button First, you'll need to head over to the Teachable Machine website and click on the \"Get Started\" button. From there, you'll be asked to choose what type of model you want to create. You can choose from image, audio, or pose. For the purpose of this article, let's choose image. !Teachable Machine's editor with the \"image\" model Next, you'll be asked to collect some images for your model. This is where the fun begins. You'll need to gather a bunch of images of whatever it is you want your model to recognize. For example, if you want to train your model to recognize different types of fruit, you'll need to gather images of apples, oranges, bananas, etc. And the best part? You get to eat the fruit once you're done! (Just kidding, don't eat the pictures. That's not how this works.) After you've collected your images, you'll need to upload them to Teachable Machine. This is where you'll start training your model. You'll need to label your images with their respective categories. For example, if you've uploaded an image of an apple, you'll need to label it as \"apple.\" You'll also need to do this for all of your other images. Once you've labeled your images, it's time to train your model. Teachable Machine will take care of the heavy lifting and train your model for you. It might take a little while, but just sit back, relax, and let the magic happen. After your model is trained, you can start using it in your own JavaScript code. Teachable Machine provides you with some code snippets that you can copy and paste into your own code (click \"Export model\" to show them). Once you've done that, you can start using your model to recognize images in your own projects.  And that's it! You've just created your very own AI using Teachable Machine and JavaScript. See, I told you it wasn't that hard. Now go out there and train your computer to recognize all sorts of wacky things. Who knows, maybe you'll be the next AI superstar. Or, at the very least, you'll impress your friends at parties! ",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda604056",
    "title": "How to measure & improve website performance with lighthouse",
    "content": "Have you ever visited a website and found yourself waiting for what feels like an eternity for it to load? If you have, you're not alone. Slow-loading websites can be frustrating for users and can cause them to abandon a site in favor of a faster one. In fact, a study by Google found that as page load time goes from second to seconds, the probability of a user bouncing increases by %. That's a lot of potential visitors lost! But fear not, dear reader, for there is hope. One tool that can help you measure and improve your website's performance is Lighthouse. Lighthouse is an open-source tool that can be run as a Chrome extension or from the command line, and it provides a comprehensive analysis of your website's performance, accessibility, best practices, and more. It's a powerful tool that can help you identify and fix issues that are slowing down your website. So, how do you get started with Lighthouse? Let's take a look. Open Lighthouse in Chrome First, you'll need to install the Lighthouse Chrome extension. Once you've done that, navigate to the website you want to test and click the Lighthouse icon in the Chrome toolbar. Run the Test Next, you'll want to run the Lighthouse test. You can choose whether you want to test on mobile or desktop, and whether you want to include or exclude certain categories (such as accessibility, SEO, and more). Review the Results Once the test is complete, Lighthouse will provide you with a detailed report of your website's performance. You'll see scores for each of the categories you selected, as well as recommendations for improvements you can make. Lighthouse will also provide you with a performance audit, which shows you how long it takes for your website to load and which elements are slowing it down. Implement the Recommendations Now that you have a list of recommendations, it's time to implement them! Lighthouse provides you with specific suggestions for improvements you can make, such as optimizing images, minifying and compressing code, and more. By following these recommendations, you can improve your website's performance and make it faster for your users.  In conclusion, if you want to improve your website's performance and make it faster, Lighthouse is an excellent tool to help you achieve your goals. By following the steps above and implementing the recommendations provided by Lighthouse, you can create a faster, more user-friendly website that your visitors will love. And who knows, you might even be able to get a few extra visitors to stick around instead of bouncing like a basketball.",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda604058",
    "title": "Data Engineering with GitHub Repos",
    "content": "Introduction In this blog, I am going to show you how to fetch all repositories that have more stars, put them in BigQuery and visualize that through Looker(Google Data Studio). I will be fetching the repo description, number of times it forked, the language used and the date of creation. Pre-requisite You should have a GCP account, with a service account(recommended). A Looker account is a must that will be used to Visualize the data. Procedure & Code Import all necessary Libraries  Make an API request to fetch the top repositories with the highest stars.  Traverse through every repository and take out needed attributes/data.  Initialize the BigQuery client and insert the data after making the table.  Visualization I'm using looker to visualize the data because it is easy to connect BigQuery data with Looker. . ```sql UPDATE `project.dataset.table` SET language = 'xyz' WHERE language IS NULL -- above language is column name ``` Conclusion Through the above, you can fetch the data from GitHub, and visualize them with Looker or any other tool. You can fetch any other data also from Github as per your need and can use Looker in a more informative way.",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda60405a",
    "title": "tricks to improve your website",
    "content": "Welcome to the wild and wonderful world of website improvement. Whether you're a seasoned website owner or a newbie, there's always something you can do to make your website better. So let's get ready to learn some cool tricks to improve your website's experience. Make it mobile-friendly Do you know what's not cool? A website that looks terrible on mobile devices. With more and more people using their phones to browse the web, it's important to make sure your website is optimized for mobile. If your website isn't mobile-friendly, you're missing out on a huge chunk of potential visitors. Keep it simple There's nothing more annoying than a website that's cluttered with unnecessary elements. Keep your design simple and clean, and make it easy for visitors to find what they're looking for. Speed it up What's even more annoying than a cluttered website? A slow website. Nobody has time to wait for your website to load, so make sure it's as fast as possible. Compress images, optimize code, and use a content delivery network (CDN) to speed up your website. Use high-quality images Speaking of images, make sure they're high-quality. Blurry or pixelated images are a surefire way to turn off visitors. Use high-resolution images that look good on all devices. Use engaging headlines Your headlines should be interesting and engaging. Nobody wants to read a boring headline, so make sure yours are attention-grabbing. Use clear calls-to-action Your website should have clear calls-to-action (CTAs). Whether you want visitors to sign up for a newsletter, make a purchase, or contact you, make sure it's easy for them to do so. Add social proof People are more likely to trust your website if they see social proof. This can be in the form of customer reviews, testimonials, or social media shares. Make it accessible Your website should be accessible to everyone, including people with disabilities. Use alt text for images, make sure your website is keyboard-friendly, and follow web accessibility guidelines. Use videos Videos are a great way to engage visitors and keep them on your website longer. Use videos to showcase products, share information, or tell a story. Test and iterate Finally, test and iterate. Use website analytics to see what's working and what's not, and make changes accordingly. A website is never truly finished, so keep tweaking and improving as you go.  So there you have it, cool tricks to improve your website's experience. Remember, a good website is one that's easy to use, engaging, and accessible to all. Happy website building!",
    "__v": 0
  },
  {
    "_id": "64e0892eb72e199dda60405c",
    "title": "The ultimate guide to ChatGPT",
    "content": "Welcome to the ultimate guide to ChatGPT! You may already know what ChatGPT is, but for those who don't, let me give you a quick rundown. ChatGPT is a state-of-the-art artificial intelligence language model developed by OpenAI. It is designed to generate human-like responses to text-based prompts and has been trained on an enormous dataset of diverse texts. So, why should you care about ChatGPT? Well, for starters, it can be an incredibly useful tool for generating content, answering questions, and even just having a conversation. But to get the most out of ChatGPT, there are a few things you should know. Here are my top tips for using ChatGPT like a pro: Be clear and specific in your prompts - ChatGPT works best when you give it clear, specific prompts. Don't be vague or overly general in your requests. Keep it concise - While ChatGPT can generate long, detailed responses, it's often better to keep your prompts short and sweet. This helps ChatGPT focus on the most important aspects of your request. Use it to generate ideas - If you're stuck on a writing project, try using ChatGPT to generate some ideas or inspiration. You might be surprised by what it comes up with! Don't take everything it says as gospel - While ChatGPT is incredibly smart, it's not infallible. Always double-check any information you get from ChatGPT before relying on it. Have fun with it! \\- ChatGPT can be a great way to kill time or just have some fun. Don't be afraid to ask it silly or random questions - you never know what kind of response you'll get! ChatGPT is an incredible tool that can be incredibly useful in a variety of contexts. By following these tips, you'll be well on your way to becoming a ChatGPT expert. So what are you waiting for? Give it a try and see what it can do for you!",
    "__v": 0
  },
  {
    "_id": "64e0892fb72e199dda60405e",
    "title": "Comparing Vercel, Netlify, and other popular alternatives",
    "content": "Web development platforms have revolutionized the way developers create and deploy web applications. But with so many options out there, it can be overwhelming to choose the right platform for your needs. That's why I've done the hard work for you and compared the most popular web development platforms available today. Vercel Vercel is a powerful platform that offers automatic scaling and instant deployment. Its advanced features make it an excellent choice for building complex projects. And they have a nice UI! But Vercel can be somewhat difficult to use compared to Replit or other competitors. Netlify Netlify is a feature-rich platform that offers continuous deployment and form handling for free. Its user-friendly interface makes it easy for developers to deploy and manage their projects. However, some users have reported issues with site speed, and its pricing plans can get expensive for advanced features. Replit Replit is a great platform that offers collaborative coding and real-time editing for free. Its community-driven approach and focus on education make it an excellent choice for students and beginner developers, altought its user interface can be a bit overwhelming for some users, the domains are really limited (projectname.username.repl.co, before it was only projectname.repl.co - clear downgrade) and it is sometimes a bit slow. Glitch Glitch is a popular platform that offers hosting for full-stack JavaScript apps and collaborative coding for free. Its simple user interface and active community make it easy for beginners to get started. It also has some basic real-time capabilities, good domains (yourproject.glitch.me), this time really INSTANT deploys (~ second to update the full website), and more. However, Glitch's features are somewhat limited compared to other platforms. GitHub Pages GitHub Pages is a basic platform that offers free hosting for static websites. It's a good choice for developers who want a simple way to host their projects, but GitHub Pages don't offer much in terms of customization or dynamic content. Also, their domain is bad. yourusername.github.io/yourproject/page? |  Not sponsored. Personal opinion only.",
    "__v": 0
  }
]
